,text1,text2,bool
0,"Official statistics are those produced by either the state, or by one of its agencies. There has been a huge increase in the number of official statistics since the Central Statistics Office was set up in 1941, and now large quantities of statistics are produced on a variety of different topics, for example income, housing and population. Although they are widely used there are many limitations of the use official statistics in sociology. In this essay I plan to talk about how the internal, or posivitist schools view statistics, looking at how they are critical of the reliability and validity of official statistics, and also how they feel that the processes involved in producing the statistics limit their uses. I will also explore the uses that statistics do have in sociology, and come to a conclusion on whether they should be use or not. The internal school of thought emphasises the need for reliability and validity in order to produce realistic statistics, however there are many problems with official statistics in this aspect, and as Hindess points out ""they cannot be taken as a...reliable account"" (Hindess, 1973). The term reliability refers to whether the statistics are replicable or not, and validity is defined as whether they paint a true picture of what they are looking at, and there are many problems with regards to both. The first problem is that of the definition of concept, and can cause problems with the reliability of the statistics. Different people define concepts in different ways so this alters the reliability of the study, and also means that readers may interpret the statistics in a different way to that which they were intended (Hindess, 1973, Shipman, 1973, May, 1997). Hindess points out that if concepts are defined differently it can have a huge result on the statistics (Hindess, 1973, cited in Irvine et al, 1979), and in Demystifying Statistics Irvine uses the example of unemployment, claiming that over the past three decades the definition of unemployment has altered many times, and this has big impact on the number of people who are defined as unemployed, according o the statistics (Irvine and Miles, 1979). The next problem is linked to validity. Official statistics go through a long process of decision making before the final result is reached (Hindess, 1973, May, 1997), even down to what methodology is used, and this process can alter the results greatly. In Social Research May uses the example of crime statistics to show this. The first decision in the process here is that of what is a crime in terms of the law. This would have a huge impact on crime statistics, for example if marital rape was not illegal then the crime statistics of rape would be very different. The second decision is made by the victim of the crime; whether to report it or not, which means that the formation of official statistics is not just influenced by those composing them, but also by decisions made by those involved in the actual area (May, 1997). Human judgement is a key factor in this criticism, but as the decision process shows it is not just the judgement of those composing the statistics that is involved (Hindess, 1973). This means that as the process continues the statistics become further distorted as different people's views and decisions become involved. This idea can be illustrated using the example of suicide. The official statistics produced on this topic are not only influenced by those collating the statistics, but also by those determining the causes of death. This can be seen by the fact that in the past Catholic priests did not like to categorise the cause of a death as 'suicide', so the term 'natural cause' would have been used instead, resulting in a very different set of statistics in the end. Cicourel also supports this idea; however he takes it one step further by saying that these decisions are influenced by people's background experiences (Cicourel, cited in Hindess, 1973). This means that even if two people from the same organisation tried to collect the same results there would be complications as they have had different experiences. Overall this shows that reliability and validity are hard to establish in official statistics, due to the number of human decisions involved, and the different ways in which people apply concepts. However according to the radical approach there are more limitations with statistics. The radical approach tells us that us that official statistics tell us more about the state than they do the topics they are exploring, therefore statistics should be viewed as a construction, and as Irvine and Miles say; ""statistics are not collected, but produced"" (Irvine and Miles, 1979). Official statistics are not neutral perceptions of a topic, but instead reflect political agendas (Irvine and Miles, 1979) and the aims of those with political power (May, 1997). One example of this is the way in which the government produced official statistics relating to unemployment under Margaret Thatcher. During her time in power the definition of unemployment changed over twenty times, as the aim of the statistics was to show how unemployment was decreasing, and the most successful way in which to achieve this was to alter the definition of the concept of unemployment. This shows how the aims and motives of those collecting the data can influence the final statistics. Radical thinkers also believe that statistics only tell us about the groups in society who hold power, and are simply a reflection of the balances of power in society (Hesteretal, 1996, cited in May, 1997). Official statistics do not exist on all topics that sociologists may want to look at, and as a result can be very limiting for them, but also only present a one sided view of society. One example of this is servants in the Victorian era. Many statistics from this time were produced by those in positions of power, for example mistresses, so they tell us little about the lives of the servants, who were working class and held little power. However, despite these limitations official statistics do have some uses. According to posivitists as long as the statistics are reliable and valid then hey are very useful, and one example of this is birth and death rates. As the registration of births and deaths are legal requirements the statistics are reliable, and paint a valid picture of the contemporary and past population levels. Statistics that are not produced using legal requirements can also be useful, however. Bulmer claims that statisticians often go to many lengths to avoid problems with reliability and validity, and he claims that as a result official statistics pose no more of a problem than sociological research. He also argues that if sufficient thought is given to the production of official statistics, and more scientific, rather than theoretical concepts are used, then these problems can be corrected, however it is hard to apply this to sociological research (Bulmer, 1984, cited in May, 1997). Using the radical approach statistics can be useful to tell us about the state and those in power, and what they are interested in. However radicals argue that if the processes involved in the construction of official statistics are carefully considered when analysing the results then the statistics can be seen to present a useful and valid picture of society (Hindess, 1973). However radicals also argue that official statistics are used most successfully, and paint the most realistic picture when they are combined with another method (May, 1997), for example self report studies when the topic of crime is being studied, as this allows the decision making process involved in the official statistics to be balanced out, as people are more likely to be honest in a self report study. Overall, according to the posivitists the main limitations of official statistics arise from the issues of reliability and validity, however they feel that in many cases these limitations are overcome by careful planning, so argue that sociologists should use official statistics. Radicals on the other hand, believe that official statistics are a construction of the state, and this means that although they are useful for examining the state and its interests they do not provide a realistic picture of society. I feel sociologists should use official statistics especially when examining those with power, or exploring the state, however they should treat official statistics produced on other topics with care, and be aware of the process of decision making and methodology that occurs before the statistics are finally produced.","Since the fourteenth century the practice of medicine has become a profession, and more importantly, a male-dominated profession. Previously medicine was seen as a female duty, however all areas of medicine have been professionalized over the centuries, and slowly become male-dominated, even the practice of obstetrics, which until the twentieth century, was still viewed as a female responsibility. This rise of medicine has had many implications for women; in this essay I will discuss them. The most important, both in terms of gender and class, was the exclusion of women from what had previously been a female occupation, and their confinement into inferior areas of medicine. However I will also examine the effects the rise of the profession of medicine had on women in relation to how women were viewed from the nineteenth century onward, and also the practical impacts that it had upon women receiving treatment, especially in the case of childbirth. In terms of class I will look at how working-class women received different treatments, as a result of the rise of medicine as a profession, and also how, consequently, the divisions between working-class and upper-class women increased. I will begin by looking at one of the most important implications; women's exclusion from medicine. In previous centuries health care tended to be carried out by female lay-healers within the community (Witz, 1992). Female lay-healers were valued members of the community and as a result gained high status. They cared for the sick and elderly, and were also relied upon to produce drugs and remedies for their patients (Moscucci, 1990). As females were seen as closer to the earth it was felt that they were most suited to carrying out the role of carer within the community, however medicine slowly began to develop as a form of science, rather than an occupation relating to nature, and male domination of the occupation began. From the 1700's the view developed that rationality and science were most important, however these were qualities associated with men, rather than women, who were seen as emotional and religious, not scientific, and as the body began to be viewed from a scientific perspective it was these qualities that were required in the practice of medicine (Donnison, 1993). Men also placed emphasis upon these qualities to justify their dominant role within medicine. Education was introduced to train people in the practice of medicine, and in 1858, the Medical Registration Act secured medicine as a profession (Witz, 1992). However, this also ensured the exclusion of women from medicine. As medicine came to be based on qualifications, rather than experience, patriarchal institutions were able to ensure women were excluded from medicine. They were unable to gain entry into the Universities offering qualifications in medicine, for example the University of Edinburgh, and prevented from taking exams that would gain them a place on the Medical Register (Witz, 1992). Although several women, including Sophie Jex Blake, managed to gain university places, they were subject to prejudice and abuse by fellow students, and following their entry onto the Medical Register, universities changed their policies, ensuring that although women could study medicine, they would not necessarily qualify to practice it (Witz, 1992). This exclusion of women can also be traced in the area of midwifery. Until the twentieth century midwifery was viewed as a female responsibility, being viewed as inferior to other areas. Although from the 1700s onwards there were several man-midwives, they did not deal with routine births, stepping in only if there was a problem; their role was similar to that of a surgeon (Donnison, 1993). However post-1730's their role changed, and they became increasingly involved in routine births, especially with the upper and middle classes. What was seen as 'a problem birth' was redefined, for example prior to this time breech births were regarded as normal, and female midwives considered able to deal with them competently, but as they came to be seen as a problem, male surgeons began to participate in an increasing number (Oakley, 1984). Despite these advances, it was not until the twentieth century that midwifery became recognised as central to general practice (Moscucci, 1990), and qualifications were deemed necessary to practice of midwifery, with the 1902 Midwives Act (Oakley, 1984). This resulted in women being further pushed out of an area which they had continued to dominate, despite the professionalisation of the rest of medicine. This also had class implications, as although women were accepted by lying-in hospitals to train in midwifery they had to pay for the privilege, ensuring that it was only those from the skilled classes who could qualify (Donnison, 1993). Previously, midwifery had been based largely on experience rather than qualification, so was practiced by many working-class women. Overall, one of the most important implications for women, was their excluded from an occupation previously regarded as female, as well as ensuring that only certain classes could enter the profession, which limited the options open to women who needed to work. It impact can still be seen today, as medicine is still a male-dominated practice, and the areas that women do enter, for example nursing, are regarded as inferior to the male-dominated areas, such as surgery. However it was not the only significant impact. Another major implication was the way in which women were depicted by the medical profession. As men wished to justify their exclusion of women from medicine, they redefined how women were viewed. As well continuing to portray them as too emotional to enter what they saw as the world of science, men also began to associate women's bodies with evil, claiming they were inherently dangerous, and therefore needed controlling (Smart, 1992). This would ensure that women were unable to enter medical profession, but also that men could retain control over women's bodies. However at the same time they also depicted women as frail, and therefore dependant upon the male medical practice (Moscucci, 1990, Smart, 1992). Both of these depictions ensured that women were unable to enter medicine, and guaranteed that it remained a male dominated profession. This had class implications for women, as those from the working classes were seen as the most dangerous, whereas frailty was largely associated with the upper and middle classes, which strengthened the divide between the upper and lower classes. It also impacted upon legislation, with further implications for women. As women were construed as dirty and dangerous by the medical profession, laws were created reflecting this point, for example the Contagious Diseases Acts in the 1860's (Smart, 1992), which created further controls for women, especially those from the working classes. The professionalisation of medicine also affected women in a more personal way. As it ensured the exclusion of women from medicine it meant that women seeking medical help had to see a male doctor (Donnison, 1993), a change that caused problems especially in childbirth. It resulted in loss of dignity for women, as many did not wish to see a male doctor in such situations, resulting in many deaths as female midwives were not trained to deal with the problems, or meant that male surgeons had 'to work blind', under the covers, creating many errors (Donnison, 1993). This implication was significant, as it was so personal, and did not just affect women who wished to enter the medical profession but all female patients. Having discussed the implications for women in relation to gender, and in some ways class, I will now focus more closely upon the class impacts. As the practice of medicine moved away from female lay-healing and towards a male profession many working-class women were denied access to medical care. As it became a profession the working classes could no longer afford to receive treatment, so many went without (Donnison, 1993), and the medical profession focused its attention on upper- class women, claiming they were more fragile and in need of extra help, which further excluded working-class women from medical treatment. The few hospitals set up to provide for the working classes were poor quality, and the women were subjected to harsh treatments, which the upper classes who could afford to pay for superior treatment were not. They were treated as if they were dirty in some way, in a similar way to which prostitutes were handled under the Contagious Diseases Acts (Moscucci, 1990). Upper-class women were able to use chloroform during birth, but no such aids were available to those from the working classes (Donnison, 1993). Such differences in treatment helped to increase the division between the working and upper classes that I mentioned previously. Overall the rise of medicine as a profession had many implications for women. It excluded them from the practice, an occupation which had previously been female, a result which had many further implications for women in both gender and class terms. It resulted in the redefining of women, into inherently dangerous, yet fragile human beings, who were therefore dependant upon men, and the male medical profession, as well as having a negative affect on women patients, as many did not wish to see male doctors. It also removed one of the final areas of society from which women could gain status, ensuring that men now acquired this standing, as well as excluding working-class women from treatment as they could no longer afford it, increasing the divisions between women of different classes. Therefore the rise of medicine as a profession turned it into an area of male-dominance, reduced women's power and ensured that male control over women increased, as did the control of the upper classes over the working classes. The rise had a largely negative effect upon women, especially those from the working classes, and its legacy can still be seen in medicine today.",True
1,"Since the fourteenth century the practice of medicine has become a profession, and more importantly, a male-dominated profession. Previously medicine was seen as a female duty, however all areas of medicine have been professionalized over the centuries, and slowly become male-dominated, even the practice of obstetrics, which until the twentieth century, was still viewed as a female responsibility. This rise of medicine has had many implications for women; in this essay I will discuss them. The most important, both in terms of gender and class, was the exclusion of women from what had previously been a female occupation, and their confinement into inferior areas of medicine. However I will also examine the effects the rise of the profession of medicine had on women in relation to how women were viewed from the nineteenth century onward, and also the practical impacts that it had upon women receiving treatment, especially in the case of childbirth. In terms of class I will look at how working-class women received different treatments, as a result of the rise of medicine as a profession, and also how, consequently, the divisions between working-class and upper-class women increased. I will begin by looking at one of the most important implications; women's exclusion from medicine. In previous centuries health care tended to be carried out by female lay-healers within the community (Witz, 1992). Female lay-healers were valued members of the community and as a result gained high status. They cared for the sick and elderly, and were also relied upon to produce drugs and remedies for their patients (Moscucci, 1990). As females were seen as closer to the earth it was felt that they were most suited to carrying out the role of carer within the community, however medicine slowly began to develop as a form of science, rather than an occupation relating to nature, and male domination of the occupation began. From the 1700's the view developed that rationality and science were most important, however these were qualities associated with men, rather than women, who were seen as emotional and religious, not scientific, and as the body began to be viewed from a scientific perspective it was these qualities that were required in the practice of medicine (Donnison, 1993). Men also placed emphasis upon these qualities to justify their dominant role within medicine. Education was introduced to train people in the practice of medicine, and in 1858, the Medical Registration Act secured medicine as a profession (Witz, 1992). However, this also ensured the exclusion of women from medicine. As medicine came to be based on qualifications, rather than experience, patriarchal institutions were able to ensure women were excluded from medicine. They were unable to gain entry into the Universities offering qualifications in medicine, for example the University of Edinburgh, and prevented from taking exams that would gain them a place on the Medical Register (Witz, 1992). Although several women, including Sophie Jex Blake, managed to gain university places, they were subject to prejudice and abuse by fellow students, and following their entry onto the Medical Register, universities changed their policies, ensuring that although women could study medicine, they would not necessarily qualify to practice it (Witz, 1992). This exclusion of women can also be traced in the area of midwifery. Until the twentieth century midwifery was viewed as a female responsibility, being viewed as inferior to other areas. Although from the 1700s onwards there were several man-midwives, they did not deal with routine births, stepping in only if there was a problem; their role was similar to that of a surgeon (Donnison, 1993). However post-1730's their role changed, and they became increasingly involved in routine births, especially with the upper and middle classes. What was seen as 'a problem birth' was redefined, for example prior to this time breech births were regarded as normal, and female midwives considered able to deal with them competently, but as they came to be seen as a problem, male surgeons began to participate in an increasing number (Oakley, 1984). Despite these advances, it was not until the twentieth century that midwifery became recognised as central to general practice (Moscucci, 1990), and qualifications were deemed necessary to practice of midwifery, with the 1902 Midwives Act (Oakley, 1984). This resulted in women being further pushed out of an area which they had continued to dominate, despite the professionalisation of the rest of medicine. This also had class implications, as although women were accepted by lying-in hospitals to train in midwifery they had to pay for the privilege, ensuring that it was only those from the skilled classes who could qualify (Donnison, 1993). Previously, midwifery had been based largely on experience rather than qualification, so was practiced by many working-class women. Overall, one of the most important implications for women, was their excluded from an occupation previously regarded as female, as well as ensuring that only certain classes could enter the profession, which limited the options open to women who needed to work. It impact can still be seen today, as medicine is still a male-dominated practice, and the areas that women do enter, for example nursing, are regarded as inferior to the male-dominated areas, such as surgery. However it was not the only significant impact. Another major implication was the way in which women were depicted by the medical profession. As men wished to justify their exclusion of women from medicine, they redefined how women were viewed. As well continuing to portray them as too emotional to enter what they saw as the world of science, men also began to associate women's bodies with evil, claiming they were inherently dangerous, and therefore needed controlling (Smart, 1992). This would ensure that women were unable to enter medical profession, but also that men could retain control over women's bodies. However at the same time they also depicted women as frail, and therefore dependant upon the male medical practice (Moscucci, 1990, Smart, 1992). Both of these depictions ensured that women were unable to enter medicine, and guaranteed that it remained a male dominated profession. This had class implications for women, as those from the working classes were seen as the most dangerous, whereas frailty was largely associated with the upper and middle classes, which strengthened the divide between the upper and lower classes. It also impacted upon legislation, with further implications for women. As women were construed as dirty and dangerous by the medical profession, laws were created reflecting this point, for example the Contagious Diseases Acts in the 1860's (Smart, 1992), which created further controls for women, especially those from the working classes. The professionalisation of medicine also affected women in a more personal way. As it ensured the exclusion of women from medicine it meant that women seeking medical help had to see a male doctor (Donnison, 1993), a change that caused problems especially in childbirth. It resulted in loss of dignity for women, as many did not wish to see a male doctor in such situations, resulting in many deaths as female midwives were not trained to deal with the problems, or meant that male surgeons had 'to work blind', under the covers, creating many errors (Donnison, 1993). This implication was significant, as it was so personal, and did not just affect women who wished to enter the medical profession but all female patients. Having discussed the implications for women in relation to gender, and in some ways class, I will now focus more closely upon the class impacts. As the practice of medicine moved away from female lay-healing and towards a male profession many working-class women were denied access to medical care. As it became a profession the working classes could no longer afford to receive treatment, so many went without (Donnison, 1993), and the medical profession focused its attention on upper- class women, claiming they were more fragile and in need of extra help, which further excluded working-class women from medical treatment. The few hospitals set up to provide for the working classes were poor quality, and the women were subjected to harsh treatments, which the upper classes who could afford to pay for superior treatment were not. They were treated as if they were dirty in some way, in a similar way to which prostitutes were handled under the Contagious Diseases Acts (Moscucci, 1990). Upper-class women were able to use chloroform during birth, but no such aids were available to those from the working classes (Donnison, 1993). Such differences in treatment helped to increase the division between the working and upper classes that I mentioned previously. Overall the rise of medicine as a profession had many implications for women. It excluded them from the practice, an occupation which had previously been female, a result which had many further implications for women in both gender and class terms. It resulted in the redefining of women, into inherently dangerous, yet fragile human beings, who were therefore dependant upon men, and the male medical profession, as well as having a negative affect on women patients, as many did not wish to see male doctors. It also removed one of the final areas of society from which women could gain status, ensuring that men now acquired this standing, as well as excluding working-class women from treatment as they could no longer afford it, increasing the divisions between women of different classes. Therefore the rise of medicine as a profession turned it into an area of male-dominance, reduced women's power and ensured that male control over women increased, as did the control of the upper classes over the working classes. The rise had a largely negative effect upon women, especially those from the working classes, and its legacy can still be seen in medicine today.","Official statistics are those produced by either the state, or by one of its agencies. There has been a huge increase in the number of official statistics since the Central Statistics Office was set up in 1941, and now large quantities of statistics are produced on a variety of different topics, for example income, housing and population. Although they are widely used there are many limitations of the use official statistics in sociology. In this essay I plan to talk about how the internal, or posivitist schools view statistics, looking at how they are critical of the reliability and validity of official statistics, and also how they feel that the processes involved in producing the statistics limit their uses. I will also explore the uses that statistics do have in sociology, and come to a conclusion on whether they should be use or not. The internal school of thought emphasises the need for reliability and validity in order to produce realistic statistics, however there are many problems with official statistics in this aspect, and as Hindess points out ""they cannot be taken as a...reliable account"" (Hindess, 1973). The term reliability refers to whether the statistics are replicable or not, and validity is defined as whether they paint a true picture of what they are looking at, and there are many problems with regards to both. The first problem is that of the definition of concept, and can cause problems with the reliability of the statistics. Different people define concepts in different ways so this alters the reliability of the study, and also means that readers may interpret the statistics in a different way to that which they were intended (Hindess, 1973, Shipman, 1973, May, 1997). Hindess points out that if concepts are defined differently it can have a huge result on the statistics (Hindess, 1973, cited in Irvine et al, 1979), and in Demystifying Statistics Irvine uses the example of unemployment, claiming that over the past three decades the definition of unemployment has altered many times, and this has big impact on the number of people who are defined as unemployed, according o the statistics (Irvine and Miles, 1979). The next problem is linked to validity. Official statistics go through a long process of decision making before the final result is reached (Hindess, 1973, May, 1997), even down to what methodology is used, and this process can alter the results greatly. In Social Research May uses the example of crime statistics to show this. The first decision in the process here is that of what is a crime in terms of the law. This would have a huge impact on crime statistics, for example if marital rape was not illegal then the crime statistics of rape would be very different. The second decision is made by the victim of the crime; whether to report it or not, which means that the formation of official statistics is not just influenced by those composing them, but also by decisions made by those involved in the actual area (May, 1997). Human judgement is a key factor in this criticism, but as the decision process shows it is not just the judgement of those composing the statistics that is involved (Hindess, 1973). This means that as the process continues the statistics become further distorted as different people's views and decisions become involved. This idea can be illustrated using the example of suicide. The official statistics produced on this topic are not only influenced by those collating the statistics, but also by those determining the causes of death. This can be seen by the fact that in the past Catholic priests did not like to categorise the cause of a death as 'suicide', so the term 'natural cause' would have been used instead, resulting in a very different set of statistics in the end. Cicourel also supports this idea; however he takes it one step further by saying that these decisions are influenced by people's background experiences (Cicourel, cited in Hindess, 1973). This means that even if two people from the same organisation tried to collect the same results there would be complications as they have had different experiences. Overall this shows that reliability and validity are hard to establish in official statistics, due to the number of human decisions involved, and the different ways in which people apply concepts. However according to the radical approach there are more limitations with statistics. The radical approach tells us that us that official statistics tell us more about the state than they do the topics they are exploring, therefore statistics should be viewed as a construction, and as Irvine and Miles say; ""statistics are not collected, but produced"" (Irvine and Miles, 1979). Official statistics are not neutral perceptions of a topic, but instead reflect political agendas (Irvine and Miles, 1979) and the aims of those with political power (May, 1997). One example of this is the way in which the government produced official statistics relating to unemployment under Margaret Thatcher. During her time in power the definition of unemployment changed over twenty times, as the aim of the statistics was to show how unemployment was decreasing, and the most successful way in which to achieve this was to alter the definition of the concept of unemployment. This shows how the aims and motives of those collecting the data can influence the final statistics. Radical thinkers also believe that statistics only tell us about the groups in society who hold power, and are simply a reflection of the balances of power in society (Hesteretal, 1996, cited in May, 1997). Official statistics do not exist on all topics that sociologists may want to look at, and as a result can be very limiting for them, but also only present a one sided view of society. One example of this is servants in the Victorian era. Many statistics from this time were produced by those in positions of power, for example mistresses, so they tell us little about the lives of the servants, who were working class and held little power. However, despite these limitations official statistics do have some uses. According to posivitists as long as the statistics are reliable and valid then hey are very useful, and one example of this is birth and death rates. As the registration of births and deaths are legal requirements the statistics are reliable, and paint a valid picture of the contemporary and past population levels. Statistics that are not produced using legal requirements can also be useful, however. Bulmer claims that statisticians often go to many lengths to avoid problems with reliability and validity, and he claims that as a result official statistics pose no more of a problem than sociological research. He also argues that if sufficient thought is given to the production of official statistics, and more scientific, rather than theoretical concepts are used, then these problems can be corrected, however it is hard to apply this to sociological research (Bulmer, 1984, cited in May, 1997). Using the radical approach statistics can be useful to tell us about the state and those in power, and what they are interested in. However radicals argue that if the processes involved in the construction of official statistics are carefully considered when analysing the results then the statistics can be seen to present a useful and valid picture of society (Hindess, 1973). However radicals also argue that official statistics are used most successfully, and paint the most realistic picture when they are combined with another method (May, 1997), for example self report studies when the topic of crime is being studied, as this allows the decision making process involved in the official statistics to be balanced out, as people are more likely to be honest in a self report study. Overall, according to the posivitists the main limitations of official statistics arise from the issues of reliability and validity, however they feel that in many cases these limitations are overcome by careful planning, so argue that sociologists should use official statistics. Radicals on the other hand, believe that official statistics are a construction of the state, and this means that although they are useful for examining the state and its interests they do not provide a realistic picture of society. I feel sociologists should use official statistics especially when examining those with power, or exploring the state, however they should treat official statistics produced on other topics with care, and be aware of the process of decision making and methodology that occurs before the statistics are finally produced.",False
2,"Since the fourteenth century the practice of medicine has become a profession, and more importantly, a male-dominated profession. Previously medicine was seen as a female duty, however all areas of medicine have been professionalized over the centuries, and slowly become male-dominated, even the practice of obstetrics, which until the twentieth century, was still viewed as a female responsibility. This rise of medicine has had many implications for women; in this essay I will discuss them. The most important, both in terms of gender and class, was the exclusion of women from what had previously been a female occupation, and their confinement into inferior areas of medicine. However I will also examine the effects the rise of the profession of medicine had on women in relation to how women were viewed from the nineteenth century onward, and also the practical impacts that it had upon women receiving treatment, especially in the case of childbirth. In terms of class I will look at how working-class women received different treatments, as a result of the rise of medicine as a profession, and also how, consequently, the divisions between working-class and upper-class women increased. I will begin by looking at one of the most important implications; women's exclusion from medicine. In previous centuries health care tended to be carried out by female lay-healers within the community (Witz, 1992). Female lay-healers were valued members of the community and as a result gained high status. They cared for the sick and elderly, and were also relied upon to produce drugs and remedies for their patients (Moscucci, 1990). As females were seen as closer to the earth it was felt that they were most suited to carrying out the role of carer within the community, however medicine slowly began to develop as a form of science, rather than an occupation relating to nature, and male domination of the occupation began. From the 1700's the view developed that rationality and science were most important, however these were qualities associated with men, rather than women, who were seen as emotional and religious, not scientific, and as the body began to be viewed from a scientific perspective it was these qualities that were required in the practice of medicine (Donnison, 1993). Men also placed emphasis upon these qualities to justify their dominant role within medicine. Education was introduced to train people in the practice of medicine, and in 1858, the Medical Registration Act secured medicine as a profession (Witz, 1992). However, this also ensured the exclusion of women from medicine. As medicine came to be based on qualifications, rather than experience, patriarchal institutions were able to ensure women were excluded from medicine. They were unable to gain entry into the Universities offering qualifications in medicine, for example the University of Edinburgh, and prevented from taking exams that would gain them a place on the Medical Register (Witz, 1992). Although several women, including Sophie Jex Blake, managed to gain university places, they were subject to prejudice and abuse by fellow students, and following their entry onto the Medical Register, universities changed their policies, ensuring that although women could study medicine, they would not necessarily qualify to practice it (Witz, 1992). This exclusion of women can also be traced in the area of midwifery. Until the twentieth century midwifery was viewed as a female responsibility, being viewed as inferior to other areas. Although from the 1700s onwards there were several man-midwives, they did not deal with routine births, stepping in only if there was a problem; their role was similar to that of a surgeon (Donnison, 1993). However post-1730's their role changed, and they became increasingly involved in routine births, especially with the upper and middle classes. What was seen as 'a problem birth' was redefined, for example prior to this time breech births were regarded as normal, and female midwives considered able to deal with them competently, but as they came to be seen as a problem, male surgeons began to participate in an increasing number (Oakley, 1984). Despite these advances, it was not until the twentieth century that midwifery became recognised as central to general practice (Moscucci, 1990), and qualifications were deemed necessary to practice of midwifery, with the 1902 Midwives Act (Oakley, 1984). This resulted in women being further pushed out of an area which they had continued to dominate, despite the professionalisation of the rest of medicine. This also had class implications, as although women were accepted by lying-in hospitals to train in midwifery they had to pay for the privilege, ensuring that it was only those from the skilled classes who could qualify (Donnison, 1993). Previously, midwifery had been based largely on experience rather than qualification, so was practiced by many working-class women. Overall, one of the most important implications for women, was their excluded from an occupation previously regarded as female, as well as ensuring that only certain classes could enter the profession, which limited the options open to women who needed to work. It impact can still be seen today, as medicine is still a male-dominated practice, and the areas that women do enter, for example nursing, are regarded as inferior to the male-dominated areas, such as surgery. However it was not the only significant impact. Another major implication was the way in which women were depicted by the medical profession. As men wished to justify their exclusion of women from medicine, they redefined how women were viewed. As well continuing to portray them as too emotional to enter what they saw as the world of science, men also began to associate women's bodies with evil, claiming they were inherently dangerous, and therefore needed controlling (Smart, 1992). This would ensure that women were unable to enter medical profession, but also that men could retain control over women's bodies. However at the same time they also depicted women as frail, and therefore dependant upon the male medical practice (Moscucci, 1990, Smart, 1992). Both of these depictions ensured that women were unable to enter medicine, and guaranteed that it remained a male dominated profession. This had class implications for women, as those from the working classes were seen as the most dangerous, whereas frailty was largely associated with the upper and middle classes, which strengthened the divide between the upper and lower classes. It also impacted upon legislation, with further implications for women. As women were construed as dirty and dangerous by the medical profession, laws were created reflecting this point, for example the Contagious Diseases Acts in the 1860's (Smart, 1992), which created further controls for women, especially those from the working classes. The professionalisation of medicine also affected women in a more personal way. As it ensured the exclusion of women from medicine it meant that women seeking medical help had to see a male doctor (Donnison, 1993), a change that caused problems especially in childbirth. It resulted in loss of dignity for women, as many did not wish to see a male doctor in such situations, resulting in many deaths as female midwives were not trained to deal with the problems, or meant that male surgeons had 'to work blind', under the covers, creating many errors (Donnison, 1993). This implication was significant, as it was so personal, and did not just affect women who wished to enter the medical profession but all female patients. Having discussed the implications for women in relation to gender, and in some ways class, I will now focus more closely upon the class impacts. As the practice of medicine moved away from female lay-healing and towards a male profession many working-class women were denied access to medical care. As it became a profession the working classes could no longer afford to receive treatment, so many went without (Donnison, 1993), and the medical profession focused its attention on upper- class women, claiming they were more fragile and in need of extra help, which further excluded working-class women from medical treatment. The few hospitals set up to provide for the working classes were poor quality, and the women were subjected to harsh treatments, which the upper classes who could afford to pay for superior treatment were not. They were treated as if they were dirty in some way, in a similar way to which prostitutes were handled under the Contagious Diseases Acts (Moscucci, 1990). Upper-class women were able to use chloroform during birth, but no such aids were available to those from the working classes (Donnison, 1993). Such differences in treatment helped to increase the division between the working and upper classes that I mentioned previously. Overall the rise of medicine as a profession had many implications for women. It excluded them from the practice, an occupation which had previously been female, a result which had many further implications for women in both gender and class terms. It resulted in the redefining of women, into inherently dangerous, yet fragile human beings, who were therefore dependant upon men, and the male medical profession, as well as having a negative affect on women patients, as many did not wish to see male doctors. It also removed one of the final areas of society from which women could gain status, ensuring that men now acquired this standing, as well as excluding working-class women from treatment as they could no longer afford it, increasing the divisions between women of different classes. Therefore the rise of medicine as a profession turned it into an area of male-dominance, reduced women's power and ensured that male control over women increased, as did the control of the upper classes over the working classes. The rise had a largely negative effect upon women, especially those from the working classes, and its legacy can still be seen in medicine today.","Racism is still a problem within our society today, and many ethnic minorities face inequalities in many areas, including education, housing, and employment. Ethnic minorities are concentrated into certain areas of the job market, such as manufacture and communication (Brown, 1992), are most likely to be the victims of assault (Abercrombie et al, 1994), and recent surveys have shown that racist ideas still exist in society. This can be seen in a survey, carried out in 1993, that asked a white sample whether they agreed or disagreed with the statement: 'Immigration has enriched the quality of life in Britain' and nearly half of the sample disagreed (Abercrombie et al, 1994, p255). In this essay I will look at what racism is, and how it is defined in contemporary society, and I will then explore why it still persists. In this section I will cover three areas that I think have contributed to the continued existence of racism; culture, economy and politics, all of which have meant that racism is an integral and internalised part of our society, however I will not be looking at individual or specific cases of racism, rather exploring a broader explanation as to why it continues. Racism is a very hard concept to define. Hall argues that it is not a static phenomenon, but is constantly changing and redefining (Hall, 1978, cited in Miles, 1989, p64). In the nineteenth century racism was a product of scientific ideas, and the belief that people could be distinguished by the term race, using particular physical features, and that certain races were superior to others (Miles, 1989). However throughout the nineteenth century racism was rejected as a scientific concept (Banton, 1987, cited in Miles, 1989, p49). It is now important to realise that racism is not just an ideology but also a practice, and in contemporary society it has been defined as a process by which subordinated groups are excluded. This idea is expanded on by Carmichael and Hamilton who argue that it is; 'the predication of decisions and policies on considerations of race for the purpose of subordinating a racial group and maintaining control over that group' (Carmichael and Hamilton, 1968, cited in Miles, 1989, p51). This acknowledges that racism is not just a belief, but also an action, but in this essay I will concentrate on why the ideology of racism is still prevalent within society, as without the continuation of the ideology then racist actions would not take place. I will now look at why racism still persists in society, and the first explanation I will consider is culture. Racist beliefs are part of a legacy of both the slave trade and colonialism as racial categorisation played a large part in the justification of both periods. The idea that humankind was split into different races and that certain races, usually white British, were superior was strong at the time, and reinforced to justify slavery and imperialism. These ideas have left a strong legacy in coloniser societies today (Fenton, 1999). Europeans constructed themselves as the representatives of superior civilisation, and in doing so also constructed those from other countries as the 'Other', a group of people who were uncivilised, and therefore inferior to the West, an idea that was reinforced through literature, as many travellers at the time focused on the abnormal characteristics of the 'Other', and helped construct the idea that they were barbaric and uncivilised (Miles, 1989). Characteristics such as cannibalism and the degradation of women were picked out as features that made those from the colonised states racially inferior, and uncivilised, and reinforced the idea that a racialist hierarchy existed that placed those from the West at the top. By defining people from these countries as the 'Other' they were automatically excluded from the society of the West (Miles, 1989). Although this construction is no longer as prevalent in society, it has impacted on racism in society today. Images and ideas that were associated with the 'Other', for example the veil in Islam, are still around today, and the racial ideas that were leant to these images still exist. There is also the idea that White Europeans wished to continue their superiority over colonised societies, so sustained the ideas associated with the inferior 'Other' in order to maintain it (Fenton, 1999). It can also be argued that racism persists through culture due to the ideas of nationalism. Nationalism is a very popular discourse in contemporary society, as it was during the period of colonialisation, and the two are very closely linked, with ideas of nationalism often associated with both the ideology and actions of racism (Fenton, 1999). Overall colonialism has impacted on the racist ideas prevalent in today's society, as although the concept of 'Other' that existed in the nineteenth century no longer continues the definition of 'Other' has changed (Miles, 1989), and it has left a legacy of racism in contemporary society. However this is not the only reason for the continuation of racism, economic factors have also had an impact. The racism apparent at the time of colonialisation has been reinforced through the labour market. During this period the racial ideas were put into practice in the economy as those constructed as the 'Other' were exploited through the trade links established with the colonies, by those in power (Miles, 1989) showing that racialism has been imbedded into the economy since before capitalism. However since the emergence of capitalism these racist ideas have been used to support the capitalist market. Capitalists have ensured that ethnic minorities are viewed as suitable only for menial jobs, as they can easily exploit them, and this has reinforced the idea that race determines your skills and abilities, therefore causing racism to be imbedded within the capitalist market (Fenton, 1999). Castles and Kosack have argued that capitalism requires a 'reserve army of labour' to ensure the successful continuation of the economic production along capitalist values, and that ethnic minorities provide this army (Procter, Lecture notes, 2004). However by constructing ethnic minorities as a reserve army of labour racist beliefs are reinforced, as they are viewed as both inferior and only suitable for inferior jobs, but also as a threat to the white workers, creating a sense of racism in white workers. Therefore the racist beliefs apparent in the culture of colonialism are recreated in the current economy, ensuring that racism persists, although there are some problems with this idea as it ignores the white people employed in menial jobs and over emphasises the idea that ethnic minorities are needed as a 'reserve army of labour' (Miles, 1989). However politics, and the policies of British political parties over the twentieth century have also ensured that these racial ideas continue. Since the Second World War British politics reinforced have the idea that those from other races are inferior. In the 1950s both the Labour and Conservative Parties were openly racist (Brown, 1992), and they constructed immigrants as a social problem. Throughout the 1950s the debates on immigration focused on the idea that the number of black immigrants entering Britain needed to be restricted as they were posing a threat to British society. Immigrants were linked with social problems, including shortages of housing and crime, and through this connection racism was reinforced, as people began to see immigrants and people from other races as a problem. Emphasis was put on their undesirable behaviour, for example crime, and prostitution, despite the fact that much of this was not carried out by immigrants (Solomos, 1989, ensuring that people linked the idea of undesirable with immigrants. This construction of immigrants as a problem continued throughout out the 1960s, and in 1962 the Commonwealth Immigrants Act was introduced, which placed restrictions on the number of black immigrants entering the country (Solomos, 1989). By the 1970s legislation had been bought in which controlled the entry of non-white immigrants through a voucher system, giving the government control over who entered the country, and what work they could do following their entry (Brown, 1992). This political racism persisted after 1979, with Margaret Thatcher again emphasising the danger to British society that immigrants pose (Solomos, 1989), and these ideas can still be seen today, with the current clamp-down on the number of asylum seekers entering the country. Overall the policies of the government since the Second World War have made racism an integral part of our society, and ensured that non-whites are still viewed with distaste and seen as inferior. In conclusion racism, which can be defined as an ideology which categorises people as inferior according to their race, and is put into practice through many policies and actions, which seek to exclude non-whites from many areas of society, can still be seen in today's society. There are three reasons for this. Firstly the culture of colonialism and the belief that the 'Other' is inferior is still apparent in our society today. Secondly these beliefs are reinforced through the economy, as capitalism exploits ethnic minorities, and ensures that they are viewed as less skilled, and finally racism is embedded in our political system through the policies of political parties and the links they make between non-white immigrants and social problems. All of these processes work together in contemporary society to ensure that racism still continues, and non-whites are excluded.",True
3,"Racism is still a problem within our society today, and many ethnic minorities face inequalities in many areas, including education, housing, and employment. Ethnic minorities are concentrated into certain areas of the job market, such as manufacture and communication (Brown, 1992), are most likely to be the victims of assault (Abercrombie et al, 1994), and recent surveys have shown that racist ideas still exist in society. This can be seen in a survey, carried out in 1993, that asked a white sample whether they agreed or disagreed with the statement: 'Immigration has enriched the quality of life in Britain' and nearly half of the sample disagreed (Abercrombie et al, 1994, p255). In this essay I will look at what racism is, and how it is defined in contemporary society, and I will then explore why it still persists. In this section I will cover three areas that I think have contributed to the continued existence of racism; culture, economy and politics, all of which have meant that racism is an integral and internalised part of our society, however I will not be looking at individual or specific cases of racism, rather exploring a broader explanation as to why it continues. Racism is a very hard concept to define. Hall argues that it is not a static phenomenon, but is constantly changing and redefining (Hall, 1978, cited in Miles, 1989, p64). In the nineteenth century racism was a product of scientific ideas, and the belief that people could be distinguished by the term race, using particular physical features, and that certain races were superior to others (Miles, 1989). However throughout the nineteenth century racism was rejected as a scientific concept (Banton, 1987, cited in Miles, 1989, p49). It is now important to realise that racism is not just an ideology but also a practice, and in contemporary society it has been defined as a process by which subordinated groups are excluded. This idea is expanded on by Carmichael and Hamilton who argue that it is; 'the predication of decisions and policies on considerations of race for the purpose of subordinating a racial group and maintaining control over that group' (Carmichael and Hamilton, 1968, cited in Miles, 1989, p51). This acknowledges that racism is not just a belief, but also an action, but in this essay I will concentrate on why the ideology of racism is still prevalent within society, as without the continuation of the ideology then racist actions would not take place. I will now look at why racism still persists in society, and the first explanation I will consider is culture. Racist beliefs are part of a legacy of both the slave trade and colonialism as racial categorisation played a large part in the justification of both periods. The idea that humankind was split into different races and that certain races, usually white British, were superior was strong at the time, and reinforced to justify slavery and imperialism. These ideas have left a strong legacy in coloniser societies today (Fenton, 1999). Europeans constructed themselves as the representatives of superior civilisation, and in doing so also constructed those from other countries as the 'Other', a group of people who were uncivilised, and therefore inferior to the West, an idea that was reinforced through literature, as many travellers at the time focused on the abnormal characteristics of the 'Other', and helped construct the idea that they were barbaric and uncivilised (Miles, 1989). Characteristics such as cannibalism and the degradation of women were picked out as features that made those from the colonised states racially inferior, and uncivilised, and reinforced the idea that a racialist hierarchy existed that placed those from the West at the top. By defining people from these countries as the 'Other' they were automatically excluded from the society of the West (Miles, 1989). Although this construction is no longer as prevalent in society, it has impacted on racism in society today. Images and ideas that were associated with the 'Other', for example the veil in Islam, are still around today, and the racial ideas that were leant to these images still exist. There is also the idea that White Europeans wished to continue their superiority over colonised societies, so sustained the ideas associated with the inferior 'Other' in order to maintain it (Fenton, 1999). It can also be argued that racism persists through culture due to the ideas of nationalism. Nationalism is a very popular discourse in contemporary society, as it was during the period of colonialisation, and the two are very closely linked, with ideas of nationalism often associated with both the ideology and actions of racism (Fenton, 1999). Overall colonialism has impacted on the racist ideas prevalent in today's society, as although the concept of 'Other' that existed in the nineteenth century no longer continues the definition of 'Other' has changed (Miles, 1989), and it has left a legacy of racism in contemporary society. However this is not the only reason for the continuation of racism, economic factors have also had an impact. The racism apparent at the time of colonialisation has been reinforced through the labour market. During this period the racial ideas were put into practice in the economy as those constructed as the 'Other' were exploited through the trade links established with the colonies, by those in power (Miles, 1989) showing that racialism has been imbedded into the economy since before capitalism. However since the emergence of capitalism these racist ideas have been used to support the capitalist market. Capitalists have ensured that ethnic minorities are viewed as suitable only for menial jobs, as they can easily exploit them, and this has reinforced the idea that race determines your skills and abilities, therefore causing racism to be imbedded within the capitalist market (Fenton, 1999). Castles and Kosack have argued that capitalism requires a 'reserve army of labour' to ensure the successful continuation of the economic production along capitalist values, and that ethnic minorities provide this army (Procter, Lecture notes, 2004). However by constructing ethnic minorities as a reserve army of labour racist beliefs are reinforced, as they are viewed as both inferior and only suitable for inferior jobs, but also as a threat to the white workers, creating a sense of racism in white workers. Therefore the racist beliefs apparent in the culture of colonialism are recreated in the current economy, ensuring that racism persists, although there are some problems with this idea as it ignores the white people employed in menial jobs and over emphasises the idea that ethnic minorities are needed as a 'reserve army of labour' (Miles, 1989). However politics, and the policies of British political parties over the twentieth century have also ensured that these racial ideas continue. Since the Second World War British politics reinforced have the idea that those from other races are inferior. In the 1950s both the Labour and Conservative Parties were openly racist (Brown, 1992), and they constructed immigrants as a social problem. Throughout the 1950s the debates on immigration focused on the idea that the number of black immigrants entering Britain needed to be restricted as they were posing a threat to British society. Immigrants were linked with social problems, including shortages of housing and crime, and through this connection racism was reinforced, as people began to see immigrants and people from other races as a problem. Emphasis was put on their undesirable behaviour, for example crime, and prostitution, despite the fact that much of this was not carried out by immigrants (Solomos, 1989, ensuring that people linked the idea of undesirable with immigrants. This construction of immigrants as a problem continued throughout out the 1960s, and in 1962 the Commonwealth Immigrants Act was introduced, which placed restrictions on the number of black immigrants entering the country (Solomos, 1989). By the 1970s legislation had been bought in which controlled the entry of non-white immigrants through a voucher system, giving the government control over who entered the country, and what work they could do following their entry (Brown, 1992). This political racism persisted after 1979, with Margaret Thatcher again emphasising the danger to British society that immigrants pose (Solomos, 1989), and these ideas can still be seen today, with the current clamp-down on the number of asylum seekers entering the country. Overall the policies of the government since the Second World War have made racism an integral part of our society, and ensured that non-whites are still viewed with distaste and seen as inferior. In conclusion racism, which can be defined as an ideology which categorises people as inferior according to their race, and is put into practice through many policies and actions, which seek to exclude non-whites from many areas of society, can still be seen in today's society. There are three reasons for this. Firstly the culture of colonialism and the belief that the 'Other' is inferior is still apparent in our society today. Secondly these beliefs are reinforced through the economy, as capitalism exploits ethnic minorities, and ensures that they are viewed as less skilled, and finally racism is embedded in our political system through the policies of political parties and the links they make between non-white immigrants and social problems. All of these processes work together in contemporary society to ensure that racism still continues, and non-whites are excluded.","Since the fourteenth century the practice of medicine has become a profession, and more importantly, a male-dominated profession. Previously medicine was seen as a female duty, however all areas of medicine have been professionalized over the centuries, and slowly become male-dominated, even the practice of obstetrics, which until the twentieth century, was still viewed as a female responsibility. This rise of medicine has had many implications for women; in this essay I will discuss them. The most important, both in terms of gender and class, was the exclusion of women from what had previously been a female occupation, and their confinement into inferior areas of medicine. However I will also examine the effects the rise of the profession of medicine had on women in relation to how women were viewed from the nineteenth century onward, and also the practical impacts that it had upon women receiving treatment, especially in the case of childbirth. In terms of class I will look at how working-class women received different treatments, as a result of the rise of medicine as a profession, and also how, consequently, the divisions between working-class and upper-class women increased. I will begin by looking at one of the most important implications; women's exclusion from medicine. In previous centuries health care tended to be carried out by female lay-healers within the community (Witz, 1992). Female lay-healers were valued members of the community and as a result gained high status. They cared for the sick and elderly, and were also relied upon to produce drugs and remedies for their patients (Moscucci, 1990). As females were seen as closer to the earth it was felt that they were most suited to carrying out the role of carer within the community, however medicine slowly began to develop as a form of science, rather than an occupation relating to nature, and male domination of the occupation began. From the 1700's the view developed that rationality and science were most important, however these were qualities associated with men, rather than women, who were seen as emotional and religious, not scientific, and as the body began to be viewed from a scientific perspective it was these qualities that were required in the practice of medicine (Donnison, 1993). Men also placed emphasis upon these qualities to justify their dominant role within medicine. Education was introduced to train people in the practice of medicine, and in 1858, the Medical Registration Act secured medicine as a profession (Witz, 1992). However, this also ensured the exclusion of women from medicine. As medicine came to be based on qualifications, rather than experience, patriarchal institutions were able to ensure women were excluded from medicine. They were unable to gain entry into the Universities offering qualifications in medicine, for example the University of Edinburgh, and prevented from taking exams that would gain them a place on the Medical Register (Witz, 1992). Although several women, including Sophie Jex Blake, managed to gain university places, they were subject to prejudice and abuse by fellow students, and following their entry onto the Medical Register, universities changed their policies, ensuring that although women could study medicine, they would not necessarily qualify to practice it (Witz, 1992). This exclusion of women can also be traced in the area of midwifery. Until the twentieth century midwifery was viewed as a female responsibility, being viewed as inferior to other areas. Although from the 1700s onwards there were several man-midwives, they did not deal with routine births, stepping in only if there was a problem; their role was similar to that of a surgeon (Donnison, 1993). However post-1730's their role changed, and they became increasingly involved in routine births, especially with the upper and middle classes. What was seen as 'a problem birth' was redefined, for example prior to this time breech births were regarded as normal, and female midwives considered able to deal with them competently, but as they came to be seen as a problem, male surgeons began to participate in an increasing number (Oakley, 1984). Despite these advances, it was not until the twentieth century that midwifery became recognised as central to general practice (Moscucci, 1990), and qualifications were deemed necessary to practice of midwifery, with the 1902 Midwives Act (Oakley, 1984). This resulted in women being further pushed out of an area which they had continued to dominate, despite the professionalisation of the rest of medicine. This also had class implications, as although women were accepted by lying-in hospitals to train in midwifery they had to pay for the privilege, ensuring that it was only those from the skilled classes who could qualify (Donnison, 1993). Previously, midwifery had been based largely on experience rather than qualification, so was practiced by many working-class women. Overall, one of the most important implications for women, was their excluded from an occupation previously regarded as female, as well as ensuring that only certain classes could enter the profession, which limited the options open to women who needed to work. It impact can still be seen today, as medicine is still a male-dominated practice, and the areas that women do enter, for example nursing, are regarded as inferior to the male-dominated areas, such as surgery. However it was not the only significant impact. Another major implication was the way in which women were depicted by the medical profession. As men wished to justify their exclusion of women from medicine, they redefined how women were viewed. As well continuing to portray them as too emotional to enter what they saw as the world of science, men also began to associate women's bodies with evil, claiming they were inherently dangerous, and therefore needed controlling (Smart, 1992). This would ensure that women were unable to enter medical profession, but also that men could retain control over women's bodies. However at the same time they also depicted women as frail, and therefore dependant upon the male medical practice (Moscucci, 1990, Smart, 1992). Both of these depictions ensured that women were unable to enter medicine, and guaranteed that it remained a male dominated profession. This had class implications for women, as those from the working classes were seen as the most dangerous, whereas frailty was largely associated with the upper and middle classes, which strengthened the divide between the upper and lower classes. It also impacted upon legislation, with further implications for women. As women were construed as dirty and dangerous by the medical profession, laws were created reflecting this point, for example the Contagious Diseases Acts in the 1860's (Smart, 1992), which created further controls for women, especially those from the working classes. The professionalisation of medicine also affected women in a more personal way. As it ensured the exclusion of women from medicine it meant that women seeking medical help had to see a male doctor (Donnison, 1993), a change that caused problems especially in childbirth. It resulted in loss of dignity for women, as many did not wish to see a male doctor in such situations, resulting in many deaths as female midwives were not trained to deal with the problems, or meant that male surgeons had 'to work blind', under the covers, creating many errors (Donnison, 1993). This implication was significant, as it was so personal, and did not just affect women who wished to enter the medical profession but all female patients. Having discussed the implications for women in relation to gender, and in some ways class, I will now focus more closely upon the class impacts. As the practice of medicine moved away from female lay-healing and towards a male profession many working-class women were denied access to medical care. As it became a profession the working classes could no longer afford to receive treatment, so many went without (Donnison, 1993), and the medical profession focused its attention on upper- class women, claiming they were more fragile and in need of extra help, which further excluded working-class women from medical treatment. The few hospitals set up to provide for the working classes were poor quality, and the women were subjected to harsh treatments, which the upper classes who could afford to pay for superior treatment were not. They were treated as if they were dirty in some way, in a similar way to which prostitutes were handled under the Contagious Diseases Acts (Moscucci, 1990). Upper-class women were able to use chloroform during birth, but no such aids were available to those from the working classes (Donnison, 1993). Such differences in treatment helped to increase the division between the working and upper classes that I mentioned previously. Overall the rise of medicine as a profession had many implications for women. It excluded them from the practice, an occupation which had previously been female, a result which had many further implications for women in both gender and class terms. It resulted in the redefining of women, into inherently dangerous, yet fragile human beings, who were therefore dependant upon men, and the male medical profession, as well as having a negative affect on women patients, as many did not wish to see male doctors. It also removed one of the final areas of society from which women could gain status, ensuring that men now acquired this standing, as well as excluding working-class women from treatment as they could no longer afford it, increasing the divisions between women of different classes. Therefore the rise of medicine as a profession turned it into an area of male-dominance, reduced women's power and ensured that male control over women increased, as did the control of the upper classes over the working classes. The rise had a largely negative effect upon women, especially those from the working classes, and its legacy can still be seen in medicine today.",False
4,"Racism is still a problem within our society today, and many ethnic minorities face inequalities in many areas, including education, housing, and employment. Ethnic minorities are concentrated into certain areas of the job market, such as manufacture and communication (Brown, 1992), are most likely to be the victims of assault (Abercrombie et al, 1994), and recent surveys have shown that racist ideas still exist in society. This can be seen in a survey, carried out in 1993, that asked a white sample whether they agreed or disagreed with the statement: 'Immigration has enriched the quality of life in Britain' and nearly half of the sample disagreed (Abercrombie et al, 1994, p255). In this essay I will look at what racism is, and how it is defined in contemporary society, and I will then explore why it still persists. In this section I will cover three areas that I think have contributed to the continued existence of racism; culture, economy and politics, all of which have meant that racism is an integral and internalised part of our society, however I will not be looking at individual or specific cases of racism, rather exploring a broader explanation as to why it continues. Racism is a very hard concept to define. Hall argues that it is not a static phenomenon, but is constantly changing and redefining (Hall, 1978, cited in Miles, 1989, p64). In the nineteenth century racism was a product of scientific ideas, and the belief that people could be distinguished by the term race, using particular physical features, and that certain races were superior to others (Miles, 1989). However throughout the nineteenth century racism was rejected as a scientific concept (Banton, 1987, cited in Miles, 1989, p49). It is now important to realise that racism is not just an ideology but also a practice, and in contemporary society it has been defined as a process by which subordinated groups are excluded. This idea is expanded on by Carmichael and Hamilton who argue that it is; 'the predication of decisions and policies on considerations of race for the purpose of subordinating a racial group and maintaining control over that group' (Carmichael and Hamilton, 1968, cited in Miles, 1989, p51). This acknowledges that racism is not just a belief, but also an action, but in this essay I will concentrate on why the ideology of racism is still prevalent within society, as without the continuation of the ideology then racist actions would not take place. I will now look at why racism still persists in society, and the first explanation I will consider is culture. Racist beliefs are part of a legacy of both the slave trade and colonialism as racial categorisation played a large part in the justification of both periods. The idea that humankind was split into different races and that certain races, usually white British, were superior was strong at the time, and reinforced to justify slavery and imperialism. These ideas have left a strong legacy in coloniser societies today (Fenton, 1999). Europeans constructed themselves as the representatives of superior civilisation, and in doing so also constructed those from other countries as the 'Other', a group of people who were uncivilised, and therefore inferior to the West, an idea that was reinforced through literature, as many travellers at the time focused on the abnormal characteristics of the 'Other', and helped construct the idea that they were barbaric and uncivilised (Miles, 1989). Characteristics such as cannibalism and the degradation of women were picked out as features that made those from the colonised states racially inferior, and uncivilised, and reinforced the idea that a racialist hierarchy existed that placed those from the West at the top. By defining people from these countries as the 'Other' they were automatically excluded from the society of the West (Miles, 1989). Although this construction is no longer as prevalent in society, it has impacted on racism in society today. Images and ideas that were associated with the 'Other', for example the veil in Islam, are still around today, and the racial ideas that were leant to these images still exist. There is also the idea that White Europeans wished to continue their superiority over colonised societies, so sustained the ideas associated with the inferior 'Other' in order to maintain it (Fenton, 1999). It can also be argued that racism persists through culture due to the ideas of nationalism. Nationalism is a very popular discourse in contemporary society, as it was during the period of colonialisation, and the two are very closely linked, with ideas of nationalism often associated with both the ideology and actions of racism (Fenton, 1999). Overall colonialism has impacted on the racist ideas prevalent in today's society, as although the concept of 'Other' that existed in the nineteenth century no longer continues the definition of 'Other' has changed (Miles, 1989), and it has left a legacy of racism in contemporary society. However this is not the only reason for the continuation of racism, economic factors have also had an impact. The racism apparent at the time of colonialisation has been reinforced through the labour market. During this period the racial ideas were put into practice in the economy as those constructed as the 'Other' were exploited through the trade links established with the colonies, by those in power (Miles, 1989) showing that racialism has been imbedded into the economy since before capitalism. However since the emergence of capitalism these racist ideas have been used to support the capitalist market. Capitalists have ensured that ethnic minorities are viewed as suitable only for menial jobs, as they can easily exploit them, and this has reinforced the idea that race determines your skills and abilities, therefore causing racism to be imbedded within the capitalist market (Fenton, 1999). Castles and Kosack have argued that capitalism requires a 'reserve army of labour' to ensure the successful continuation of the economic production along capitalist values, and that ethnic minorities provide this army (Procter, Lecture notes, 2004). However by constructing ethnic minorities as a reserve army of labour racist beliefs are reinforced, as they are viewed as both inferior and only suitable for inferior jobs, but also as a threat to the white workers, creating a sense of racism in white workers. Therefore the racist beliefs apparent in the culture of colonialism are recreated in the current economy, ensuring that racism persists, although there are some problems with this idea as it ignores the white people employed in menial jobs and over emphasises the idea that ethnic minorities are needed as a 'reserve army of labour' (Miles, 1989). However politics, and the policies of British political parties over the twentieth century have also ensured that these racial ideas continue. Since the Second World War British politics reinforced have the idea that those from other races are inferior. In the 1950s both the Labour and Conservative Parties were openly racist (Brown, 1992), and they constructed immigrants as a social problem. Throughout the 1950s the debates on immigration focused on the idea that the number of black immigrants entering Britain needed to be restricted as they were posing a threat to British society. Immigrants were linked with social problems, including shortages of housing and crime, and through this connection racism was reinforced, as people began to see immigrants and people from other races as a problem. Emphasis was put on their undesirable behaviour, for example crime, and prostitution, despite the fact that much of this was not carried out by immigrants (Solomos, 1989, ensuring that people linked the idea of undesirable with immigrants. This construction of immigrants as a problem continued throughout out the 1960s, and in 1962 the Commonwealth Immigrants Act was introduced, which placed restrictions on the number of black immigrants entering the country (Solomos, 1989). By the 1970s legislation had been bought in which controlled the entry of non-white immigrants through a voucher system, giving the government control over who entered the country, and what work they could do following their entry (Brown, 1992). This political racism persisted after 1979, with Margaret Thatcher again emphasising the danger to British society that immigrants pose (Solomos, 1989), and these ideas can still be seen today, with the current clamp-down on the number of asylum seekers entering the country. Overall the policies of the government since the Second World War have made racism an integral part of our society, and ensured that non-whites are still viewed with distaste and seen as inferior. In conclusion racism, which can be defined as an ideology which categorises people as inferior according to their race, and is put into practice through many policies and actions, which seek to exclude non-whites from many areas of society, can still be seen in today's society. There are three reasons for this. Firstly the culture of colonialism and the belief that the 'Other' is inferior is still apparent in our society today. Secondly these beliefs are reinforced through the economy, as capitalism exploits ethnic minorities, and ensures that they are viewed as less skilled, and finally racism is embedded in our political system through the policies of political parties and the links they make between non-white immigrants and social problems. All of these processes work together in contemporary society to ensure that racism still continues, and non-whites are excluded.","Much more reproductive choice is now available to women, with the ability to chose whether or not to have children, when to have them and in what context, now a reproductive right given to women. This, combined with shifting social and economic opportunities for women, has lead to an increase in the number of childless women, with, in 1991, twice as many saying that they expected to remain childless as in 1986 (Gillespie, 2001). However the anticipated number of children per woman in Europe and the USA is still near or above two (Bongaarts, 1999), showing that many are still having children. In this essay I will explore why women have children, even though there is now more opportunity for them not to, and why those who do not have children do not do so. I will begin by looking at why women do have children, exploring the idea of push and pull in the decision, introduced by Morrell (1994), looking at factors identified as 'pulling' women into motherhood; psychological, social and economic factors, as well as looking at the influences of 'push' factors such as national discourses on motherhood and social policy. I will then examine why women do not have children or remain, as Gillespie describes, as voluntarily childless, (2001), considering the influence of personal, social and economic factors, and changing opportunities for women. In considering all of these influences on why women do or do not have children I will look at how they all work together to form a complex process, not just a simple decision. Morrell argues that women do not choose to have children only because it is something that they would like to experience and are attracted to, but also because they are almost forced into it by national discourses on motherhood and social policies, illustrated when she says; 'women are not only pulled to mothering, women are pushed in this direction in part by cultural representations of both motherhood and childlessness' (1994, p145). This means that a complex combination of personal choices and cultural influences combine in women's decisions to have children. Many personal reasons for women having children have been identified. The society in which we now live is disaggregated and lonely, and it is argued that women may want children to compensate for this (Andorka, 1978), as they are seen by many as a constant source of love and affection, and are usually totally dependant upon their parents. This idea is supported by Coldwell, who says; 'one's own children provide a unique pleasure which is not substitutable' (1982, cited in Schoen et al, 1997, p335), showing that many women chose to have children for the unique personal rewards that they carry. It is also argued that children are a source of self expansion, enabling women to feel they are moral and altruistic people for producing them (Hoffman and Hoffman, 1973, cited in Andorka, 1978, p338), and that they are a way of reproducing ones self and ensuring ones characteristics remain, an influence becoming perhaps more important as life after death is no longer considered a reality (Gittins, 1993). However as well as personal factors attracting women to motherhood, economic and social factors may also be influential. Lancaster has argued that children do provide some economic profit, as they can be a way of claiming higher benefits and gaining access to housing, (1965, cited in Andorka, 1978, p364). However it is now felt that economic factors do not influence the decision as much as in previous times, as women are no longer economically dependant upon men, and children are not needed to provide a source of income. Instead social factors have more of an input. Coleman claims that children are a form of 'social capital' (1988, cited in Schoen et al, 1997, p337), as they provide women with social benefits that they might not otherwise have, and this gain can influence the decision to have children. Having children can mean the development of social relationships that can be beneficial to women, and ensure higher standings within a social network (Schoen et al, 1997). Becoming a parent signals adult status has been reached, and this can result in acceptance into certain prestigious social groups, or the chance for social mobility (Hoffman and Hoffman, 1973, cited in Andorka, 1978, p338), for example some workplaces reward workers who have children as they are seen as mature and responsible. Although this is more applicable to male workers, it can mean that if a man is given promotion the whole family will be seen differently by society. It is also suggested that having children can produce better social relationships with kin networks, meaning more physical, emotional and financial support that may not otherwise have been achieved (Schoen et al, 1997). Having children can also provide women specifically with a higher status in society, as mothers are glorified (McDaniel, 1996), and many women may wish to be regarded highly by society. It is also argued that children can give women more power in the domestic sphere, as nurturing is seen as naturally linked to women, so they are given control over this area (Gittins, 1993), and therefore see it as a way of gaining more power within domestic relationships. Security is another form of social capital that can be created by having children. As well as being a source of security in the form of constant love, they can also provide social and financial security, especially in old age. Morrell found that many women feared old age, as women are now living longer than men (1994) and this fear can mean that women choose to have children to ensure their security, therefore making them an important social resource. These more individual and personal reasons that influence the choice of women to have children often combine in a complex way, however they are not the only factors that affect the decision. National discourses on motherhood and social policy have subtle influences that provide a background on which the more explicit influences of personal fulfilment and economic and social benefits are established. Patriarchal ideology prescribes particular behaviour for marriage and reproduction; the idea that marriage is based upon heterosexual relationships and that having children is the next natural step (Gittins, 1993), meaning that many women see getting married and having children as a natural progression through life, even in modern society. This is reinforced by strong historical discourses that link femininity with motherhood, claiming that 'true womanhood' is not reached unless women have children, or see having children as their biological and ideological goals (McDaniel, 1996). Culture dictates that it is natural for women to be mothers, and prescribes the number of children that is seen as respectable within our society (Schneider and Schneider, 1995, cited in Schoen et al, 1997, p335). Religious, political and scientific leaders stress that it is only through motherhood that women can be fulfilled, and Gillespie supports this, highlighting the way in which motherhood is seen as natural in modern society; 'the idea that motherhood is essentialist, deterministic, fixed, inevitably fulfilling and central to feminine identity is firmly entrenched in industrial, urban and rural societies' (2001, p141). This discourse, as well as placing motherhood as natural and essential for defining women, also ensures that women in society without children, both voluntarily and involuntarily, are seen as failed and not proper women. Culture also dictates that those who do not have children are selfish and immoral (Gillespie, 2001), and motherhood is seen as a signifier of normality (Morrell, 1994). Women feel that they should fit in with what is 'normal and natural', meaning that they chose to have children. These ideas are evident in social policy. As the nation relies on reproduction for its continuation, the state has much interest in levels of fertility and tries to encourage women to reproduce. This is done by reinforcing the discourses of motherhood as natural, and through more explicit social policy. This is evident in the USA where abortion and contraception are closely controlled by the state to ensure that women consider having children as important (McDaniel, 1996). These discourses in society influence the more personal factors that are involved on the decision to become a mother, as they promote the idea that children are a natural progression for women and their main source of happiness. Therefore many women feel that they should have children as it will make them happy, and see it as natural in order for them to fulfil their role in society, and gain status, as society attributes status to women through motherhood in these discourses. Consequently these cultural ideas running through society influence and combine with personal, economic and social factors to create a complex combination of reasons as to why women have children. However, despite these dominant discourses in society not all women have children. Although this may be down to infertility in some cases, I will be looking at the issue of voluntary childlessness, where women choose not to have children, rather than being unable to. As with the decision to have children, there are many personal, economic and social reasons why some women choose childlessness. Some women simply do not like children (Gillespie, 2000), or fear the effects that having a child will have upon their figure. Others may feel that due to crime and poverty the world now is not a suitable place to bring up children, or that the conditions of the town in which they live, for example lack of space, are not equitable to having children (Andorka, 1978). Children are also a significant emotional responsibility and some women may feel they do not wish to take this on, or may already have carried such a responsibility, either for siblings or parents and not want to endure it again (Gillespie, 2000). There is also the influence of economic factors to consider. Children are no longer economic assets, but are in fact very financially costly, and many women may decide they cannot afford to have children (Schoen et al, 1997). Fears about economic security have lead to more women deciding that children are not a viable option, and that the financial burden is too high a price to pay. Aspirations for certain standards of living may also influence the decision as some will feel that the cost of children would not fit in with the lifestyle they wish to have (Andorka, 1978). This links quite closely to social factors, as a desired lifestyle can be quite important when considering whether or not to have children. Opportunities other than motherhood are now open to women, especially in economic structures, and some women choose to explore these opportunities, rather than experience motherhood. Women's increasing participation in employment means that they are able to develop careers of their own, and many feel that they gain their identity from paid work and do not want to lose this by having children (Gillespie, 2001). Women now wish to have interests separate from the domestic sphere, and some may believe that having children places restrictions on the possibilities for this sort of lifestyle. Morrell describes it as a desire for 'freedom from and freedom to', where women wish to have freedom from the ties, both financially and emotionally, of being mothers, but also want the freedom to experience a different lifestyle, with flexibility and no guilt. She also found that many did not like the context under which motherhood was supposed to take place, feeling that it would enforce a break from paid employment, therefore economic dependence, and confinement to the home ensuring a sexual division of labour (1994). These desires for a lifestyle that does not involve children have been made possible by small changes in employment structures, which mean that women can now have a career, and the emergence of discourses, albeit not mainstream ones which allow women greater sexual freedom. Gillespie describes this as 'the post-modern model of reproduction', in which social forces have impacted upon motherhood, meaning that that it is becoming disaggregated with the natural (2001), so motherhood is no longer seen as synonymous with femininity. This shows that as well as there being personal, economic and social factors influencing the decision not to have children, changes in employment structures and new discourses emerging on motherhood and reproduction mean that these factors are possible in modern society. Therefore it is a complex combination of social ideals and personal responses that results in women becoming voluntarily childless. Overall this shows that there are many complex influences upon the decision of whether or not have children. In the decision to have children discourses on motherhood and reproduction create the ideal that motherhood is both natural and fulfilling for women, and this impacts upon personal, economic and social factors, including security, happiness and social capital, that lead to women choosing to have children. The same idea can be seen in the decision not to have children, as changing opportunities for women, and emerging discourses are making it possible for women to decide that for personal, economic and social reasons, for example lifestyle choices, they wish to remain childless. In conclusion why women do or do not have children is a complex process influenced by many factors, and based upon a variety of discourses and opportunities ingrained within society, not simply whether or not a woman likes children.",True
5,"Much more reproductive choice is now available to women, with the ability to chose whether or not to have children, when to have them and in what context, now a reproductive right given to women. This, combined with shifting social and economic opportunities for women, has lead to an increase in the number of childless women, with, in 1991, twice as many saying that they expected to remain childless as in 1986 (Gillespie, 2001). However the anticipated number of children per woman in Europe and the USA is still near or above two (Bongaarts, 1999), showing that many are still having children. In this essay I will explore why women have children, even though there is now more opportunity for them not to, and why those who do not have children do not do so. I will begin by looking at why women do have children, exploring the idea of push and pull in the decision, introduced by Morrell (1994), looking at factors identified as 'pulling' women into motherhood; psychological, social and economic factors, as well as looking at the influences of 'push' factors such as national discourses on motherhood and social policy. I will then examine why women do not have children or remain, as Gillespie describes, as voluntarily childless, (2001), considering the influence of personal, social and economic factors, and changing opportunities for women. In considering all of these influences on why women do or do not have children I will look at how they all work together to form a complex process, not just a simple decision. Morrell argues that women do not choose to have children only because it is something that they would like to experience and are attracted to, but also because they are almost forced into it by national discourses on motherhood and social policies, illustrated when she says; 'women are not only pulled to mothering, women are pushed in this direction in part by cultural representations of both motherhood and childlessness' (1994, p145). This means that a complex combination of personal choices and cultural influences combine in women's decisions to have children. Many personal reasons for women having children have been identified. The society in which we now live is disaggregated and lonely, and it is argued that women may want children to compensate for this (Andorka, 1978), as they are seen by many as a constant source of love and affection, and are usually totally dependant upon their parents. This idea is supported by Coldwell, who says; 'one's own children provide a unique pleasure which is not substitutable' (1982, cited in Schoen et al, 1997, p335), showing that many women chose to have children for the unique personal rewards that they carry. It is also argued that children are a source of self expansion, enabling women to feel they are moral and altruistic people for producing them (Hoffman and Hoffman, 1973, cited in Andorka, 1978, p338), and that they are a way of reproducing ones self and ensuring ones characteristics remain, an influence becoming perhaps more important as life after death is no longer considered a reality (Gittins, 1993). However as well as personal factors attracting women to motherhood, economic and social factors may also be influential. Lancaster has argued that children do provide some economic profit, as they can be a way of claiming higher benefits and gaining access to housing, (1965, cited in Andorka, 1978, p364). However it is now felt that economic factors do not influence the decision as much as in previous times, as women are no longer economically dependant upon men, and children are not needed to provide a source of income. Instead social factors have more of an input. Coleman claims that children are a form of 'social capital' (1988, cited in Schoen et al, 1997, p337), as they provide women with social benefits that they might not otherwise have, and this gain can influence the decision to have children. Having children can mean the development of social relationships that can be beneficial to women, and ensure higher standings within a social network (Schoen et al, 1997). Becoming a parent signals adult status has been reached, and this can result in acceptance into certain prestigious social groups, or the chance for social mobility (Hoffman and Hoffman, 1973, cited in Andorka, 1978, p338), for example some workplaces reward workers who have children as they are seen as mature and responsible. Although this is more applicable to male workers, it can mean that if a man is given promotion the whole family will be seen differently by society. It is also suggested that having children can produce better social relationships with kin networks, meaning more physical, emotional and financial support that may not otherwise have been achieved (Schoen et al, 1997). Having children can also provide women specifically with a higher status in society, as mothers are glorified (McDaniel, 1996), and many women may wish to be regarded highly by society. It is also argued that children can give women more power in the domestic sphere, as nurturing is seen as naturally linked to women, so they are given control over this area (Gittins, 1993), and therefore see it as a way of gaining more power within domestic relationships. Security is another form of social capital that can be created by having children. As well as being a source of security in the form of constant love, they can also provide social and financial security, especially in old age. Morrell found that many women feared old age, as women are now living longer than men (1994) and this fear can mean that women choose to have children to ensure their security, therefore making them an important social resource. These more individual and personal reasons that influence the choice of women to have children often combine in a complex way, however they are not the only factors that affect the decision. National discourses on motherhood and social policy have subtle influences that provide a background on which the more explicit influences of personal fulfilment and economic and social benefits are established. Patriarchal ideology prescribes particular behaviour for marriage and reproduction; the idea that marriage is based upon heterosexual relationships and that having children is the next natural step (Gittins, 1993), meaning that many women see getting married and having children as a natural progression through life, even in modern society. This is reinforced by strong historical discourses that link femininity with motherhood, claiming that 'true womanhood' is not reached unless women have children, or see having children as their biological and ideological goals (McDaniel, 1996). Culture dictates that it is natural for women to be mothers, and prescribes the number of children that is seen as respectable within our society (Schneider and Schneider, 1995, cited in Schoen et al, 1997, p335). Religious, political and scientific leaders stress that it is only through motherhood that women can be fulfilled, and Gillespie supports this, highlighting the way in which motherhood is seen as natural in modern society; 'the idea that motherhood is essentialist, deterministic, fixed, inevitably fulfilling and central to feminine identity is firmly entrenched in industrial, urban and rural societies' (2001, p141). This discourse, as well as placing motherhood as natural and essential for defining women, also ensures that women in society without children, both voluntarily and involuntarily, are seen as failed and not proper women. Culture also dictates that those who do not have children are selfish and immoral (Gillespie, 2001), and motherhood is seen as a signifier of normality (Morrell, 1994). Women feel that they should fit in with what is 'normal and natural', meaning that they chose to have children. These ideas are evident in social policy. As the nation relies on reproduction for its continuation, the state has much interest in levels of fertility and tries to encourage women to reproduce. This is done by reinforcing the discourses of motherhood as natural, and through more explicit social policy. This is evident in the USA where abortion and contraception are closely controlled by the state to ensure that women consider having children as important (McDaniel, 1996). These discourses in society influence the more personal factors that are involved on the decision to become a mother, as they promote the idea that children are a natural progression for women and their main source of happiness. Therefore many women feel that they should have children as it will make them happy, and see it as natural in order for them to fulfil their role in society, and gain status, as society attributes status to women through motherhood in these discourses. Consequently these cultural ideas running through society influence and combine with personal, economic and social factors to create a complex combination of reasons as to why women have children. However, despite these dominant discourses in society not all women have children. Although this may be down to infertility in some cases, I will be looking at the issue of voluntary childlessness, where women choose not to have children, rather than being unable to. As with the decision to have children, there are many personal, economic and social reasons why some women choose childlessness. Some women simply do not like children (Gillespie, 2000), or fear the effects that having a child will have upon their figure. Others may feel that due to crime and poverty the world now is not a suitable place to bring up children, or that the conditions of the town in which they live, for example lack of space, are not equitable to having children (Andorka, 1978). Children are also a significant emotional responsibility and some women may feel they do not wish to take this on, or may already have carried such a responsibility, either for siblings or parents and not want to endure it again (Gillespie, 2000). There is also the influence of economic factors to consider. Children are no longer economic assets, but are in fact very financially costly, and many women may decide they cannot afford to have children (Schoen et al, 1997). Fears about economic security have lead to more women deciding that children are not a viable option, and that the financial burden is too high a price to pay. Aspirations for certain standards of living may also influence the decision as some will feel that the cost of children would not fit in with the lifestyle they wish to have (Andorka, 1978). This links quite closely to social factors, as a desired lifestyle can be quite important when considering whether or not to have children. Opportunities other than motherhood are now open to women, especially in economic structures, and some women choose to explore these opportunities, rather than experience motherhood. Women's increasing participation in employment means that they are able to develop careers of their own, and many feel that they gain their identity from paid work and do not want to lose this by having children (Gillespie, 2001). Women now wish to have interests separate from the domestic sphere, and some may believe that having children places restrictions on the possibilities for this sort of lifestyle. Morrell describes it as a desire for 'freedom from and freedom to', where women wish to have freedom from the ties, both financially and emotionally, of being mothers, but also want the freedom to experience a different lifestyle, with flexibility and no guilt. She also found that many did not like the context under which motherhood was supposed to take place, feeling that it would enforce a break from paid employment, therefore economic dependence, and confinement to the home ensuring a sexual division of labour (1994). These desires for a lifestyle that does not involve children have been made possible by small changes in employment structures, which mean that women can now have a career, and the emergence of discourses, albeit not mainstream ones which allow women greater sexual freedom. Gillespie describes this as 'the post-modern model of reproduction', in which social forces have impacted upon motherhood, meaning that that it is becoming disaggregated with the natural (2001), so motherhood is no longer seen as synonymous with femininity. This shows that as well as there being personal, economic and social factors influencing the decision not to have children, changes in employment structures and new discourses emerging on motherhood and reproduction mean that these factors are possible in modern society. Therefore it is a complex combination of social ideals and personal responses that results in women becoming voluntarily childless. Overall this shows that there are many complex influences upon the decision of whether or not have children. In the decision to have children discourses on motherhood and reproduction create the ideal that motherhood is both natural and fulfilling for women, and this impacts upon personal, economic and social factors, including security, happiness and social capital, that lead to women choosing to have children. The same idea can be seen in the decision not to have children, as changing opportunities for women, and emerging discourses are making it possible for women to decide that for personal, economic and social reasons, for example lifestyle choices, they wish to remain childless. In conclusion why women do or do not have children is a complex process influenced by many factors, and based upon a variety of discourses and opportunities ingrained within society, not simply whether or not a woman likes children.","Racism is still a problem within our society today, and many ethnic minorities face inequalities in many areas, including education, housing, and employment. Ethnic minorities are concentrated into certain areas of the job market, such as manufacture and communication (Brown, 1992), are most likely to be the victims of assault (Abercrombie et al, 1994), and recent surveys have shown that racist ideas still exist in society. This can be seen in a survey, carried out in 1993, that asked a white sample whether they agreed or disagreed with the statement: 'Immigration has enriched the quality of life in Britain' and nearly half of the sample disagreed (Abercrombie et al, 1994, p255). In this essay I will look at what racism is, and how it is defined in contemporary society, and I will then explore why it still persists. In this section I will cover three areas that I think have contributed to the continued existence of racism; culture, economy and politics, all of which have meant that racism is an integral and internalised part of our society, however I will not be looking at individual or specific cases of racism, rather exploring a broader explanation as to why it continues. Racism is a very hard concept to define. Hall argues that it is not a static phenomenon, but is constantly changing and redefining (Hall, 1978, cited in Miles, 1989, p64). In the nineteenth century racism was a product of scientific ideas, and the belief that people could be distinguished by the term race, using particular physical features, and that certain races were superior to others (Miles, 1989). However throughout the nineteenth century racism was rejected as a scientific concept (Banton, 1987, cited in Miles, 1989, p49). It is now important to realise that racism is not just an ideology but also a practice, and in contemporary society it has been defined as a process by which subordinated groups are excluded. This idea is expanded on by Carmichael and Hamilton who argue that it is; 'the predication of decisions and policies on considerations of race for the purpose of subordinating a racial group and maintaining control over that group' (Carmichael and Hamilton, 1968, cited in Miles, 1989, p51). This acknowledges that racism is not just a belief, but also an action, but in this essay I will concentrate on why the ideology of racism is still prevalent within society, as without the continuation of the ideology then racist actions would not take place. I will now look at why racism still persists in society, and the first explanation I will consider is culture. Racist beliefs are part of a legacy of both the slave trade and colonialism as racial categorisation played a large part in the justification of both periods. The idea that humankind was split into different races and that certain races, usually white British, were superior was strong at the time, and reinforced to justify slavery and imperialism. These ideas have left a strong legacy in coloniser societies today (Fenton, 1999). Europeans constructed themselves as the representatives of superior civilisation, and in doing so also constructed those from other countries as the 'Other', a group of people who were uncivilised, and therefore inferior to the West, an idea that was reinforced through literature, as many travellers at the time focused on the abnormal characteristics of the 'Other', and helped construct the idea that they were barbaric and uncivilised (Miles, 1989). Characteristics such as cannibalism and the degradation of women were picked out as features that made those from the colonised states racially inferior, and uncivilised, and reinforced the idea that a racialist hierarchy existed that placed those from the West at the top. By defining people from these countries as the 'Other' they were automatically excluded from the society of the West (Miles, 1989). Although this construction is no longer as prevalent in society, it has impacted on racism in society today. Images and ideas that were associated with the 'Other', for example the veil in Islam, are still around today, and the racial ideas that were leant to these images still exist. There is also the idea that White Europeans wished to continue their superiority over colonised societies, so sustained the ideas associated with the inferior 'Other' in order to maintain it (Fenton, 1999). It can also be argued that racism persists through culture due to the ideas of nationalism. Nationalism is a very popular discourse in contemporary society, as it was during the period of colonialisation, and the two are very closely linked, with ideas of nationalism often associated with both the ideology and actions of racism (Fenton, 1999). Overall colonialism has impacted on the racist ideas prevalent in today's society, as although the concept of 'Other' that existed in the nineteenth century no longer continues the definition of 'Other' has changed (Miles, 1989), and it has left a legacy of racism in contemporary society. However this is not the only reason for the continuation of racism, economic factors have also had an impact. The racism apparent at the time of colonialisation has been reinforced through the labour market. During this period the racial ideas were put into practice in the economy as those constructed as the 'Other' were exploited through the trade links established with the colonies, by those in power (Miles, 1989) showing that racialism has been imbedded into the economy since before capitalism. However since the emergence of capitalism these racist ideas have been used to support the capitalist market. Capitalists have ensured that ethnic minorities are viewed as suitable only for menial jobs, as they can easily exploit them, and this has reinforced the idea that race determines your skills and abilities, therefore causing racism to be imbedded within the capitalist market (Fenton, 1999). Castles and Kosack have argued that capitalism requires a 'reserve army of labour' to ensure the successful continuation of the economic production along capitalist values, and that ethnic minorities provide this army (Procter, Lecture notes, 2004). However by constructing ethnic minorities as a reserve army of labour racist beliefs are reinforced, as they are viewed as both inferior and only suitable for inferior jobs, but also as a threat to the white workers, creating a sense of racism in white workers. Therefore the racist beliefs apparent in the culture of colonialism are recreated in the current economy, ensuring that racism persists, although there are some problems with this idea as it ignores the white people employed in menial jobs and over emphasises the idea that ethnic minorities are needed as a 'reserve army of labour' (Miles, 1989). However politics, and the policies of British political parties over the twentieth century have also ensured that these racial ideas continue. Since the Second World War British politics reinforced have the idea that those from other races are inferior. In the 1950s both the Labour and Conservative Parties were openly racist (Brown, 1992), and they constructed immigrants as a social problem. Throughout the 1950s the debates on immigration focused on the idea that the number of black immigrants entering Britain needed to be restricted as they were posing a threat to British society. Immigrants were linked with social problems, including shortages of housing and crime, and through this connection racism was reinforced, as people began to see immigrants and people from other races as a problem. Emphasis was put on their undesirable behaviour, for example crime, and prostitution, despite the fact that much of this was not carried out by immigrants (Solomos, 1989, ensuring that people linked the idea of undesirable with immigrants. This construction of immigrants as a problem continued throughout out the 1960s, and in 1962 the Commonwealth Immigrants Act was introduced, which placed restrictions on the number of black immigrants entering the country (Solomos, 1989). By the 1970s legislation had been bought in which controlled the entry of non-white immigrants through a voucher system, giving the government control over who entered the country, and what work they could do following their entry (Brown, 1992). This political racism persisted after 1979, with Margaret Thatcher again emphasising the danger to British society that immigrants pose (Solomos, 1989), and these ideas can still be seen today, with the current clamp-down on the number of asylum seekers entering the country. Overall the policies of the government since the Second World War have made racism an integral part of our society, and ensured that non-whites are still viewed with distaste and seen as inferior. In conclusion racism, which can be defined as an ideology which categorises people as inferior according to their race, and is put into practice through many policies and actions, which seek to exclude non-whites from many areas of society, can still be seen in today's society. There are three reasons for this. Firstly the culture of colonialism and the belief that the 'Other' is inferior is still apparent in our society today. Secondly these beliefs are reinforced through the economy, as capitalism exploits ethnic minorities, and ensures that they are viewed as less skilled, and finally racism is embedded in our political system through the policies of political parties and the links they make between non-white immigrants and social problems. All of these processes work together in contemporary society to ensure that racism still continues, and non-whites are excluded.",False
6,"Much more reproductive choice is now available to women, with the ability to chose whether or not to have children, when to have them and in what context, now a reproductive right given to women. This, combined with shifting social and economic opportunities for women, has lead to an increase in the number of childless women, with, in 1991, twice as many saying that they expected to remain childless as in 1986 (Gillespie, 2001). However the anticipated number of children per woman in Europe and the USA is still near or above two (Bongaarts, 1999), showing that many are still having children. In this essay I will explore why women have children, even though there is now more opportunity for them not to, and why those who do not have children do not do so. I will begin by looking at why women do have children, exploring the idea of push and pull in the decision, introduced by Morrell (1994), looking at factors identified as 'pulling' women into motherhood; psychological, social and economic factors, as well as looking at the influences of 'push' factors such as national discourses on motherhood and social policy. I will then examine why women do not have children or remain, as Gillespie describes, as voluntarily childless, (2001), considering the influence of personal, social and economic factors, and changing opportunities for women. In considering all of these influences on why women do or do not have children I will look at how they all work together to form a complex process, not just a simple decision. Morrell argues that women do not choose to have children only because it is something that they would like to experience and are attracted to, but also because they are almost forced into it by national discourses on motherhood and social policies, illustrated when she says; 'women are not only pulled to mothering, women are pushed in this direction in part by cultural representations of both motherhood and childlessness' (1994, p145). This means that a complex combination of personal choices and cultural influences combine in women's decisions to have children. Many personal reasons for women having children have been identified. The society in which we now live is disaggregated and lonely, and it is argued that women may want children to compensate for this (Andorka, 1978), as they are seen by many as a constant source of love and affection, and are usually totally dependant upon their parents. This idea is supported by Coldwell, who says; 'one's own children provide a unique pleasure which is not substitutable' (1982, cited in Schoen et al, 1997, p335), showing that many women chose to have children for the unique personal rewards that they carry. It is also argued that children are a source of self expansion, enabling women to feel they are moral and altruistic people for producing them (Hoffman and Hoffman, 1973, cited in Andorka, 1978, p338), and that they are a way of reproducing ones self and ensuring ones characteristics remain, an influence becoming perhaps more important as life after death is no longer considered a reality (Gittins, 1993). However as well as personal factors attracting women to motherhood, economic and social factors may also be influential. Lancaster has argued that children do provide some economic profit, as they can be a way of claiming higher benefits and gaining access to housing, (1965, cited in Andorka, 1978, p364). However it is now felt that economic factors do not influence the decision as much as in previous times, as women are no longer economically dependant upon men, and children are not needed to provide a source of income. Instead social factors have more of an input. Coleman claims that children are a form of 'social capital' (1988, cited in Schoen et al, 1997, p337), as they provide women with social benefits that they might not otherwise have, and this gain can influence the decision to have children. Having children can mean the development of social relationships that can be beneficial to women, and ensure higher standings within a social network (Schoen et al, 1997). Becoming a parent signals adult status has been reached, and this can result in acceptance into certain prestigious social groups, or the chance for social mobility (Hoffman and Hoffman, 1973, cited in Andorka, 1978, p338), for example some workplaces reward workers who have children as they are seen as mature and responsible. Although this is more applicable to male workers, it can mean that if a man is given promotion the whole family will be seen differently by society. It is also suggested that having children can produce better social relationships with kin networks, meaning more physical, emotional and financial support that may not otherwise have been achieved (Schoen et al, 1997). Having children can also provide women specifically with a higher status in society, as mothers are glorified (McDaniel, 1996), and many women may wish to be regarded highly by society. It is also argued that children can give women more power in the domestic sphere, as nurturing is seen as naturally linked to women, so they are given control over this area (Gittins, 1993), and therefore see it as a way of gaining more power within domestic relationships. Security is another form of social capital that can be created by having children. As well as being a source of security in the form of constant love, they can also provide social and financial security, especially in old age. Morrell found that many women feared old age, as women are now living longer than men (1994) and this fear can mean that women choose to have children to ensure their security, therefore making them an important social resource. These more individual and personal reasons that influence the choice of women to have children often combine in a complex way, however they are not the only factors that affect the decision. National discourses on motherhood and social policy have subtle influences that provide a background on which the more explicit influences of personal fulfilment and economic and social benefits are established. Patriarchal ideology prescribes particular behaviour for marriage and reproduction; the idea that marriage is based upon heterosexual relationships and that having children is the next natural step (Gittins, 1993), meaning that many women see getting married and having children as a natural progression through life, even in modern society. This is reinforced by strong historical discourses that link femininity with motherhood, claiming that 'true womanhood' is not reached unless women have children, or see having children as their biological and ideological goals (McDaniel, 1996). Culture dictates that it is natural for women to be mothers, and prescribes the number of children that is seen as respectable within our society (Schneider and Schneider, 1995, cited in Schoen et al, 1997, p335). Religious, political and scientific leaders stress that it is only through motherhood that women can be fulfilled, and Gillespie supports this, highlighting the way in which motherhood is seen as natural in modern society; 'the idea that motherhood is essentialist, deterministic, fixed, inevitably fulfilling and central to feminine identity is firmly entrenched in industrial, urban and rural societies' (2001, p141). This discourse, as well as placing motherhood as natural and essential for defining women, also ensures that women in society without children, both voluntarily and involuntarily, are seen as failed and not proper women. Culture also dictates that those who do not have children are selfish and immoral (Gillespie, 2001), and motherhood is seen as a signifier of normality (Morrell, 1994). Women feel that they should fit in with what is 'normal and natural', meaning that they chose to have children. These ideas are evident in social policy. As the nation relies on reproduction for its continuation, the state has much interest in levels of fertility and tries to encourage women to reproduce. This is done by reinforcing the discourses of motherhood as natural, and through more explicit social policy. This is evident in the USA where abortion and contraception are closely controlled by the state to ensure that women consider having children as important (McDaniel, 1996). These discourses in society influence the more personal factors that are involved on the decision to become a mother, as they promote the idea that children are a natural progression for women and their main source of happiness. Therefore many women feel that they should have children as it will make them happy, and see it as natural in order for them to fulfil their role in society, and gain status, as society attributes status to women through motherhood in these discourses. Consequently these cultural ideas running through society influence and combine with personal, economic and social factors to create a complex combination of reasons as to why women have children. However, despite these dominant discourses in society not all women have children. Although this may be down to infertility in some cases, I will be looking at the issue of voluntary childlessness, where women choose not to have children, rather than being unable to. As with the decision to have children, there are many personal, economic and social reasons why some women choose childlessness. Some women simply do not like children (Gillespie, 2000), or fear the effects that having a child will have upon their figure. Others may feel that due to crime and poverty the world now is not a suitable place to bring up children, or that the conditions of the town in which they live, for example lack of space, are not equitable to having children (Andorka, 1978). Children are also a significant emotional responsibility and some women may feel they do not wish to take this on, or may already have carried such a responsibility, either for siblings or parents and not want to endure it again (Gillespie, 2000). There is also the influence of economic factors to consider. Children are no longer economic assets, but are in fact very financially costly, and many women may decide they cannot afford to have children (Schoen et al, 1997). Fears about economic security have lead to more women deciding that children are not a viable option, and that the financial burden is too high a price to pay. Aspirations for certain standards of living may also influence the decision as some will feel that the cost of children would not fit in with the lifestyle they wish to have (Andorka, 1978). This links quite closely to social factors, as a desired lifestyle can be quite important when considering whether or not to have children. Opportunities other than motherhood are now open to women, especially in economic structures, and some women choose to explore these opportunities, rather than experience motherhood. Women's increasing participation in employment means that they are able to develop careers of their own, and many feel that they gain their identity from paid work and do not want to lose this by having children (Gillespie, 2001). Women now wish to have interests separate from the domestic sphere, and some may believe that having children places restrictions on the possibilities for this sort of lifestyle. Morrell describes it as a desire for 'freedom from and freedom to', where women wish to have freedom from the ties, both financially and emotionally, of being mothers, but also want the freedom to experience a different lifestyle, with flexibility and no guilt. She also found that many did not like the context under which motherhood was supposed to take place, feeling that it would enforce a break from paid employment, therefore economic dependence, and confinement to the home ensuring a sexual division of labour (1994). These desires for a lifestyle that does not involve children have been made possible by small changes in employment structures, which mean that women can now have a career, and the emergence of discourses, albeit not mainstream ones which allow women greater sexual freedom. Gillespie describes this as 'the post-modern model of reproduction', in which social forces have impacted upon motherhood, meaning that that it is becoming disaggregated with the natural (2001), so motherhood is no longer seen as synonymous with femininity. This shows that as well as there being personal, economic and social factors influencing the decision not to have children, changes in employment structures and new discourses emerging on motherhood and reproduction mean that these factors are possible in modern society. Therefore it is a complex combination of social ideals and personal responses that results in women becoming voluntarily childless. Overall this shows that there are many complex influences upon the decision of whether or not have children. In the decision to have children discourses on motherhood and reproduction create the ideal that motherhood is both natural and fulfilling for women, and this impacts upon personal, economic and social factors, including security, happiness and social capital, that lead to women choosing to have children. The same idea can be seen in the decision not to have children, as changing opportunities for women, and emerging discourses are making it possible for women to decide that for personal, economic and social reasons, for example lifestyle choices, they wish to remain childless. In conclusion why women do or do not have children is a complex process influenced by many factors, and based upon a variety of discourses and opportunities ingrained within society, not simply whether or not a woman likes children.","In the twenty five years following the Second World War the government was fully committed to full employment, however in the 1970s unemployment began to rise drastically (Vickerstaff, 2003). The number of unemployed rose from 1.3 million in 1979 to 3 million in 1982 (Jones and Novak, 1999, p39, Glynn, 1999, p190), and as Burnett notes these figures remained high throughout the 1980s and 1990s (1994). However although unemployment has always existed in periods of history before, this increase was significant, as new patterns emerged, with rising youth and ethnic minority unemployment, and unemployment concentrated in certain areas of the country (Ashton and Maguire, 1991). In this essay I will briefly outline why mass unemployment emerged at this time, including factors of global changes, technological advances and government policies. I will then move on to examine which groups were most at risk from these changes, looking closely at the young, those in industrial towns, men, especially the unskilled, and finally ethnic minorities, and exploring the reasons why they were so vulnerable. I will not look at several other groups who were also vulnerable; those with disabilities, few qualifications and the elderly, as they had been at risk from unemployment before, and it was not the situation in the 1970s that caused this risk, as it was with the groups I will look at. There were many reasons as to why unemployment grew in the 1980s and 1990s. The neo-liberal and social democratic approaches have opposing views, with the neo-liberals arguing that unemployment was down to individual attitudes, and that deregulating markets would solve this problem, whilst social democratic approaches argued that it was a structural problem, that regulated markets and welfare would lessen (Vickerstaff, 2003). However there were many more complicated factors involved in the steep decline during this period. One of the main reasons was global changes. Firstly the 1973 oil crisis lead to a decline in the global economy, and the British economy, causing unemployment to rise (Burnett, 1994), especially as this resulted in a shift of trade against Britain, as she was not an oil producer (Glynn, 1999). Secondly, throughout the recession global markets began to spread, meaning that British exports began to decrease, and more low cost products were imported from abroad, so many companies went bust (Ashton and Maguire, 1991). Also those companies who did not began to change their labour market strategy, either moving production to other countries, where labour was cheaper, or making redundancies and moving towards a more flexible workforce (Glynn, 1999). All of these changes in the global market had a dramatic effect on the labour market within Britain, but also to industry as a whole, as it caused a massive decline in the manufacturing industry (Ashton and Maguire, 1991), which fell in production by 15.75% between 1979 and 1983 (Glynn, 1999, p270). As manufacturing was a major employer in Britain at the time, inevitably unemployment rose, as although the service sector grew as manufacturing declined, it was not to the same extent, and fewer jobs were created. These global changes were also further compounded by increases in technology, and government policies. Technology displaced or made more flexible many jobs in the parts of the manufacturing industry that survived (Burnett, 1994) and meant that fewer workers were needed in the growing service sector. It also meant that many unskilled and semi skilled jobs disappeared from the market, as technology could now be used (Mizen, 2003). Ashton and Maguire argue that government policy made this problem worse however, as it weakened trade union power and made these changes easier for employers to introduce (1991), and Professor Laynard agrees, claiming that rising inflation, fiscal monetary policies and a lack of commitment to full employment caused dramatic increases in unemployment (Laynard, cited in Burnett, 1994, p268). One of the groups most affected by these changes was the young, and Makeham claims that they are now especially vulnerable to general trends in employment (1980, cited in Coles, 1995, p 32). In 1960 the unemployment rate for the under 25s was only 2.4%, however by 1979 it had increased to 12.2% and by 1981 it had risen further to 21.4% (Coles, 1995, p32). This trend is one that still continues today, and in 1996, half a million 19-24 year olds were classified as unemployed (Jones and Novak, 1999, p64). These statistics can further be placed in context by looking at the fact that although general unemployment was rising at the time, which could simply be reflected in these figures, it only rose by 45%, whereas youth unemployment rose by 120% (Coles, 1995, p32). There are many reasons as to why the young are so vulnerable to these changes. It is argued that they are especially vulnerable to the business cycle anyway, as in times of recession businesses cut their recruitment and training and inflate the experience and qualifications needed for financial reasons, as do Trade Unions, who aim to protect their existing members, meaning that the young have very little opportunity to enter into the work force. Also due to statutory redundancy payments it is cheaper to sack those who have been in work for the shortest period, and this usually applied to the young (Mizen, 2003). However, although this does make the young more vulnerable, the reasons why they became one of the highest groups in unemployment in the 1980s and 1990s has to be placed in context. Firstly the global changes and resulting decline in the manufacturing industry impacted significantly upon them, as it was previously a large source of youth employment, with one third of school leavers entering the industry (Mizen, 2003). This loss should have been compensated by the growth in the service sector, however many employers preferred adult female workers, and began to reject the young, reflecting the fact that Maguire claims, employers often view the young as irresponsible and lacking training and experience (1992), therefore making them more at risk in times of high unemployment. Technological advances also impacted as many of the jobs they displaced were in the manufacturing sector, which was a major source of youth employment, but also because it meant that fewer apprenticeships were offered (Coles, 1995), so the young could not get the training or the experience needed to enter the labour market initially. Burnett supports the idea that it meant that the young lacked the higher criteria requested by employers due to economic decline, but also points out that at the time adult wages were changed to apply to 18 year olds as well, not just older workers (1994), so it was now no cheaper for employers to hire young people. As well as being especially vulnerable to these changes in the labour market structure, the risk posed to the young was further increased by government policies. They blamed youth unemployment on what is known as the 'deficiency model' (Mizen, 2003), which argued that the reason the young were out of work was because they were restless in work, had no incentive to be there, or did not have the qualities needed for employment. This meant they received little support from the government to protect them from the dangers of unemployment. Therefore overall the recent changes in the labour market, technological advances and government policies have made the young a group particularly at risk of unemployment. Another group at risk to high unemployment were those living in traditional industrial towns, especially in the North and West. Statistics show that whilst the rate of official unemployment was only 8.4% in the South East, in the North West it was 14.7%, and in the North 16.6%, indicating that industrial areas were at a higher risk of unemployment (Jones and Novak, 1999, p41). Within these areas places such as Yorkshire, Scotland and Humberside were the worst affected, and Ashton et al found that a young man's chance of being unemployed in Sunderland was one in three, compared to only one in thirty three in St Albans (1988, cited in Coles, 1995, p39). Pissarides and Wadsworth note that this is largely due to the occupational distribution of employment (1992). These areas were where manufacturing production was concentrated, and when this went into decline due to global changes, these areas lost their main source of jobs. They also did not experience the benefits bought about by the increase in the service sector, as the growth of business was concentrated mostly in the South East, growing by 20% per year in these areas, compared to only 7.2% in the North West (McLaughlin, 1992, p14). Burnett supports this argument, saying that the structural changes that have occurred in the labour market, most notably the change from manufacturing production to service sector domination, have led to high job losses in certain areas, especially in industrial towns (1994). Technological advances have further increased the risk of unemployment in these areas as they have meant that many of the remaining jobs have become flexible (Vickerstaff, 2003). The people living within these areas were made even more vulnerable to these risks, as they become largely cut off from other cities where the service sector is now dominating, meaning they were excluded from job information networks and fell into a downward spiral ending in a culture of unemployment (Ashton and Maguire, 1991). These reasons have all combined to ensure that over the past thirty years people living in these industrial areas have been more affected by unemployment. Men, especially the unskilled, are another group at risk from unemployment, and who suffered during the mass unemployment of the 1980s and 1990s. Unemployment for men over 25 has increased four fold since the 1970s (Faggio and Nickell, 2003, p 31), with 2.3 million fewer men in work now than twenty years ago (Jones and Novak, 1999, p40). There are several reasons for this decline in male employment. Firstly, they were the main workers in the manufacturing industry, so when this declined their levels of unemployment increased (Ashton and Maguire, 1991). Although the service sector grew to replace manufacturing as a source of employment, women were favoured by employers due to their characteristics, and the way they were perceived as more caring and understanding, and also reliable, especially if they were married (Maguire, 1992). This idea is demonstrated clearly by Ashton and Maguire when they say; 'the new service sector jobs have provided low skilled, part-time and often casual employment, primarily for females' (1991, p53). The position of men, especially the unskilled, in the labour market has been further undermined by the global changes and technological advances that were taking place in the 1970s (Faggio and Nickell, 2003). The movement of production to developing countries meant that many unskilled and manual jobs were displaced, and it was previously men filled these positions. These developments therefore make men more vulnerable to unemployment, as the jobs they formerly had no longer exist, and the new jobs emerging in the service sector were seen as better suited to the innate skills of women. The final group vulnerable to unemployment are ethnic minorities, however it should be noted that this category needs to be disaggregated, as unemployment is especially significant for Afro-Caribbean's and Bangladeshi workers, but does not affect Asian workers, especially those from India or China, to the same extent. Pissarides and Wadsworth have identified this group as vulnerable, however, saying; 'race has a large effect on unemployment' (1992, p66). Statistics show that by 1986 the rate of white unemployment was 9.1% compared to 15.1% for non-whites (Pissarides and Wadsworth, 1992, p66). More recently Coles found that those identifying themselves as black were twice as likely to be unemployed (1995, p40). There are several reasons for this. Firstly many of them have lower educational achievements, with 40% of Bangladeshis having no formal qualifications (Wadsworth, 2003, p119), and in times of recession, as was occurring in the 1980s employers raise the qualifications necessary to move into employment, so ethnic minorities would have been put at risk. This could also be linked to the fact that many have a lack of language skills and familiarity with British institutions (Wadsworth, 2003), which employers may no longer have found acceptable in a time when a large reserve army of labour existed. Secondly racial discrimination existed, and still exists in the labour market, with employers perceiving ethnic minorities as lacking an innate ability (McLaughlin, 1992). If employers base the criteria for workers upon this it puts some ethnic minorities at a very high risk of unemployment. Finally Jenkins argues that as with those in industrial towns, ethnic minorities have limited access to job information networks, so they do not get the same opportunities as those with access (1986, cited in Maguire, 1992). This shows that some ethnic minorities are at a very high risk as they have lower qualification and skill levels, but also face exclusion and discrimination within the labour market, especially one with more focus upon the service sector than manufacturing. In conclusion there were four main groups who were at risk from unemployment, and therefore most vulnerable to mass unemployment in the 1980s and 1990s. The young were at risk due to global changes in the labour market; which caused a decline in manufacturing in Britain, technological advances, and government policies, and those in industrial areas suffered largely for similar reasons, as the decline in manufacturing hit these areas hard. Again it was largely the global changes and decline in manufacturing that put men at risk from unemployment, as well as ethnic minorities, although they also have to contend with issues of discrimination which place them at an even higher risk. Overall this shows that the global changes, technological advances and government policies in the 1970s put certain groups of people at risk, and they were especially vulnerable to unemployment.",True
7,"In the twenty five years following the Second World War the government was fully committed to full employment, however in the 1970s unemployment began to rise drastically (Vickerstaff, 2003). The number of unemployed rose from 1.3 million in 1979 to 3 million in 1982 (Jones and Novak, 1999, p39, Glynn, 1999, p190), and as Burnett notes these figures remained high throughout the 1980s and 1990s (1994). However although unemployment has always existed in periods of history before, this increase was significant, as new patterns emerged, with rising youth and ethnic minority unemployment, and unemployment concentrated in certain areas of the country (Ashton and Maguire, 1991). In this essay I will briefly outline why mass unemployment emerged at this time, including factors of global changes, technological advances and government policies. I will then move on to examine which groups were most at risk from these changes, looking closely at the young, those in industrial towns, men, especially the unskilled, and finally ethnic minorities, and exploring the reasons why they were so vulnerable. I will not look at several other groups who were also vulnerable; those with disabilities, few qualifications and the elderly, as they had been at risk from unemployment before, and it was not the situation in the 1970s that caused this risk, as it was with the groups I will look at. There were many reasons as to why unemployment grew in the 1980s and 1990s. The neo-liberal and social democratic approaches have opposing views, with the neo-liberals arguing that unemployment was down to individual attitudes, and that deregulating markets would solve this problem, whilst social democratic approaches argued that it was a structural problem, that regulated markets and welfare would lessen (Vickerstaff, 2003). However there were many more complicated factors involved in the steep decline during this period. One of the main reasons was global changes. Firstly the 1973 oil crisis lead to a decline in the global economy, and the British economy, causing unemployment to rise (Burnett, 1994), especially as this resulted in a shift of trade against Britain, as she was not an oil producer (Glynn, 1999). Secondly, throughout the recession global markets began to spread, meaning that British exports began to decrease, and more low cost products were imported from abroad, so many companies went bust (Ashton and Maguire, 1991). Also those companies who did not began to change their labour market strategy, either moving production to other countries, where labour was cheaper, or making redundancies and moving towards a more flexible workforce (Glynn, 1999). All of these changes in the global market had a dramatic effect on the labour market within Britain, but also to industry as a whole, as it caused a massive decline in the manufacturing industry (Ashton and Maguire, 1991), which fell in production by 15.75% between 1979 and 1983 (Glynn, 1999, p270). As manufacturing was a major employer in Britain at the time, inevitably unemployment rose, as although the service sector grew as manufacturing declined, it was not to the same extent, and fewer jobs were created. These global changes were also further compounded by increases in technology, and government policies. Technology displaced or made more flexible many jobs in the parts of the manufacturing industry that survived (Burnett, 1994) and meant that fewer workers were needed in the growing service sector. It also meant that many unskilled and semi skilled jobs disappeared from the market, as technology could now be used (Mizen, 2003). Ashton and Maguire argue that government policy made this problem worse however, as it weakened trade union power and made these changes easier for employers to introduce (1991), and Professor Laynard agrees, claiming that rising inflation, fiscal monetary policies and a lack of commitment to full employment caused dramatic increases in unemployment (Laynard, cited in Burnett, 1994, p268). One of the groups most affected by these changes was the young, and Makeham claims that they are now especially vulnerable to general trends in employment (1980, cited in Coles, 1995, p 32). In 1960 the unemployment rate for the under 25s was only 2.4%, however by 1979 it had increased to 12.2% and by 1981 it had risen further to 21.4% (Coles, 1995, p32). This trend is one that still continues today, and in 1996, half a million 19-24 year olds were classified as unemployed (Jones and Novak, 1999, p64). These statistics can further be placed in context by looking at the fact that although general unemployment was rising at the time, which could simply be reflected in these figures, it only rose by 45%, whereas youth unemployment rose by 120% (Coles, 1995, p32). There are many reasons as to why the young are so vulnerable to these changes. It is argued that they are especially vulnerable to the business cycle anyway, as in times of recession businesses cut their recruitment and training and inflate the experience and qualifications needed for financial reasons, as do Trade Unions, who aim to protect their existing members, meaning that the young have very little opportunity to enter into the work force. Also due to statutory redundancy payments it is cheaper to sack those who have been in work for the shortest period, and this usually applied to the young (Mizen, 2003). However, although this does make the young more vulnerable, the reasons why they became one of the highest groups in unemployment in the 1980s and 1990s has to be placed in context. Firstly the global changes and resulting decline in the manufacturing industry impacted significantly upon them, as it was previously a large source of youth employment, with one third of school leavers entering the industry (Mizen, 2003). This loss should have been compensated by the growth in the service sector, however many employers preferred adult female workers, and began to reject the young, reflecting the fact that Maguire claims, employers often view the young as irresponsible and lacking training and experience (1992), therefore making them more at risk in times of high unemployment. Technological advances also impacted as many of the jobs they displaced were in the manufacturing sector, which was a major source of youth employment, but also because it meant that fewer apprenticeships were offered (Coles, 1995), so the young could not get the training or the experience needed to enter the labour market initially. Burnett supports the idea that it meant that the young lacked the higher criteria requested by employers due to economic decline, but also points out that at the time adult wages were changed to apply to 18 year olds as well, not just older workers (1994), so it was now no cheaper for employers to hire young people. As well as being especially vulnerable to these changes in the labour market structure, the risk posed to the young was further increased by government policies. They blamed youth unemployment on what is known as the 'deficiency model' (Mizen, 2003), which argued that the reason the young were out of work was because they were restless in work, had no incentive to be there, or did not have the qualities needed for employment. This meant they received little support from the government to protect them from the dangers of unemployment. Therefore overall the recent changes in the labour market, technological advances and government policies have made the young a group particularly at risk of unemployment. Another group at risk to high unemployment were those living in traditional industrial towns, especially in the North and West. Statistics show that whilst the rate of official unemployment was only 8.4% in the South East, in the North West it was 14.7%, and in the North 16.6%, indicating that industrial areas were at a higher risk of unemployment (Jones and Novak, 1999, p41). Within these areas places such as Yorkshire, Scotland and Humberside were the worst affected, and Ashton et al found that a young man's chance of being unemployed in Sunderland was one in three, compared to only one in thirty three in St Albans (1988, cited in Coles, 1995, p39). Pissarides and Wadsworth note that this is largely due to the occupational distribution of employment (1992). These areas were where manufacturing production was concentrated, and when this went into decline due to global changes, these areas lost their main source of jobs. They also did not experience the benefits bought about by the increase in the service sector, as the growth of business was concentrated mostly in the South East, growing by 20% per year in these areas, compared to only 7.2% in the North West (McLaughlin, 1992, p14). Burnett supports this argument, saying that the structural changes that have occurred in the labour market, most notably the change from manufacturing production to service sector domination, have led to high job losses in certain areas, especially in industrial towns (1994). Technological advances have further increased the risk of unemployment in these areas as they have meant that many of the remaining jobs have become flexible (Vickerstaff, 2003). The people living within these areas were made even more vulnerable to these risks, as they become largely cut off from other cities where the service sector is now dominating, meaning they were excluded from job information networks and fell into a downward spiral ending in a culture of unemployment (Ashton and Maguire, 1991). These reasons have all combined to ensure that over the past thirty years people living in these industrial areas have been more affected by unemployment. Men, especially the unskilled, are another group at risk from unemployment, and who suffered during the mass unemployment of the 1980s and 1990s. Unemployment for men over 25 has increased four fold since the 1970s (Faggio and Nickell, 2003, p 31), with 2.3 million fewer men in work now than twenty years ago (Jones and Novak, 1999, p40). There are several reasons for this decline in male employment. Firstly, they were the main workers in the manufacturing industry, so when this declined their levels of unemployment increased (Ashton and Maguire, 1991). Although the service sector grew to replace manufacturing as a source of employment, women were favoured by employers due to their characteristics, and the way they were perceived as more caring and understanding, and also reliable, especially if they were married (Maguire, 1992). This idea is demonstrated clearly by Ashton and Maguire when they say; 'the new service sector jobs have provided low skilled, part-time and often casual employment, primarily for females' (1991, p53). The position of men, especially the unskilled, in the labour market has been further undermined by the global changes and technological advances that were taking place in the 1970s (Faggio and Nickell, 2003). The movement of production to developing countries meant that many unskilled and manual jobs were displaced, and it was previously men filled these positions. These developments therefore make men more vulnerable to unemployment, as the jobs they formerly had no longer exist, and the new jobs emerging in the service sector were seen as better suited to the innate skills of women. The final group vulnerable to unemployment are ethnic minorities, however it should be noted that this category needs to be disaggregated, as unemployment is especially significant for Afro-Caribbean's and Bangladeshi workers, but does not affect Asian workers, especially those from India or China, to the same extent. Pissarides and Wadsworth have identified this group as vulnerable, however, saying; 'race has a large effect on unemployment' (1992, p66). Statistics show that by 1986 the rate of white unemployment was 9.1% compared to 15.1% for non-whites (Pissarides and Wadsworth, 1992, p66). More recently Coles found that those identifying themselves as black were twice as likely to be unemployed (1995, p40). There are several reasons for this. Firstly many of them have lower educational achievements, with 40% of Bangladeshis having no formal qualifications (Wadsworth, 2003, p119), and in times of recession, as was occurring in the 1980s employers raise the qualifications necessary to move into employment, so ethnic minorities would have been put at risk. This could also be linked to the fact that many have a lack of language skills and familiarity with British institutions (Wadsworth, 2003), which employers may no longer have found acceptable in a time when a large reserve army of labour existed. Secondly racial discrimination existed, and still exists in the labour market, with employers perceiving ethnic minorities as lacking an innate ability (McLaughlin, 1992). If employers base the criteria for workers upon this it puts some ethnic minorities at a very high risk of unemployment. Finally Jenkins argues that as with those in industrial towns, ethnic minorities have limited access to job information networks, so they do not get the same opportunities as those with access (1986, cited in Maguire, 1992). This shows that some ethnic minorities are at a very high risk as they have lower qualification and skill levels, but also face exclusion and discrimination within the labour market, especially one with more focus upon the service sector than manufacturing. In conclusion there were four main groups who were at risk from unemployment, and therefore most vulnerable to mass unemployment in the 1980s and 1990s. The young were at risk due to global changes in the labour market; which caused a decline in manufacturing in Britain, technological advances, and government policies, and those in industrial areas suffered largely for similar reasons, as the decline in manufacturing hit these areas hard. Again it was largely the global changes and decline in manufacturing that put men at risk from unemployment, as well as ethnic minorities, although they also have to contend with issues of discrimination which place them at an even higher risk. Overall this shows that the global changes, technological advances and government policies in the 1970s put certain groups of people at risk, and they were especially vulnerable to unemployment.","Much more reproductive choice is now available to women, with the ability to chose whether or not to have children, when to have them and in what context, now a reproductive right given to women. This, combined with shifting social and economic opportunities for women, has lead to an increase in the number of childless women, with, in 1991, twice as many saying that they expected to remain childless as in 1986 (Gillespie, 2001). However the anticipated number of children per woman in Europe and the USA is still near or above two (Bongaarts, 1999), showing that many are still having children. In this essay I will explore why women have children, even though there is now more opportunity for them not to, and why those who do not have children do not do so. I will begin by looking at why women do have children, exploring the idea of push and pull in the decision, introduced by Morrell (1994), looking at factors identified as 'pulling' women into motherhood; psychological, social and economic factors, as well as looking at the influences of 'push' factors such as national discourses on motherhood and social policy. I will then examine why women do not have children or remain, as Gillespie describes, as voluntarily childless, (2001), considering the influence of personal, social and economic factors, and changing opportunities for women. In considering all of these influences on why women do or do not have children I will look at how they all work together to form a complex process, not just a simple decision. Morrell argues that women do not choose to have children only because it is something that they would like to experience and are attracted to, but also because they are almost forced into it by national discourses on motherhood and social policies, illustrated when she says; 'women are not only pulled to mothering, women are pushed in this direction in part by cultural representations of both motherhood and childlessness' (1994, p145). This means that a complex combination of personal choices and cultural influences combine in women's decisions to have children. Many personal reasons for women having children have been identified. The society in which we now live is disaggregated and lonely, and it is argued that women may want children to compensate for this (Andorka, 1978), as they are seen by many as a constant source of love and affection, and are usually totally dependant upon their parents. This idea is supported by Coldwell, who says; 'one's own children provide a unique pleasure which is not substitutable' (1982, cited in Schoen et al, 1997, p335), showing that many women chose to have children for the unique personal rewards that they carry. It is also argued that children are a source of self expansion, enabling women to feel they are moral and altruistic people for producing them (Hoffman and Hoffman, 1973, cited in Andorka, 1978, p338), and that they are a way of reproducing ones self and ensuring ones characteristics remain, an influence becoming perhaps more important as life after death is no longer considered a reality (Gittins, 1993). However as well as personal factors attracting women to motherhood, economic and social factors may also be influential. Lancaster has argued that children do provide some economic profit, as they can be a way of claiming higher benefits and gaining access to housing, (1965, cited in Andorka, 1978, p364). However it is now felt that economic factors do not influence the decision as much as in previous times, as women are no longer economically dependant upon men, and children are not needed to provide a source of income. Instead social factors have more of an input. Coleman claims that children are a form of 'social capital' (1988, cited in Schoen et al, 1997, p337), as they provide women with social benefits that they might not otherwise have, and this gain can influence the decision to have children. Having children can mean the development of social relationships that can be beneficial to women, and ensure higher standings within a social network (Schoen et al, 1997). Becoming a parent signals adult status has been reached, and this can result in acceptance into certain prestigious social groups, or the chance for social mobility (Hoffman and Hoffman, 1973, cited in Andorka, 1978, p338), for example some workplaces reward workers who have children as they are seen as mature and responsible. Although this is more applicable to male workers, it can mean that if a man is given promotion the whole family will be seen differently by society. It is also suggested that having children can produce better social relationships with kin networks, meaning more physical, emotional and financial support that may not otherwise have been achieved (Schoen et al, 1997). Having children can also provide women specifically with a higher status in society, as mothers are glorified (McDaniel, 1996), and many women may wish to be regarded highly by society. It is also argued that children can give women more power in the domestic sphere, as nurturing is seen as naturally linked to women, so they are given control over this area (Gittins, 1993), and therefore see it as a way of gaining more power within domestic relationships. Security is another form of social capital that can be created by having children. As well as being a source of security in the form of constant love, they can also provide social and financial security, especially in old age. Morrell found that many women feared old age, as women are now living longer than men (1994) and this fear can mean that women choose to have children to ensure their security, therefore making them an important social resource. These more individual and personal reasons that influence the choice of women to have children often combine in a complex way, however they are not the only factors that affect the decision. National discourses on motherhood and social policy have subtle influences that provide a background on which the more explicit influences of personal fulfilment and economic and social benefits are established. Patriarchal ideology prescribes particular behaviour for marriage and reproduction; the idea that marriage is based upon heterosexual relationships and that having children is the next natural step (Gittins, 1993), meaning that many women see getting married and having children as a natural progression through life, even in modern society. This is reinforced by strong historical discourses that link femininity with motherhood, claiming that 'true womanhood' is not reached unless women have children, or see having children as their biological and ideological goals (McDaniel, 1996). Culture dictates that it is natural for women to be mothers, and prescribes the number of children that is seen as respectable within our society (Schneider and Schneider, 1995, cited in Schoen et al, 1997, p335). Religious, political and scientific leaders stress that it is only through motherhood that women can be fulfilled, and Gillespie supports this, highlighting the way in which motherhood is seen as natural in modern society; 'the idea that motherhood is essentialist, deterministic, fixed, inevitably fulfilling and central to feminine identity is firmly entrenched in industrial, urban and rural societies' (2001, p141). This discourse, as well as placing motherhood as natural and essential for defining women, also ensures that women in society without children, both voluntarily and involuntarily, are seen as failed and not proper women. Culture also dictates that those who do not have children are selfish and immoral (Gillespie, 2001), and motherhood is seen as a signifier of normality (Morrell, 1994). Women feel that they should fit in with what is 'normal and natural', meaning that they chose to have children. These ideas are evident in social policy. As the nation relies on reproduction for its continuation, the state has much interest in levels of fertility and tries to encourage women to reproduce. This is done by reinforcing the discourses of motherhood as natural, and through more explicit social policy. This is evident in the USA where abortion and contraception are closely controlled by the state to ensure that women consider having children as important (McDaniel, 1996). These discourses in society influence the more personal factors that are involved on the decision to become a mother, as they promote the idea that children are a natural progression for women and their main source of happiness. Therefore many women feel that they should have children as it will make them happy, and see it as natural in order for them to fulfil their role in society, and gain status, as society attributes status to women through motherhood in these discourses. Consequently these cultural ideas running through society influence and combine with personal, economic and social factors to create a complex combination of reasons as to why women have children. However, despite these dominant discourses in society not all women have children. Although this may be down to infertility in some cases, I will be looking at the issue of voluntary childlessness, where women choose not to have children, rather than being unable to. As with the decision to have children, there are many personal, economic and social reasons why some women choose childlessness. Some women simply do not like children (Gillespie, 2000), or fear the effects that having a child will have upon their figure. Others may feel that due to crime and poverty the world now is not a suitable place to bring up children, or that the conditions of the town in which they live, for example lack of space, are not equitable to having children (Andorka, 1978). Children are also a significant emotional responsibility and some women may feel they do not wish to take this on, or may already have carried such a responsibility, either for siblings or parents and not want to endure it again (Gillespie, 2000). There is also the influence of economic factors to consider. Children are no longer economic assets, but are in fact very financially costly, and many women may decide they cannot afford to have children (Schoen et al, 1997). Fears about economic security have lead to more women deciding that children are not a viable option, and that the financial burden is too high a price to pay. Aspirations for certain standards of living may also influence the decision as some will feel that the cost of children would not fit in with the lifestyle they wish to have (Andorka, 1978). This links quite closely to social factors, as a desired lifestyle can be quite important when considering whether or not to have children. Opportunities other than motherhood are now open to women, especially in economic structures, and some women choose to explore these opportunities, rather than experience motherhood. Women's increasing participation in employment means that they are able to develop careers of their own, and many feel that they gain their identity from paid work and do not want to lose this by having children (Gillespie, 2001). Women now wish to have interests separate from the domestic sphere, and some may believe that having children places restrictions on the possibilities for this sort of lifestyle. Morrell describes it as a desire for 'freedom from and freedom to', where women wish to have freedom from the ties, both financially and emotionally, of being mothers, but also want the freedom to experience a different lifestyle, with flexibility and no guilt. She also found that many did not like the context under which motherhood was supposed to take place, feeling that it would enforce a break from paid employment, therefore economic dependence, and confinement to the home ensuring a sexual division of labour (1994). These desires for a lifestyle that does not involve children have been made possible by small changes in employment structures, which mean that women can now have a career, and the emergence of discourses, albeit not mainstream ones which allow women greater sexual freedom. Gillespie describes this as 'the post-modern model of reproduction', in which social forces have impacted upon motherhood, meaning that that it is becoming disaggregated with the natural (2001), so motherhood is no longer seen as synonymous with femininity. This shows that as well as there being personal, economic and social factors influencing the decision not to have children, changes in employment structures and new discourses emerging on motherhood and reproduction mean that these factors are possible in modern society. Therefore it is a complex combination of social ideals and personal responses that results in women becoming voluntarily childless. Overall this shows that there are many complex influences upon the decision of whether or not have children. In the decision to have children discourses on motherhood and reproduction create the ideal that motherhood is both natural and fulfilling for women, and this impacts upon personal, economic and social factors, including security, happiness and social capital, that lead to women choosing to have children. The same idea can be seen in the decision not to have children, as changing opportunities for women, and emerging discourses are making it possible for women to decide that for personal, economic and social reasons, for example lifestyle choices, they wish to remain childless. In conclusion why women do or do not have children is a complex process influenced by many factors, and based upon a variety of discourses and opportunities ingrained within society, not simply whether or not a woman likes children.",False
8,"Victorian notions of women's madness were largely influenced by the Victorian Domestic Ideology; paramount during the nineteenth century in defining gender roles, responsibilities and acceptable behaviour. This primarily concerned the middle classes, establishing ideals of womanhood and femininity that were arguably damaging both to those directly targeted as well as other groups within Victorian society striving to emulate the middle class depiction of morality, sobriety and modesty. At the same time, the notion of madness as a disease was becoming increasingly apparent within Victorian society, further highlighting gender and class divisions through varying perceptions of madness and its origins. Whilst there were significant implications in terms of gender, this essay will mainly deal with the effects of social class on women's experience of madness in Victorian England. Firstly, it is important to understand that madness, in all its various forms, was seen as a female illness. Femininity became inextricably linked with hysteria, along with neurasthenia and anorexia nervosa. The main reasoning for this is to be found in the prevailing misogynistic understanding of female sexuality in nineteenth century England. Women were viewed as emotionally unstable as a result of the functions of their reproductive organs, the precise workings of which received limited medical investigation. The popular theory put forward by physicians claimed that madness was caused by 'the action of the reproductive organs on an unstable nervous system',1 showing how women were seen as intrinsically weak and therefore having a predisposition to insanity simply through being female. This was a reductionist view of madness, using biology to ascertain causes of the illness rather than analysing social and cultural factors relevant to the women affected. In this view, female biological functions were seen as the cause of madness, namely the onset of menstruation, pregnancy, childbirth and the menopause. From this it can be seen how madness was viewed in a feminine context, confining women to the subject of psychoanalysis throughout the nineteenth and twentieth centuries. The primary origins of women's madness were considered a result of their diseased sex organs, insatiable sexual desire, sedentary lifestyles and inferior minds.2 However, this could only be applied to the middle classes, confined to the domestic sphere and not engaging in paid employment. For middle class women, the omnipresent Victorian Domestic Ideology carried much weight with regard to their lifestyles and expected behaviour. Its restrictive nature left middle class women little freedom to choose their path in life or have any influence over their own decisions. Instead, they were expected to conform to social ideals of womanhood, in which they should be obedient, meek and, ultimately, subordinate to the male figures in their lives. This patriarchal system, both in the small scale of family life and the large scale of society as a whole, meant that women constantly had to conform to men's definitions and expectations, in which 'their [women's] sanity was often judged according to their compliance with middle-class standards of fashion'.3 In this way, masculinity was the accepted norm and femininity was a deviation from this. Thus, while masculinity was synonymous with sanity and emotional health, femininity was synonymous with insanity and corrupt mentality. Being the moral guardians of the domestic sphere, this proved a problem as it threatened the morality of the home and future generations, thus male figures of authority were forced to control female madness to safeguard the population. Ironically, it was not the female half of the population that was protected and helped, instead they were further oppressed. Particularly frustrating for middle class women, puberty was a time when they were presented with the clear distinction between themselves and their brothers. Women came to see that their confinement to the home was the accepted woman's role, depriving them of intellectual and cultural stimulation and highlighting how 'puberty... gives to woman the conviction of her dependence'.4 This may explain to some extent why puberty was seen as such a volatile period for women, many of whom would have resented the expectation that they would adopt the feminine ideal as their own. With limited outlets for creativity and economic independence, marriage was the only viable option for the majority of middle class women. However, far from being a safe and protected haven it was observed that it was chiefly 'those women who are married, not in paid employment, with children living at home, who are most at risk of being positioned as mad'.5 This serves to highlight the restriction of domesticity, channelling women's energy into servitude of the family whilst not recognising her intellectual and creative potential. This feeling of helplessness and isolation would have caused considerable distress to middle class women who aspired to achieve more, particularly those wealthy, literate and educated individuals who saw the possibility to engage in public life but were prevented from doing so on the basis of their sex.6 It could be argued that, under such conditions, madness was the middle class woman's protest against her subordination and oppression. By directly rejecting her gender- and class-specific roles, the madwoman embodied everything that confronted the Victorian Domestic Ideology serving to imprison her within the domestic sphere. The leading psychoanalyst of the time, Freud, believed it was the case that women were beginning to 'resist the social definitions that confine them to the doll's house of bourgeois femininity'.7 Such women were notable for their sexual deviance, in terms of promiscuity, bearing children out of wedlock, nymphomania, masturbation and homosexuality, all of which were thought to be signs of madness. Such practices received considerable attention, largely from men trying to suppress such behaviour for fear it would corrupt their womenfolk and disrupt society. These views relate to medieval associations of womanhood with witchcraft, in which increased sexuality was prominent in labelling women's deviant status. The link between women's heightened sexual desire and fornication with the Devil served to oppress female sexuality with the threat of accused witchcraft and being sentenced to death. During the nineteenth century this translated to the control of female sexuality with the threat of accused madness and subsequent institutionalisation or castration of the sex organs. Middle class women may have found madness a suitable outlet for their anger and frustration, with no other ways to communicate their distress other than attracting attention to their deteriorating health and radical behaviour. It would certainly have gained them the sympathy and attention from those around them, if not the change in their lifestyles that they truly desired.8 However, the extent to which such action was profitable is debatable, with evidence suggesting that 'hysteria was at best a private, ineffectual response to the frustrations of women's lives... [in which] the sympathy of the family, [and] the attention of the physician were slight in relation to its costs in powerlessness and silence'.9 The prevailing ideals of femininity would have appeared insurmountable for a middle class woman of the time, particularly those with professional aspirations of earning a living as artists or writers, in which cases it was thought that 'rebelliousness could produce nervous disorder',10 resulting in few women practicing their chosen career free from the associated stigma of madness. A different perspective on madness can be found within the working class, largely disregarded in nineteenth century psychiatry. A predominating view of the time was that 'insanity was an expensive disease'11, reserved for the affluent middle and upper classes where 'sickness and invalidism for women were almost in vogue'12, signifying plentiful leisure time and the fact that they did not have to enter into paid employment. Whilst middle and upper class women were retiring to bed with various nervous afflictions, their working class counterparts were busy working to support their families; hence they suffered the dual oppression of class and gender in the suppression of their activities and outlets.13 Having even less autonomy, working class women had more to protest against, added to the fact that the Victorian Domestic Ideology was being imposed upon them too in an attempt to universally enforce middle class standards. This created friction between the desired standards the working class strived to emulate and the reality of what they could achieve given their limited economic and social means, inevitably leading to feelings of frustration and helplessness. Whereas uncontrollable sexuality was a key feature of middle class madness, assumed moral weakness of the working class was the main reason given as the cause of their madness, asserting that 'Poverty was, after all, one of the moral causes of insanity'.14 Furthermore, different manifestations of insanity were identified between the classes, with hysteria being largely associated with the working class whilst neurasthenia was more commonly identified with the middle and upper classes. Though both showed similar symptoms, the latter was considered a 'more prestigious and attractive form of female nervousness than hysteria'15, thus reserved for the elite members of society. The treatment of diagnosed madness available to middle and upper class women would have further inhibited their freedom and creativity, usually in the form of the rest cure.16 In such a practice, women were prevented from any intellectual or moral growth, being inescapably confined to the domestic environment and prevented from exercising any pursuits of their own. Whilst there may have existed the notion that 'Sickness presents a tempting escape from the contingency of the feminine role'17, in the sense that women were temporarily freed of household burdens, it is probably more accurate to see such treatment as exceedingly damaging to their existing oppressed positions. It is contestable as to whether many of the women diagnosed as mad were genuinely of ill mental health or whether this was a simple yet effective way to repress those not conforming to social ideals constructed by men. Coincidentally, at the same time that middle class women began demanding more rights and opportunities, 'the female nervous disorders of anorexia nervosa, hysteria, and neurasthenia became epidemic'.18 This emphasises the notion of patriarchal control over the subordinate members of society, namely women, through the legitimacy of professional medicalisation. In contrast, working class women could not hope to receive what they would have perceived as a reward in such treatment, instead they would have faced incarceration in public county asylums. They too were shunned from society rather than treated, further adding to their humiliation when 'visits to the asylum, to view the mad, became a routine part of social life'.19 Conversely, their isolation was a public matter, unlike the private affair of the middle and upper classes, with their literary depiction of the 'madwoman in the attic'.20 So marked was this difference in institutionalisation that by the end of the nineteenth century, ninety one per cent of all the institutionalised insane were paupers.21 Again, middle class ideals were incorporated into working class life, present within the asylum itself. The design of these institutions was seen as 'inevitably reproducing structures of class and gender'22, thus aiming to recreate social and moral norms in an artificial environment through appropriate segregation. Working class madwomen suffered extreme stigma under the Poor Law in county asylums, whilst the middle and upper class women were more often, when institutionalised, put into private asylums, thus accentuating how the 'affluent and middling groups had very different experiences... [the] divide between private and state institutions reflected... a basic class division'.23 Ultimately, it is apparent that both perceived causes and treatments of madness differed between the working and middle classes, affecting the lives of many Victorian women, as well as re-emphasising class divisions in society. Whereas working class women may have used madness as a protest against their economical oppression, middle and upper class women were more likely to use it to protest against the social ideals forced upon them, which also permeated through to the working class as part of the Victorian Domestic Ideology. It can be concluded that madness was gendered in class specific ways, with distinctly separate discourses on the subject regarding each class.","The development of feminist thought and action in Japan is distinct from that of its emergence in the West, more specifically within the United States and Britain. Though influenced by Western politics, the development of feminism in Japan was also initiated by numerous other social and political factors that affected the political climate at that specific time and place. Adherence to Confucian ideologies severely restricted women within Japanese society, as well as subordinating the working classes and those not owning property. A growing political awareness and imperialist competition later led to the struggle for social reform, of which women were arguably at the forefront. This essay will deal with such social and political factors as these to account for the development of feminism in Japan. The subordinate position of women within Japanese society came to be highlighted at the end of the nineteenth century, whereas previously notions of the 'natural inferiority of women'1 had been largely unchallenged. This had implications for the rights of women concerning property, marriage and divorce, amongst other issues. Adopting Chinese Confucian ethics, the Japanese sought to uphold and promote to younger generations the established patriarchy through regulating the autonomy and independence of women. This was achieved through the establishment of a patrilineal family system that demanded filial respect within the family, which was then reflected in the necessity for obedience to the head of state.2 It can be observed from this that women were subject to conform to male ruling, both in familial terms and within the wider political sphere. This originated from the Confucian teaching that women were to be understood as yin, hence passive and in need of ruling by the male yang. This therefore left women no place to rule, either within the family or within the feudalistic structure prevailing in Japan at the time.3 Such oppression of female autonomy was further reinforced through the Civil Code of 1897, which successfully restricted women's power as it granted control of marital property and possessions to the husband, specifying that a wife could not act without her husband's permission.4 This subordination of Japanese women was only challenged after the re-ascendance of the Emperor Meiji in 1868, termed the Meiji Restoration (1868-1912). This signalled 'a process of reform, modernisation and democratisation'.5 From this point, widespread social and political reform took place within Japan, primarily to compete with Western imperialistic powers and assert a national strength. Western intellectuals linked the Japanese subordination of women to their weakness as a nation; a theory which was to be adopted by Japanese reformers such as those involved with the Meiji Six Journal, seeking to modernise and civilise their nation.6 It was seen as paramount to liberate women in society in order to 'gain international respectability for Japan'7 and thus to compete successfully in the global political economy. The Meiji Restoration was a more liberal form of government than had been in place previously and directly questioned the status of women, holding them to be 'at the centre of change' for reform of society and modernisation.8 It was arguably this revolution which made the woman question accessible to Japanese society, paving the way for women to make their own voices heard in the changing political context and thus providing a powerful medium for this oppressed section of society. During the struggle for women's liberation, nationalism gained increasing interest and received considerably more attention from Japanese intellectuals, reformers and citizens. Feminism and nationalism were interlinked in discourses on social reform, though many believed the latter to take precedence through exploiting the former for its own aims, observing how the state 'moulded women's options and energies into national rather than individual goals'.9 This may be explained in terms of Japan's increasing need to assert itself economically against Western capitalist structures, thus necessitating the mobilisation of the female half of the population, though this could only be achieved after their inclusion within the political sphere. However, the extent to which this was truly implemented is questionable. The fact that women were subordinate to men in Japanese society led many to suggest that this was the reason for Japan's economic and military weakness and consequent subordination as a nation to the West.10 Women were thus implicated in the development of a stronger nation and were relied upon heavily in new empire building. That is not to say that women were independent of the state, in fact the issue of support for mothers was highly debated. There existed, to some extent, interdependency between women and the state, though Yosano Akiko, a prominent feminist at the time, argued that 'it is slave morality for women to be dependent on men because of their procreational role [and therefore women] must refuse dependency on the state for the very same reason'.11 This serves to highlight the role assigned to women; primarily viewed as mothers providing the next generation of patriotic, nationalist fighters for the country. An additional factor affecting women was arguably their education. Debated by reformers, intellectuals and feminists, many continued to feel that this was a large barrier to women's struggle. It was observed that the 'inadequacy of women's education reinforced their low status'12 and was thus a cause for concern and in need of reform. In 1873, forty percent of boys and fifteen percent of girls attended primary school, though by 1910, nearly ninety nine percent of boys and ninety seven percent of girls were in this position.13 In 1885 the Minister of Education, Mori Arinori, a 'nationalist who said publicly that education was for the benefit of the state, not the individual'14 promoted the idea that women were to be educated for a purpose other than their own emancipation. This originated from the emphasis placed by the both state and traditional society on the need to produce good wives and wise mothers, or ryosai kembo.15 Hence, the limited education women received was aimed at developing their domestic and nurturing skills, rather than instilling them with intellectual desires and political awareness. The issue of political awareness and activity held the key to women's liberation in Japan. Though reformist ideologies became increasingly more popular, legislation remained highly conservative, reflected in laws such as the 1890 Peace Preservation Law regulating 'subversive activities', a move largely aimed at feminists, and also within laws preventing women from attending political meetings.16 However, with the reinstatement of the Emperor Meiji there was a shift towards democracy, which led women, as well as other oppressed groups, to question their position in society and to reject 'taxation without representation'.17 The growth of capitalism and the erosion of the feudalistic power structure supported the expansion of the middle class; a new group demanding more political rights and voicing their oppression through the right-wing People's Rights Movement, established in 1874. By 1878 the Movement had expanded membership to women, recognising them as 'a force to be mobilised', whereas they had previously been unconcerned with women's rights.18 Within a few years, Kishida Toshiko spoke at a public meeting on women's rights; the first woman to do so within the Movement. This signified a large step forward in women's liberation and inspired numerous other women to act, including Fukuda Hideko.19 Other notable groups formed due to the feeling within the middle classes that there was not sufficient political freedom and representation. For example, the Tokyo Women's Reform Society was set up chiefly by middle class Christians and campaigned largely against social evils oppressing women, such as prostitution and concubinage.20 Due to rapid industrialisation taking place in Japan as an attempt to keep up with Western capitalism and the newly-introduced free trade market, there ensued an influx of women into the labour force. Whilst in 1930 there were ten million female workers in Japan, by 1944 this figure had increased to thirteen million, as well as showing a marked decrease in the percentage of men in the workforce.21 Women gradually entered such professions as teaching, nursing and clerical work, thus providing a strong basis for the working women's movement. However, women soon began to rise up against oppressive working conditions and laws, resulting in a strike in the Amamiya Silk Mill in Kofu. Here, as in other strikes taking place, women workers protested against worsening conditions of employment, wage decreases and abuse from male employers.22 Such dissatisfaction stemmed principally from the exploitative way in which female labour was being harnessed, so that industries 'could be made competitive in world markets if they were able to utilise cheap female labour'23 in order to then generate capital for use in other investments. A growing awareness of unfair working conditions led to a proliferation of working women's trade unions, thus granting improved representation and a medium through which they could make themselves heard in the rapidly modernising and industrialising nation. A further medium through which women gained representation was via the Seito journal, first published in 1911 by a women's literary appreciation group.24 Though not originally intended as a political group, it soon developed into such due to increasing pressure from the government to terminate its publication; feared as a threat to patriarchy and Japanese tradition. Eventually, the journal and members of the 'Bluestockings' group, such as Hiratsuka Raicho, came to question women's issues such as maternity provision, equal pay, the family system and marriage. The group demonstrably made the personal political and engaged many middle class Japanese women with the issue of their subordination, causing the government to enforce its termination in 1916.25 Ultimately, it can be seen that a varied combination of changing social and political factors led to the development of feminism in Japan. Through increased political awareness and changing structures within the government, women were able to challenge their position for the first time. In the national struggle for increased economic and military power, gender identities were redefined in order to benefit the state. Women were arguably used in this process before being liberated themselves. Thereby, 'in the process of refusing subordinate status [as a nation], Japan constructed a new womanhood as a demonstration of its equality with the West and as a means of proving its civilised status'.26 It was this use of women for national goals that led to increased feelings of oppression and eventual action against exploitation and subordination, resulting in a strong feminist movement in Japan towards the end of the nineteenth century.",True
9,"The development of feminist thought and action in Japan is distinct from that of its emergence in the West, more specifically within the United States and Britain. Though influenced by Western politics, the development of feminism in Japan was also initiated by numerous other social and political factors that affected the political climate at that specific time and place. Adherence to Confucian ideologies severely restricted women within Japanese society, as well as subordinating the working classes and those not owning property. A growing political awareness and imperialist competition later led to the struggle for social reform, of which women were arguably at the forefront. This essay will deal with such social and political factors as these to account for the development of feminism in Japan. The subordinate position of women within Japanese society came to be highlighted at the end of the nineteenth century, whereas previously notions of the 'natural inferiority of women'1 had been largely unchallenged. This had implications for the rights of women concerning property, marriage and divorce, amongst other issues. Adopting Chinese Confucian ethics, the Japanese sought to uphold and promote to younger generations the established patriarchy through regulating the autonomy and independence of women. This was achieved through the establishment of a patrilineal family system that demanded filial respect within the family, which was then reflected in the necessity for obedience to the head of state.2 It can be observed from this that women were subject to conform to male ruling, both in familial terms and within the wider political sphere. This originated from the Confucian teaching that women were to be understood as yin, hence passive and in need of ruling by the male yang. This therefore left women no place to rule, either within the family or within the feudalistic structure prevailing in Japan at the time.3 Such oppression of female autonomy was further reinforced through the Civil Code of 1897, which successfully restricted women's power as it granted control of marital property and possessions to the husband, specifying that a wife could not act without her husband's permission.4 This subordination of Japanese women was only challenged after the re-ascendance of the Emperor Meiji in 1868, termed the Meiji Restoration (1868-1912). This signalled 'a process of reform, modernisation and democratisation'.5 From this point, widespread social and political reform took place within Japan, primarily to compete with Western imperialistic powers and assert a national strength. Western intellectuals linked the Japanese subordination of women to their weakness as a nation; a theory which was to be adopted by Japanese reformers such as those involved with the Meiji Six Journal, seeking to modernise and civilise their nation.6 It was seen as paramount to liberate women in society in order to 'gain international respectability for Japan'7 and thus to compete successfully in the global political economy. The Meiji Restoration was a more liberal form of government than had been in place previously and directly questioned the status of women, holding them to be 'at the centre of change' for reform of society and modernisation.8 It was arguably this revolution which made the woman question accessible to Japanese society, paving the way for women to make their own voices heard in the changing political context and thus providing a powerful medium for this oppressed section of society. During the struggle for women's liberation, nationalism gained increasing interest and received considerably more attention from Japanese intellectuals, reformers and citizens. Feminism and nationalism were interlinked in discourses on social reform, though many believed the latter to take precedence through exploiting the former for its own aims, observing how the state 'moulded women's options and energies into national rather than individual goals'.9 This may be explained in terms of Japan's increasing need to assert itself economically against Western capitalist structures, thus necessitating the mobilisation of the female half of the population, though this could only be achieved after their inclusion within the political sphere. However, the extent to which this was truly implemented is questionable. The fact that women were subordinate to men in Japanese society led many to suggest that this was the reason for Japan's economic and military weakness and consequent subordination as a nation to the West.10 Women were thus implicated in the development of a stronger nation and were relied upon heavily in new empire building. That is not to say that women were independent of the state, in fact the issue of support for mothers was highly debated. There existed, to some extent, interdependency between women and the state, though Yosano Akiko, a prominent feminist at the time, argued that 'it is slave morality for women to be dependent on men because of their procreational role [and therefore women] must refuse dependency on the state for the very same reason'.11 This serves to highlight the role assigned to women; primarily viewed as mothers providing the next generation of patriotic, nationalist fighters for the country. An additional factor affecting women was arguably their education. Debated by reformers, intellectuals and feminists, many continued to feel that this was a large barrier to women's struggle. It was observed that the 'inadequacy of women's education reinforced their low status'12 and was thus a cause for concern and in need of reform. In 1873, forty percent of boys and fifteen percent of girls attended primary school, though by 1910, nearly ninety nine percent of boys and ninety seven percent of girls were in this position.13 In 1885 the Minister of Education, Mori Arinori, a 'nationalist who said publicly that education was for the benefit of the state, not the individual'14 promoted the idea that women were to be educated for a purpose other than their own emancipation. This originated from the emphasis placed by the both state and traditional society on the need to produce good wives and wise mothers, or ryosai kembo.15 Hence, the limited education women received was aimed at developing their domestic and nurturing skills, rather than instilling them with intellectual desires and political awareness. The issue of political awareness and activity held the key to women's liberation in Japan. Though reformist ideologies became increasingly more popular, legislation remained highly conservative, reflected in laws such as the 1890 Peace Preservation Law regulating 'subversive activities', a move largely aimed at feminists, and also within laws preventing women from attending political meetings.16 However, with the reinstatement of the Emperor Meiji there was a shift towards democracy, which led women, as well as other oppressed groups, to question their position in society and to reject 'taxation without representation'.17 The growth of capitalism and the erosion of the feudalistic power structure supported the expansion of the middle class; a new group demanding more political rights and voicing their oppression through the right-wing People's Rights Movement, established in 1874. By 1878 the Movement had expanded membership to women, recognising them as 'a force to be mobilised', whereas they had previously been unconcerned with women's rights.18 Within a few years, Kishida Toshiko spoke at a public meeting on women's rights; the first woman to do so within the Movement. This signified a large step forward in women's liberation and inspired numerous other women to act, including Fukuda Hideko.19 Other notable groups formed due to the feeling within the middle classes that there was not sufficient political freedom and representation. For example, the Tokyo Women's Reform Society was set up chiefly by middle class Christians and campaigned largely against social evils oppressing women, such as prostitution and concubinage.20 Due to rapid industrialisation taking place in Japan as an attempt to keep up with Western capitalism and the newly-introduced free trade market, there ensued an influx of women into the labour force. Whilst in 1930 there were ten million female workers in Japan, by 1944 this figure had increased to thirteen million, as well as showing a marked decrease in the percentage of men in the workforce.21 Women gradually entered such professions as teaching, nursing and clerical work, thus providing a strong basis for the working women's movement. However, women soon began to rise up against oppressive working conditions and laws, resulting in a strike in the Amamiya Silk Mill in Kofu. Here, as in other strikes taking place, women workers protested against worsening conditions of employment, wage decreases and abuse from male employers.22 Such dissatisfaction stemmed principally from the exploitative way in which female labour was being harnessed, so that industries 'could be made competitive in world markets if they were able to utilise cheap female labour'23 in order to then generate capital for use in other investments. A growing awareness of unfair working conditions led to a proliferation of working women's trade unions, thus granting improved representation and a medium through which they could make themselves heard in the rapidly modernising and industrialising nation. A further medium through which women gained representation was via the Seito journal, first published in 1911 by a women's literary appreciation group.24 Though not originally intended as a political group, it soon developed into such due to increasing pressure from the government to terminate its publication; feared as a threat to patriarchy and Japanese tradition. Eventually, the journal and members of the 'Bluestockings' group, such as Hiratsuka Raicho, came to question women's issues such as maternity provision, equal pay, the family system and marriage. The group demonstrably made the personal political and engaged many middle class Japanese women with the issue of their subordination, causing the government to enforce its termination in 1916.25 Ultimately, it can be seen that a varied combination of changing social and political factors led to the development of feminism in Japan. Through increased political awareness and changing structures within the government, women were able to challenge their position for the first time. In the national struggle for increased economic and military power, gender identities were redefined in order to benefit the state. Women were arguably used in this process before being liberated themselves. Thereby, 'in the process of refusing subordinate status [as a nation], Japan constructed a new womanhood as a demonstration of its equality with the West and as a means of proving its civilised status'.26 It was this use of women for national goals that led to increased feelings of oppression and eventual action against exploitation and subordination, resulting in a strong feminist movement in Japan towards the end of the nineteenth century.","Victorian notions of women's madness were largely influenced by the Victorian Domestic Ideology; paramount during the nineteenth century in defining gender roles, responsibilities and acceptable behaviour. This primarily concerned the middle classes, establishing ideals of womanhood and femininity that were arguably damaging both to those directly targeted as well as other groups within Victorian society striving to emulate the middle class depiction of morality, sobriety and modesty. At the same time, the notion of madness as a disease was becoming increasingly apparent within Victorian society, further highlighting gender and class divisions through varying perceptions of madness and its origins. Whilst there were significant implications in terms of gender, this essay will mainly deal with the effects of social class on women's experience of madness in Victorian England. Firstly, it is important to understand that madness, in all its various forms, was seen as a female illness. Femininity became inextricably linked with hysteria, along with neurasthenia and anorexia nervosa. The main reasoning for this is to be found in the prevailing misogynistic understanding of female sexuality in nineteenth century England. Women were viewed as emotionally unstable as a result of the functions of their reproductive organs, the precise workings of which received limited medical investigation. The popular theory put forward by physicians claimed that madness was caused by 'the action of the reproductive organs on an unstable nervous system',1 showing how women were seen as intrinsically weak and therefore having a predisposition to insanity simply through being female. This was a reductionist view of madness, using biology to ascertain causes of the illness rather than analysing social and cultural factors relevant to the women affected. In this view, female biological functions were seen as the cause of madness, namely the onset of menstruation, pregnancy, childbirth and the menopause. From this it can be seen how madness was viewed in a feminine context, confining women to the subject of psychoanalysis throughout the nineteenth and twentieth centuries. The primary origins of women's madness were considered a result of their diseased sex organs, insatiable sexual desire, sedentary lifestyles and inferior minds.2 However, this could only be applied to the middle classes, confined to the domestic sphere and not engaging in paid employment. For middle class women, the omnipresent Victorian Domestic Ideology carried much weight with regard to their lifestyles and expected behaviour. Its restrictive nature left middle class women little freedom to choose their path in life or have any influence over their own decisions. Instead, they were expected to conform to social ideals of womanhood, in which they should be obedient, meek and, ultimately, subordinate to the male figures in their lives. This patriarchal system, both in the small scale of family life and the large scale of society as a whole, meant that women constantly had to conform to men's definitions and expectations, in which 'their [women's] sanity was often judged according to their compliance with middle-class standards of fashion'.3 In this way, masculinity was the accepted norm and femininity was a deviation from this. Thus, while masculinity was synonymous with sanity and emotional health, femininity was synonymous with insanity and corrupt mentality. Being the moral guardians of the domestic sphere, this proved a problem as it threatened the morality of the home and future generations, thus male figures of authority were forced to control female madness to safeguard the population. Ironically, it was not the female half of the population that was protected and helped, instead they were further oppressed. Particularly frustrating for middle class women, puberty was a time when they were presented with the clear distinction between themselves and their brothers. Women came to see that their confinement to the home was the accepted woman's role, depriving them of intellectual and cultural stimulation and highlighting how 'puberty... gives to woman the conviction of her dependence'.4 This may explain to some extent why puberty was seen as such a volatile period for women, many of whom would have resented the expectation that they would adopt the feminine ideal as their own. With limited outlets for creativity and economic independence, marriage was the only viable option for the majority of middle class women. However, far from being a safe and protected haven it was observed that it was chiefly 'those women who are married, not in paid employment, with children living at home, who are most at risk of being positioned as mad'.5 This serves to highlight the restriction of domesticity, channelling women's energy into servitude of the family whilst not recognising her intellectual and creative potential. This feeling of helplessness and isolation would have caused considerable distress to middle class women who aspired to achieve more, particularly those wealthy, literate and educated individuals who saw the possibility to engage in public life but were prevented from doing so on the basis of their sex.6 It could be argued that, under such conditions, madness was the middle class woman's protest against her subordination and oppression. By directly rejecting her gender- and class-specific roles, the madwoman embodied everything that confronted the Victorian Domestic Ideology serving to imprison her within the domestic sphere. The leading psychoanalyst of the time, Freud, believed it was the case that women were beginning to 'resist the social definitions that confine them to the doll's house of bourgeois femininity'.7 Such women were notable for their sexual deviance, in terms of promiscuity, bearing children out of wedlock, nymphomania, masturbation and homosexuality, all of which were thought to be signs of madness. Such practices received considerable attention, largely from men trying to suppress such behaviour for fear it would corrupt their womenfolk and disrupt society. These views relate to medieval associations of womanhood with witchcraft, in which increased sexuality was prominent in labelling women's deviant status. The link between women's heightened sexual desire and fornication with the Devil served to oppress female sexuality with the threat of accused witchcraft and being sentenced to death. During the nineteenth century this translated to the control of female sexuality with the threat of accused madness and subsequent institutionalisation or castration of the sex organs. Middle class women may have found madness a suitable outlet for their anger and frustration, with no other ways to communicate their distress other than attracting attention to their deteriorating health and radical behaviour. It would certainly have gained them the sympathy and attention from those around them, if not the change in their lifestyles that they truly desired.8 However, the extent to which such action was profitable is debatable, with evidence suggesting that 'hysteria was at best a private, ineffectual response to the frustrations of women's lives... [in which] the sympathy of the family, [and] the attention of the physician were slight in relation to its costs in powerlessness and silence'.9 The prevailing ideals of femininity would have appeared insurmountable for a middle class woman of the time, particularly those with professional aspirations of earning a living as artists or writers, in which cases it was thought that 'rebelliousness could produce nervous disorder',10 resulting in few women practicing their chosen career free from the associated stigma of madness. A different perspective on madness can be found within the working class, largely disregarded in nineteenth century psychiatry. A predominating view of the time was that 'insanity was an expensive disease'11, reserved for the affluent middle and upper classes where 'sickness and invalidism for women were almost in vogue'12, signifying plentiful leisure time and the fact that they did not have to enter into paid employment. Whilst middle and upper class women were retiring to bed with various nervous afflictions, their working class counterparts were busy working to support their families; hence they suffered the dual oppression of class and gender in the suppression of their activities and outlets.13 Having even less autonomy, working class women had more to protest against, added to the fact that the Victorian Domestic Ideology was being imposed upon them too in an attempt to universally enforce middle class standards. This created friction between the desired standards the working class strived to emulate and the reality of what they could achieve given their limited economic and social means, inevitably leading to feelings of frustration and helplessness. Whereas uncontrollable sexuality was a key feature of middle class madness, assumed moral weakness of the working class was the main reason given as the cause of their madness, asserting that 'Poverty was, after all, one of the moral causes of insanity'.14 Furthermore, different manifestations of insanity were identified between the classes, with hysteria being largely associated with the working class whilst neurasthenia was more commonly identified with the middle and upper classes. Though both showed similar symptoms, the latter was considered a 'more prestigious and attractive form of female nervousness than hysteria'15, thus reserved for the elite members of society. The treatment of diagnosed madness available to middle and upper class women would have further inhibited their freedom and creativity, usually in the form of the rest cure.16 In such a practice, women were prevented from any intellectual or moral growth, being inescapably confined to the domestic environment and prevented from exercising any pursuits of their own. Whilst there may have existed the notion that 'Sickness presents a tempting escape from the contingency of the feminine role'17, in the sense that women were temporarily freed of household burdens, it is probably more accurate to see such treatment as exceedingly damaging to their existing oppressed positions. It is contestable as to whether many of the women diagnosed as mad were genuinely of ill mental health or whether this was a simple yet effective way to repress those not conforming to social ideals constructed by men. Coincidentally, at the same time that middle class women began demanding more rights and opportunities, 'the female nervous disorders of anorexia nervosa, hysteria, and neurasthenia became epidemic'.18 This emphasises the notion of patriarchal control over the subordinate members of society, namely women, through the legitimacy of professional medicalisation. In contrast, working class women could not hope to receive what they would have perceived as a reward in such treatment, instead they would have faced incarceration in public county asylums. They too were shunned from society rather than treated, further adding to their humiliation when 'visits to the asylum, to view the mad, became a routine part of social life'.19 Conversely, their isolation was a public matter, unlike the private affair of the middle and upper classes, with their literary depiction of the 'madwoman in the attic'.20 So marked was this difference in institutionalisation that by the end of the nineteenth century, ninety one per cent of all the institutionalised insane were paupers.21 Again, middle class ideals were incorporated into working class life, present within the asylum itself. The design of these institutions was seen as 'inevitably reproducing structures of class and gender'22, thus aiming to recreate social and moral norms in an artificial environment through appropriate segregation. Working class madwomen suffered extreme stigma under the Poor Law in county asylums, whilst the middle and upper class women were more often, when institutionalised, put into private asylums, thus accentuating how the 'affluent and middling groups had very different experiences... [the] divide between private and state institutions reflected... a basic class division'.23 Ultimately, it is apparent that both perceived causes and treatments of madness differed between the working and middle classes, affecting the lives of many Victorian women, as well as re-emphasising class divisions in society. Whereas working class women may have used madness as a protest against their economical oppression, middle and upper class women were more likely to use it to protest against the social ideals forced upon them, which also permeated through to the working class as part of the Victorian Domestic Ideology. It can be concluded that madness was gendered in class specific ways, with distinctly separate discourses on the subject regarding each class.",False
10,"On Friday afternoon, the 26 th of November, I conducted my ethnographic observation in an urban setting in the centre of Leamington. My first intention was to observe activity and interaction in a second hand book shop, paying special attention to variables such as gender and age, and their potential relation to areas of interest. This, as we will see, proved to be difficult. Also, I intended to record noteworthy or unusual behaviour that might be caused by social circumstances particular to the shop. Around 1.45 pm I entered the shop, and at that moment there were about three customers around, apart from the two shopkeepers. The shop, called Portland Books has an authentic and old-fashioned atmosphere, with its wooden floor, narrow space and antique, hardback books on old wooden shelves. Besides old and new books, the shop also sells postcards and old placard photos. The counter is situated downstairs, in the middle of the shop. A small staircase leads to a first floor, where a small room with academic literature and a big room with novels and poetry can be found. I decided to stay upstairs for a while. In the big room, there were two customers, besides me and a friend that I had brought along. The upstairs area creates a lesser degree of constraint, as there is no shopkeeper around and the street can not be seen. This might create a more closed-off environment. Still, the customers whispered, as would have been the case in a library. After a while, the low number of customers made it pointless to stay in the shop for very long because of the absence of interaction to observe. At 2.15 pm I decided to change my approach and find another shop with more people in it. I found a better case study in another book store called Waterstone's, a bigger shop that sells more mainstream literature and does not offer second-hand books. Additionally, Waterstone's is situation more centrally at the entrance of a shopping centre and near to the Parade. Here I spent around two hours observing the shopping public. Although also not very busy, Waterstone's is a quite big store, like the first shop with two floors, and there was interesting material for observation. Most of the interaction was situated around the counter, where there was constant conversation between customers and one of the many members of the shop's personnel. The atmosphere in the shop is of the style of a 'walk-in shopping centre'. Many people come in to look around briefly, and nearly everybody does this by themselves. The counter downstairs is placed in the middle of the shop, presumably to make it easier for the personnel to make sure no theft takes place. Classical music sounds throughout the store; not the type of sensitive piano music, but rather dynamic orchestra music. However, the music is played quite softly, so that I observed no noticeable response to the music of customers or personnel. A large stairway leads the way the store's first floor. This floor is even larger than the ground floor. Differently from the first shop I visited, this floor also has a counter which is located centrally. Although there appears to be less constraint (which might be due to the size of the store, and to the music, creating a more 'informal' atmosphere) customers among each other still spoke in a lowered tone. Again, only around the counter people spoke aloud. It is striking that, although there was a considerable number of people on this floor (around 30 of more), by far the greatest number of them seemed to have come alone, or at least went around very individually. The customers also spread out quite evenly over the available sections of books (categorised in sections such as 'history', 'pets', 'linguistics', 'biography' etc. ), most of the time seemingly avoiding sections where other people were looking for books. Sometimes two people entered the shop together, although throughout the whole period of observation I did not see a larger group of customers entering the shop. Upon entering, a pair usually split to find the books of their own interest, and hardly interacted until the shop visit was ending. As such, it can be said that I observed a fairly static social atmosphere in the shop. It appeared that people were trying to pretend that others were not around them, or influencing their preferred method of going around the shop. When I observed this aspect, I decided to test to what extent a reaction could be caused among customers by intruding their private sphere. I went to stand next to a woman in a corner of the shop, to see if this would disturb her private sphere, and thus change her course of activity, or behaviour. But, although I stood next to her for a while (pretending to look for books), and took books from the shelf she was looking at, she did not react to my presence at all. Even if my presence did irritate her or created some form of discomfort, she was able to conceal this. In another instance, I approached a shelf where two people stood in search for books, without speaking. Although I stood behind them for a while, looking at the shelves they stood in front of, they did not react to this. Whether it happens consciously or not, most of the customers conform to seclusion of each individual. I would even argue that a quiet consensus exists to uphold a sort of polite distance, seemingly dependent on consciously avoiding each other, after which the customer does not have to assess constantly the situation in which it is appropriate to react to others. This, of course, creates the somewhat silent and static atmosphere of which I spoke earlier. About the type of people, I can say the following. On the first floor, there was a majority of women among the customers. Most of them I guessed were over forty years of age. Interestingly, between the two floors there seemed to be something of a difference between the types of customers. Generally, customers downstairs were younger of age, and they spent less time in the shop. Moreover, the customers downstairs seemed to be more reactive to other customers. They seemed, perhaps, less accustomed to the unwritten rules of the 'bookshop etiquette'. What underlines this mode of thinking, also, is the method of classifying the books it offers. Children's books, bestsellers, as well as popular scientific books, are all on offer on the ground floor, while some books were displayed on tables on the ground floor, as well as on shelves at the top floor. Although at first it may appear inefficient and chaotic to place one book in two places in the store, we must look for an explanation to it. It may be presumed that Waterstone's has deliberately implemented a categorisation of books that suits its customers and therefore works effectively as the most profitable way of advertising its books. Whereas regular bookshop visitors have certain sections of books they might be most interested in (particularly specialist literature qualifies for this, as people interested in one type of book are likely to buy more books of that type), others might be on the outlook for a particular book they have been recommended, or in search of a suitable gift. An average customer at Waterstone's is therefore likely to come to the store with an idea of what to buy. The store then tries to cater for any type of customer, so that they might easily find what they are looking for if they have no idea what to buy, or in the second case, may look around undisturbed for books of their interest. It is furthermore interesting to note that the store succeeds to a high degree to create an optimal consumer atmosphere. There is little talking, people who intend to stay for a longer period easily find their way in search for their category of books, and the first floor, by concealing the street and placing the salon tables and couch, create an atmosphere where people can stay longer without being disturbed. It is likely that the customers recognise and reinforce aspects of this way of thinking. This is shown by the fact that people search for books by themselves, even if they enter the shop as part of a group. Other evidence is the maintenance of the 'polite distance' of which I spoke earlier. It is evident that consumerism decides to a high degree what forms of social interaction are accepted, and which not. In this, there is no great difference of interest between personnel and customer. The nineteenth century was, until recently, predominantly seen as a century of rapid industrialisation which set the stage for profound social change. This way of thinking about the birth of a capitalist society, however, fails to recognise the two sides to the changes that took place. After all, not only was there rapid increase in supply, also was demand of consumer goods ever increasing (McCracken 1990:5). Earlier examples of cultural consumerism in various forms of frequent local gift exchange could already be found in the late eighteenth-century (Finn 2000:143). However, consumerism in the Victorian period took a different and much more broad-based shape through various agents, of which most importantly department stores and an 'unparalleled advertising craze' (Loeb 1994:5). And while men in the eighteenth-century were themselves participating consumers in an informal, pre-capitalist local economy, in the last decades of the nineteenth century mass advertisement of an impersonal character began to target almost only women as their potential customers (Loeb 1994:8) in a highly formalised mass commercial economy. The departure from a semi-formal, local economy to this increasingly enlarged scale of consumption of various kinds of consumer goods did not occur without protest or debate (Rappaport 1996). By the 1850s, the middle class had come to project a self-image of a virtuous class, with an identity based upon plainness and self-discipline. Especially into the nineteenth century, as the middle class began to emerge as a distinctive class, emphasised and luxurious clothing were replaced by more austere and sober dress (Finn 2000). Indeed sobriety and democratic value formed part of its claim to political power. But after the great wave of revolutions, from which middle class all over Europe once again arose reaffirmed in their political position, these democratic values came to be reinterpreted (Loeb 1994), slowly replacing sobriety for the desired acquisition of an 'ornamental delirium' for members of the middle class and the world that surrounded them (McCracken 1990:24). Perhaps the Great Exhibition of 1851 may be said to have served as a starting point of the spread of consumerism. Upon its opening in the Crystal Palace in London, it attracted a recorded total of six million visitors, equal to one-third of the country's population (Weintraub 2004). Moreover, for the first time, 'ordinary commodities [were turned] into cultural signifiers' (Rappaport 1996:65), affirming the results of a rational and sober life. Also William Whiteley, owner of London's first department store, had seen the Crystal Palace, and desired to open a store that made available those goods that remained unattainable at the Great Exhibition (Rappaport 1996). The decades after 1851 then set the stage for an ambiguous public debate among the middle classes, in which critics of new department stores such as Whiteley's expressed 'fears about the morality of consumption' (Rappaport 1996:66), while on the other side a nearly fantastic range of new goods and unprecedented luxury and comfort triggered a great appraisal of 'material democracy' (Loeb 1994:7). McCracken has argued that this middle class tendency toward mass consumerism represented a 'fundamental shift in the culture of the [...] modern world' (McCracken 1990:3) in which the middle class increasingly defined itself in material terms, while within its body others based their elitist or democratic values upon a paraphernalia of objects symbolically highly charged. Initially, the growing number of middle class women engaging themselves in the pastime of shopping was a sight that conflicted with the traditional Victorian 'angel in the household'. While some critics despised the idea of women drinking liquor during such shopping trips to the department store, others went even further to compare these consuming women with prostitutes (Finn 1998:36). In this criticism we may find a fear that was both classed and gendered; after all, prostitutes represented a disrupting female working class challenge to middle class Victorian idea of the gentile, meek and submissive wife. Shopping, however, remained viewed and actively gendered as a feminine task (Loeb 1994:12, Rappaport 1996:67) and the role of middle class women became increasingly an ornamental one. In the streets of London, 'Female shoppers, like the glittering objects on display, became a central part of the urban spectacle' (Rappaport 1996:80), allowing them to become walking statements, sometimes of middle class democratising success at large, other times drawing from an individualistic elitist identity (McCracken 1990). Advertisement did not play the least role in this newly constructed female middle class identity. From an early point, advertisers came to accept that 'the hand that rocks the cradle is the hand that buys the boots' (Loeb 1994:8). Throughout the second half of the nineteenth century, techniques of persuasion became increasingly refined (McCracken 1990:29), shifting the portrayal of the household mistress in a shape of the 'perfect lady' through images in an often historical, near-mythological style (Loeb 1994). Such portrayal of rather powerful, controlling women attempted to equal the act of shopping to having 'the opportunity to', as a critic wrote disdainfully, '""luxuriate"" in a deep and intense ""sense of power""' (Rappaport 1996:76). Growing consumerism, besides reforming the gender ideal, was also championed as a new definition of class difference. As goods on sale in the department store increasingly became 'material symbols', which set prescriptions of how the middle class was to dress, what material conveniences they could posses, which, in turn said something about 'how they should spend their leisure time' (McCracken 1990:27). Businesses and advertisers were eager to support this civilising, 'rational' view on middle class consumption. Their profits came mostly from the middle class, as a journal of the trade wrote: 'The buyers of the world are the great MIDDLE CLASS PEOPLE [sic]' (Loeb 1994:8). Thus, the Army and Navy Co-operative Society, a private department store, instructed the doorkeepers to keep servants and messengers out to avoid class mingling and provide safety for the female audience (Rappaport 1996). On the other hand, products that served to define middle class status were sold as necessities through the creation of great fear. Parents read in advertisements that without Mellin's Food, their child '[would] not last long' (Loeb 1994:14), while ""Raw Milk"" was advertised as so dangerous, that 'those who allow it to be served to their families take a great responsibility' (Loeb 1994:14). The arousal of fear can be said to have been used in advertisement and by the department stores in reaffirming middle class values through consumption, for which shopping could comfortably be conducted in the safe, exclusive environment of the department store. As mentioned earlier, this image of the safe middle class 'Universal Provider' taking care of you 'from the cradle to the grave' (Rappaport 1996:73) was continually criticised. Opponents of Whiteley's business tried to 'expose' the degenerate consumption behaviours of his customers (Rappaport: 1996:61), while hyperboloid advertisement was often dismissed as 'puffery' by its audience (Loeb 1994:5). Moreover, small shopkeepers tried to stand in the way of business men such as Whiteley whenever they could (Rappaport 1996). However, such opposition was overcome in various ways. First of all, great businesses such as Whiteley's received continual impulses through the middle class desire to attain novelties aroused by mass advertisement in periodicals of all kinds. Even as the country slid into an economical recession in the 1870s, the middle class 'seemed determined to perpetuate an invigorating sense of material possibility' (Loeb 1994:3). Objections to a valueless, banal middle class were diverted by the emergence of the eccentric, elitist model of the dandy, who, by claims of superior aesthetic standards created a new elite from within the middle class (McCracken 1990). Retailers with shops around department stores came to take these stores for a fact. It was soon found that, if the neighbourhood cooperated with the department store, markets expanded handsomely, also for smaller retailers (Rappaport 1996). Eventually, by the end of the nineteenth century all criticism had been channelled into the new consumerist middle class model. Except, perhaps, for some moral criticism, which was labelled as old-fashioned (Rappaport 1996). Consumption in the nineteenth century was nothing new (McCracken 1990). However, its extent and the way in which it changed the middle class was an enormous step further toward the modern capitalist model as we know it today. Where before the Victorian era consumption remained limited to a small number, it now took a flight to mass proportions. The way in which the public came to see the department store was transformed 'from ""the halls of temptation"" into a recognised and cherished ""social sight""' (Rappaport 1996:61). Whereas social changes and higher living standards were previously seen as the result of industrialisation and the great increase of supply of a wide range of goods, the driving forces on the 'demand side' are now becoming increasingly appreciated and researched (McCracken 1990). One of the reasons for this enormous increase in consumption may be found in the fact that new markets were sought to create a demand for this surplus of goods (Rappaport 1996:81). However, this does not tell the entire story. The general enthusiasm with which new material achievements were hailed had a profound effect on society and the way in which the middle class defined itself as part of it. Also men, and more particularly women within the middle class were both changed and changed their identities consciously with increasing consumerism. As 'shopping' was essentially gendered as a female activity in the second half of the nineteenth century, changes of gender perception were most acutely visible with women. Women were now encouraged to beautify themselves and surround their middle class homes with a paraphernalia of gentility.","The first meeting of Marx and Engels in Paris in 1844 marked the starting point of a legendary friendship and cooperation of two thinkers who left an enormous mark on political development in the twentieth century and whose influence, in the words of a critic, can only be compared to that of Jesus or Mohammed. V.I. Lenin, 'Karl Marx', in Marx - Engels - Marxism (Moscow, 1951), p. 17. Peter Singer, Marx (Rotterdam, 1999), p. 10. As the philosophes of the enlightenment had posed a theory of a developing and progressing world, Marx and Engels concerned themselves with applying this model of development to society. Marx placed all history in terms of classes struggling through their relation to material welfare and the means of production. As atheistic followers of Hegel, Marx and Engels thus came to believe that 'the struggle against existing wrong and prevalent evil, is also rooted in the universal law of eternal development'. V.I. Lenin, 'Frederick Engels' in Marx - Engels - Marxism, p. 58. Marx and Engels lived at a time in which rapid industrialisation brought about exceeding disparities between the property-owning class (the bourgeoisie) and the working class (the proletariat). Believing capitalism to be only another step toward a society without class antagonisms, Marx turned to studying history and economics, while Engels in 1845 published The Condition of the Working Class in England. In this work, Engels drew from direct experience and a wide range of sources, arguing that the working class was becoming increasingly conscious of its place and relation to the bourgeoisie, which would in the end inevitably lead to a proletarian revolution. Marx and Engels were deeply impressed when, in 1844, factory workers revolted in Silesia and Bohemia. They observed unrest all over Europe, and thus became convinced of the revolutionary potential of the proletariat; a potential which would eventually carry humanity into the next stage of socialism. Lenin, 'Frederick Engels', p. 60. Victor Kiernan, 'History', in David McLellan (ed. ), Marx: The First Hundred Years (Oxford, 1983), p. 60. From circumstance and experience, Marx and Engels were not so much concerned with democracy, but rather with the greater concept of freedom and creating a freer society, in which, they argued, the state would cease to be necessary. Parliamentary democracy, in the eyes of Marx and Engel, was nothing more than the crystallisation of the contradictory interests between the bourgeoisie and the proletariat, and could not be different. Moreover, by separating the government from civil society, 'the bourgeois state and its attendant ideas of political rights and common national interests functioned, like religion, as an opium, an illusory compensation and thus a support for the injustices of civil society'. A radical change was needed for the advance at first of the proletariat and eventually of society as a whole. In their own words 'Not criticism but revolution is the driving force of history'. Singer, Marx, p. 104. David McLellan, 'Politics', in David McLellan (ed. ), Marx: The First Hundred Years (Oxford, 1983), pp. 145-46. Kiernan, 'History', p. 62. Marx and Engels became convinced that they had discovered a scientific model of society and its inevitable future. Marx's drive to act upon theory soon brought him into international communist circles. After heated debate at a congress in London in 1847, Marx and Engels were commissioned to write a political programme for the Communist League. Its result, the Communist Manifesto, can be regarded as Marx's and Engels' most outspoken vision on class struggle and the predestined faith of the proletariat. 'The proletariat', the Communist Manifesto stated confidently, 'goes through various stages of development'. Then, the manifesto goes on to speak in deterministic terms that the proletarian 'mission is to destroy all previous securities for, and insurances of, individual property'. Singer, Marx, pp. 14-15. Karl Marx and Friedrich Engels, The Communist Manifesto, cited in University of Warwick Insite, Department of History (20 November 2003),  URL  (21 December 2004). Ibid. If this scenario were to be the inevitable course of events, the logical question would be: why? Analysing history, Marx and Engels had discovered a continuous struggle for power between a powerful minority and a weak majority. They noted that 'All previous historical movements were movements of minorities, or in the interest of minorities'. However, it seemed that the proletariat would be able to break through this tendency. 'The proletarian movement is the self-conscious, independent movement of the immense majority, in the interest of the immense majority'. In this sense, it can be argued that for Marx and Engels, a primitive idea of democratic, or majority, rights served to justify a complex social theory of inevitable revolutionary struggle. Convinced about the power and improvement value of the theory of class struggle, 'His [Engels'] and Marx's historical vision was optimistic about the future, tragic when it turned to the past, as a record so dreadfully bad that any sacrifice was worth while to get away from it'. Ibid. Kiernan, 'History', p. 72. When Lenin became leader of the Bolsheviks in the early twentieth century, a debate among Marxists had been around for nearly fifteen years concerning the best way to bring about communism. One section of the Second International of 1889, under the leadership of Bernstein, had come to embrace the idea of social democracy, and of participation in parliamentary politics. Where Marx had been careful in his wording, or more abstract in his discussion of the democracy question, Lenin was more articulate in his discussion of the matter. He briefly defined the state as 'a special organization of force: it is an organization of violence for the suppression of some class'. Therefore, he argued, a revolution was the necessary agent for the transition from capitalism to socialism. McLellan, 'Politics', p. 167. In 1905 Lenin wrote Two Tactics, an article discussing the political direction Russia should take after the revolution. In this article he advocated a 'revolutionary democratic dictatorship of the proletariat and peasantry', a term that Marx had used only once, in a letter to a friend. What contrasted the Bolsheviks from the anarchists, Lenin argued, was that Bolsheviks thought it necessary to establish a transitory government to place the proletariat firm in the seat of power. However, months before the October revolution, in April 1917, he wrote that this would be 'a state without a standing army, without a police opposed to the people, without an officialdom placed above the people', based on Marx's example of his view of the Paris Commune. Eventually, in Engels' words, the government would slowly 'wither away' because it would become unnecessary. Ibid, p. 161. Ibid, pp. 154-55. V.I. Lenin, 'Letters on Tactics' in Marx - Engels - Marxism, p. 58. McLellan, 'Politics', p. 168 Some of Lenin's writing, though, already revealed some of the authoritarian traces that were to characterise the Bolsheviks' policy and the Soviet Union's later political development. Convinced, like Marx, of the inevitability of a proletarian revolution, he remarked: 'I said that there can be no government (apart from a bourgeois government) in Russia other than a government of the Soviets of Workers', Agricultural Labourers', Soldiers' and Peasants' Deputies'. Lenin, 'Letters on Tactics', pp. 389-90. By 1917, Lenin observed that the key conditions were fulfilled in Russia for a proletarian revolution to take place. An imperialist war was raging over Europe, according to Marxist theory calling in the culmination of capitalism. Conditions in Russia, he argued, were peculiar, as the country had already seen a revolution in February 1917. 'We have existing side by side, together, simultaneously, both the rule of the bourgeoisie [...] and a revolutionary-democratic dictatorship of the proletariat and the peasantry, which is voluntarily ceding power to the bourgeoisie, voluntarily transforming itself into an appendage of the bourgeoisie'. According to Lenin, the masses had 'been confused, led astray and deceived' by the temporary government of Lvov and Kerensky. He adopted Trotsky's idea of 'permanent revolution', arguing that the proletariat could not stop after overthrowing the bourgeois government. By not completing the dictatorship of the proletariat, the government itself had gone over to the side of the bourgeoisie. Columbia Encyclopedia, 'Vladimir Ilych Lenin: Theoretician and Revolutionary', Encyclopedia.com (2004),  URL  (27 December 2004). Lenin, 'Letters on Tactics', p. 386. V.I. Lenin, 'The Task of the Proletariat in Our Revolution' in Marx - Engels - Marxism, p. 380. In practice, much of Lenin's writing at the dawn of the October revolution appears to excuse the Bolsheviks' grasp for power and subsequent policy. Eventually, Bolshevik rule directly after the revolution proved to be highly undemocratic. In 1918, all political parties that did not support the Bolsheviks were systematically suppressed; while a temporary oligarchic, highly centralised government was formed, taking in members by co-option rather than by democratic elections. Rosa Luxemburg was among the prominent critics of Lenin's post-revolution, oppressive policy. 'The negative, the tearing down, can be decreed; the building up, the positive cannot', she argued. According to her, Lenin was standing in the way of a democratic learning process of the proletariat. 'The broadest democracy was needed', otherwise 'socialism will be decreed from behind a few official desks by a dozen intellectuals'. Although Lenin argued that such policy was needed in 'defence against counterrevolution and the actual elimination of everything that contradicted the sovereignty of the people', this, according to Luxemburg, was 'not the dictatorship of the proletariat, [...] but only the dictatorship of a handful of politicians'. McLellan, 'Politics', p. 170. Ibid, p. 163. Rosa Luxemburg, The Russian Revolution and Leninism or Marxism? , cited in University of Warwick Insite, Department of History (20 November 2003),  URL  (27 December 2004). V.I. Lenin, 'Two Tactics of Social Democracy in the Democratic Revolution' in Marx - Engels - Marxism, p. 177. Rosa Luxemburg, The Russian Revolution. By 1921, Lenin described the Soviet Union as 'a workers' state with bureaucratic distortion'. Trotsky actually became regretful, stating in a meeting that 'things are heading towards ruin'. Stalin, however, asserted in 1918 that at the time, the 'Bolsheviks did not doubt that bourgeois parliamentarism and the bourgeois-democratic republic represented a past stage of the revolution'. McLellan, 'Politics', p. 171. Joseph Stalin, The October Revolution (London, 1936), p. 168. Stalin, The October Revolution, p. 22. It is hard to place Stalin's policy in an unmitigated Marxist light. Despite its rhetoric, the Soviet Union of Stalin appeared to many something other than socialism. Theory was increasingly not matching policy. The state did not wither away, instead, a status quo of power was developing; one in which all lines came together in Moscow. An elite of scientifically based professional revolutionaries would 'show the way, formulating the program and policies, educating the people, and working out the strategy and tactics'. The character of the government had changed since the October revolution. Stalin's biographer, Deutscher, wrote: 'It has assumed office as a government of the people, by the people and for the people [...][now,] it ceases to be government by the people'. Indeed, it is arguable that the rapid industrialisation and agricultural reforms represented some elevated aspiration of the proletariat and peasants, however, the price of these reforms (The 1936 Great Terror and show trials) was disproportionate and tragic. Moreover, they revealed a highly undemocratic and unrepresentative government that stood separate from the masses. McLellan, 'Politics', p.173. Albert Weisbord, 'Bolshevism, Fraudulent Practice Of Democratic Centralism' cited in Weisbord.org (November/December 1976),  URL  (28 December 2004). I. Deutscher, Stalin: A Political Biography (Oxford, 1950), p. 175. McLellan, 'Politics', p. 173. The approximately hundred-year period over which the intellectual and political activity of Marx, Engels, Lenin and Stalin spread has seen the emergence of an immense social and political experiment of revolution and the move toward a communist state. The concept of democracy has over this time been much debated and scrutinised in order to deprive it of its 'bourgeois' elements. However, in the theories of Marx and Engels, there has been too little space left open for a pluralistic approach to state organisation. Likewise, Lenin and Stalin never achieved the 'withering away of the state' into a 'revolutionary-democratic dictatorship of the proletariat' upon the gap that fell after the October revolution of 1917. Communism, or some pure form of democracy within society's organisation has never been achieved. Singer, Marx, p. 103.",True
11,"The first meeting of Marx and Engels in Paris in 1844 marked the starting point of a legendary friendship and cooperation of two thinkers who left an enormous mark on political development in the twentieth century and whose influence, in the words of a critic, can only be compared to that of Jesus or Mohammed. V.I. Lenin, 'Karl Marx', in Marx - Engels - Marxism (Moscow, 1951), p. 17. Peter Singer, Marx (Rotterdam, 1999), p. 10. As the philosophes of the enlightenment had posed a theory of a developing and progressing world, Marx and Engels concerned themselves with applying this model of development to society. Marx placed all history in terms of classes struggling through their relation to material welfare and the means of production. As atheistic followers of Hegel, Marx and Engels thus came to believe that 'the struggle against existing wrong and prevalent evil, is also rooted in the universal law of eternal development'. V.I. Lenin, 'Frederick Engels' in Marx - Engels - Marxism, p. 58. Marx and Engels lived at a time in which rapid industrialisation brought about exceeding disparities between the property-owning class (the bourgeoisie) and the working class (the proletariat). Believing capitalism to be only another step toward a society without class antagonisms, Marx turned to studying history and economics, while Engels in 1845 published The Condition of the Working Class in England. In this work, Engels drew from direct experience and a wide range of sources, arguing that the working class was becoming increasingly conscious of its place and relation to the bourgeoisie, which would in the end inevitably lead to a proletarian revolution. Marx and Engels were deeply impressed when, in 1844, factory workers revolted in Silesia and Bohemia. They observed unrest all over Europe, and thus became convinced of the revolutionary potential of the proletariat; a potential which would eventually carry humanity into the next stage of socialism. Lenin, 'Frederick Engels', p. 60. Victor Kiernan, 'History', in David McLellan (ed. ), Marx: The First Hundred Years (Oxford, 1983), p. 60. From circumstance and experience, Marx and Engels were not so much concerned with democracy, but rather with the greater concept of freedom and creating a freer society, in which, they argued, the state would cease to be necessary. Parliamentary democracy, in the eyes of Marx and Engel, was nothing more than the crystallisation of the contradictory interests between the bourgeoisie and the proletariat, and could not be different. Moreover, by separating the government from civil society, 'the bourgeois state and its attendant ideas of political rights and common national interests functioned, like religion, as an opium, an illusory compensation and thus a support for the injustices of civil society'. A radical change was needed for the advance at first of the proletariat and eventually of society as a whole. In their own words 'Not criticism but revolution is the driving force of history'. Singer, Marx, p. 104. David McLellan, 'Politics', in David McLellan (ed. ), Marx: The First Hundred Years (Oxford, 1983), pp. 145-46. Kiernan, 'History', p. 62. Marx and Engels became convinced that they had discovered a scientific model of society and its inevitable future. Marx's drive to act upon theory soon brought him into international communist circles. After heated debate at a congress in London in 1847, Marx and Engels were commissioned to write a political programme for the Communist League. Its result, the Communist Manifesto, can be regarded as Marx's and Engels' most outspoken vision on class struggle and the predestined faith of the proletariat. 'The proletariat', the Communist Manifesto stated confidently, 'goes through various stages of development'. Then, the manifesto goes on to speak in deterministic terms that the proletarian 'mission is to destroy all previous securities for, and insurances of, individual property'. Singer, Marx, pp. 14-15. Karl Marx and Friedrich Engels, The Communist Manifesto, cited in University of Warwick Insite, Department of History (20 November 2003),  URL  (21 December 2004). Ibid. If this scenario were to be the inevitable course of events, the logical question would be: why? Analysing history, Marx and Engels had discovered a continuous struggle for power between a powerful minority and a weak majority. They noted that 'All previous historical movements were movements of minorities, or in the interest of minorities'. However, it seemed that the proletariat would be able to break through this tendency. 'The proletarian movement is the self-conscious, independent movement of the immense majority, in the interest of the immense majority'. In this sense, it can be argued that for Marx and Engels, a primitive idea of democratic, or majority, rights served to justify a complex social theory of inevitable revolutionary struggle. Convinced about the power and improvement value of the theory of class struggle, 'His [Engels'] and Marx's historical vision was optimistic about the future, tragic when it turned to the past, as a record so dreadfully bad that any sacrifice was worth while to get away from it'. Ibid. Kiernan, 'History', p. 72. When Lenin became leader of the Bolsheviks in the early twentieth century, a debate among Marxists had been around for nearly fifteen years concerning the best way to bring about communism. One section of the Second International of 1889, under the leadership of Bernstein, had come to embrace the idea of social democracy, and of participation in parliamentary politics. Where Marx had been careful in his wording, or more abstract in his discussion of the democracy question, Lenin was more articulate in his discussion of the matter. He briefly defined the state as 'a special organization of force: it is an organization of violence for the suppression of some class'. Therefore, he argued, a revolution was the necessary agent for the transition from capitalism to socialism. McLellan, 'Politics', p. 167. In 1905 Lenin wrote Two Tactics, an article discussing the political direction Russia should take after the revolution. In this article he advocated a 'revolutionary democratic dictatorship of the proletariat and peasantry', a term that Marx had used only once, in a letter to a friend. What contrasted the Bolsheviks from the anarchists, Lenin argued, was that Bolsheviks thought it necessary to establish a transitory government to place the proletariat firm in the seat of power. However, months before the October revolution, in April 1917, he wrote that this would be 'a state without a standing army, without a police opposed to the people, without an officialdom placed above the people', based on Marx's example of his view of the Paris Commune. Eventually, in Engels' words, the government would slowly 'wither away' because it would become unnecessary. Ibid, p. 161. Ibid, pp. 154-55. V.I. Lenin, 'Letters on Tactics' in Marx - Engels - Marxism, p. 58. McLellan, 'Politics', p. 168 Some of Lenin's writing, though, already revealed some of the authoritarian traces that were to characterise the Bolsheviks' policy and the Soviet Union's later political development. Convinced, like Marx, of the inevitability of a proletarian revolution, he remarked: 'I said that there can be no government (apart from a bourgeois government) in Russia other than a government of the Soviets of Workers', Agricultural Labourers', Soldiers' and Peasants' Deputies'. Lenin, 'Letters on Tactics', pp. 389-90. By 1917, Lenin observed that the key conditions were fulfilled in Russia for a proletarian revolution to take place. An imperialist war was raging over Europe, according to Marxist theory calling in the culmination of capitalism. Conditions in Russia, he argued, were peculiar, as the country had already seen a revolution in February 1917. 'We have existing side by side, together, simultaneously, both the rule of the bourgeoisie [...] and a revolutionary-democratic dictatorship of the proletariat and the peasantry, which is voluntarily ceding power to the bourgeoisie, voluntarily transforming itself into an appendage of the bourgeoisie'. According to Lenin, the masses had 'been confused, led astray and deceived' by the temporary government of Lvov and Kerensky. He adopted Trotsky's idea of 'permanent revolution', arguing that the proletariat could not stop after overthrowing the bourgeois government. By not completing the dictatorship of the proletariat, the government itself had gone over to the side of the bourgeoisie. Columbia Encyclopedia, 'Vladimir Ilych Lenin: Theoretician and Revolutionary', Encyclopedia.com (2004),  URL  (27 December 2004). Lenin, 'Letters on Tactics', p. 386. V.I. Lenin, 'The Task of the Proletariat in Our Revolution' in Marx - Engels - Marxism, p. 380. In practice, much of Lenin's writing at the dawn of the October revolution appears to excuse the Bolsheviks' grasp for power and subsequent policy. Eventually, Bolshevik rule directly after the revolution proved to be highly undemocratic. In 1918, all political parties that did not support the Bolsheviks were systematically suppressed; while a temporary oligarchic, highly centralised government was formed, taking in members by co-option rather than by democratic elections. Rosa Luxemburg was among the prominent critics of Lenin's post-revolution, oppressive policy. 'The negative, the tearing down, can be decreed; the building up, the positive cannot', she argued. According to her, Lenin was standing in the way of a democratic learning process of the proletariat. 'The broadest democracy was needed', otherwise 'socialism will be decreed from behind a few official desks by a dozen intellectuals'. Although Lenin argued that such policy was needed in 'defence against counterrevolution and the actual elimination of everything that contradicted the sovereignty of the people', this, according to Luxemburg, was 'not the dictatorship of the proletariat, [...] but only the dictatorship of a handful of politicians'. McLellan, 'Politics', p. 170. Ibid, p. 163. Rosa Luxemburg, The Russian Revolution and Leninism or Marxism? , cited in University of Warwick Insite, Department of History (20 November 2003),  URL  (27 December 2004). V.I. Lenin, 'Two Tactics of Social Democracy in the Democratic Revolution' in Marx - Engels - Marxism, p. 177. Rosa Luxemburg, The Russian Revolution. By 1921, Lenin described the Soviet Union as 'a workers' state with bureaucratic distortion'. Trotsky actually became regretful, stating in a meeting that 'things are heading towards ruin'. Stalin, however, asserted in 1918 that at the time, the 'Bolsheviks did not doubt that bourgeois parliamentarism and the bourgeois-democratic republic represented a past stage of the revolution'. McLellan, 'Politics', p. 171. Joseph Stalin, The October Revolution (London, 1936), p. 168. Stalin, The October Revolution, p. 22. It is hard to place Stalin's policy in an unmitigated Marxist light. Despite its rhetoric, the Soviet Union of Stalin appeared to many something other than socialism. Theory was increasingly not matching policy. The state did not wither away, instead, a status quo of power was developing; one in which all lines came together in Moscow. An elite of scientifically based professional revolutionaries would 'show the way, formulating the program and policies, educating the people, and working out the strategy and tactics'. The character of the government had changed since the October revolution. Stalin's biographer, Deutscher, wrote: 'It has assumed office as a government of the people, by the people and for the people [...][now,] it ceases to be government by the people'. Indeed, it is arguable that the rapid industrialisation and agricultural reforms represented some elevated aspiration of the proletariat and peasants, however, the price of these reforms (The 1936 Great Terror and show trials) was disproportionate and tragic. Moreover, they revealed a highly undemocratic and unrepresentative government that stood separate from the masses. McLellan, 'Politics', p.173. Albert Weisbord, 'Bolshevism, Fraudulent Practice Of Democratic Centralism' cited in Weisbord.org (November/December 1976),  URL  (28 December 2004). I. Deutscher, Stalin: A Political Biography (Oxford, 1950), p. 175. McLellan, 'Politics', p. 173. The approximately hundred-year period over which the intellectual and political activity of Marx, Engels, Lenin and Stalin spread has seen the emergence of an immense social and political experiment of revolution and the move toward a communist state. The concept of democracy has over this time been much debated and scrutinised in order to deprive it of its 'bourgeois' elements. However, in the theories of Marx and Engels, there has been too little space left open for a pluralistic approach to state organisation. Likewise, Lenin and Stalin never achieved the 'withering away of the state' into a 'revolutionary-democratic dictatorship of the proletariat' upon the gap that fell after the October revolution of 1917. Communism, or some pure form of democracy within society's organisation has never been achieved. Singer, Marx, p. 103.","On Friday afternoon, the 26 th of November, I conducted my ethnographic observation in an urban setting in the centre of Leamington. My first intention was to observe activity and interaction in a second hand book shop, paying special attention to variables such as gender and age, and their potential relation to areas of interest. This, as we will see, proved to be difficult. Also, I intended to record noteworthy or unusual behaviour that might be caused by social circumstances particular to the shop. Around 1.45 pm I entered the shop, and at that moment there were about three customers around, apart from the two shopkeepers. The shop, called Portland Books has an authentic and old-fashioned atmosphere, with its wooden floor, narrow space and antique, hardback books on old wooden shelves. Besides old and new books, the shop also sells postcards and old placard photos. The counter is situated downstairs, in the middle of the shop. A small staircase leads to a first floor, where a small room with academic literature and a big room with novels and poetry can be found. I decided to stay upstairs for a while. In the big room, there were two customers, besides me and a friend that I had brought along. The upstairs area creates a lesser degree of constraint, as there is no shopkeeper around and the street can not be seen. This might create a more closed-off environment. Still, the customers whispered, as would have been the case in a library. After a while, the low number of customers made it pointless to stay in the shop for very long because of the absence of interaction to observe. At 2.15 pm I decided to change my approach and find another shop with more people in it. I found a better case study in another book store called Waterstone's, a bigger shop that sells more mainstream literature and does not offer second-hand books. Additionally, Waterstone's is situation more centrally at the entrance of a shopping centre and near to the Parade. Here I spent around two hours observing the shopping public. Although also not very busy, Waterstone's is a quite big store, like the first shop with two floors, and there was interesting material for observation. Most of the interaction was situated around the counter, where there was constant conversation between customers and one of the many members of the shop's personnel. The atmosphere in the shop is of the style of a 'walk-in shopping centre'. Many people come in to look around briefly, and nearly everybody does this by themselves. The counter downstairs is placed in the middle of the shop, presumably to make it easier for the personnel to make sure no theft takes place. Classical music sounds throughout the store; not the type of sensitive piano music, but rather dynamic orchestra music. However, the music is played quite softly, so that I observed no noticeable response to the music of customers or personnel. A large stairway leads the way the store's first floor. This floor is even larger than the ground floor. Differently from the first shop I visited, this floor also has a counter which is located centrally. Although there appears to be less constraint (which might be due to the size of the store, and to the music, creating a more 'informal' atmosphere) customers among each other still spoke in a lowered tone. Again, only around the counter people spoke aloud. It is striking that, although there was a considerable number of people on this floor (around 30 of more), by far the greatest number of them seemed to have come alone, or at least went around very individually. The customers also spread out quite evenly over the available sections of books (categorised in sections such as 'history', 'pets', 'linguistics', 'biography' etc. ), most of the time seemingly avoiding sections where other people were looking for books. Sometimes two people entered the shop together, although throughout the whole period of observation I did not see a larger group of customers entering the shop. Upon entering, a pair usually split to find the books of their own interest, and hardly interacted until the shop visit was ending. As such, it can be said that I observed a fairly static social atmosphere in the shop. It appeared that people were trying to pretend that others were not around them, or influencing their preferred method of going around the shop. When I observed this aspect, I decided to test to what extent a reaction could be caused among customers by intruding their private sphere. I went to stand next to a woman in a corner of the shop, to see if this would disturb her private sphere, and thus change her course of activity, or behaviour. But, although I stood next to her for a while (pretending to look for books), and took books from the shelf she was looking at, she did not react to my presence at all. Even if my presence did irritate her or created some form of discomfort, she was able to conceal this. In another instance, I approached a shelf where two people stood in search for books, without speaking. Although I stood behind them for a while, looking at the shelves they stood in front of, they did not react to this. Whether it happens consciously or not, most of the customers conform to seclusion of each individual. I would even argue that a quiet consensus exists to uphold a sort of polite distance, seemingly dependent on consciously avoiding each other, after which the customer does not have to assess constantly the situation in which it is appropriate to react to others. This, of course, creates the somewhat silent and static atmosphere of which I spoke earlier. About the type of people, I can say the following. On the first floor, there was a majority of women among the customers. Most of them I guessed were over forty years of age. Interestingly, between the two floors there seemed to be something of a difference between the types of customers. Generally, customers downstairs were younger of age, and they spent less time in the shop. Moreover, the customers downstairs seemed to be more reactive to other customers. They seemed, perhaps, less accustomed to the unwritten rules of the 'bookshop etiquette'. What underlines this mode of thinking, also, is the method of classifying the books it offers. Children's books, bestsellers, as well as popular scientific books, are all on offer on the ground floor, while some books were displayed on tables on the ground floor, as well as on shelves at the top floor. Although at first it may appear inefficient and chaotic to place one book in two places in the store, we must look for an explanation to it. It may be presumed that Waterstone's has deliberately implemented a categorisation of books that suits its customers and therefore works effectively as the most profitable way of advertising its books. Whereas regular bookshop visitors have certain sections of books they might be most interested in (particularly specialist literature qualifies for this, as people interested in one type of book are likely to buy more books of that type), others might be on the outlook for a particular book they have been recommended, or in search of a suitable gift. An average customer at Waterstone's is therefore likely to come to the store with an idea of what to buy. The store then tries to cater for any type of customer, so that they might easily find what they are looking for if they have no idea what to buy, or in the second case, may look around undisturbed for books of their interest. It is furthermore interesting to note that the store succeeds to a high degree to create an optimal consumer atmosphere. There is little talking, people who intend to stay for a longer period easily find their way in search for their category of books, and the first floor, by concealing the street and placing the salon tables and couch, create an atmosphere where people can stay longer without being disturbed. It is likely that the customers recognise and reinforce aspects of this way of thinking. This is shown by the fact that people search for books by themselves, even if they enter the shop as part of a group. Other evidence is the maintenance of the 'polite distance' of which I spoke earlier. It is evident that consumerism decides to a high degree what forms of social interaction are accepted, and which not. In this, there is no great difference of interest between personnel and customer. The nineteenth century was, until recently, predominantly seen as a century of rapid industrialisation which set the stage for profound social change. This way of thinking about the birth of a capitalist society, however, fails to recognise the two sides to the changes that took place. After all, not only was there rapid increase in supply, also was demand of consumer goods ever increasing (McCracken 1990:5). Earlier examples of cultural consumerism in various forms of frequent local gift exchange could already be found in the late eighteenth-century (Finn 2000:143). However, consumerism in the Victorian period took a different and much more broad-based shape through various agents, of which most importantly department stores and an 'unparalleled advertising craze' (Loeb 1994:5). And while men in the eighteenth-century were themselves participating consumers in an informal, pre-capitalist local economy, in the last decades of the nineteenth century mass advertisement of an impersonal character began to target almost only women as their potential customers (Loeb 1994:8) in a highly formalised mass commercial economy. The departure from a semi-formal, local economy to this increasingly enlarged scale of consumption of various kinds of consumer goods did not occur without protest or debate (Rappaport 1996). By the 1850s, the middle class had come to project a self-image of a virtuous class, with an identity based upon plainness and self-discipline. Especially into the nineteenth century, as the middle class began to emerge as a distinctive class, emphasised and luxurious clothing were replaced by more austere and sober dress (Finn 2000). Indeed sobriety and democratic value formed part of its claim to political power. But after the great wave of revolutions, from which middle class all over Europe once again arose reaffirmed in their political position, these democratic values came to be reinterpreted (Loeb 1994), slowly replacing sobriety for the desired acquisition of an 'ornamental delirium' for members of the middle class and the world that surrounded them (McCracken 1990:24). Perhaps the Great Exhibition of 1851 may be said to have served as a starting point of the spread of consumerism. Upon its opening in the Crystal Palace in London, it attracted a recorded total of six million visitors, equal to one-third of the country's population (Weintraub 2004). Moreover, for the first time, 'ordinary commodities [were turned] into cultural signifiers' (Rappaport 1996:65), affirming the results of a rational and sober life. Also William Whiteley, owner of London's first department store, had seen the Crystal Palace, and desired to open a store that made available those goods that remained unattainable at the Great Exhibition (Rappaport 1996). The decades after 1851 then set the stage for an ambiguous public debate among the middle classes, in which critics of new department stores such as Whiteley's expressed 'fears about the morality of consumption' (Rappaport 1996:66), while on the other side a nearly fantastic range of new goods and unprecedented luxury and comfort triggered a great appraisal of 'material democracy' (Loeb 1994:7). McCracken has argued that this middle class tendency toward mass consumerism represented a 'fundamental shift in the culture of the [...] modern world' (McCracken 1990:3) in which the middle class increasingly defined itself in material terms, while within its body others based their elitist or democratic values upon a paraphernalia of objects symbolically highly charged. Initially, the growing number of middle class women engaging themselves in the pastime of shopping was a sight that conflicted with the traditional Victorian 'angel in the household'. While some critics despised the idea of women drinking liquor during such shopping trips to the department store, others went even further to compare these consuming women with prostitutes (Finn 1998:36). In this criticism we may find a fear that was both classed and gendered; after all, prostitutes represented a disrupting female working class challenge to middle class Victorian idea of the gentile, meek and submissive wife. Shopping, however, remained viewed and actively gendered as a feminine task (Loeb 1994:12, Rappaport 1996:67) and the role of middle class women became increasingly an ornamental one. In the streets of London, 'Female shoppers, like the glittering objects on display, became a central part of the urban spectacle' (Rappaport 1996:80), allowing them to become walking statements, sometimes of middle class democratising success at large, other times drawing from an individualistic elitist identity (McCracken 1990). Advertisement did not play the least role in this newly constructed female middle class identity. From an early point, advertisers came to accept that 'the hand that rocks the cradle is the hand that buys the boots' (Loeb 1994:8). Throughout the second half of the nineteenth century, techniques of persuasion became increasingly refined (McCracken 1990:29), shifting the portrayal of the household mistress in a shape of the 'perfect lady' through images in an often historical, near-mythological style (Loeb 1994). Such portrayal of rather powerful, controlling women attempted to equal the act of shopping to having 'the opportunity to', as a critic wrote disdainfully, '""luxuriate"" in a deep and intense ""sense of power""' (Rappaport 1996:76). Growing consumerism, besides reforming the gender ideal, was also championed as a new definition of class difference. As goods on sale in the department store increasingly became 'material symbols', which set prescriptions of how the middle class was to dress, what material conveniences they could posses, which, in turn said something about 'how they should spend their leisure time' (McCracken 1990:27). Businesses and advertisers were eager to support this civilising, 'rational' view on middle class consumption. Their profits came mostly from the middle class, as a journal of the trade wrote: 'The buyers of the world are the great MIDDLE CLASS PEOPLE [sic]' (Loeb 1994:8). Thus, the Army and Navy Co-operative Society, a private department store, instructed the doorkeepers to keep servants and messengers out to avoid class mingling and provide safety for the female audience (Rappaport 1996). On the other hand, products that served to define middle class status were sold as necessities through the creation of great fear. Parents read in advertisements that without Mellin's Food, their child '[would] not last long' (Loeb 1994:14), while ""Raw Milk"" was advertised as so dangerous, that 'those who allow it to be served to their families take a great responsibility' (Loeb 1994:14). The arousal of fear can be said to have been used in advertisement and by the department stores in reaffirming middle class values through consumption, for which shopping could comfortably be conducted in the safe, exclusive environment of the department store. As mentioned earlier, this image of the safe middle class 'Universal Provider' taking care of you 'from the cradle to the grave' (Rappaport 1996:73) was continually criticised. Opponents of Whiteley's business tried to 'expose' the degenerate consumption behaviours of his customers (Rappaport: 1996:61), while hyperboloid advertisement was often dismissed as 'puffery' by its audience (Loeb 1994:5). Moreover, small shopkeepers tried to stand in the way of business men such as Whiteley whenever they could (Rappaport 1996). However, such opposition was overcome in various ways. First of all, great businesses such as Whiteley's received continual impulses through the middle class desire to attain novelties aroused by mass advertisement in periodicals of all kinds. Even as the country slid into an economical recession in the 1870s, the middle class 'seemed determined to perpetuate an invigorating sense of material possibility' (Loeb 1994:3). Objections to a valueless, banal middle class were diverted by the emergence of the eccentric, elitist model of the dandy, who, by claims of superior aesthetic standards created a new elite from within the middle class (McCracken 1990). Retailers with shops around department stores came to take these stores for a fact. It was soon found that, if the neighbourhood cooperated with the department store, markets expanded handsomely, also for smaller retailers (Rappaport 1996). Eventually, by the end of the nineteenth century all criticism had been channelled into the new consumerist middle class model. Except, perhaps, for some moral criticism, which was labelled as old-fashioned (Rappaport 1996). Consumption in the nineteenth century was nothing new (McCracken 1990). However, its extent and the way in which it changed the middle class was an enormous step further toward the modern capitalist model as we know it today. Where before the Victorian era consumption remained limited to a small number, it now took a flight to mass proportions. The way in which the public came to see the department store was transformed 'from ""the halls of temptation"" into a recognised and cherished ""social sight""' (Rappaport 1996:61). Whereas social changes and higher living standards were previously seen as the result of industrialisation and the great increase of supply of a wide range of goods, the driving forces on the 'demand side' are now becoming increasingly appreciated and researched (McCracken 1990). One of the reasons for this enormous increase in consumption may be found in the fact that new markets were sought to create a demand for this surplus of goods (Rappaport 1996:81). However, this does not tell the entire story. The general enthusiasm with which new material achievements were hailed had a profound effect on society and the way in which the middle class defined itself as part of it. Also men, and more particularly women within the middle class were both changed and changed their identities consciously with increasing consumerism. As 'shopping' was essentially gendered as a female activity in the second half of the nineteenth century, changes of gender perception were most acutely visible with women. Women were now encouraged to beautify themselves and surround their middle class homes with a paraphernalia of gentility.",False
12,"The first meeting of Marx and Engels in Paris in 1844 marked the starting point of a legendary friendship and cooperation of two thinkers who left an enormous mark on political development in the twentieth century and whose influence, in the words of a critic, can only be compared to that of Jesus or Mohammed. V.I. Lenin, 'Karl Marx', in Marx - Engels - Marxism (Moscow, 1951), p. 17. Peter Singer, Marx (Rotterdam, 1999), p. 10. As the philosophes of the enlightenment had posed a theory of a developing and progressing world, Marx and Engels concerned themselves with applying this model of development to society. Marx placed all history in terms of classes struggling through their relation to material welfare and the means of production. As atheistic followers of Hegel, Marx and Engels thus came to believe that 'the struggle against existing wrong and prevalent evil, is also rooted in the universal law of eternal development'. V.I. Lenin, 'Frederick Engels' in Marx - Engels - Marxism, p. 58. Marx and Engels lived at a time in which rapid industrialisation brought about exceeding disparities between the property-owning class (the bourgeoisie) and the working class (the proletariat). Believing capitalism to be only another step toward a society without class antagonisms, Marx turned to studying history and economics, while Engels in 1845 published The Condition of the Working Class in England. In this work, Engels drew from direct experience and a wide range of sources, arguing that the working class was becoming increasingly conscious of its place and relation to the bourgeoisie, which would in the end inevitably lead to a proletarian revolution. Marx and Engels were deeply impressed when, in 1844, factory workers revolted in Silesia and Bohemia. They observed unrest all over Europe, and thus became convinced of the revolutionary potential of the proletariat; a potential which would eventually carry humanity into the next stage of socialism. Lenin, 'Frederick Engels', p. 60. Victor Kiernan, 'History', in David McLellan (ed. ), Marx: The First Hundred Years (Oxford, 1983), p. 60. From circumstance and experience, Marx and Engels were not so much concerned with democracy, but rather with the greater concept of freedom and creating a freer society, in which, they argued, the state would cease to be necessary. Parliamentary democracy, in the eyes of Marx and Engel, was nothing more than the crystallisation of the contradictory interests between the bourgeoisie and the proletariat, and could not be different. Moreover, by separating the government from civil society, 'the bourgeois state and its attendant ideas of political rights and common national interests functioned, like religion, as an opium, an illusory compensation and thus a support for the injustices of civil society'. A radical change was needed for the advance at first of the proletariat and eventually of society as a whole. In their own words 'Not criticism but revolution is the driving force of history'. Singer, Marx, p. 104. David McLellan, 'Politics', in David McLellan (ed. ), Marx: The First Hundred Years (Oxford, 1983), pp. 145-46. Kiernan, 'History', p. 62. Marx and Engels became convinced that they had discovered a scientific model of society and its inevitable future. Marx's drive to act upon theory soon brought him into international communist circles. After heated debate at a congress in London in 1847, Marx and Engels were commissioned to write a political programme for the Communist League. Its result, the Communist Manifesto, can be regarded as Marx's and Engels' most outspoken vision on class struggle and the predestined faith of the proletariat. 'The proletariat', the Communist Manifesto stated confidently, 'goes through various stages of development'. Then, the manifesto goes on to speak in deterministic terms that the proletarian 'mission is to destroy all previous securities for, and insurances of, individual property'. Singer, Marx, pp. 14-15. Karl Marx and Friedrich Engels, The Communist Manifesto, cited in University of Warwick Insite, Department of History (20 November 2003),  URL  (21 December 2004). Ibid. If this scenario were to be the inevitable course of events, the logical question would be: why? Analysing history, Marx and Engels had discovered a continuous struggle for power between a powerful minority and a weak majority. They noted that 'All previous historical movements were movements of minorities, or in the interest of minorities'. However, it seemed that the proletariat would be able to break through this tendency. 'The proletarian movement is the self-conscious, independent movement of the immense majority, in the interest of the immense majority'. In this sense, it can be argued that for Marx and Engels, a primitive idea of democratic, or majority, rights served to justify a complex social theory of inevitable revolutionary struggle. Convinced about the power and improvement value of the theory of class struggle, 'His [Engels'] and Marx's historical vision was optimistic about the future, tragic when it turned to the past, as a record so dreadfully bad that any sacrifice was worth while to get away from it'. Ibid. Kiernan, 'History', p. 72. When Lenin became leader of the Bolsheviks in the early twentieth century, a debate among Marxists had been around for nearly fifteen years concerning the best way to bring about communism. One section of the Second International of 1889, under the leadership of Bernstein, had come to embrace the idea of social democracy, and of participation in parliamentary politics. Where Marx had been careful in his wording, or more abstract in his discussion of the democracy question, Lenin was more articulate in his discussion of the matter. He briefly defined the state as 'a special organization of force: it is an organization of violence for the suppression of some class'. Therefore, he argued, a revolution was the necessary agent for the transition from capitalism to socialism. McLellan, 'Politics', p. 167. In 1905 Lenin wrote Two Tactics, an article discussing the political direction Russia should take after the revolution. In this article he advocated a 'revolutionary democratic dictatorship of the proletariat and peasantry', a term that Marx had used only once, in a letter to a friend. What contrasted the Bolsheviks from the anarchists, Lenin argued, was that Bolsheviks thought it necessary to establish a transitory government to place the proletariat firm in the seat of power. However, months before the October revolution, in April 1917, he wrote that this would be 'a state without a standing army, without a police opposed to the people, without an officialdom placed above the people', based on Marx's example of his view of the Paris Commune. Eventually, in Engels' words, the government would slowly 'wither away' because it would become unnecessary. Ibid, p. 161. Ibid, pp. 154-55. V.I. Lenin, 'Letters on Tactics' in Marx - Engels - Marxism, p. 58. McLellan, 'Politics', p. 168 Some of Lenin's writing, though, already revealed some of the authoritarian traces that were to characterise the Bolsheviks' policy and the Soviet Union's later political development. Convinced, like Marx, of the inevitability of a proletarian revolution, he remarked: 'I said that there can be no government (apart from a bourgeois government) in Russia other than a government of the Soviets of Workers', Agricultural Labourers', Soldiers' and Peasants' Deputies'. Lenin, 'Letters on Tactics', pp. 389-90. By 1917, Lenin observed that the key conditions were fulfilled in Russia for a proletarian revolution to take place. An imperialist war was raging over Europe, according to Marxist theory calling in the culmination of capitalism. Conditions in Russia, he argued, were peculiar, as the country had already seen a revolution in February 1917. 'We have existing side by side, together, simultaneously, both the rule of the bourgeoisie [...] and a revolutionary-democratic dictatorship of the proletariat and the peasantry, which is voluntarily ceding power to the bourgeoisie, voluntarily transforming itself into an appendage of the bourgeoisie'. According to Lenin, the masses had 'been confused, led astray and deceived' by the temporary government of Lvov and Kerensky. He adopted Trotsky's idea of 'permanent revolution', arguing that the proletariat could not stop after overthrowing the bourgeois government. By not completing the dictatorship of the proletariat, the government itself had gone over to the side of the bourgeoisie. Columbia Encyclopedia, 'Vladimir Ilych Lenin: Theoretician and Revolutionary', Encyclopedia.com (2004),  URL  (27 December 2004). Lenin, 'Letters on Tactics', p. 386. V.I. Lenin, 'The Task of the Proletariat in Our Revolution' in Marx - Engels - Marxism, p. 380. In practice, much of Lenin's writing at the dawn of the October revolution appears to excuse the Bolsheviks' grasp for power and subsequent policy. Eventually, Bolshevik rule directly after the revolution proved to be highly undemocratic. In 1918, all political parties that did not support the Bolsheviks were systematically suppressed; while a temporary oligarchic, highly centralised government was formed, taking in members by co-option rather than by democratic elections. Rosa Luxemburg was among the prominent critics of Lenin's post-revolution, oppressive policy. 'The negative, the tearing down, can be decreed; the building up, the positive cannot', she argued. According to her, Lenin was standing in the way of a democratic learning process of the proletariat. 'The broadest democracy was needed', otherwise 'socialism will be decreed from behind a few official desks by a dozen intellectuals'. Although Lenin argued that such policy was needed in 'defence against counterrevolution and the actual elimination of everything that contradicted the sovereignty of the people', this, according to Luxemburg, was 'not the dictatorship of the proletariat, [...] but only the dictatorship of a handful of politicians'. McLellan, 'Politics', p. 170. Ibid, p. 163. Rosa Luxemburg, The Russian Revolution and Leninism or Marxism? , cited in University of Warwick Insite, Department of History (20 November 2003),  URL  (27 December 2004). V.I. Lenin, 'Two Tactics of Social Democracy in the Democratic Revolution' in Marx - Engels - Marxism, p. 177. Rosa Luxemburg, The Russian Revolution. By 1921, Lenin described the Soviet Union as 'a workers' state with bureaucratic distortion'. Trotsky actually became regretful, stating in a meeting that 'things are heading towards ruin'. Stalin, however, asserted in 1918 that at the time, the 'Bolsheviks did not doubt that bourgeois parliamentarism and the bourgeois-democratic republic represented a past stage of the revolution'. McLellan, 'Politics', p. 171. Joseph Stalin, The October Revolution (London, 1936), p. 168. Stalin, The October Revolution, p. 22. It is hard to place Stalin's policy in an unmitigated Marxist light. Despite its rhetoric, the Soviet Union of Stalin appeared to many something other than socialism. Theory was increasingly not matching policy. The state did not wither away, instead, a status quo of power was developing; one in which all lines came together in Moscow. An elite of scientifically based professional revolutionaries would 'show the way, formulating the program and policies, educating the people, and working out the strategy and tactics'. The character of the government had changed since the October revolution. Stalin's biographer, Deutscher, wrote: 'It has assumed office as a government of the people, by the people and for the people [...][now,] it ceases to be government by the people'. Indeed, it is arguable that the rapid industrialisation and agricultural reforms represented some elevated aspiration of the proletariat and peasants, however, the price of these reforms (The 1936 Great Terror and show trials) was disproportionate and tragic. Moreover, they revealed a highly undemocratic and unrepresentative government that stood separate from the masses. McLellan, 'Politics', p.173. Albert Weisbord, 'Bolshevism, Fraudulent Practice Of Democratic Centralism' cited in Weisbord.org (November/December 1976),  URL  (28 December 2004). I. Deutscher, Stalin: A Political Biography (Oxford, 1950), p. 175. McLellan, 'Politics', p. 173. The approximately hundred-year period over which the intellectual and political activity of Marx, Engels, Lenin and Stalin spread has seen the emergence of an immense social and political experiment of revolution and the move toward a communist state. The concept of democracy has over this time been much debated and scrutinised in order to deprive it of its 'bourgeois' elements. However, in the theories of Marx and Engels, there has been too little space left open for a pluralistic approach to state organisation. Likewise, Lenin and Stalin never achieved the 'withering away of the state' into a 'revolutionary-democratic dictatorship of the proletariat' upon the gap that fell after the October revolution of 1917. Communism, or some pure form of democracy within society's organisation has never been achieved. Singer, Marx, p. 103.","The publication of The Bell Curve in 1994, the issues it discussed and conclusions it drew have triggered an enormous controversy over the years after its publication (Keenan 1999). This comes mostly from its assumed political incorrectness, claimed implications for public policy, and doubts from the scientific community as to its scientific claims (Keenan 1999). In this essay I shall attempt to take a closer look at some sociologically significant conclusions reached in the book. It seems, however, impossible to review the book without references to the sharp criticism it sparked upon publication. I shall therefore occasionally refer to critics or related academic work. The book sets out in an ambitious tone. In its introductory note, the authors forebode sensational conclusions, expecting 'that many people will have a ""This can't possibly be true"" reaction' (p. xix). It soon becomes clear that the book is intended for a wide audience, and throughout the book the authors continually insinuate this presumption. It is a point that is well worth noting, as we shall later see. The intention of the book, however, is not very clear at the outset. Vague notions such as 'for the last thirty years, the concept of intelligence has been the pariah in the world of ideas' (p. 1), and further, 'received wisdom in the media is roughly 180 degrees opposite from each of the six points [that we assume as a basis]' (p. 23) leave the reader with the idea that the authors are to set straight a grave misunderstanding. The six points which the authors write about are of great importance throughout the book. They are six assumptions from the classicist school about intelligence (or cognitive ability, two terms that are used interchangeably in the book) and the place IQ testing takes in expressing a person's intelligence. Especially claim five and six, respectively, that properly administered IQ tests are not biased, and that intelligence is 'substantially heritable [...] no less that 40 percent and no more than 80 percent' (p. 23) are facts that the authors lean on particularly heavily. According to critics, the problem is that these claims involve more complication and nuance than an average reader may understand, and that the consensus among psychometricians that the authors mention is not as broad as their own claims. A critic of the book, professor in psychology Robert Sternberg reacts as follows: 'I think that there is definitely some heritability of intelligence in the White population. Almost every psychologist believes there is some heritability of IQ and I agree. But the public may not understand just what that means. If you accept the use of the heritability statistic, about .5 [50%] is probably right' (Miele 1995). Throughout the book, moderate claims moulded into more radical views and outcomes are commonplace. This becomes especially controversial and fragile as topics around intelligence difference between races are being discussed (Frisby 1995). The virulent attack on the most controversial matters discussed in the book, according to Murray, 'often read like an unintentional confirmation of our view of the ""cognitive elite"" as a new caste', while 'when the Sturm und Drang has subsided, nothing important in The Bell Curve will have been overturned. I say this not because Herrnstein and I were especially far-sighted, but because our conclusions are so cautiously phrased and our findings anchored so securely in the middle of the scientific road' (Upstream). To put these claims in context, it is necessary to look at some of the criticism of the book. The Bell Curve has been accused of presenting conclusions on a pseudo-scientific basis. As the book is quite substantial, and the authors continually moderate the argument besides cross-referencing chapters, it is difficult to either speak completely for or against the type of science that The Bell Curve presents the reader with. However, the idea that Sternberg brings up is worth noting. He says: 'The way that book is written is to, I think, say X on page 605 in sentence 8, with an appropriate caution, and then invite the reader to a somewhat more extreme conclusion elsewhere' (Miele 1995). In line with this claim, we find the authors introducing the idea of a possible constant deviation in intelligence between ethnic groups in a casual way: 'Americans have watched the spectacular economic success of the Pacific rim nations' (p. 272), as if this were a logical consequence of a population's higher IQ average. In the unfolding chapter 13, which deals with variations of cognitive ability between ethnic groups, the book continues to provide the evidence for trends in intelligence difference between 'Asians' and 'Whites'. For this evidence, the book leans heavily upon the work of Lynn. A key claim, that IQ differences in the results are not generally found in culturally biased assignments, is here backed by only one source, Jensen. The work of this scholar, however, is certainly not uncontroversial and may even be racially motivated (Keenan 1999). Thus, where Murray and Herrnstein claim that 'there is no longer an important technical debate' (p. 282), Sternberg counters: 'To the extent that there is a consensus it is certainly not Herrnstein's and Murray's' (Miele 1995). Nonetheless, it is no more than fair to put forward Murray's response in the afterword of the book: 'Never mind that The Bell Curve draws its evidence from more than a thousand scholars. Never mind that among [the accused scholars] are some of the most respected psychologists of our time' (p. 564). Possibly the complication with the data used lies not so much in its collectors as in the theory behind it. In order to further their argument, the authors take a great and careless leap beyond their use of the concepts of 'race' and 'ethnicity', concepts which, according to Frisby, are 'best left to specialists' (Frisby 1995), while a UNESCO report argues that 'The term [race] at best is at present time not really allowable on any score in man' (Miles 1982). How then do the authors of The Bell Curve justify their use of ethnicity (which, again, may be taken as interchangeable in the book)? Only a paragraph seems enough: 'The studies of ""blacks"" or ""Latinos"" or ""Asians"" who live in America generally denote people who say they are black, Latino or Asian - no more, no less' (p. 271), and further they write 'race differences are varied and complex - and they make the human species more adaptable and more interesting' (p. 272). In this way, the authors appear to draw a direct connection between personally perceived ethnic differences and the distribution of cognitive abilities for different human groupings. It can, however, conversely be argued that, like all over the world humans are expected to be born with two arms and two legs, the distribution of intelligence may be expected to fit into a similar predictable model worldwide. Opinions are still so divided about the issue that different conclusions are reached from the same data. As an example, the conclusions of Flynn are refuted by the authors as follows: 'He also says that Asian-Americans actually have the same non-verbal intelligence as whites', but 'Lynn disagrees and concludes from the same data [my italics] that there is an ethnic difference in overall IQ as well' (p. 273). Perhaps the contrast cannot be presented starker than through the following example. Whilst the authors argue that 'tests results are matched by analyses of occupational and scientific attainment by Jews, which consistently show their disproportionate level of success' (p.275), Keenan (1999) insists: 'The Jews provide another example of the blending of racial distinctions. They are all presumably derived from one population [...] Gene frequency studies, however, show that now Jewish populations in different parts of the world tend to resemble their surrounding populations at least as much, if not more, than they do each other'. The book consists of four parts comprising some 500 pages of textual body and several appendices providing more background. Part III, nonetheless, presents the most important and controversial findings. The authors recognise this, claiming, 'that many readers have turned first to this chapter indicates how sensitive the issue has become' (p. 270). It is difficult to accuse the authors of racist motives. Placing possibly controversial outcomes into a thought exercise, the authors state 'some people would interpret the news as a license for treating all whites as intellectually superior to all blacks [but it becomes] obvious how illogical [...] such reactions would be' (p. 312). It can, however, easily be said that the authors jump to conclusions about racial differences too soon, to go on suggesting changes in public policy in part IV. The problem with this book thus lies with its scientific inconsistency. It sets out with no clearly stated objective, while the use of scholarly caution and polite integrity is broken soon after the authors bring it up. While the authors promise in the introduction to 'employ the more neutral term cognitive ability as often as possible' (p. 22), synonyms such as 'smart' (p. 127), 'very smart... Very dumb...' (p. 133) that occasionally crop up nearly indicate the carelessness with which the authors draw their conclusions. Frisby claims that 'to parade facts about racial differences before the general public is akin to putting a lit match to gasoline' (Frisby 1995). The authors of The Bell Curve are eager to draw conclusions and that it is what lends this work its importance, but also its legions of hostile critics. In the words of Sternberg: 'If you were to ask, ""What inference do Herrnstein and Murray invite their readers to draw?"" they go beyond what they know' (Miele 1995). It was in 1903 that Emmeline Pankhurst and her daughter Christabel founded the Women's Social and Political Union (WSPU), following around forty years of organised campaign for female suffrage organisations in Britain (Banks 1981). After another fifteen years of campaign, interrupted by the First World War, women were finally granted the vote in 1918 through the Representation of People Act. It remains, however, a controversial matter as to whether the militant campaigns of the WSPU actually helped to quicken the vote for women, or not. In order to get a clearer picture of the tactics of the suffragettes, it is worthwhile to take a closer look at their use of the female body in violent, unconventional and often illegal ways, to draw attention to their cause. The connection of British femininity with a high morality, and the idea of gender equalities through historical argumentation were common ground arguments for the vote in the late nineteenth century. Although arguments of the 'constitutionalists' always sought to stay within the boundaries of middle class respectability, they certainly incorporated argumentation based upon the female body (Holton 1998). The most evident examples of this can be found in racist theories. Female authors attempted to present an image of a superior British race, of which women, had, by necessity, always been part. Charlotte Carmichael Stopes', in her book British Freewomen of 1894, argued that women's right to political participation originated from the ancient Britons. 'The moral codes, sexual equality in physical height [my italics]' were, in her book presented as arguments for women's suffrage (Holton 1998: 158). Constitutionalist feminists increasingly began to make use of racial reasoning to support their campaign for the vote (Holton 1998). This provided the movement with a legal means of enhancing female respectability and high morale in a way that was compatible and in harmony with society. However, after forty years of such campaigns, the women's vote was still nearly as far away as it had been at the outset. This realisation caused the WSPU to seek to pressurise the government, for they were responsible for the problem (Pugh 2000). From a harmony model, the dominant suffrage campaigns thence shifted to a model of conflict, bringing the movement into a new phase (Banks 1981, Holton 1998). The suffragettes, as the WSPU activists came to be known, sought to cut right through to the core of the problem by addressing the government directly. They sought to point out the inherent contradictions of the political system as it was: the partial inclusion of women into an essentially male-dominated environment (Lawrence 2000). From insisting on politicians' support in public meetings, the suffragettes soon radicalised (Vicinus 1985). They felt that the suffrage question was not dealt with seriously, and from there the WSPU leader Christabel Pankhurst set out to phrase the problem more directly: '[i]f Mr Asquith [PM] would not let her vote, she was not prepared to let him talk' (Lawrence 2001). This meant a great leap away from the Victorian feminist movement; suffragettes sought to replace the passive, homely housewife with a campaigning activist, a political being. In the words of the prominent suffragette Emmeline Pethick-Lawrence: 'Not the Vote only, but what the Vote means - the moral, the mental, economic, the spiritual enfranchisement of Womanhood, the release of women...' was of vital importance to women (Vicinus 1985: 249). A new stage of interrupting meetings started. Suffragettes continually disrupted parliamentary debates, by shouting out against the speaker. Increasingly coordinated, the suffragettes were sometimes able to spoil a complete speech, by disrupting the speaker in turns. In 1908, a speech by Lloyd George was continually interrupted for two hours, with fifty women carried out (Pugh 2000). Although this obstruction of the political process was arguably playing into the hands anti-suffragists, such forceful, physical practices of politics can be said to have been part of masculine politics as it was conducted by male politicians (Lawrence 2001). Indeed, when members of the audience could be forcefully removed from a public political meeting, this might be interpreted as a threat to the civil liberties (Pugh 2000). This meant a moral victory for the suffragettes, especially since gentlemanliness towards women was expected of politicians. Mass assault and arrests of women were not an uncommon sight any longer, which the particularly violent events of 'Black Friday' in November 1910 highlighted (Vicinus 1985). Hence, the suffragettes increasingly highlighted this state brutality by a number of means. Hunger striking was one of them, first begun on the initiative of Marion Wallace Dunlop in July 1909 (Pugh 2000). Protesting against the government's refusal to grant the suffragettes the status of political prisoners, the WSPU soon managed to place the campaign for female suffrage on a moral high ground, as the government had to face the issue of the prisoners' treatment. The WSPU brilliantly publicised this moral strength, speaking of 'moral right triumphing over physical force' (Vicinus 1985). Forcible feeding of hunger striking suffragettes soon received criticism from doctors (Pugh 2000) and graphic representation presented a shocking picture of the treatment of women in prisons. The problem continued, and leaving even the parliament divided (John 2001). In 1913, the Cat and Mouse Act was thus passed, which dismissed the policy of forcible feeding and was aimed at avoiding the negative publicity deriving from it. To some extent, the act succeeded in doing so, as many people argued that their suffering was 'self-imposed and their martyrdom as in some sense staged' (Harrison 1978: 180). This loss of sympathy and moral and intellectual high ground was however enhanced by the suffragettes increasing radicalisation and alienation from sympathisers (Pugh 2000). The suffragettes showed a radical impatience and determination that eventually led them to virtually abandon any techniques of persuasion (Pugh 2000). The years in the run towards World War I resulted in the most passionate outbursts of the suffragettes attack upon male domination of the political system (Vicinus 1985). A first systematic window breaking campaign had been undertaken in 1909, of which Sylvia Pankhurst said, 'let it be the windows of the Government, not the bodies of women which shall be broken' (Lawrence 2001: 222). A near concession of the parliament in 1910, which would have resulted in a limited vote for women, could not be passed by a much divided parliament. Winston Churchill, usually a sympathiser of the women's suffrage cause, voted against the bill in resentment of the WSPU's violent tactics (Harrison 1978). However, this failure so outraged the rank-and-file WSPU members, that another window breaking campaign was started by the end of 1911 (Vicinus 1985). Popular support for the WSPU was on the decline. The violence of the suffragettes' campaign made it possible to argue that women did not demonstrate the capability to participate in politics due to their rash and unreasonable behaviour. Criticism of the Union's leaders, Emmeline and Christabel Pankhurst, even enhanced this view. 'They are not calm women,' Teresa Billington-Greig claimed, 'but irritable, yielding to sudden tempest' (Harrison 1978: 176). And whilst, 'the police, in early stages [...] avoided heavy-handed treatment of the women' (Pugh 2000: 193), they still insisted on continued provocation and imprisonment (Pugh 2000). In 1912 Christabel Pankhurst fled to Paris after the police raided the WSPU headquarters in London. As the movement was losing grip of its popular support, so did Christabel break with some of the high-ranking campaigners, among them the Pethick-Lawrences whom had up until then financed the several branches of the organisation (Pugh 2000). The publication of the 'The Freewoman', a periodical by a number of suffragettes, between 1911 and 1913, 'reawakened the free love fears that had haunted the feminist movements since its beginning' (Banks 1981: 132). The Pankhursts, however, had always insisted on dressing in feminine and respectable manner to disarm opponents from criticism (Pugh 2000). And despite arson, window breaking and other violent behaviour of WSPU movements, the organised anti-suffragist movement was cautiously following the skill with which the suffragettes sought publicity. As correspondence of October 1913 shows, the anti-suffragist movement was well aware of the press attention that the suffragettes managed to obtain. '[P]ublicity in the Press', an executive committee member wrote, 'is our greatest need and our opponents' chief advantage over us' (Harrison 1978: 175). It was this sparked public debate, and the attitude of directness and briskness in contrast to the vagueness and eventual moral weakness of the anti-suffragists over the real debate of women's suffrage, that eventually led the parliament to extend the vote to women over thirty. The suffragettes' efforts can be said to be characterised by a curious mixture of Victorian respectability and morality blended with a continual and controversial insistence on their rights. An immense confidence in the female gender to the extent of superior feelings (Vicinus 1985) combined with a strong sisterhood and sense of sacrifice for the cause led Sylvia Pankhurst to burst out even in 1954 against anti-suffragist Violet Markham: 'that foul traitor - who, while the suffragettes were hunger striking, appeared on the Albert Hall platform, [...] protesting against women having the vote' (Harrison 1978: 13). Resultantly, at all times, the suffragettes used their bodies as a symbol of sovereignty and moral superiority over the established political power, and this to point out the flaws in the anti-suffrage argumentation.",True
13,"The publication of The Bell Curve in 1994, the issues it discussed and conclusions it drew have triggered an enormous controversy over the years after its publication (Keenan 1999). This comes mostly from its assumed political incorrectness, claimed implications for public policy, and doubts from the scientific community as to its scientific claims (Keenan 1999). In this essay I shall attempt to take a closer look at some sociologically significant conclusions reached in the book. It seems, however, impossible to review the book without references to the sharp criticism it sparked upon publication. I shall therefore occasionally refer to critics or related academic work. The book sets out in an ambitious tone. In its introductory note, the authors forebode sensational conclusions, expecting 'that many people will have a ""This can't possibly be true"" reaction' (p. xix). It soon becomes clear that the book is intended for a wide audience, and throughout the book the authors continually insinuate this presumption. It is a point that is well worth noting, as we shall later see. The intention of the book, however, is not very clear at the outset. Vague notions such as 'for the last thirty years, the concept of intelligence has been the pariah in the world of ideas' (p. 1), and further, 'received wisdom in the media is roughly 180 degrees opposite from each of the six points [that we assume as a basis]' (p. 23) leave the reader with the idea that the authors are to set straight a grave misunderstanding. The six points which the authors write about are of great importance throughout the book. They are six assumptions from the classicist school about intelligence (or cognitive ability, two terms that are used interchangeably in the book) and the place IQ testing takes in expressing a person's intelligence. Especially claim five and six, respectively, that properly administered IQ tests are not biased, and that intelligence is 'substantially heritable [...] no less that 40 percent and no more than 80 percent' (p. 23) are facts that the authors lean on particularly heavily. According to critics, the problem is that these claims involve more complication and nuance than an average reader may understand, and that the consensus among psychometricians that the authors mention is not as broad as their own claims. A critic of the book, professor in psychology Robert Sternberg reacts as follows: 'I think that there is definitely some heritability of intelligence in the White population. Almost every psychologist believes there is some heritability of IQ and I agree. But the public may not understand just what that means. If you accept the use of the heritability statistic, about .5 [50%] is probably right' (Miele 1995). Throughout the book, moderate claims moulded into more radical views and outcomes are commonplace. This becomes especially controversial and fragile as topics around intelligence difference between races are being discussed (Frisby 1995). The virulent attack on the most controversial matters discussed in the book, according to Murray, 'often read like an unintentional confirmation of our view of the ""cognitive elite"" as a new caste', while 'when the Sturm und Drang has subsided, nothing important in The Bell Curve will have been overturned. I say this not because Herrnstein and I were especially far-sighted, but because our conclusions are so cautiously phrased and our findings anchored so securely in the middle of the scientific road' (Upstream). To put these claims in context, it is necessary to look at some of the criticism of the book. The Bell Curve has been accused of presenting conclusions on a pseudo-scientific basis. As the book is quite substantial, and the authors continually moderate the argument besides cross-referencing chapters, it is difficult to either speak completely for or against the type of science that The Bell Curve presents the reader with. However, the idea that Sternberg brings up is worth noting. He says: 'The way that book is written is to, I think, say X on page 605 in sentence 8, with an appropriate caution, and then invite the reader to a somewhat more extreme conclusion elsewhere' (Miele 1995). In line with this claim, we find the authors introducing the idea of a possible constant deviation in intelligence between ethnic groups in a casual way: 'Americans have watched the spectacular economic success of the Pacific rim nations' (p. 272), as if this were a logical consequence of a population's higher IQ average. In the unfolding chapter 13, which deals with variations of cognitive ability between ethnic groups, the book continues to provide the evidence for trends in intelligence difference between 'Asians' and 'Whites'. For this evidence, the book leans heavily upon the work of Lynn. A key claim, that IQ differences in the results are not generally found in culturally biased assignments, is here backed by only one source, Jensen. The work of this scholar, however, is certainly not uncontroversial and may even be racially motivated (Keenan 1999). Thus, where Murray and Herrnstein claim that 'there is no longer an important technical debate' (p. 282), Sternberg counters: 'To the extent that there is a consensus it is certainly not Herrnstein's and Murray's' (Miele 1995). Nonetheless, it is no more than fair to put forward Murray's response in the afterword of the book: 'Never mind that The Bell Curve draws its evidence from more than a thousand scholars. Never mind that among [the accused scholars] are some of the most respected psychologists of our time' (p. 564). Possibly the complication with the data used lies not so much in its collectors as in the theory behind it. In order to further their argument, the authors take a great and careless leap beyond their use of the concepts of 'race' and 'ethnicity', concepts which, according to Frisby, are 'best left to specialists' (Frisby 1995), while a UNESCO report argues that 'The term [race] at best is at present time not really allowable on any score in man' (Miles 1982). How then do the authors of The Bell Curve justify their use of ethnicity (which, again, may be taken as interchangeable in the book)? Only a paragraph seems enough: 'The studies of ""blacks"" or ""Latinos"" or ""Asians"" who live in America generally denote people who say they are black, Latino or Asian - no more, no less' (p. 271), and further they write 'race differences are varied and complex - and they make the human species more adaptable and more interesting' (p. 272). In this way, the authors appear to draw a direct connection between personally perceived ethnic differences and the distribution of cognitive abilities for different human groupings. It can, however, conversely be argued that, like all over the world humans are expected to be born with two arms and two legs, the distribution of intelligence may be expected to fit into a similar predictable model worldwide. Opinions are still so divided about the issue that different conclusions are reached from the same data. As an example, the conclusions of Flynn are refuted by the authors as follows: 'He also says that Asian-Americans actually have the same non-verbal intelligence as whites', but 'Lynn disagrees and concludes from the same data [my italics] that there is an ethnic difference in overall IQ as well' (p. 273). Perhaps the contrast cannot be presented starker than through the following example. Whilst the authors argue that 'tests results are matched by analyses of occupational and scientific attainment by Jews, which consistently show their disproportionate level of success' (p.275), Keenan (1999) insists: 'The Jews provide another example of the blending of racial distinctions. They are all presumably derived from one population [...] Gene frequency studies, however, show that now Jewish populations in different parts of the world tend to resemble their surrounding populations at least as much, if not more, than they do each other'. The book consists of four parts comprising some 500 pages of textual body and several appendices providing more background. Part III, nonetheless, presents the most important and controversial findings. The authors recognise this, claiming, 'that many readers have turned first to this chapter indicates how sensitive the issue has become' (p. 270). It is difficult to accuse the authors of racist motives. Placing possibly controversial outcomes into a thought exercise, the authors state 'some people would interpret the news as a license for treating all whites as intellectually superior to all blacks [but it becomes] obvious how illogical [...] such reactions would be' (p. 312). It can, however, easily be said that the authors jump to conclusions about racial differences too soon, to go on suggesting changes in public policy in part IV. The problem with this book thus lies with its scientific inconsistency. It sets out with no clearly stated objective, while the use of scholarly caution and polite integrity is broken soon after the authors bring it up. While the authors promise in the introduction to 'employ the more neutral term cognitive ability as often as possible' (p. 22), synonyms such as 'smart' (p. 127), 'very smart... Very dumb...' (p. 133) that occasionally crop up nearly indicate the carelessness with which the authors draw their conclusions. Frisby claims that 'to parade facts about racial differences before the general public is akin to putting a lit match to gasoline' (Frisby 1995). The authors of The Bell Curve are eager to draw conclusions and that it is what lends this work its importance, but also its legions of hostile critics. In the words of Sternberg: 'If you were to ask, ""What inference do Herrnstein and Murray invite their readers to draw?"" they go beyond what they know' (Miele 1995). It was in 1903 that Emmeline Pankhurst and her daughter Christabel founded the Women's Social and Political Union (WSPU), following around forty years of organised campaign for female suffrage organisations in Britain (Banks 1981). After another fifteen years of campaign, interrupted by the First World War, women were finally granted the vote in 1918 through the Representation of People Act. It remains, however, a controversial matter as to whether the militant campaigns of the WSPU actually helped to quicken the vote for women, or not. In order to get a clearer picture of the tactics of the suffragettes, it is worthwhile to take a closer look at their use of the female body in violent, unconventional and often illegal ways, to draw attention to their cause. The connection of British femininity with a high morality, and the idea of gender equalities through historical argumentation were common ground arguments for the vote in the late nineteenth century. Although arguments of the 'constitutionalists' always sought to stay within the boundaries of middle class respectability, they certainly incorporated argumentation based upon the female body (Holton 1998). The most evident examples of this can be found in racist theories. Female authors attempted to present an image of a superior British race, of which women, had, by necessity, always been part. Charlotte Carmichael Stopes', in her book British Freewomen of 1894, argued that women's right to political participation originated from the ancient Britons. 'The moral codes, sexual equality in physical height [my italics]' were, in her book presented as arguments for women's suffrage (Holton 1998: 158). Constitutionalist feminists increasingly began to make use of racial reasoning to support their campaign for the vote (Holton 1998). This provided the movement with a legal means of enhancing female respectability and high morale in a way that was compatible and in harmony with society. However, after forty years of such campaigns, the women's vote was still nearly as far away as it had been at the outset. This realisation caused the WSPU to seek to pressurise the government, for they were responsible for the problem (Pugh 2000). From a harmony model, the dominant suffrage campaigns thence shifted to a model of conflict, bringing the movement into a new phase (Banks 1981, Holton 1998). The suffragettes, as the WSPU activists came to be known, sought to cut right through to the core of the problem by addressing the government directly. They sought to point out the inherent contradictions of the political system as it was: the partial inclusion of women into an essentially male-dominated environment (Lawrence 2000). From insisting on politicians' support in public meetings, the suffragettes soon radicalised (Vicinus 1985). They felt that the suffrage question was not dealt with seriously, and from there the WSPU leader Christabel Pankhurst set out to phrase the problem more directly: '[i]f Mr Asquith [PM] would not let her vote, she was not prepared to let him talk' (Lawrence 2001). This meant a great leap away from the Victorian feminist movement; suffragettes sought to replace the passive, homely housewife with a campaigning activist, a political being. In the words of the prominent suffragette Emmeline Pethick-Lawrence: 'Not the Vote only, but what the Vote means - the moral, the mental, economic, the spiritual enfranchisement of Womanhood, the release of women...' was of vital importance to women (Vicinus 1985: 249). A new stage of interrupting meetings started. Suffragettes continually disrupted parliamentary debates, by shouting out against the speaker. Increasingly coordinated, the suffragettes were sometimes able to spoil a complete speech, by disrupting the speaker in turns. In 1908, a speech by Lloyd George was continually interrupted for two hours, with fifty women carried out (Pugh 2000). Although this obstruction of the political process was arguably playing into the hands anti-suffragists, such forceful, physical practices of politics can be said to have been part of masculine politics as it was conducted by male politicians (Lawrence 2001). Indeed, when members of the audience could be forcefully removed from a public political meeting, this might be interpreted as a threat to the civil liberties (Pugh 2000). This meant a moral victory for the suffragettes, especially since gentlemanliness towards women was expected of politicians. Mass assault and arrests of women were not an uncommon sight any longer, which the particularly violent events of 'Black Friday' in November 1910 highlighted (Vicinus 1985). Hence, the suffragettes increasingly highlighted this state brutality by a number of means. Hunger striking was one of them, first begun on the initiative of Marion Wallace Dunlop in July 1909 (Pugh 2000). Protesting against the government's refusal to grant the suffragettes the status of political prisoners, the WSPU soon managed to place the campaign for female suffrage on a moral high ground, as the government had to face the issue of the prisoners' treatment. The WSPU brilliantly publicised this moral strength, speaking of 'moral right triumphing over physical force' (Vicinus 1985). Forcible feeding of hunger striking suffragettes soon received criticism from doctors (Pugh 2000) and graphic representation presented a shocking picture of the treatment of women in prisons. The problem continued, and leaving even the parliament divided (John 2001). In 1913, the Cat and Mouse Act was thus passed, which dismissed the policy of forcible feeding and was aimed at avoiding the negative publicity deriving from it. To some extent, the act succeeded in doing so, as many people argued that their suffering was 'self-imposed and their martyrdom as in some sense staged' (Harrison 1978: 180). This loss of sympathy and moral and intellectual high ground was however enhanced by the suffragettes increasing radicalisation and alienation from sympathisers (Pugh 2000). The suffragettes showed a radical impatience and determination that eventually led them to virtually abandon any techniques of persuasion (Pugh 2000). The years in the run towards World War I resulted in the most passionate outbursts of the suffragettes attack upon male domination of the political system (Vicinus 1985). A first systematic window breaking campaign had been undertaken in 1909, of which Sylvia Pankhurst said, 'let it be the windows of the Government, not the bodies of women which shall be broken' (Lawrence 2001: 222). A near concession of the parliament in 1910, which would have resulted in a limited vote for women, could not be passed by a much divided parliament. Winston Churchill, usually a sympathiser of the women's suffrage cause, voted against the bill in resentment of the WSPU's violent tactics (Harrison 1978). However, this failure so outraged the rank-and-file WSPU members, that another window breaking campaign was started by the end of 1911 (Vicinus 1985). Popular support for the WSPU was on the decline. The violence of the suffragettes' campaign made it possible to argue that women did not demonstrate the capability to participate in politics due to their rash and unreasonable behaviour. Criticism of the Union's leaders, Emmeline and Christabel Pankhurst, even enhanced this view. 'They are not calm women,' Teresa Billington-Greig claimed, 'but irritable, yielding to sudden tempest' (Harrison 1978: 176). And whilst, 'the police, in early stages [...] avoided heavy-handed treatment of the women' (Pugh 2000: 193), they still insisted on continued provocation and imprisonment (Pugh 2000). In 1912 Christabel Pankhurst fled to Paris after the police raided the WSPU headquarters in London. As the movement was losing grip of its popular support, so did Christabel break with some of the high-ranking campaigners, among them the Pethick-Lawrences whom had up until then financed the several branches of the organisation (Pugh 2000). The publication of the 'The Freewoman', a periodical by a number of suffragettes, between 1911 and 1913, 'reawakened the free love fears that had haunted the feminist movements since its beginning' (Banks 1981: 132). The Pankhursts, however, had always insisted on dressing in feminine and respectable manner to disarm opponents from criticism (Pugh 2000). And despite arson, window breaking and other violent behaviour of WSPU movements, the organised anti-suffragist movement was cautiously following the skill with which the suffragettes sought publicity. As correspondence of October 1913 shows, the anti-suffragist movement was well aware of the press attention that the suffragettes managed to obtain. '[P]ublicity in the Press', an executive committee member wrote, 'is our greatest need and our opponents' chief advantage over us' (Harrison 1978: 175). It was this sparked public debate, and the attitude of directness and briskness in contrast to the vagueness and eventual moral weakness of the anti-suffragists over the real debate of women's suffrage, that eventually led the parliament to extend the vote to women over thirty. The suffragettes' efforts can be said to be characterised by a curious mixture of Victorian respectability and morality blended with a continual and controversial insistence on their rights. An immense confidence in the female gender to the extent of superior feelings (Vicinus 1985) combined with a strong sisterhood and sense of sacrifice for the cause led Sylvia Pankhurst to burst out even in 1954 against anti-suffragist Violet Markham: 'that foul traitor - who, while the suffragettes were hunger striking, appeared on the Albert Hall platform, [...] protesting against women having the vote' (Harrison 1978: 13). Resultantly, at all times, the suffragettes used their bodies as a symbol of sovereignty and moral superiority over the established political power, and this to point out the flaws in the anti-suffrage argumentation.","The first meeting of Marx and Engels in Paris in 1844 marked the starting point of a legendary friendship and cooperation of two thinkers who left an enormous mark on political development in the twentieth century and whose influence, in the words of a critic, can only be compared to that of Jesus or Mohammed. V.I. Lenin, 'Karl Marx', in Marx - Engels - Marxism (Moscow, 1951), p. 17. Peter Singer, Marx (Rotterdam, 1999), p. 10. As the philosophes of the enlightenment had posed a theory of a developing and progressing world, Marx and Engels concerned themselves with applying this model of development to society. Marx placed all history in terms of classes struggling through their relation to material welfare and the means of production. As atheistic followers of Hegel, Marx and Engels thus came to believe that 'the struggle against existing wrong and prevalent evil, is also rooted in the universal law of eternal development'. V.I. Lenin, 'Frederick Engels' in Marx - Engels - Marxism, p. 58. Marx and Engels lived at a time in which rapid industrialisation brought about exceeding disparities between the property-owning class (the bourgeoisie) and the working class (the proletariat). Believing capitalism to be only another step toward a society without class antagonisms, Marx turned to studying history and economics, while Engels in 1845 published The Condition of the Working Class in England. In this work, Engels drew from direct experience and a wide range of sources, arguing that the working class was becoming increasingly conscious of its place and relation to the bourgeoisie, which would in the end inevitably lead to a proletarian revolution. Marx and Engels were deeply impressed when, in 1844, factory workers revolted in Silesia and Bohemia. They observed unrest all over Europe, and thus became convinced of the revolutionary potential of the proletariat; a potential which would eventually carry humanity into the next stage of socialism. Lenin, 'Frederick Engels', p. 60. Victor Kiernan, 'History', in David McLellan (ed. ), Marx: The First Hundred Years (Oxford, 1983), p. 60. From circumstance and experience, Marx and Engels were not so much concerned with democracy, but rather with the greater concept of freedom and creating a freer society, in which, they argued, the state would cease to be necessary. Parliamentary democracy, in the eyes of Marx and Engel, was nothing more than the crystallisation of the contradictory interests between the bourgeoisie and the proletariat, and could not be different. Moreover, by separating the government from civil society, 'the bourgeois state and its attendant ideas of political rights and common national interests functioned, like religion, as an opium, an illusory compensation and thus a support for the injustices of civil society'. A radical change was needed for the advance at first of the proletariat and eventually of society as a whole. In their own words 'Not criticism but revolution is the driving force of history'. Singer, Marx, p. 104. David McLellan, 'Politics', in David McLellan (ed. ), Marx: The First Hundred Years (Oxford, 1983), pp. 145-46. Kiernan, 'History', p. 62. Marx and Engels became convinced that they had discovered a scientific model of society and its inevitable future. Marx's drive to act upon theory soon brought him into international communist circles. After heated debate at a congress in London in 1847, Marx and Engels were commissioned to write a political programme for the Communist League. Its result, the Communist Manifesto, can be regarded as Marx's and Engels' most outspoken vision on class struggle and the predestined faith of the proletariat. 'The proletariat', the Communist Manifesto stated confidently, 'goes through various stages of development'. Then, the manifesto goes on to speak in deterministic terms that the proletarian 'mission is to destroy all previous securities for, and insurances of, individual property'. Singer, Marx, pp. 14-15. Karl Marx and Friedrich Engels, The Communist Manifesto, cited in University of Warwick Insite, Department of History (20 November 2003),  URL  (21 December 2004). Ibid. If this scenario were to be the inevitable course of events, the logical question would be: why? Analysing history, Marx and Engels had discovered a continuous struggle for power between a powerful minority and a weak majority. They noted that 'All previous historical movements were movements of minorities, or in the interest of minorities'. However, it seemed that the proletariat would be able to break through this tendency. 'The proletarian movement is the self-conscious, independent movement of the immense majority, in the interest of the immense majority'. In this sense, it can be argued that for Marx and Engels, a primitive idea of democratic, or majority, rights served to justify a complex social theory of inevitable revolutionary struggle. Convinced about the power and improvement value of the theory of class struggle, 'His [Engels'] and Marx's historical vision was optimistic about the future, tragic when it turned to the past, as a record so dreadfully bad that any sacrifice was worth while to get away from it'. Ibid. Kiernan, 'History', p. 72. When Lenin became leader of the Bolsheviks in the early twentieth century, a debate among Marxists had been around for nearly fifteen years concerning the best way to bring about communism. One section of the Second International of 1889, under the leadership of Bernstein, had come to embrace the idea of social democracy, and of participation in parliamentary politics. Where Marx had been careful in his wording, or more abstract in his discussion of the democracy question, Lenin was more articulate in his discussion of the matter. He briefly defined the state as 'a special organization of force: it is an organization of violence for the suppression of some class'. Therefore, he argued, a revolution was the necessary agent for the transition from capitalism to socialism. McLellan, 'Politics', p. 167. In 1905 Lenin wrote Two Tactics, an article discussing the political direction Russia should take after the revolution. In this article he advocated a 'revolutionary democratic dictatorship of the proletariat and peasantry', a term that Marx had used only once, in a letter to a friend. What contrasted the Bolsheviks from the anarchists, Lenin argued, was that Bolsheviks thought it necessary to establish a transitory government to place the proletariat firm in the seat of power. However, months before the October revolution, in April 1917, he wrote that this would be 'a state without a standing army, without a police opposed to the people, without an officialdom placed above the people', based on Marx's example of his view of the Paris Commune. Eventually, in Engels' words, the government would slowly 'wither away' because it would become unnecessary. Ibid, p. 161. Ibid, pp. 154-55. V.I. Lenin, 'Letters on Tactics' in Marx - Engels - Marxism, p. 58. McLellan, 'Politics', p. 168 Some of Lenin's writing, though, already revealed some of the authoritarian traces that were to characterise the Bolsheviks' policy and the Soviet Union's later political development. Convinced, like Marx, of the inevitability of a proletarian revolution, he remarked: 'I said that there can be no government (apart from a bourgeois government) in Russia other than a government of the Soviets of Workers', Agricultural Labourers', Soldiers' and Peasants' Deputies'. Lenin, 'Letters on Tactics', pp. 389-90. By 1917, Lenin observed that the key conditions were fulfilled in Russia for a proletarian revolution to take place. An imperialist war was raging over Europe, according to Marxist theory calling in the culmination of capitalism. Conditions in Russia, he argued, were peculiar, as the country had already seen a revolution in February 1917. 'We have existing side by side, together, simultaneously, both the rule of the bourgeoisie [...] and a revolutionary-democratic dictatorship of the proletariat and the peasantry, which is voluntarily ceding power to the bourgeoisie, voluntarily transforming itself into an appendage of the bourgeoisie'. According to Lenin, the masses had 'been confused, led astray and deceived' by the temporary government of Lvov and Kerensky. He adopted Trotsky's idea of 'permanent revolution', arguing that the proletariat could not stop after overthrowing the bourgeois government. By not completing the dictatorship of the proletariat, the government itself had gone over to the side of the bourgeoisie. Columbia Encyclopedia, 'Vladimir Ilych Lenin: Theoretician and Revolutionary', Encyclopedia.com (2004),  URL  (27 December 2004). Lenin, 'Letters on Tactics', p. 386. V.I. Lenin, 'The Task of the Proletariat in Our Revolution' in Marx - Engels - Marxism, p. 380. In practice, much of Lenin's writing at the dawn of the October revolution appears to excuse the Bolsheviks' grasp for power and subsequent policy. Eventually, Bolshevik rule directly after the revolution proved to be highly undemocratic. In 1918, all political parties that did not support the Bolsheviks were systematically suppressed; while a temporary oligarchic, highly centralised government was formed, taking in members by co-option rather than by democratic elections. Rosa Luxemburg was among the prominent critics of Lenin's post-revolution, oppressive policy. 'The negative, the tearing down, can be decreed; the building up, the positive cannot', she argued. According to her, Lenin was standing in the way of a democratic learning process of the proletariat. 'The broadest democracy was needed', otherwise 'socialism will be decreed from behind a few official desks by a dozen intellectuals'. Although Lenin argued that such policy was needed in 'defence against counterrevolution and the actual elimination of everything that contradicted the sovereignty of the people', this, according to Luxemburg, was 'not the dictatorship of the proletariat, [...] but only the dictatorship of a handful of politicians'. McLellan, 'Politics', p. 170. Ibid, p. 163. Rosa Luxemburg, The Russian Revolution and Leninism or Marxism? , cited in University of Warwick Insite, Department of History (20 November 2003),  URL  (27 December 2004). V.I. Lenin, 'Two Tactics of Social Democracy in the Democratic Revolution' in Marx - Engels - Marxism, p. 177. Rosa Luxemburg, The Russian Revolution. By 1921, Lenin described the Soviet Union as 'a workers' state with bureaucratic distortion'. Trotsky actually became regretful, stating in a meeting that 'things are heading towards ruin'. Stalin, however, asserted in 1918 that at the time, the 'Bolsheviks did not doubt that bourgeois parliamentarism and the bourgeois-democratic republic represented a past stage of the revolution'. McLellan, 'Politics', p. 171. Joseph Stalin, The October Revolution (London, 1936), p. 168. Stalin, The October Revolution, p. 22. It is hard to place Stalin's policy in an unmitigated Marxist light. Despite its rhetoric, the Soviet Union of Stalin appeared to many something other than socialism. Theory was increasingly not matching policy. The state did not wither away, instead, a status quo of power was developing; one in which all lines came together in Moscow. An elite of scientifically based professional revolutionaries would 'show the way, formulating the program and policies, educating the people, and working out the strategy and tactics'. The character of the government had changed since the October revolution. Stalin's biographer, Deutscher, wrote: 'It has assumed office as a government of the people, by the people and for the people [...][now,] it ceases to be government by the people'. Indeed, it is arguable that the rapid industrialisation and agricultural reforms represented some elevated aspiration of the proletariat and peasants, however, the price of these reforms (The 1936 Great Terror and show trials) was disproportionate and tragic. Moreover, they revealed a highly undemocratic and unrepresentative government that stood separate from the masses. McLellan, 'Politics', p.173. Albert Weisbord, 'Bolshevism, Fraudulent Practice Of Democratic Centralism' cited in Weisbord.org (November/December 1976),  URL  (28 December 2004). I. Deutscher, Stalin: A Political Biography (Oxford, 1950), p. 175. McLellan, 'Politics', p. 173. The approximately hundred-year period over which the intellectual and political activity of Marx, Engels, Lenin and Stalin spread has seen the emergence of an immense social and political experiment of revolution and the move toward a communist state. The concept of democracy has over this time been much debated and scrutinised in order to deprive it of its 'bourgeois' elements. However, in the theories of Marx and Engels, there has been too little space left open for a pluralistic approach to state organisation. Likewise, Lenin and Stalin never achieved the 'withering away of the state' into a 'revolutionary-democratic dictatorship of the proletariat' upon the gap that fell after the October revolution of 1917. Communism, or some pure form of democracy within society's organisation has never been achieved. Singer, Marx, p. 103.",False
14,"The publication of The Bell Curve in 1994, the issues it discussed and conclusions it drew have triggered an enormous controversy over the years after its publication (Keenan 1999). This comes mostly from its assumed political incorrectness, claimed implications for public policy, and doubts from the scientific community as to its scientific claims (Keenan 1999). In this essay I shall attempt to take a closer look at some sociologically significant conclusions reached in the book. It seems, however, impossible to review the book without references to the sharp criticism it sparked upon publication. I shall therefore occasionally refer to critics or related academic work. The book sets out in an ambitious tone. In its introductory note, the authors forebode sensational conclusions, expecting 'that many people will have a ""This can't possibly be true"" reaction' (p. xix). It soon becomes clear that the book is intended for a wide audience, and throughout the book the authors continually insinuate this presumption. It is a point that is well worth noting, as we shall later see. The intention of the book, however, is not very clear at the outset. Vague notions such as 'for the last thirty years, the concept of intelligence has been the pariah in the world of ideas' (p. 1), and further, 'received wisdom in the media is roughly 180 degrees opposite from each of the six points [that we assume as a basis]' (p. 23) leave the reader with the idea that the authors are to set straight a grave misunderstanding. The six points which the authors write about are of great importance throughout the book. They are six assumptions from the classicist school about intelligence (or cognitive ability, two terms that are used interchangeably in the book) and the place IQ testing takes in expressing a person's intelligence. Especially claim five and six, respectively, that properly administered IQ tests are not biased, and that intelligence is 'substantially heritable [...] no less that 40 percent and no more than 80 percent' (p. 23) are facts that the authors lean on particularly heavily. According to critics, the problem is that these claims involve more complication and nuance than an average reader may understand, and that the consensus among psychometricians that the authors mention is not as broad as their own claims. A critic of the book, professor in psychology Robert Sternberg reacts as follows: 'I think that there is definitely some heritability of intelligence in the White population. Almost every psychologist believes there is some heritability of IQ and I agree. But the public may not understand just what that means. If you accept the use of the heritability statistic, about .5 [50%] is probably right' (Miele 1995). Throughout the book, moderate claims moulded into more radical views and outcomes are commonplace. This becomes especially controversial and fragile as topics around intelligence difference between races are being discussed (Frisby 1995). The virulent attack on the most controversial matters discussed in the book, according to Murray, 'often read like an unintentional confirmation of our view of the ""cognitive elite"" as a new caste', while 'when the Sturm und Drang has subsided, nothing important in The Bell Curve will have been overturned. I say this not because Herrnstein and I were especially far-sighted, but because our conclusions are so cautiously phrased and our findings anchored so securely in the middle of the scientific road' (Upstream). To put these claims in context, it is necessary to look at some of the criticism of the book. The Bell Curve has been accused of presenting conclusions on a pseudo-scientific basis. As the book is quite substantial, and the authors continually moderate the argument besides cross-referencing chapters, it is difficult to either speak completely for or against the type of science that The Bell Curve presents the reader with. However, the idea that Sternberg brings up is worth noting. He says: 'The way that book is written is to, I think, say X on page 605 in sentence 8, with an appropriate caution, and then invite the reader to a somewhat more extreme conclusion elsewhere' (Miele 1995). In line with this claim, we find the authors introducing the idea of a possible constant deviation in intelligence between ethnic groups in a casual way: 'Americans have watched the spectacular economic success of the Pacific rim nations' (p. 272), as if this were a logical consequence of a population's higher IQ average. In the unfolding chapter 13, which deals with variations of cognitive ability between ethnic groups, the book continues to provide the evidence for trends in intelligence difference between 'Asians' and 'Whites'. For this evidence, the book leans heavily upon the work of Lynn. A key claim, that IQ differences in the results are not generally found in culturally biased assignments, is here backed by only one source, Jensen. The work of this scholar, however, is certainly not uncontroversial and may even be racially motivated (Keenan 1999). Thus, where Murray and Herrnstein claim that 'there is no longer an important technical debate' (p. 282), Sternberg counters: 'To the extent that there is a consensus it is certainly not Herrnstein's and Murray's' (Miele 1995). Nonetheless, it is no more than fair to put forward Murray's response in the afterword of the book: 'Never mind that The Bell Curve draws its evidence from more than a thousand scholars. Never mind that among [the accused scholars] are some of the most respected psychologists of our time' (p. 564). Possibly the complication with the data used lies not so much in its collectors as in the theory behind it. In order to further their argument, the authors take a great and careless leap beyond their use of the concepts of 'race' and 'ethnicity', concepts which, according to Frisby, are 'best left to specialists' (Frisby 1995), while a UNESCO report argues that 'The term [race] at best is at present time not really allowable on any score in man' (Miles 1982). How then do the authors of The Bell Curve justify their use of ethnicity (which, again, may be taken as interchangeable in the book)? Only a paragraph seems enough: 'The studies of ""blacks"" or ""Latinos"" or ""Asians"" who live in America generally denote people who say they are black, Latino or Asian - no more, no less' (p. 271), and further they write 'race differences are varied and complex - and they make the human species more adaptable and more interesting' (p. 272). In this way, the authors appear to draw a direct connection between personally perceived ethnic differences and the distribution of cognitive abilities for different human groupings. It can, however, conversely be argued that, like all over the world humans are expected to be born with two arms and two legs, the distribution of intelligence may be expected to fit into a similar predictable model worldwide. Opinions are still so divided about the issue that different conclusions are reached from the same data. As an example, the conclusions of Flynn are refuted by the authors as follows: 'He also says that Asian-Americans actually have the same non-verbal intelligence as whites', but 'Lynn disagrees and concludes from the same data [my italics] that there is an ethnic difference in overall IQ as well' (p. 273). Perhaps the contrast cannot be presented starker than through the following example. Whilst the authors argue that 'tests results are matched by analyses of occupational and scientific attainment by Jews, which consistently show their disproportionate level of success' (p.275), Keenan (1999) insists: 'The Jews provide another example of the blending of racial distinctions. They are all presumably derived from one population [...] Gene frequency studies, however, show that now Jewish populations in different parts of the world tend to resemble their surrounding populations at least as much, if not more, than they do each other'. The book consists of four parts comprising some 500 pages of textual body and several appendices providing more background. Part III, nonetheless, presents the most important and controversial findings. The authors recognise this, claiming, 'that many readers have turned first to this chapter indicates how sensitive the issue has become' (p. 270). It is difficult to accuse the authors of racist motives. Placing possibly controversial outcomes into a thought exercise, the authors state 'some people would interpret the news as a license for treating all whites as intellectually superior to all blacks [but it becomes] obvious how illogical [...] such reactions would be' (p. 312). It can, however, easily be said that the authors jump to conclusions about racial differences too soon, to go on suggesting changes in public policy in part IV. The problem with this book thus lies with its scientific inconsistency. It sets out with no clearly stated objective, while the use of scholarly caution and polite integrity is broken soon after the authors bring it up. While the authors promise in the introduction to 'employ the more neutral term cognitive ability as often as possible' (p. 22), synonyms such as 'smart' (p. 127), 'very smart... Very dumb...' (p. 133) that occasionally crop up nearly indicate the carelessness with which the authors draw their conclusions. Frisby claims that 'to parade facts about racial differences before the general public is akin to putting a lit match to gasoline' (Frisby 1995). The authors of The Bell Curve are eager to draw conclusions and that it is what lends this work its importance, but also its legions of hostile critics. In the words of Sternberg: 'If you were to ask, ""What inference do Herrnstein and Murray invite their readers to draw?"" they go beyond what they know' (Miele 1995). It was in 1903 that Emmeline Pankhurst and her daughter Christabel founded the Women's Social and Political Union (WSPU), following around forty years of organised campaign for female suffrage organisations in Britain (Banks 1981). After another fifteen years of campaign, interrupted by the First World War, women were finally granted the vote in 1918 through the Representation of People Act. It remains, however, a controversial matter as to whether the militant campaigns of the WSPU actually helped to quicken the vote for women, or not. In order to get a clearer picture of the tactics of the suffragettes, it is worthwhile to take a closer look at their use of the female body in violent, unconventional and often illegal ways, to draw attention to their cause. The connection of British femininity with a high morality, and the idea of gender equalities through historical argumentation were common ground arguments for the vote in the late nineteenth century. Although arguments of the 'constitutionalists' always sought to stay within the boundaries of middle class respectability, they certainly incorporated argumentation based upon the female body (Holton 1998). The most evident examples of this can be found in racist theories. Female authors attempted to present an image of a superior British race, of which women, had, by necessity, always been part. Charlotte Carmichael Stopes', in her book British Freewomen of 1894, argued that women's right to political participation originated from the ancient Britons. 'The moral codes, sexual equality in physical height [my italics]' were, in her book presented as arguments for women's suffrage (Holton 1998: 158). Constitutionalist feminists increasingly began to make use of racial reasoning to support their campaign for the vote (Holton 1998). This provided the movement with a legal means of enhancing female respectability and high morale in a way that was compatible and in harmony with society. However, after forty years of such campaigns, the women's vote was still nearly as far away as it had been at the outset. This realisation caused the WSPU to seek to pressurise the government, for they were responsible for the problem (Pugh 2000). From a harmony model, the dominant suffrage campaigns thence shifted to a model of conflict, bringing the movement into a new phase (Banks 1981, Holton 1998). The suffragettes, as the WSPU activists came to be known, sought to cut right through to the core of the problem by addressing the government directly. They sought to point out the inherent contradictions of the political system as it was: the partial inclusion of women into an essentially male-dominated environment (Lawrence 2000). From insisting on politicians' support in public meetings, the suffragettes soon radicalised (Vicinus 1985). They felt that the suffrage question was not dealt with seriously, and from there the WSPU leader Christabel Pankhurst set out to phrase the problem more directly: '[i]f Mr Asquith [PM] would not let her vote, she was not prepared to let him talk' (Lawrence 2001). This meant a great leap away from the Victorian feminist movement; suffragettes sought to replace the passive, homely housewife with a campaigning activist, a political being. In the words of the prominent suffragette Emmeline Pethick-Lawrence: 'Not the Vote only, but what the Vote means - the moral, the mental, economic, the spiritual enfranchisement of Womanhood, the release of women...' was of vital importance to women (Vicinus 1985: 249). A new stage of interrupting meetings started. Suffragettes continually disrupted parliamentary debates, by shouting out against the speaker. Increasingly coordinated, the suffragettes were sometimes able to spoil a complete speech, by disrupting the speaker in turns. In 1908, a speech by Lloyd George was continually interrupted for two hours, with fifty women carried out (Pugh 2000). Although this obstruction of the political process was arguably playing into the hands anti-suffragists, such forceful, physical practices of politics can be said to have been part of masculine politics as it was conducted by male politicians (Lawrence 2001). Indeed, when members of the audience could be forcefully removed from a public political meeting, this might be interpreted as a threat to the civil liberties (Pugh 2000). This meant a moral victory for the suffragettes, especially since gentlemanliness towards women was expected of politicians. Mass assault and arrests of women were not an uncommon sight any longer, which the particularly violent events of 'Black Friday' in November 1910 highlighted (Vicinus 1985). Hence, the suffragettes increasingly highlighted this state brutality by a number of means. Hunger striking was one of them, first begun on the initiative of Marion Wallace Dunlop in July 1909 (Pugh 2000). Protesting against the government's refusal to grant the suffragettes the status of political prisoners, the WSPU soon managed to place the campaign for female suffrage on a moral high ground, as the government had to face the issue of the prisoners' treatment. The WSPU brilliantly publicised this moral strength, speaking of 'moral right triumphing over physical force' (Vicinus 1985). Forcible feeding of hunger striking suffragettes soon received criticism from doctors (Pugh 2000) and graphic representation presented a shocking picture of the treatment of women in prisons. The problem continued, and leaving even the parliament divided (John 2001). In 1913, the Cat and Mouse Act was thus passed, which dismissed the policy of forcible feeding and was aimed at avoiding the negative publicity deriving from it. To some extent, the act succeeded in doing so, as many people argued that their suffering was 'self-imposed and their martyrdom as in some sense staged' (Harrison 1978: 180). This loss of sympathy and moral and intellectual high ground was however enhanced by the suffragettes increasing radicalisation and alienation from sympathisers (Pugh 2000). The suffragettes showed a radical impatience and determination that eventually led them to virtually abandon any techniques of persuasion (Pugh 2000). The years in the run towards World War I resulted in the most passionate outbursts of the suffragettes attack upon male domination of the political system (Vicinus 1985). A first systematic window breaking campaign had been undertaken in 1909, of which Sylvia Pankhurst said, 'let it be the windows of the Government, not the bodies of women which shall be broken' (Lawrence 2001: 222). A near concession of the parliament in 1910, which would have resulted in a limited vote for women, could not be passed by a much divided parliament. Winston Churchill, usually a sympathiser of the women's suffrage cause, voted against the bill in resentment of the WSPU's violent tactics (Harrison 1978). However, this failure so outraged the rank-and-file WSPU members, that another window breaking campaign was started by the end of 1911 (Vicinus 1985). Popular support for the WSPU was on the decline. The violence of the suffragettes' campaign made it possible to argue that women did not demonstrate the capability to participate in politics due to their rash and unreasonable behaviour. Criticism of the Union's leaders, Emmeline and Christabel Pankhurst, even enhanced this view. 'They are not calm women,' Teresa Billington-Greig claimed, 'but irritable, yielding to sudden tempest' (Harrison 1978: 176). And whilst, 'the police, in early stages [...] avoided heavy-handed treatment of the women' (Pugh 2000: 193), they still insisted on continued provocation and imprisonment (Pugh 2000). In 1912 Christabel Pankhurst fled to Paris after the police raided the WSPU headquarters in London. As the movement was losing grip of its popular support, so did Christabel break with some of the high-ranking campaigners, among them the Pethick-Lawrences whom had up until then financed the several branches of the organisation (Pugh 2000). The publication of the 'The Freewoman', a periodical by a number of suffragettes, between 1911 and 1913, 'reawakened the free love fears that had haunted the feminist movements since its beginning' (Banks 1981: 132). The Pankhursts, however, had always insisted on dressing in feminine and respectable manner to disarm opponents from criticism (Pugh 2000). And despite arson, window breaking and other violent behaviour of WSPU movements, the organised anti-suffragist movement was cautiously following the skill with which the suffragettes sought publicity. As correspondence of October 1913 shows, the anti-suffragist movement was well aware of the press attention that the suffragettes managed to obtain. '[P]ublicity in the Press', an executive committee member wrote, 'is our greatest need and our opponents' chief advantage over us' (Harrison 1978: 175). It was this sparked public debate, and the attitude of directness and briskness in contrast to the vagueness and eventual moral weakness of the anti-suffragists over the real debate of women's suffrage, that eventually led the parliament to extend the vote to women over thirty. The suffragettes' efforts can be said to be characterised by a curious mixture of Victorian respectability and morality blended with a continual and controversial insistence on their rights. An immense confidence in the female gender to the extent of superior feelings (Vicinus 1985) combined with a strong sisterhood and sense of sacrifice for the cause led Sylvia Pankhurst to burst out even in 1954 against anti-suffragist Violet Markham: 'that foul traitor - who, while the suffragettes were hunger striking, appeared on the Albert Hall platform, [...] protesting against women having the vote' (Harrison 1978: 13). Resultantly, at all times, the suffragettes used their bodies as a symbol of sovereignty and moral superiority over the established political power, and this to point out the flaws in the anti-suffrage argumentation.","Rousseau has always been typified as an intellectual outsider; it was, however, only in the twentieth century that his socio-political theory came to be seen as a threat to the individual's liberty. Emphasis thereby is often laid on his concept of a 'general will', through which personal liberty is handed over to the collective. 'The citizens having but one interest', Rousseau argues, 'the people [have] but a single will'. And although Rousseau favours the ballot vote, he argues for the infallibility of its ruling: 'when the opinion contrary to my own prevails, it proves nothing more than that I made a mistake, and that what I took to be the general will was not'. From the point of view of Rousseau, most importantly protection should be rendered against the emergence of a minority rule, or 'particular will', rather than against the state, which, after all, represents the sovereignty of the lawgiver as it corresponds with the community's best interest, the general will. The question I will look at here is thus to what extent the critical assertion holds truth that 'the more safeguards he established against despotism, the more weapons he forged for tyranny', within Rousseau's system of the social contract. Iain Hampsher-Monk, 'Rousseau and totalitarianism: with hindsight?' in Robert Wokler (ed), Rousseau and Liberty (Manchester/New York, 1995), p. 268. Jean-Jacques Rousseau, The Social Contract and Discourses (London/New York, 1952), p. 87. Christopher Bertram, Routledge Philosophy Guidebook to Rousseau and The Social Contract (London, 2004), p. 120. Hampsher-Monk, 'Rousseau and totalitarianism: with hindsight? ', p. 271. Recent criticism has argued that the Cold War split has caused intellectuals to make an over-simplified distinction between 'individualistic liberalism and state collectivism', stating that Rousseau was on the totalitarian side of the argument. Rousseau, however, from the outset entertains an awareness of the problem of new forms of subjugation that hide behind a powerful state, and thus sets out to construct a model for a 'form of association [...] in which each, while uniting himself with all, may still obey himself alone, and remain free as before'. In this sense, Rousseau undertakes to find a form in which freedom can be exercised on a public, political level. The important distinction that has to be made here though, is the type of freedom at stake. Arguing that pre-modern, pre-social man exercised an individual sovereignty free from social obligation or commitment, knew a natural, brutal kind of liberty. When people come to regular interaction, they become interdependent, which goes accompanied by fierce passions. It is only within society that morality comes to exist, through virtue and pitié, a form of compassion which counters man's corruption by society. What is then needed is the construction of an association in which passions are tempered and virtue is encouraged. In this sense, envisaging a social contract is merely a theoretical exercise, to find a form in which such an exercise of sovereignty can be best expressed, in other words, 'Rousseau's purpose is to discover a rule which has moral validity'. Hampsher-Monk, 'Rousseau and totalitarianism: with hindsight? ', p. 268. Rousseau, The Social Contract, p. 12. Patrick Neal, Liberalism and Its Discontents (London, 1997), p. 61. Rousseau, The Social Contract, p. 5. Maurizio Viroli, Jean-Jacques Rousseau and the 'well-ordered society' (Cambridge, 1988), p. 132. Christopher Bertram, Routledge Philosophy Guidebook to Rousseau and The Social Contract (London, 2004), p. 19. Viroli, The 'well-ordered society', p. 121. Another point of view insists on the juxtaposition of democracy and liberalism, positioning Rousseau in a radically democratic framework. Certainly, this view depends greatly on the view one takes of liberalism. Liberalism can be taken at an individual level, stressing the removal of any constraint on human action (negative liberty), and placing theory in wider context, that of the body politic (positive liberty). Rousseau is quite explicit about what form of liberty we are to seek. 'Man is born free; and everywhere he is in chains', he argues, 'One thinks himself the master of others, and still remains a greater slave than they'. It is therefore clear that Rousseau seeks a wider answer to the question of liberty, one for which 'it is necessary to ensure that the conditions in which [people] live do not pose a threat to their survival'. This condition can be brought about by the realisation that it is the best interest of all citizens to care for other's interest and well-being. Here we find the distinction that Rousseau makes between a narrow form of personal interest and a wider, more coherent form of enlightened interest, being the best for the community, and thus, indirectly, each citizen's actual will, as it secures the best-considered proposal and enjoys general support of the citizenry. Subordination to the law subsequently only becomes an exercise of one's liberty. Carol Blum, Rousseau and the Republic of Virtue. The Language of Politics in the French Revolution (Ithaca/London, 1986), p. 32. Hampsher-Monk, 'Rousseau and totalitarianism: with hindsight? ', p. 271. Rousseau, The Social Contract, p. 3. Viroli, The 'well-ordered society', p. 112. Bertram, Rousseau and The Social Contract, p. 44. Exercising one's freedom through abiding by the law is only reasonable if lawgiver and subject are the same person. This is where Rousseau takes a radically democratic stance, criticising Kant's very open assertion that law can still be legitimate only by the requirement that the requirement 'that the proposal could have been agreed to had the citizens been consulted and were they fully rational'. Hence Rousseau's claim that 'power indeed may be transmitted, but not the will': for the general will to function, it requires the individual standpoints of each citizen. Unanimity is thereby not needed; it is in fact a sign of an unhealthy body politic. The only law that requires unanimity of all citizens is the social pact, since by this each individual hands over their individual sovereignty to the higher expression of the community. Within the body politic, the general will aims to protect rather than to attack its participants, and 'it is not', therefore, 'to be assumed that any of the parties then expects to get hanged'. Likewise, peace with neighbouring communities is required by natural law. It has therefore been claimed that the general will represents a model for the well-being of all, and thus reflects 'each individual's good or real will'. Neal, Liberalism, p. 63. Rousseau, The Social Contract, p. 20. Ibid, pp. 87-88. Ibid, p. 28. Hampsher-Monk, 'Rousseau and totalitarianism: with hindsight? ', p. 270. Ibid, p. 270. The emphasis that Rousseau lays on the collective, however, is today regarded with mistrust. After all, the functioning of the general will depends entirely upon the full dedication of the citizenry to the social contract, and we find 'no restriction of principle on what it may command in that form'. It appears at times as though Rousseau puts all individual risks at right for an ideologically unified community. As soon as all sovereignty passes from the individual to the state, a greater power comes to exist which '[man] cannot use without the help of others'. Contrarily, there is no 'safeguards for the individual against the state'. The Rousseauean state results in collective socialisation, only rendered legitimate through the consent of its participants. In a wider interpretation of the social contract as merely a thought experiment, this socialisation obtains a somewhat enforced character. Bertram, Rousseau and The Social Contract, p. 193. Blum, Republic of Virtue, p. 111. Hampsher-Monk, 'Rousseau and totalitarianism: with hindsight? ', p. 272. Ibid, p. 268. One form of criticism of the general will draws parallels with the Jacobin rule during the French revolution. Both reveal traces of absolutist totalitarianism: 'It is absolutely necessary that evil come from others and from them alone'. Despite the validity of a critique of Rousseau's reassertion that the general will is infallible (which indeed shows traces of a majority intolerance), it is difficult if not altogether impossible to find examples of the Rousseauean state in practice. Eastern block socialism, which has been suggested as imposing some for of a state-directed will upon its citizens, has been severely criticised. Also the Jacobin adoption of Rousseau's general will has been seen as of symbolic value rather than a precise influence of his thought. Lacking everywhere has been the binding together of lawgiver and subject, which according to Rousseau results only in the imposition a particular will of a state governed by a political elite. Jean Starobinski, 'La Mise en accusation de la Société', cited in Carol Blum, Rousseau and the Republic of Virtue. The Language of Politics in the French Revolution (Ithaca/London, 1986), p. 218. Blum, Republic of Virtue, p. 32. Hampsher-Monk, 'Rousseau and totalitarianism: with hindsight? ', p. 272. It is true that Rousseau is most concerned with the organisation of social order based upon the exercise of virtue, rather than on preserving the individual's relative freedom of action. It is therefore perhaps surprising that Rousseau leans entirely on the exercise of a general will in ensuring the best for all citizens. Through this construct, Rousseau categorically denies the individual any sovereignty except for a participatory role in the community. Effectively, the state attains a strongly fortified position in which ' [t]he more the natural strengths are dead and destroyed, the more the acquired ones are great and durable, and the more solid and perfect this institution is'. Such a radical step toward an active, creative form of liberalism challenges liberals into the debate of the equalisation of the citizenry, since 'serious economic inequalities destroy liberty'. Nonetheless, it does not suffice to, as a consequence of this conclusion, leave all individual property and grounds for action within the confines of the community's sovereignty. Viroli, The 'well-ordered society', p. 113. Blum, Republic of Virtue, p. 111. Hampsher-Monk, 'Rousseau and totalitarianism: with hindsight? ', p. 275. One of Rousseau's ideas of human nature was that the strongest of human passions was amour the soi (love of self), of which virtue and piti éwere manifestations within society. 'No-one has any interest in making them [the conditions of association] burdensome to others' This may be seen as a problem for the protection of the individual's rights: Rousseau does not expect the intrusion of state into the individual's life and thus includes no precautions to safeguard certain domains of private life. 'If this leads to intervention in 'areas which are perceived as illiberal, such as the social and economic conditions of citizen's lives and with their beliefs and values', this is seen by Rousseau as a decision of the general will in order to secure the necessary conditions for socio-political liberty. To some extent the modern welfare state can be seen as a moderate outcome of Rousseau's general will, be it on a representative basis. Executive and lawmaking government of the welfare state is now generally expected to create policy that benefits the community in a broad social sense. Taxation, health security and economic intervention are put in place to equalise conditions and ensure a minimum standard of welfare for each citizen, whereas participation in elections for government positions is open to each citizen. This form of moderate liberal welfare state, however, fails to pass the test, since in this model the law does not come from all. Bertram, Rousseau and The Social Contract, p. 24. Hampsher-Monk, 'Rousseau and totalitarianism: with hindsight? ', p. 281. Ibid, p, 275. Bertram, Rousseau and The Social Contract, p. 157. The dichotomy between modern, individualist liberalism and the Rousseauean state lies predominantly in their different interpretations of liberty. Whereas the former stress the lack of protection for the individual in Rousseau's social contract, Rousseau in turn entertains concerns with the crude, unsocial kind of liberty that individualists profess. Rather does he see liberty in a community best exercised at the highest level, thus enhancing liberty in an ordered way. For order to exist, there needs to be unity based on a reasonable principle; the social contract offers just that. As liberty exercised only on the most individual, narrow level only furthers contradiction in interests, Rousseau proposes the pursuit of community interest, which, if carried out fully, secures the welfare and safety of all its citizens. What Rousseau does not display enough attention to, however, was the fact that 'there is never a perfect fit between moral requirements of our rational nature and requirements of citizenship'. This results in a lack of consideration of the nature of the general will, and its proper domain. Rousseau, on the contrary, believes in the stabilising role of the general will, and the inalienability of the will in general. As the general will represent the only genuine method of determining the sovereign will of the citizenry, parallels drawn between totalitarian ideological states and Rousseau's general will are, strictly theoretical, incorrect. The problem, nonetheless, lies with the radical solution Rousseau's general will poses. Asserting that '[t]o be governed by appetite alone is slavery, while obedience to a law one prescribes to oneself is freedom', leaves very little space for individual action. Thus, a community taking its legislative task to the extreme, might indeed force its citizens into abide by the law. The question becomes, then, not to what extent Rousseau can be used for totalitarian ends, which he explicitly sets out to prevent, but rather how a feasible system may be envisaged in which particular interpretations of the general good can obtain a stable construct in which the individual is both more protected and better positioned to criticise the state's pursuit of the general good. Viroli, The 'well-ordered society', p. 38. Christopher Bertram, Routledge Philosophy Guidebook to Rousseau and The Social Contract (London, 2004), p. 33. Rousseau, The Social Contract, p. 20. Neal, Liberalism, p. 60. Christopher Bertram, Routledge Philosophy Guidebook to Rousseau and The Social Contract (London, 2004), p. 194. IntroductionWhen we talk about the 1930s in terms of social history, we often refer to the strong emergence of ideology into the public domain. Moreover, for Latin American countries, the first half of the twentieth century marked the coming of 'modernity'. Other factors, however, such as immigration and a rumbling social hierarchy in the aftermath of slavery, tended to influence society in more informal ways. Self-representation through culture was usually closest at hand for the lower classes, and often provided a colourful means of communicating a multi-faceted message. In this essay we shall look at such cultural representations in Argentina, Brazil and Mexico, and assess the extent to which they contributed to the construction of a national identity. Naturally, art forms such as dance and music were, even though powerful, not the only way of expressing one's national sentiments. The first films with sound managed to reach a mass audience and demonstrated what it was like to be a true Argentine, or Brazilian, in a light package. Moreover, the aftermath of key political events (Argentina, the 'infamous decade' after the fall of Yrigoyen; Brazil, the 1930 coup of Vargas; Mexico, the heritage of the revolution) and the subsequent split among the people brought a new literature of thinkers focussing with urgency upon 'national character and the metaphysics of the Mexican, Argentinian or Brazilian 'being''. The assessment of each country's culture climate thus provides interesting insights into the attempts made at a culturally constructed nationhood. Edwin Williamson, The Penguin History of Latin America, (London, 1992), p. 513. Gerald Martin, Journeys through the Labyrinth. Latin American Fiction in the Twentieth Century, (London/New York, 1989), p. 67. Argentina: 'a land of exiles' Jorge Luis Borges, cited in in Gabriel Nouzeilles and Graciela Montaldo (eds. ), The Argentina Reader. History, Culture, Politics, (Durham, London, 2002), p. 1.The Argentina of the 1930s saw increasing modernisation and a changing political climate. This did not leave the notion of nationalism untouched; and, after the social positivism of the Irigoyen government that was overthrown in 1930, cultural representations of the Argentine nation became increasingly ambiguous. The 'Década Infame' brought a reconsideration of Argentine patriotism and the essence of 'Argentineness'. A new definition of the Argentine identity proved, however, not quite unproblematic. A nation of immigrants, Argentina had many faces and was, with the coming of further modernity and political maturing of the masses, thrust into a further set of cultural disparities. These were based upon dichotomies as varied as those of machismo versus feminism, the European-orientated metropolitan versus the patriotic gaucho, and that of the definition of a high culture versus that of commercially formed mass culture. Essentially, thus, the concept of argentinidad was politically and culturally utilised in a multitude of ideological representations: 'The references to a collective ideal of the argentinidad were continuous [...], although this ideal was defined in the most imprecise way and was basically the product of the whims and wishful thinking of the ideologues who wrote on the subject, more than an ideal that could be identified empirically as a trait of Argentina's culture'. Carlos Escudé, 'The Case of Argentina',  URL , (2000), viewed 2 May 2005. An early elaboration upon Argentine culture and its forms can be found in Sarmiento's Facundo of 1845. Sarmiento, Argentina's president between 1868 and 1874, regarded the countryside, inhabited by the mestizo gaucho Argentine, as uncivilized, chaotic - even barbarous: 'dirty children covered in rags, living amid packs of dogs; men stretched out on the ground, in utter inactivity; filth and poverty everywhere'. The country's biggest cities, on the contrary, contained every aspect 'that characterizes cultured people'. Later, the popular poem about Martín Fierro created a distinction between the good, heroic and patriotic gaucho, and the barbarous and violent bad gaucho. The early twentieth century thereafter saw a complete rehabilitation of the gaucho icon, when it was presented as an example of argentinidad in an increasingly immigrant-dominated society. This was, however, merely a constructed and idealised identity, and the by the end of the twenties, the plurality of immigrant cultures were gaining space from more traditional social and cultural dance clubs with names like 'The Patriotic Gauchos'. In its stead came a new form of dance, less courteous and more fitting in the context of the big metropolis Buenos Aires: tango. Domingo Faustino Sarmiento, Facundo. Civilization and Barbarism, (London, 2003), p. 51. Ibid, p. 52. William Rowe and Vivian Schelling, Memory and Modernity. Popular Culture in Latin America, (London, New York, 1991), p. 34. Leopoldo Logunes, 'National Identity in a Cosmopolitan Society' in The Argentina Reader, p. 209. William Rowe and Vivian Schelling, Memory and Modernity. Popular Culture in Latin America, (London, New York, 1991), p. 35. The thirties in Argentina were marked by modernising development from above and from below. The tango started its ascending popularity from below. A dance of immigrants, it was originally popular in the packed, miserable and crime-ridden suburbs of Buenos Aires. Its association with knife-fighting and gangs was famously put into words in Borges' short story 'Streetcorner Man' of 1933. In it, two gang leaders head into confrontation on an illegal tango party. Ibid, p. 35. 'The only reason I don't carve you up is because you sicken me,' the Butcher said to Rosendo, threatening to strike him. That same moment, La Lujanera threw her arms around the Butcher's neck, fixing those eyes of hers on him, and said in a fury, 'Let him alone - making us think he was a man.' For a minute of so [the Butcher] was bewildered. Then, wrapping his arms around La Lujanera, he called to the musicians to play loud and strong, and he ordered the rest of us to dance. Jorge Luis Borges, The Aleph and Other Stories, (London, 1973), p. 27. These aspects - machismo, violence, illegitimacy, made dancing the tango into a rebellious act, associated with characters on the edge of society, devoid of patriotic virtue or morality. This was soon to change, as the tango was brought to Europe and became popular in the salons of avant-garde Paris. Before this shift, the tango only gained marginal recognition as a truly Argentine metropolitan dance, as in the poem 'Milonga' by Oliverio Girondo. Rowe and Schelling, Memory and Modernity. Popular Culture in Latin America, (London, New York, 1991), p. 36. Males whose bodies rupture in a ritual court, their heads sunk low between their shoulders, their lips thick with coarse remarks. Females with their nervous rumps, bits of foam at their armpits and their eyes looking much too oily.Oliverio Girondo, 'Poems to Be Read on a Trolley Car' in The Argentina Reader, p. 252-53. A second exponent of tango, nevertheless, soon found through tango a new formula for political criticism, woven into poetical lyrics. The song 'Cambalache' by Enrique Santos Discépolo, written in the early thirties, counterposed the liberal positivistic nationalism with a gloomy, destructive view of the new Argentina. 'Those that don't cry don't feed', Santos sang, 'and those that don't steal are fools'. Enrique Santos Discépolo, 'Cambalache', in The Argentina Reader, p. 267. Despite this loathing with politics, the middle class adopted the tango as an aspect of the new, dynamic and modern culture that Argentina was now adopting. In this, the cinema played a big, yet not unambiguous role. Argentina was steadily adopting the Northern American cinema culture. In 1930, around a thousand film screens could be found throughout the country. However, already by 1926 an estimated 95 percent of screen time was filled by films from the United States. Tango reached a mass audience when sound was added to cinema films, and a number of glamorous films with appealing tunes lent this formula its great success. Unfortunately this also caused tango to become a mass cultural phenomenon, alienated from its former basis of charismatic metropolitanism. By the late thirties, fifty such films were produced a year, also serving as a charming and successful export product for other Latin American countries. John King, Magic Reels. A History of Cinema in Latin America, (London, New York, 2000), p. 9. Ibid, p. 11. Ibid, p. 38. While the metropolitan immigrant gained commercial ground and mass appeal, other cultural forces were working from above, posing questions at a more political and intellectual level. X-Ray of the Pampa was published in 1933, melancholically referring back to inner country Argentina as a real source of Argentine national identity. In the essay, Ezequiel Martínez Estrada accused the central government in the metropolitan centre of cultivating city values at the cost of the 'countryside, which is full of truth and life'. Minority immigrant groups, faced by the dominance and lack of understanding of the overwhelmingly Catholic Hispanic cultural majority, attempted to secure themselves a place within the Argentine identity. Jews, who were often particularly victim of racism, attempted, with relative success, to bring the issue to attention in the theatre. A number of Jewish playwrights wrote plays on Argentine Jewish families and their cultural problems, with strategies varying from the complete abandonment of any distinctive Jewish identity', to the incorporation [...] of Argentina's many immigrant groups, their concerns and their idioms'. The playwright César Tiempo in 1937 even won a prestigious theatre prize for one such play with a light-hearted tone, Pan criollo [Native Bread]. Ezequiel Martínez Estrada, 'X-Ray of the Pampa' in The Argentina Reader, p. 262. David William Foster, Cultural Diversity in Latin American Literature (Albuquerque, 1994), p. 97. Foster, Cultural Diversity, pp. 110-11. Ibid, p. 111. The literature of Jorge Luis Borges forms a totally separate category in Argentina's intellectual life of the thirties. Whilst his outlook towards nationalism was complicated and peculiar, both the political left and right came to agree 'that what the fictions display is mastery'. Initially, Borges became part of the capital's avant-garde poetry circle, whose most famous representative was Oliverio Girondo, and his first published poems still breathe the excitement of metropolitan life. By the 1930s, however, Borges came to reject Martín Fierro's nativistic idea of argentinidad as well as the avant-garde as 'infantile disorders'. Perhaps due to some drastic changes in Borges' life over the second half of the thirties, among which the death of his father, a short period of serious illness and a mindless job as a librarian, Borges increasingly came to see all experiences as individual. Increasingly, Borges disapproved of simplistic, mono-cultured concepts of Argentine nationalism, in which some groups were downplayed in favour of others. Instead, he proposed that Jean Franco, Critical Passions: Selected Essays (Durham/London, 1999), p. 328. Martin, Journeys through the Labyrinth, p. 153. Franco, Critical Passions, p. 331. Martin, Journeys through the Labyrint, p. 153. we cannot limit ourselves to purely Argentine subjects in order to be Argentine; for either being Argentine is an inescapable act of fate - and in that case we shall be so in all events - or being Argentine is a mere affectation, a mask.Jorge Luis Borges, cited in Martin, Journeys through the Labyrinth, p. 155. This deconstructionist view of argentinidad cut short all aspirations toward an inward-looking, 'authentic' national identity, and hence, Borges later came to be regarded as an early exponent of postmodernism. Throughout the 1930s, Borges was still very much in search of new values, comparing the Argentine with the Jew, whom, according to him both had no 'particular national tradition', while the entrenchment in a narrow set of national values was 'negative, or, at the very least, uncreative'. Soon, he would move on to a path of displacing, often surprising literary creations, revealing elements of both ironic genius as elitism. Franco, Critical Passions, p. 327. Ibid, pp. 330-31. Williamson, The Penguin History, p. 516. Martin, Journeys through the Labyrint, p. 159. Brazil: 'All hail! This samba's going to end in jail'  Patrícia Galvão, 'Where They about Rosa Luxemburg', in The Brazil Reader, p. 166.The 1930s as a definable decade for Brazil started in October 1930, when a group of young army officers overthrew the long-standing São Paulo oligarchy and brought GetúlioVargas to power - a dictatorship that was to last until 1954. This inaugurated an era of modernisation to the country, and a new search for a national identity. Gradually, art forms and cultural expressions of the lowest classes (in Brazil this mainly meant the former slaves) became accepted by a wider public, and eventually even representative as national symbols. Dances like the samba and capoeira and the carnival of Rio de Janeiro - though often in commercialised and regularised forms- now became celebrated aspects of the Brazilian national identity. The film genre of the chanchada came hereby to play an important role, as it familiarised people from the cities with the cultural life of the favelas, the neighbourhoods of the poor black people, revealing 'its misery and at the same time its tremendous beauty'. Literature and poetry, on the contrary, became increasingly politicised throughout the thirties. Already in 1931, many of the writers in the Modernist movement were aware of the ideological clash that the Vargas government was driving the cultural community into. This drove a number of intellectuals to the Communists' side and often into imprisonment, as they presented a 'proletarian nation' in their work. The most lasting national cultural legacy, nonetheless, comes from the bambas of the favelas. Williamson, The Penguin History, pp. 416-22. Robert Stam, Tropical Multiculturalism. A Comparative History of Race in Brazilian Cinema and Culture, (London, 1997), p. 80. Ibid, p. 82. Mike Gonzales and David Treece, The Gathering of Voices. The Twentieth-Century Poetry of Latin America, (London, New York, 1992), p. 154. Martin, Journeys through the Labyrinth, pp. 69-72. While in the early decades of the twentieth century public concerts of samba were out of question, the 1930s marked a slow shift toward toleration of the music and dance. Whereas previously lovers of samba 'had to sing it far from the police', gradually a case was being made for acceptance of ""black"" art forms. Amidst the still prevalent racism in Brazil, Gilberto Freyes presented his theory of 'racial democracy' in 1933 with the publishing of his book The Masters and the Slaves. In it, Freyes argued that the contribution of Afro Brazilians added to the rich cultural mix in which Brazilian tradition was embedded. It has been argued, in fact, that consumption of Afro Brazilian food and music were already prevalent among all classes in a discrete sense since the nineteenth century. However, now supported by the state, who saw in the lower classes an important element for the maintenance of the status quo, the samba culture increasingly gained space in the public sphere. The gatherings and parties held in the house of Tia Ciata have been said to serve as a microcosmic example of Brazilian society. Each room in her house had different musical genre performances, with in the front rooms -near to the street- respectable dances such as the polka, and toward the back samba and percussive, ""ethnic"" rhythms. Over time, as the samba gained momentum and respectability, it started to refine and develop in a multitude of exponents. Fusing African rhythms and European tonal scales into a more accessible style, the samba now became acceptable for the wealthier white classes. Stam, Tropical Multiculturalism, p. 80. Ibid, p. 79. Paul Austerlitz, 'Review of The Mystery of Samba',  URL , (1999), viewed 10 May 2005. Rowe and Schelling, Memory and Modernity, p. 129. Austerlitz, 'Review of The Mystery of Samba'. Central to the samba scene in the favelas were the batuqueiros, or bambas. These dancers performed the capoeira, a self-defence technique disguised as a dance, which had been developed by runaway slaves. In street carnivals, where performers paraded through the streets, the bambas were singled out for their particular talent for improvised dancing. As the carnival was becoming bigger, it became also more organised. So-called Escolas de Samba (samba schools) started to predominate the street carnival, now participating in officially organised competitions for monetary prizes and social recognition. Thus, gradually, the samba culture came to serve as a means for increasing respectability and social mobility, with the samba schools as benefactors of neighbourhoods, bringing education and increasing unity in Rio de Janeiro. Rowe and Schelling, Memory and Modernity, p. 129. Ibid, p. 136. W.W. Norton, 'Samba: Mass Culture from the Bottom Up',  URL  (2002), viewed 10 May 2005. Whether empowering the black lower classes, or simply reducing their contribution to 'the picturesque and the folkloric', sure is that samba contributed to Brazilian nation-building as it transcended the regional differences and immensely varying dialects. Over time, the samba thus became a national symbol. The production of Brazilian films brought a major contribution in the transformation of samba into a mass culture, and thus to some extent in the emancipation of black Brazilians. With the coming of sound in films, the most popular film genre became, as had been the case in Argentina, a light story incorporating popular songs. The Brazilian version of such films was the chanchada, which, although never entirely unambiguous, has been described in its widest sense as 'the generic name given to all the comedies and musical comedies, with popular pretensions'. As could be expected, Rio de Janeiro emerged as the centre of Brazilian cinema in the 1930s, closely linked to the samba schools and the carnivals, in the sense that chanchada films embraced similar values and were often released around the time of carnival. In films such as Alô Alô Brasil (1935), featuring Brazilian film star Carmen Miranda, and Alô Alô Carnaval (1936), featuring her sister Aurora, blacks played minor roles as musicians in the songs. The presence of blacks in chanchadas, however marginal, were rather significant as they meant the open incorporation of ""black culture"" in Brazilian national culture. Stam, Tropical Multiculturalism, p. 80. Norton, 'Samba: Mass Culture from the Bottom Up'. Rowe and Schelling, Memory and Modernity, p. 131. King, Magical Reels, p. 55. Ibid, p. 55. Stam, Tropical Multiculturalism, p. 83. Ibid, p. 81. The extent to which the Vargas government influenced the incorporation of samba culture is blurred. Although Vargas showed some minor active support for the movement by founding the National Institute of Educational Film (INCE) in 1937, which was to serve national integration, this institute was actually short-lived, while the number of long feature films decreased toward the end of the decade. Moreover, the culture of the favelas brought with it the 'dialectic of malandragem', which was a concept of using the established order for illegal interests and to avoid 'the punishment of work'. Malandragem attempted to function alongside the order of society, thus obscuring the brisk line between order and disorder. It is therefore likely that these cultural shifts were rather a wave that accompanied the emancipation of the former slave classes than a conscious attempt at nation-building for the Estado Novo. King, Magical Reels, p. 56. Rowe and Schelling, Memory and Modernity, p. 128. Traditional high culture in Brazil's thirties was preoccupied with another struggle, that against the right-wing dictatorship. Much of the intelligentsia and artistic scene, united in the Popular Front, chose the direction of regionalism. Upper class painter Tarsila do Amaral, after having explored aspects in Brazilian nativism in her work, now started to focus on representing traditional every-day scenes of the countryside. According to Gilberto Freyes, the 'soul of Brazil' was to be sought in the rural north, which was struck by poverty and where many sugar plantations were situated. The regionalist movement, however, incorporated a wide range of ideologies opposed to the Vargas regime. Raquel de Queirós, in 1930 published Back in '15, a novel on the plight of the northern region during the famine period of 1915. In 1937 she was imprisoned for her radical Trotskyist political outlook. Regionalism thus often served to point class inequalities in a radical leftist outlook of Brazilian society. A less covered ideological viewpoint is provided in Patrícia Galvão's 'proletarian novel' Industrial Park (1933), where she focuses on the injustice of the Brazilian system. Elements of nationalism are obviously subjugated for the purpose of class struggle, as in the following passage. Gonzales and Treece, The Gathering of Voices, p. 152. Carol Damian and Christina Mehrtens, 'Tarsila and the 1920s', in Robert M. Levine and John J. Crocitti (eds. ), The Brazil Reader. History, Culture, Politics, (London, 1999) p. 315. Martin, Journeys through the Labyrinth, p. 66. Ibid, p. 69. Alfredo tries to like the simple and poorly prepared food. He feels happy. He doesn't find Brazil abhorrent, as before. He doesn't need to drown his individualistic irritability in any picturesque scenes, neither in the ovens of the Sahara nor in theglacial Arctic Ocean. He wants them to leave him in Braz. Eating that revolutionary food. Without longing for Cairo hotels or French wines.Galvão, 'Where They about Rosa Luxemburg', in The Brazil Reader, p. 169. The way in which this passage points out any Brazilian national identity is clearly negative; it argues in favour of a 'simple', social Brazil instead of an internationally incorporated, capitalist nation. Elements of Brazilian grassroots national identity are ironically referred to as expressions of a repressed class:  All Hail! All Hail! This samba's Going to land in jail.Ibid, in The Brazil Reader, p. 166. Other poetry, although not as explicitly communist as Galvão (who, for this reason, spent four years in prison as a political prisoner), focussed on the rough and oppressive facets of the regime through from a national perspective. The liberal Mário de Andrade in his poem 'The Wagon of Misery' ridicules the regime with deliberate spelling mistakes. Bump! The Wagon of Misery' s here From the Eyetalian carnival! Old Ma Misery is dressed in honour The colour of the brass of our age Gathering spittle behind her. The colonil the generul the kaptin The pure the heroic the well-meaning Childrin of the Brazilyun factory Colombus's people are swinging. Gonzales and Treece, The Gathering of Voices, p. 157. Brazilian literature and poetry of the modernist movement revealed thus revealed a highly politicised, polemic style, much embedded in the opposition of the dictatorship and united through this opposition rather than through a shared outlook of Brazilian nationalism proper. Throughout the thirties, Brazil remained still very much in search of a form of national cohesion. Martin, Journeys through the Labyrinth, p. 68. Mexico: 'A dark Indian, grateful to the party'In Mexico, the revolution proved to be the 'defining event', shaping the nation for the remainder of the century. For many, the success of the 1910s revolution was the culmination of revolutionary nationalism: the yoke of a century of oppression had now been thrown off. In an attempt to create a ""new man"" and ""new woman"", the new government turned to art, popular culture and education to create a new sense of nationhood for the Mexicans. The new Mexican nation now came to incorporate and celebrate a new group of inhabitants that had been previously been seen as passive and inferior: the Mexican Indian. The art of the 1920s and 1930s is thus marked by a fresh interest in Aztec art and architecture, and a celebration of the 'Indian race' can be found in nationalistic essays. Gilbert M. Joseph and Timothy J. Henderson (eds. ), The Mexico Reader. History, Culture, Politics, (Durham, London, 2002), p. 333. Alan Knight, 'Popular Culture and the Revolutionary State in Mexico, 1910-1940', Hispanic American Historical Review, 74:3 (1994), p. 395. Ibid, p. 395. Alexander S. Dawson, 'From Models for the Nation to Model Citizens: Indigenismo and the 'Revindication' of the Mexican Indian, 1920-1940', Journal of Latin American Studies, 30 (1998), p. 285. By the thirties, a new political machine, the National Revolutionary Party, had been put in place and the work of myth-making of Mexican heroes of the revolution started. This period has been marked the post-revolutionary period, as legitimising the state, as well as instilling a paradoxical blend of conservative revolutionary sentiments in Mexicans. This new ideological stream, indigenismo, was reflected in the mural paintings of Mexico's most prominent modern artists, in the work of intellectuals and in the number of epic novels about the revolution that were published in the twenties and thirties. It has, however, been held that for the government the aim was not 'to 'indigenize' Mexico, but to 'Mexicanize' the Indian'. Knight, 'Popular Culture and the Revolutionary State', p. 393. Martin, Journeys through the Labyrinth, p. 45. Rowe and Schelling, Memory and Modernity, p. 184. Indiginismo involved the reinterpretation of both the Indian and the nation. It involved, on the one hand, the creation of the myth of the radically social and progressive new Mexican nation, and on the other hand, a glorification of the Mexican Indian and their past. By the 1930s, however, the emphasis was shifted from race to culture. An interesting example can be found in the issue of bank notes. The depiction of a 'native woman, wearing traditional costume, standing before the Aztec calendar stone' was never circulated, and instead the images of Zaragoza and Madero were preferred. Now, notions of 'literacy, nationalism, notions of citizenship, sobriety, hygiene, and hard work' became part of the ideological mores; the Indians were believed to possess those qualities to an exceptional extent. They were, in fact, 'rational political actors with modern sensibilities'. To fit this singular representation, a new kind of Indian peasant emerged in literature: Dawson, 'Indigenismo and the 'Revindication' of the Mexican Indian', p. 282. Ibid, p. 285. Ibid, p. 294. Joseph M. Galloy, 'Symbols of Identity and Nationalism in Mexican and Central-American Currency',  URL  , (2000), viewed 11 May 2005. Knight, 'Popular Culture and the Revolutionary State', p. 395. Dawson, 'Indigenismo and the 'Revindication' of the Mexican Indian', p. 279. A dark Indian dressed in white, who wants to learn to read and write in Spanish and is grateful to the party; his children are healthy and can choose whether to stay in the rancho or migrate to the new life in the cities. Martin, Journeys through the Labyrinth, p. 46. Another aspect of this indigenismo were the rural ballads, corridos. They are transformed to be used on mass scale, into three-minute songs to be played on the radio Agustín Lara became the icon of this typically Mexican musical exponent. Often in appraisal of revolutionary heroes, corridos with their simple rhyme were easy to remember and thus accessible to considerable percentage of illiterate Mexicans. The following is an example of a corrido describing the death of Francisco Villa. Rowe and Schelling, Memory and Modernity, p. 100. Poor Pancho Villa! His destiny was very sad; To die in an ambush And in the middle of the way. He went, leaving Parral handling his horse, the brave general author of ""La Cucaracha"" [the cockroach] [own translation] Armando de Maria y Campos, La Revolucion Mexicana a Traves de los Corridos Populares, (Mexico City 1962), p. 245. Yet another way in which the revolutionaries were eternalised and epitomised as heroes of the Mexican nation was through the paintings of Diego Rivera, Jose Orozco and David Alfaro Siqueiros. These modern artists - 'Los Tres Grandes'- had started in the twenties to experiment with mural painting, a style which they thereafter used effectively for the depiction of Mexico's indigenous history, scenes of the revolution and of the modern, industrialised, progressive state. Supported and often commissioned by the government, these painters fulfilled 'the visual component of [the] need to create that Mexican citizen necessary for the survival of the post-Revolutionary state'. Martin, Journeys through the Labyrinth, p. 46. Anthony W. Lee, 'Mural Painting and Social Revolution in Mexico: 1920-1940, Art of the New Order - Review',  URL , (1999) viewed 11 May 2005. Ibid. In terms of literature, the 1930s saw a continuation of the style employed in the twenties. Epic works with 'the man on horseback', the archetypical revolutionary also depicted in the mural paintings of the post-revolutionaries. Where Guzmán had in 1926 recounted his memories in the revolutionary army of Villa with The Eagle and the Serpent, Gregorio López y Fuentes in 1932 created an epic hero out of Zapata. His novel The Land provided the sense of heroic Mexican leadership that legitimised the Mexican revolution. Martin, Journeys through the Labyrinth, p. 38. It is passed on in whispers, in huddles, by men at the plough and women at the well. He has been seen. So it's true he's not dead... 'Do you know who saw him? Old Albina. [...] This is what she told me: ""[...] I recognised him straight off. Hat pushed back, like always, them same trousers with the buttons, and that moustache you could tell a mile away. [...]"" ' Ibid, p. 45. There were, of course, also many writers opposed to this mythical image. In fact, the Mexican revolution brought no generation of poets of any significance; most of them were to be sought with the defeated reactionaries. Gómez Marín, who lived in London at the time wrote in a letter: 'From here, Mexico is something dark and bloody'. Moreover, the corrido, typical Mexican icon, could naturally also be used for other purposes than the glorification of the revolution. The following extract comes from a corrido praising the bravery of a christian Cristero rebellion. Gonzales and Treece, The Gathering of Voices, p. 29. Franco, Critical Passions, p. 456. Then the general said: ""I am prepared to grant you a pardon If you will tell me Where I might find the local priest."" Valentín promptly answered him: ""That I cannot say; I'd rather you kill me Than give up a friend.""... Before they shot him, Before he went up the hill, Valentín cried: ""O mother of Guadalupe! For your religion they will kill me. ""...Anonynous, 'The Ballad of Valentín of the Sierra', in Gilbert M. Joseph and Timothy J. Henderson (eds. ), The Mexico Reader. History, Culture, Politics, (Durham, London, 2002), p. 420. Arguably, illiteracy rates stood in the way of effective nation-building through literature. Some have thus asserted that 'it [was] film, not writing, which create[d] [...] the 'imagined community' of the nation'. Indeed, when the first film with sound was made in 1931, it was sponsored by state funds. Such films, which like in the case of Argentina and Brazil, were capable of reaching as mass audience, dealt with popular themes such as destitute prostitutes (which, of course, fuelled the macho discourse of the frail woman in need of protection) and went accompanied by popular tunes, most notably those of Agustín Lara's soundtracks. The state was quite involved in the ideological message such films conveyed. Thus when the Soviet filmmaker Eisenstein, visiting Mexico, planned to make a film on the history of Mexico, the government withdrew their subsidies for fear that such a film might cause an outrage amongst the oligarchic hacienda owners. King, Magical Reels, p. 99. Ibid, p. 42. Ibid, p. 42. Ibid, p. 44. In the aftermath of the revolutionary decade (1910-1920) in Mexico, the new authorities then energetically set about constructing the archetype of a new ""ideal Mexican"". This was based upon a renewed enthusiasm about the Indian, who was henceforth to be considered as a part of the new Mexican state. Indigenismo and, from the late 1920s into the thirties, revolutionary myth making played a big role in such representations. Nationalism was hence instilled through popular means: mural painting, the corrido ballad, film, and to some extent, epic literature. By actively interfering and negotiating these art forms, the state sought to channel the multitude of opinions into a glorification of the new Mexicanidad. Conclusion Nationhood is a fluid concept. It can be manipulated to incorporate whatever ideological ideas, and exclude or obscure what one does not find appropriate to it. Argentina and Brazil in the 1930s saw the beginning of a great transformation of nationhood, while Mexico had already begun that project directly after the Mexican revolution. Although often the state attempted to control cultural representations of its nationalism, intellectuals and poorer minorities did not let this happen easily. Borges refused to accept a simple, patriotic nationalism, thereby brilliantly revealing the paradoxes underlying such a concept; in Vargas' Brazil, the government was in a constant state of war with its poets and authors on the left, while former slaves of the favelas proved to be able to provide Brazil's most uniting and lasting national identity. Perhaps the Mexican state can be said to have succeeded best at imposing an official picture of nationalism. Their corridos and murals invoked a revolutionary-ideological Mexicanidad, party-style; and it would only be by the 1990s that Mexican natives started to wage war on that identity. Nevertheless, the successes in Mexico also knew their limits. The Catholic religion proved impossible to curb, while student protests in the 1970s already revealed the violent antagonism underlying the picture of Mexico that the government offered its people. Representations of national identity, then, are inherently linked with political ideology and power. Brazilian authors with tendencies toward communism provide the example; they employ a highly politicised image of the 'true Brazilian' proletariat. In Argentina, such representations resulted in two opposing views, the gauchoesque patriot, modelled after Martin Fierro stood in contrast to the liberal modernist 'rational' Argentine inspired by Sarmiento's Facundo. When tango, the voice of the minorities, and Borges came into the equation, a mono-cultured Argentine ideal became more complicated and troublesome. Similar in each of the three countries discussed, however, are the influences of 'lower' cultural forms. Tango, initially despised by all respectable layers of society, became the epitome of Argentinidad and proud export product after its success in the Parisian salons, while the samba of the Brazilian blacks became a national virtue. In Mexico, the corrido and new popular film both served the ideal of praising the spiritual idealism that belonged inherently to the Mexican man and woman, as opposed to the violent pragmatism of Northern American men, and the dehumanised star beauty of the Hollywood actress. Nationalism in all countries discussed, became increasingly represented in popular ways, reaching out to a mass audience. The constructors of such nationalistic forms sought to reach out to an unprecedented number of people, instilling upon them these new forms of nationalism. In Brazil, chanchadas united the country despite its immense differences in dialect, while in Mexico the party did their best to involve nationalism in every possible cultural and educational aspect. In Argentina it culminated in the decade of Peron's popular dictatorship. It can thus be said, that across these nations, the 1930s transformed nationalism into a popular and far-reaching construct.",True
15,"Rousseau has always been typified as an intellectual outsider; it was, however, only in the twentieth century that his socio-political theory came to be seen as a threat to the individual's liberty. Emphasis thereby is often laid on his concept of a 'general will', through which personal liberty is handed over to the collective. 'The citizens having but one interest', Rousseau argues, 'the people [have] but a single will'. And although Rousseau favours the ballot vote, he argues for the infallibility of its ruling: 'when the opinion contrary to my own prevails, it proves nothing more than that I made a mistake, and that what I took to be the general will was not'. From the point of view of Rousseau, most importantly protection should be rendered against the emergence of a minority rule, or 'particular will', rather than against the state, which, after all, represents the sovereignty of the lawgiver as it corresponds with the community's best interest, the general will. The question I will look at here is thus to what extent the critical assertion holds truth that 'the more safeguards he established against despotism, the more weapons he forged for tyranny', within Rousseau's system of the social contract. Iain Hampsher-Monk, 'Rousseau and totalitarianism: with hindsight?' in Robert Wokler (ed), Rousseau and Liberty (Manchester/New York, 1995), p. 268. Jean-Jacques Rousseau, The Social Contract and Discourses (London/New York, 1952), p. 87. Christopher Bertram, Routledge Philosophy Guidebook to Rousseau and The Social Contract (London, 2004), p. 120. Hampsher-Monk, 'Rousseau and totalitarianism: with hindsight? ', p. 271. Recent criticism has argued that the Cold War split has caused intellectuals to make an over-simplified distinction between 'individualistic liberalism and state collectivism', stating that Rousseau was on the totalitarian side of the argument. Rousseau, however, from the outset entertains an awareness of the problem of new forms of subjugation that hide behind a powerful state, and thus sets out to construct a model for a 'form of association [...] in which each, while uniting himself with all, may still obey himself alone, and remain free as before'. In this sense, Rousseau undertakes to find a form in which freedom can be exercised on a public, political level. The important distinction that has to be made here though, is the type of freedom at stake. Arguing that pre-modern, pre-social man exercised an individual sovereignty free from social obligation or commitment, knew a natural, brutal kind of liberty. When people come to regular interaction, they become interdependent, which goes accompanied by fierce passions. It is only within society that morality comes to exist, through virtue and pitié, a form of compassion which counters man's corruption by society. What is then needed is the construction of an association in which passions are tempered and virtue is encouraged. In this sense, envisaging a social contract is merely a theoretical exercise, to find a form in which such an exercise of sovereignty can be best expressed, in other words, 'Rousseau's purpose is to discover a rule which has moral validity'. Hampsher-Monk, 'Rousseau and totalitarianism: with hindsight? ', p. 268. Rousseau, The Social Contract, p. 12. Patrick Neal, Liberalism and Its Discontents (London, 1997), p. 61. Rousseau, The Social Contract, p. 5. Maurizio Viroli, Jean-Jacques Rousseau and the 'well-ordered society' (Cambridge, 1988), p. 132. Christopher Bertram, Routledge Philosophy Guidebook to Rousseau and The Social Contract (London, 2004), p. 19. Viroli, The 'well-ordered society', p. 121. Another point of view insists on the juxtaposition of democracy and liberalism, positioning Rousseau in a radically democratic framework. Certainly, this view depends greatly on the view one takes of liberalism. Liberalism can be taken at an individual level, stressing the removal of any constraint on human action (negative liberty), and placing theory in wider context, that of the body politic (positive liberty). Rousseau is quite explicit about what form of liberty we are to seek. 'Man is born free; and everywhere he is in chains', he argues, 'One thinks himself the master of others, and still remains a greater slave than they'. It is therefore clear that Rousseau seeks a wider answer to the question of liberty, one for which 'it is necessary to ensure that the conditions in which [people] live do not pose a threat to their survival'. This condition can be brought about by the realisation that it is the best interest of all citizens to care for other's interest and well-being. Here we find the distinction that Rousseau makes between a narrow form of personal interest and a wider, more coherent form of enlightened interest, being the best for the community, and thus, indirectly, each citizen's actual will, as it secures the best-considered proposal and enjoys general support of the citizenry. Subordination to the law subsequently only becomes an exercise of one's liberty. Carol Blum, Rousseau and the Republic of Virtue. The Language of Politics in the French Revolution (Ithaca/London, 1986), p. 32. Hampsher-Monk, 'Rousseau and totalitarianism: with hindsight? ', p. 271. Rousseau, The Social Contract, p. 3. Viroli, The 'well-ordered society', p. 112. Bertram, Rousseau and The Social Contract, p. 44. Exercising one's freedom through abiding by the law is only reasonable if lawgiver and subject are the same person. This is where Rousseau takes a radically democratic stance, criticising Kant's very open assertion that law can still be legitimate only by the requirement that the requirement 'that the proposal could have been agreed to had the citizens been consulted and were they fully rational'. Hence Rousseau's claim that 'power indeed may be transmitted, but not the will': for the general will to function, it requires the individual standpoints of each citizen. Unanimity is thereby not needed; it is in fact a sign of an unhealthy body politic. The only law that requires unanimity of all citizens is the social pact, since by this each individual hands over their individual sovereignty to the higher expression of the community. Within the body politic, the general will aims to protect rather than to attack its participants, and 'it is not', therefore, 'to be assumed that any of the parties then expects to get hanged'. Likewise, peace with neighbouring communities is required by natural law. It has therefore been claimed that the general will represents a model for the well-being of all, and thus reflects 'each individual's good or real will'. Neal, Liberalism, p. 63. Rousseau, The Social Contract, p. 20. Ibid, pp. 87-88. Ibid, p. 28. Hampsher-Monk, 'Rousseau and totalitarianism: with hindsight? ', p. 270. Ibid, p. 270. The emphasis that Rousseau lays on the collective, however, is today regarded with mistrust. After all, the functioning of the general will depends entirely upon the full dedication of the citizenry to the social contract, and we find 'no restriction of principle on what it may command in that form'. It appears at times as though Rousseau puts all individual risks at right for an ideologically unified community. As soon as all sovereignty passes from the individual to the state, a greater power comes to exist which '[man] cannot use without the help of others'. Contrarily, there is no 'safeguards for the individual against the state'. The Rousseauean state results in collective socialisation, only rendered legitimate through the consent of its participants. In a wider interpretation of the social contract as merely a thought experiment, this socialisation obtains a somewhat enforced character. Bertram, Rousseau and The Social Contract, p. 193. Blum, Republic of Virtue, p. 111. Hampsher-Monk, 'Rousseau and totalitarianism: with hindsight? ', p. 272. Ibid, p. 268. One form of criticism of the general will draws parallels with the Jacobin rule during the French revolution. Both reveal traces of absolutist totalitarianism: 'It is absolutely necessary that evil come from others and from them alone'. Despite the validity of a critique of Rousseau's reassertion that the general will is infallible (which indeed shows traces of a majority intolerance), it is difficult if not altogether impossible to find examples of the Rousseauean state in practice. Eastern block socialism, which has been suggested as imposing some for of a state-directed will upon its citizens, has been severely criticised. Also the Jacobin adoption of Rousseau's general will has been seen as of symbolic value rather than a precise influence of his thought. Lacking everywhere has been the binding together of lawgiver and subject, which according to Rousseau results only in the imposition a particular will of a state governed by a political elite. Jean Starobinski, 'La Mise en accusation de la Société', cited in Carol Blum, Rousseau and the Republic of Virtue. The Language of Politics in the French Revolution (Ithaca/London, 1986), p. 218. Blum, Republic of Virtue, p. 32. Hampsher-Monk, 'Rousseau and totalitarianism: with hindsight? ', p. 272. It is true that Rousseau is most concerned with the organisation of social order based upon the exercise of virtue, rather than on preserving the individual's relative freedom of action. It is therefore perhaps surprising that Rousseau leans entirely on the exercise of a general will in ensuring the best for all citizens. Through this construct, Rousseau categorically denies the individual any sovereignty except for a participatory role in the community. Effectively, the state attains a strongly fortified position in which ' [t]he more the natural strengths are dead and destroyed, the more the acquired ones are great and durable, and the more solid and perfect this institution is'. Such a radical step toward an active, creative form of liberalism challenges liberals into the debate of the equalisation of the citizenry, since 'serious economic inequalities destroy liberty'. Nonetheless, it does not suffice to, as a consequence of this conclusion, leave all individual property and grounds for action within the confines of the community's sovereignty. Viroli, The 'well-ordered society', p. 113. Blum, Republic of Virtue, p. 111. Hampsher-Monk, 'Rousseau and totalitarianism: with hindsight? ', p. 275. One of Rousseau's ideas of human nature was that the strongest of human passions was amour the soi (love of self), of which virtue and piti éwere manifestations within society. 'No-one has any interest in making them [the conditions of association] burdensome to others' This may be seen as a problem for the protection of the individual's rights: Rousseau does not expect the intrusion of state into the individual's life and thus includes no precautions to safeguard certain domains of private life. 'If this leads to intervention in 'areas which are perceived as illiberal, such as the social and economic conditions of citizen's lives and with their beliefs and values', this is seen by Rousseau as a decision of the general will in order to secure the necessary conditions for socio-political liberty. To some extent the modern welfare state can be seen as a moderate outcome of Rousseau's general will, be it on a representative basis. Executive and lawmaking government of the welfare state is now generally expected to create policy that benefits the community in a broad social sense. Taxation, health security and economic intervention are put in place to equalise conditions and ensure a minimum standard of welfare for each citizen, whereas participation in elections for government positions is open to each citizen. This form of moderate liberal welfare state, however, fails to pass the test, since in this model the law does not come from all. Bertram, Rousseau and The Social Contract, p. 24. Hampsher-Monk, 'Rousseau and totalitarianism: with hindsight? ', p. 281. Ibid, p, 275. Bertram, Rousseau and The Social Contract, p. 157. The dichotomy between modern, individualist liberalism and the Rousseauean state lies predominantly in their different interpretations of liberty. Whereas the former stress the lack of protection for the individual in Rousseau's social contract, Rousseau in turn entertains concerns with the crude, unsocial kind of liberty that individualists profess. Rather does he see liberty in a community best exercised at the highest level, thus enhancing liberty in an ordered way. For order to exist, there needs to be unity based on a reasonable principle; the social contract offers just that. As liberty exercised only on the most individual, narrow level only furthers contradiction in interests, Rousseau proposes the pursuit of community interest, which, if carried out fully, secures the welfare and safety of all its citizens. What Rousseau does not display enough attention to, however, was the fact that 'there is never a perfect fit between moral requirements of our rational nature and requirements of citizenship'. This results in a lack of consideration of the nature of the general will, and its proper domain. Rousseau, on the contrary, believes in the stabilising role of the general will, and the inalienability of the will in general. As the general will represent the only genuine method of determining the sovereign will of the citizenry, parallels drawn between totalitarian ideological states and Rousseau's general will are, strictly theoretical, incorrect. The problem, nonetheless, lies with the radical solution Rousseau's general will poses. Asserting that '[t]o be governed by appetite alone is slavery, while obedience to a law one prescribes to oneself is freedom', leaves very little space for individual action. Thus, a community taking its legislative task to the extreme, might indeed force its citizens into abide by the law. The question becomes, then, not to what extent Rousseau can be used for totalitarian ends, which he explicitly sets out to prevent, but rather how a feasible system may be envisaged in which particular interpretations of the general good can obtain a stable construct in which the individual is both more protected and better positioned to criticise the state's pursuit of the general good. Viroli, The 'well-ordered society', p. 38. Christopher Bertram, Routledge Philosophy Guidebook to Rousseau and The Social Contract (London, 2004), p. 33. Rousseau, The Social Contract, p. 20. Neal, Liberalism, p. 60. Christopher Bertram, Routledge Philosophy Guidebook to Rousseau and The Social Contract (London, 2004), p. 194. IntroductionWhen we talk about the 1930s in terms of social history, we often refer to the strong emergence of ideology into the public domain. Moreover, for Latin American countries, the first half of the twentieth century marked the coming of 'modernity'. Other factors, however, such as immigration and a rumbling social hierarchy in the aftermath of slavery, tended to influence society in more informal ways. Self-representation through culture was usually closest at hand for the lower classes, and often provided a colourful means of communicating a multi-faceted message. In this essay we shall look at such cultural representations in Argentina, Brazil and Mexico, and assess the extent to which they contributed to the construction of a national identity. Naturally, art forms such as dance and music were, even though powerful, not the only way of expressing one's national sentiments. The first films with sound managed to reach a mass audience and demonstrated what it was like to be a true Argentine, or Brazilian, in a light package. Moreover, the aftermath of key political events (Argentina, the 'infamous decade' after the fall of Yrigoyen; Brazil, the 1930 coup of Vargas; Mexico, the heritage of the revolution) and the subsequent split among the people brought a new literature of thinkers focussing with urgency upon 'national character and the metaphysics of the Mexican, Argentinian or Brazilian 'being''. The assessment of each country's culture climate thus provides interesting insights into the attempts made at a culturally constructed nationhood. Edwin Williamson, The Penguin History of Latin America, (London, 1992), p. 513. Gerald Martin, Journeys through the Labyrinth. Latin American Fiction in the Twentieth Century, (London/New York, 1989), p. 67. Argentina: 'a land of exiles' Jorge Luis Borges, cited in in Gabriel Nouzeilles and Graciela Montaldo (eds. ), The Argentina Reader. History, Culture, Politics, (Durham, London, 2002), p. 1.The Argentina of the 1930s saw increasing modernisation and a changing political climate. This did not leave the notion of nationalism untouched; and, after the social positivism of the Irigoyen government that was overthrown in 1930, cultural representations of the Argentine nation became increasingly ambiguous. The 'Década Infame' brought a reconsideration of Argentine patriotism and the essence of 'Argentineness'. A new definition of the Argentine identity proved, however, not quite unproblematic. A nation of immigrants, Argentina had many faces and was, with the coming of further modernity and political maturing of the masses, thrust into a further set of cultural disparities. These were based upon dichotomies as varied as those of machismo versus feminism, the European-orientated metropolitan versus the patriotic gaucho, and that of the definition of a high culture versus that of commercially formed mass culture. Essentially, thus, the concept of argentinidad was politically and culturally utilised in a multitude of ideological representations: 'The references to a collective ideal of the argentinidad were continuous [...], although this ideal was defined in the most imprecise way and was basically the product of the whims and wishful thinking of the ideologues who wrote on the subject, more than an ideal that could be identified empirically as a trait of Argentina's culture'. Carlos Escudé, 'The Case of Argentina',  URL , (2000), viewed 2 May 2005. An early elaboration upon Argentine culture and its forms can be found in Sarmiento's Facundo of 1845. Sarmiento, Argentina's president between 1868 and 1874, regarded the countryside, inhabited by the mestizo gaucho Argentine, as uncivilized, chaotic - even barbarous: 'dirty children covered in rags, living amid packs of dogs; men stretched out on the ground, in utter inactivity; filth and poverty everywhere'. The country's biggest cities, on the contrary, contained every aspect 'that characterizes cultured people'. Later, the popular poem about Martín Fierro created a distinction between the good, heroic and patriotic gaucho, and the barbarous and violent bad gaucho. The early twentieth century thereafter saw a complete rehabilitation of the gaucho icon, when it was presented as an example of argentinidad in an increasingly immigrant-dominated society. This was, however, merely a constructed and idealised identity, and the by the end of the twenties, the plurality of immigrant cultures were gaining space from more traditional social and cultural dance clubs with names like 'The Patriotic Gauchos'. In its stead came a new form of dance, less courteous and more fitting in the context of the big metropolis Buenos Aires: tango. Domingo Faustino Sarmiento, Facundo. Civilization and Barbarism, (London, 2003), p. 51. Ibid, p. 52. William Rowe and Vivian Schelling, Memory and Modernity. Popular Culture in Latin America, (London, New York, 1991), p. 34. Leopoldo Logunes, 'National Identity in a Cosmopolitan Society' in The Argentina Reader, p. 209. William Rowe and Vivian Schelling, Memory and Modernity. Popular Culture in Latin America, (London, New York, 1991), p. 35. The thirties in Argentina were marked by modernising development from above and from below. The tango started its ascending popularity from below. A dance of immigrants, it was originally popular in the packed, miserable and crime-ridden suburbs of Buenos Aires. Its association with knife-fighting and gangs was famously put into words in Borges' short story 'Streetcorner Man' of 1933. In it, two gang leaders head into confrontation on an illegal tango party. Ibid, p. 35. 'The only reason I don't carve you up is because you sicken me,' the Butcher said to Rosendo, threatening to strike him. That same moment, La Lujanera threw her arms around the Butcher's neck, fixing those eyes of hers on him, and said in a fury, 'Let him alone - making us think he was a man.' For a minute of so [the Butcher] was bewildered. Then, wrapping his arms around La Lujanera, he called to the musicians to play loud and strong, and he ordered the rest of us to dance. Jorge Luis Borges, The Aleph and Other Stories, (London, 1973), p. 27. These aspects - machismo, violence, illegitimacy, made dancing the tango into a rebellious act, associated with characters on the edge of society, devoid of patriotic virtue or morality. This was soon to change, as the tango was brought to Europe and became popular in the salons of avant-garde Paris. Before this shift, the tango only gained marginal recognition as a truly Argentine metropolitan dance, as in the poem 'Milonga' by Oliverio Girondo. Rowe and Schelling, Memory and Modernity. Popular Culture in Latin America, (London, New York, 1991), p. 36. Males whose bodies rupture in a ritual court, their heads sunk low between their shoulders, their lips thick with coarse remarks. Females with their nervous rumps, bits of foam at their armpits and their eyes looking much too oily.Oliverio Girondo, 'Poems to Be Read on a Trolley Car' in The Argentina Reader, p. 252-53. A second exponent of tango, nevertheless, soon found through tango a new formula for political criticism, woven into poetical lyrics. The song 'Cambalache' by Enrique Santos Discépolo, written in the early thirties, counterposed the liberal positivistic nationalism with a gloomy, destructive view of the new Argentina. 'Those that don't cry don't feed', Santos sang, 'and those that don't steal are fools'. Enrique Santos Discépolo, 'Cambalache', in The Argentina Reader, p. 267. Despite this loathing with politics, the middle class adopted the tango as an aspect of the new, dynamic and modern culture that Argentina was now adopting. In this, the cinema played a big, yet not unambiguous role. Argentina was steadily adopting the Northern American cinema culture. In 1930, around a thousand film screens could be found throughout the country. However, already by 1926 an estimated 95 percent of screen time was filled by films from the United States. Tango reached a mass audience when sound was added to cinema films, and a number of glamorous films with appealing tunes lent this formula its great success. Unfortunately this also caused tango to become a mass cultural phenomenon, alienated from its former basis of charismatic metropolitanism. By the late thirties, fifty such films were produced a year, also serving as a charming and successful export product for other Latin American countries. John King, Magic Reels. A History of Cinema in Latin America, (London, New York, 2000), p. 9. Ibid, p. 11. Ibid, p. 38. While the metropolitan immigrant gained commercial ground and mass appeal, other cultural forces were working from above, posing questions at a more political and intellectual level. X-Ray of the Pampa was published in 1933, melancholically referring back to inner country Argentina as a real source of Argentine national identity. In the essay, Ezequiel Martínez Estrada accused the central government in the metropolitan centre of cultivating city values at the cost of the 'countryside, which is full of truth and life'. Minority immigrant groups, faced by the dominance and lack of understanding of the overwhelmingly Catholic Hispanic cultural majority, attempted to secure themselves a place within the Argentine identity. Jews, who were often particularly victim of racism, attempted, with relative success, to bring the issue to attention in the theatre. A number of Jewish playwrights wrote plays on Argentine Jewish families and their cultural problems, with strategies varying from the complete abandonment of any distinctive Jewish identity', to the incorporation [...] of Argentina's many immigrant groups, their concerns and their idioms'. The playwright César Tiempo in 1937 even won a prestigious theatre prize for one such play with a light-hearted tone, Pan criollo [Native Bread]. Ezequiel Martínez Estrada, 'X-Ray of the Pampa' in The Argentina Reader, p. 262. David William Foster, Cultural Diversity in Latin American Literature (Albuquerque, 1994), p. 97. Foster, Cultural Diversity, pp. 110-11. Ibid, p. 111. The literature of Jorge Luis Borges forms a totally separate category in Argentina's intellectual life of the thirties. Whilst his outlook towards nationalism was complicated and peculiar, both the political left and right came to agree 'that what the fictions display is mastery'. Initially, Borges became part of the capital's avant-garde poetry circle, whose most famous representative was Oliverio Girondo, and his first published poems still breathe the excitement of metropolitan life. By the 1930s, however, Borges came to reject Martín Fierro's nativistic idea of argentinidad as well as the avant-garde as 'infantile disorders'. Perhaps due to some drastic changes in Borges' life over the second half of the thirties, among which the death of his father, a short period of serious illness and a mindless job as a librarian, Borges increasingly came to see all experiences as individual. Increasingly, Borges disapproved of simplistic, mono-cultured concepts of Argentine nationalism, in which some groups were downplayed in favour of others. Instead, he proposed that Jean Franco, Critical Passions: Selected Essays (Durham/London, 1999), p. 328. Martin, Journeys through the Labyrinth, p. 153. Franco, Critical Passions, p. 331. Martin, Journeys through the Labyrint, p. 153. we cannot limit ourselves to purely Argentine subjects in order to be Argentine; for either being Argentine is an inescapable act of fate - and in that case we shall be so in all events - or being Argentine is a mere affectation, a mask.Jorge Luis Borges, cited in Martin, Journeys through the Labyrinth, p. 155. This deconstructionist view of argentinidad cut short all aspirations toward an inward-looking, 'authentic' national identity, and hence, Borges later came to be regarded as an early exponent of postmodernism. Throughout the 1930s, Borges was still very much in search of new values, comparing the Argentine with the Jew, whom, according to him both had no 'particular national tradition', while the entrenchment in a narrow set of national values was 'negative, or, at the very least, uncreative'. Soon, he would move on to a path of displacing, often surprising literary creations, revealing elements of both ironic genius as elitism. Franco, Critical Passions, p. 327. Ibid, pp. 330-31. Williamson, The Penguin History, p. 516. Martin, Journeys through the Labyrint, p. 159. Brazil: 'All hail! This samba's going to end in jail'  Patrícia Galvão, 'Where They about Rosa Luxemburg', in The Brazil Reader, p. 166.The 1930s as a definable decade for Brazil started in October 1930, when a group of young army officers overthrew the long-standing São Paulo oligarchy and brought GetúlioVargas to power - a dictatorship that was to last until 1954. This inaugurated an era of modernisation to the country, and a new search for a national identity. Gradually, art forms and cultural expressions of the lowest classes (in Brazil this mainly meant the former slaves) became accepted by a wider public, and eventually even representative as national symbols. Dances like the samba and capoeira and the carnival of Rio de Janeiro - though often in commercialised and regularised forms- now became celebrated aspects of the Brazilian national identity. The film genre of the chanchada came hereby to play an important role, as it familiarised people from the cities with the cultural life of the favelas, the neighbourhoods of the poor black people, revealing 'its misery and at the same time its tremendous beauty'. Literature and poetry, on the contrary, became increasingly politicised throughout the thirties. Already in 1931, many of the writers in the Modernist movement were aware of the ideological clash that the Vargas government was driving the cultural community into. This drove a number of intellectuals to the Communists' side and often into imprisonment, as they presented a 'proletarian nation' in their work. The most lasting national cultural legacy, nonetheless, comes from the bambas of the favelas. Williamson, The Penguin History, pp. 416-22. Robert Stam, Tropical Multiculturalism. A Comparative History of Race in Brazilian Cinema and Culture, (London, 1997), p. 80. Ibid, p. 82. Mike Gonzales and David Treece, The Gathering of Voices. The Twentieth-Century Poetry of Latin America, (London, New York, 1992), p. 154. Martin, Journeys through the Labyrinth, pp. 69-72. While in the early decades of the twentieth century public concerts of samba were out of question, the 1930s marked a slow shift toward toleration of the music and dance. Whereas previously lovers of samba 'had to sing it far from the police', gradually a case was being made for acceptance of ""black"" art forms. Amidst the still prevalent racism in Brazil, Gilberto Freyes presented his theory of 'racial democracy' in 1933 with the publishing of his book The Masters and the Slaves. In it, Freyes argued that the contribution of Afro Brazilians added to the rich cultural mix in which Brazilian tradition was embedded. It has been argued, in fact, that consumption of Afro Brazilian food and music were already prevalent among all classes in a discrete sense since the nineteenth century. However, now supported by the state, who saw in the lower classes an important element for the maintenance of the status quo, the samba culture increasingly gained space in the public sphere. The gatherings and parties held in the house of Tia Ciata have been said to serve as a microcosmic example of Brazilian society. Each room in her house had different musical genre performances, with in the front rooms -near to the street- respectable dances such as the polka, and toward the back samba and percussive, ""ethnic"" rhythms. Over time, as the samba gained momentum and respectability, it started to refine and develop in a multitude of exponents. Fusing African rhythms and European tonal scales into a more accessible style, the samba now became acceptable for the wealthier white classes. Stam, Tropical Multiculturalism, p. 80. Ibid, p. 79. Paul Austerlitz, 'Review of The Mystery of Samba',  URL , (1999), viewed 10 May 2005. Rowe and Schelling, Memory and Modernity, p. 129. Austerlitz, 'Review of The Mystery of Samba'. Central to the samba scene in the favelas were the batuqueiros, or bambas. These dancers performed the capoeira, a self-defence technique disguised as a dance, which had been developed by runaway slaves. In street carnivals, where performers paraded through the streets, the bambas were singled out for their particular talent for improvised dancing. As the carnival was becoming bigger, it became also more organised. So-called Escolas de Samba (samba schools) started to predominate the street carnival, now participating in officially organised competitions for monetary prizes and social recognition. Thus, gradually, the samba culture came to serve as a means for increasing respectability and social mobility, with the samba schools as benefactors of neighbourhoods, bringing education and increasing unity in Rio de Janeiro. Rowe and Schelling, Memory and Modernity, p. 129. Ibid, p. 136. W.W. Norton, 'Samba: Mass Culture from the Bottom Up',  URL  (2002), viewed 10 May 2005. Whether empowering the black lower classes, or simply reducing their contribution to 'the picturesque and the folkloric', sure is that samba contributed to Brazilian nation-building as it transcended the regional differences and immensely varying dialects. Over time, the samba thus became a national symbol. The production of Brazilian films brought a major contribution in the transformation of samba into a mass culture, and thus to some extent in the emancipation of black Brazilians. With the coming of sound in films, the most popular film genre became, as had been the case in Argentina, a light story incorporating popular songs. The Brazilian version of such films was the chanchada, which, although never entirely unambiguous, has been described in its widest sense as 'the generic name given to all the comedies and musical comedies, with popular pretensions'. As could be expected, Rio de Janeiro emerged as the centre of Brazilian cinema in the 1930s, closely linked to the samba schools and the carnivals, in the sense that chanchada films embraced similar values and were often released around the time of carnival. In films such as Alô Alô Brasil (1935), featuring Brazilian film star Carmen Miranda, and Alô Alô Carnaval (1936), featuring her sister Aurora, blacks played minor roles as musicians in the songs. The presence of blacks in chanchadas, however marginal, were rather significant as they meant the open incorporation of ""black culture"" in Brazilian national culture. Stam, Tropical Multiculturalism, p. 80. Norton, 'Samba: Mass Culture from the Bottom Up'. Rowe and Schelling, Memory and Modernity, p. 131. King, Magical Reels, p. 55. Ibid, p. 55. Stam, Tropical Multiculturalism, p. 83. Ibid, p. 81. The extent to which the Vargas government influenced the incorporation of samba culture is blurred. Although Vargas showed some minor active support for the movement by founding the National Institute of Educational Film (INCE) in 1937, which was to serve national integration, this institute was actually short-lived, while the number of long feature films decreased toward the end of the decade. Moreover, the culture of the favelas brought with it the 'dialectic of malandragem', which was a concept of using the established order for illegal interests and to avoid 'the punishment of work'. Malandragem attempted to function alongside the order of society, thus obscuring the brisk line between order and disorder. It is therefore likely that these cultural shifts were rather a wave that accompanied the emancipation of the former slave classes than a conscious attempt at nation-building for the Estado Novo. King, Magical Reels, p. 56. Rowe and Schelling, Memory and Modernity, p. 128. Traditional high culture in Brazil's thirties was preoccupied with another struggle, that against the right-wing dictatorship. Much of the intelligentsia and artistic scene, united in the Popular Front, chose the direction of regionalism. Upper class painter Tarsila do Amaral, after having explored aspects in Brazilian nativism in her work, now started to focus on representing traditional every-day scenes of the countryside. According to Gilberto Freyes, the 'soul of Brazil' was to be sought in the rural north, which was struck by poverty and where many sugar plantations were situated. The regionalist movement, however, incorporated a wide range of ideologies opposed to the Vargas regime. Raquel de Queirós, in 1930 published Back in '15, a novel on the plight of the northern region during the famine period of 1915. In 1937 she was imprisoned for her radical Trotskyist political outlook. Regionalism thus often served to point class inequalities in a radical leftist outlook of Brazilian society. A less covered ideological viewpoint is provided in Patrícia Galvão's 'proletarian novel' Industrial Park (1933), where she focuses on the injustice of the Brazilian system. Elements of nationalism are obviously subjugated for the purpose of class struggle, as in the following passage. Gonzales and Treece, The Gathering of Voices, p. 152. Carol Damian and Christina Mehrtens, 'Tarsila and the 1920s', in Robert M. Levine and John J. Crocitti (eds. ), The Brazil Reader. History, Culture, Politics, (London, 1999) p. 315. Martin, Journeys through the Labyrinth, p. 66. Ibid, p. 69. Alfredo tries to like the simple and poorly prepared food. He feels happy. He doesn't find Brazil abhorrent, as before. He doesn't need to drown his individualistic irritability in any picturesque scenes, neither in the ovens of the Sahara nor in theglacial Arctic Ocean. He wants them to leave him in Braz. Eating that revolutionary food. Without longing for Cairo hotels or French wines.Galvão, 'Where They about Rosa Luxemburg', in The Brazil Reader, p. 169. The way in which this passage points out any Brazilian national identity is clearly negative; it argues in favour of a 'simple', social Brazil instead of an internationally incorporated, capitalist nation. Elements of Brazilian grassroots national identity are ironically referred to as expressions of a repressed class:  All Hail! All Hail! This samba's Going to land in jail.Ibid, in The Brazil Reader, p. 166. Other poetry, although not as explicitly communist as Galvão (who, for this reason, spent four years in prison as a political prisoner), focussed on the rough and oppressive facets of the regime through from a national perspective. The liberal Mário de Andrade in his poem 'The Wagon of Misery' ridicules the regime with deliberate spelling mistakes. Bump! The Wagon of Misery' s here From the Eyetalian carnival! Old Ma Misery is dressed in honour The colour of the brass of our age Gathering spittle behind her. The colonil the generul the kaptin The pure the heroic the well-meaning Childrin of the Brazilyun factory Colombus's people are swinging. Gonzales and Treece, The Gathering of Voices, p. 157. Brazilian literature and poetry of the modernist movement revealed thus revealed a highly politicised, polemic style, much embedded in the opposition of the dictatorship and united through this opposition rather than through a shared outlook of Brazilian nationalism proper. Throughout the thirties, Brazil remained still very much in search of a form of national cohesion. Martin, Journeys through the Labyrinth, p. 68. Mexico: 'A dark Indian, grateful to the party'In Mexico, the revolution proved to be the 'defining event', shaping the nation for the remainder of the century. For many, the success of the 1910s revolution was the culmination of revolutionary nationalism: the yoke of a century of oppression had now been thrown off. In an attempt to create a ""new man"" and ""new woman"", the new government turned to art, popular culture and education to create a new sense of nationhood for the Mexicans. The new Mexican nation now came to incorporate and celebrate a new group of inhabitants that had been previously been seen as passive and inferior: the Mexican Indian. The art of the 1920s and 1930s is thus marked by a fresh interest in Aztec art and architecture, and a celebration of the 'Indian race' can be found in nationalistic essays. Gilbert M. Joseph and Timothy J. Henderson (eds. ), The Mexico Reader. History, Culture, Politics, (Durham, London, 2002), p. 333. Alan Knight, 'Popular Culture and the Revolutionary State in Mexico, 1910-1940', Hispanic American Historical Review, 74:3 (1994), p. 395. Ibid, p. 395. Alexander S. Dawson, 'From Models for the Nation to Model Citizens: Indigenismo and the 'Revindication' of the Mexican Indian, 1920-1940', Journal of Latin American Studies, 30 (1998), p. 285. By the thirties, a new political machine, the National Revolutionary Party, had been put in place and the work of myth-making of Mexican heroes of the revolution started. This period has been marked the post-revolutionary period, as legitimising the state, as well as instilling a paradoxical blend of conservative revolutionary sentiments in Mexicans. This new ideological stream, indigenismo, was reflected in the mural paintings of Mexico's most prominent modern artists, in the work of intellectuals and in the number of epic novels about the revolution that were published in the twenties and thirties. It has, however, been held that for the government the aim was not 'to 'indigenize' Mexico, but to 'Mexicanize' the Indian'. Knight, 'Popular Culture and the Revolutionary State', p. 393. Martin, Journeys through the Labyrinth, p. 45. Rowe and Schelling, Memory and Modernity, p. 184. Indiginismo involved the reinterpretation of both the Indian and the nation. It involved, on the one hand, the creation of the myth of the radically social and progressive new Mexican nation, and on the other hand, a glorification of the Mexican Indian and their past. By the 1930s, however, the emphasis was shifted from race to culture. An interesting example can be found in the issue of bank notes. The depiction of a 'native woman, wearing traditional costume, standing before the Aztec calendar stone' was never circulated, and instead the images of Zaragoza and Madero were preferred. Now, notions of 'literacy, nationalism, notions of citizenship, sobriety, hygiene, and hard work' became part of the ideological mores; the Indians were believed to possess those qualities to an exceptional extent. They were, in fact, 'rational political actors with modern sensibilities'. To fit this singular representation, a new kind of Indian peasant emerged in literature: Dawson, 'Indigenismo and the 'Revindication' of the Mexican Indian', p. 282. Ibid, p. 285. Ibid, p. 294. Joseph M. Galloy, 'Symbols of Identity and Nationalism in Mexican and Central-American Currency',  URL  , (2000), viewed 11 May 2005. Knight, 'Popular Culture and the Revolutionary State', p. 395. Dawson, 'Indigenismo and the 'Revindication' of the Mexican Indian', p. 279. A dark Indian dressed in white, who wants to learn to read and write in Spanish and is grateful to the party; his children are healthy and can choose whether to stay in the rancho or migrate to the new life in the cities. Martin, Journeys through the Labyrinth, p. 46. Another aspect of this indigenismo were the rural ballads, corridos. They are transformed to be used on mass scale, into three-minute songs to be played on the radio Agustín Lara became the icon of this typically Mexican musical exponent. Often in appraisal of revolutionary heroes, corridos with their simple rhyme were easy to remember and thus accessible to considerable percentage of illiterate Mexicans. The following is an example of a corrido describing the death of Francisco Villa. Rowe and Schelling, Memory and Modernity, p. 100. Poor Pancho Villa! His destiny was very sad; To die in an ambush And in the middle of the way. He went, leaving Parral handling his horse, the brave general author of ""La Cucaracha"" [the cockroach] [own translation] Armando de Maria y Campos, La Revolucion Mexicana a Traves de los Corridos Populares, (Mexico City 1962), p. 245. Yet another way in which the revolutionaries were eternalised and epitomised as heroes of the Mexican nation was through the paintings of Diego Rivera, Jose Orozco and David Alfaro Siqueiros. These modern artists - 'Los Tres Grandes'- had started in the twenties to experiment with mural painting, a style which they thereafter used effectively for the depiction of Mexico's indigenous history, scenes of the revolution and of the modern, industrialised, progressive state. Supported and often commissioned by the government, these painters fulfilled 'the visual component of [the] need to create that Mexican citizen necessary for the survival of the post-Revolutionary state'. Martin, Journeys through the Labyrinth, p. 46. Anthony W. Lee, 'Mural Painting and Social Revolution in Mexico: 1920-1940, Art of the New Order - Review',  URL , (1999) viewed 11 May 2005. Ibid. In terms of literature, the 1930s saw a continuation of the style employed in the twenties. Epic works with 'the man on horseback', the archetypical revolutionary also depicted in the mural paintings of the post-revolutionaries. Where Guzmán had in 1926 recounted his memories in the revolutionary army of Villa with The Eagle and the Serpent, Gregorio López y Fuentes in 1932 created an epic hero out of Zapata. His novel The Land provided the sense of heroic Mexican leadership that legitimised the Mexican revolution. Martin, Journeys through the Labyrinth, p. 38. It is passed on in whispers, in huddles, by men at the plough and women at the well. He has been seen. So it's true he's not dead... 'Do you know who saw him? Old Albina. [...] This is what she told me: ""[...] I recognised him straight off. Hat pushed back, like always, them same trousers with the buttons, and that moustache you could tell a mile away. [...]"" ' Ibid, p. 45. There were, of course, also many writers opposed to this mythical image. In fact, the Mexican revolution brought no generation of poets of any significance; most of them were to be sought with the defeated reactionaries. Gómez Marín, who lived in London at the time wrote in a letter: 'From here, Mexico is something dark and bloody'. Moreover, the corrido, typical Mexican icon, could naturally also be used for other purposes than the glorification of the revolution. The following extract comes from a corrido praising the bravery of a christian Cristero rebellion. Gonzales and Treece, The Gathering of Voices, p. 29. Franco, Critical Passions, p. 456. Then the general said: ""I am prepared to grant you a pardon If you will tell me Where I might find the local priest."" Valentín promptly answered him: ""That I cannot say; I'd rather you kill me Than give up a friend.""... Before they shot him, Before he went up the hill, Valentín cried: ""O mother of Guadalupe! For your religion they will kill me. ""...Anonynous, 'The Ballad of Valentín of the Sierra', in Gilbert M. Joseph and Timothy J. Henderson (eds. ), The Mexico Reader. History, Culture, Politics, (Durham, London, 2002), p. 420. Arguably, illiteracy rates stood in the way of effective nation-building through literature. Some have thus asserted that 'it [was] film, not writing, which create[d] [...] the 'imagined community' of the nation'. Indeed, when the first film with sound was made in 1931, it was sponsored by state funds. Such films, which like in the case of Argentina and Brazil, were capable of reaching as mass audience, dealt with popular themes such as destitute prostitutes (which, of course, fuelled the macho discourse of the frail woman in need of protection) and went accompanied by popular tunes, most notably those of Agustín Lara's soundtracks. The state was quite involved in the ideological message such films conveyed. Thus when the Soviet filmmaker Eisenstein, visiting Mexico, planned to make a film on the history of Mexico, the government withdrew their subsidies for fear that such a film might cause an outrage amongst the oligarchic hacienda owners. King, Magical Reels, p. 99. Ibid, p. 42. Ibid, p. 42. Ibid, p. 44. In the aftermath of the revolutionary decade (1910-1920) in Mexico, the new authorities then energetically set about constructing the archetype of a new ""ideal Mexican"". This was based upon a renewed enthusiasm about the Indian, who was henceforth to be considered as a part of the new Mexican state. Indigenismo and, from the late 1920s into the thirties, revolutionary myth making played a big role in such representations. Nationalism was hence instilled through popular means: mural painting, the corrido ballad, film, and to some extent, epic literature. By actively interfering and negotiating these art forms, the state sought to channel the multitude of opinions into a glorification of the new Mexicanidad. Conclusion Nationhood is a fluid concept. It can be manipulated to incorporate whatever ideological ideas, and exclude or obscure what one does not find appropriate to it. Argentina and Brazil in the 1930s saw the beginning of a great transformation of nationhood, while Mexico had already begun that project directly after the Mexican revolution. Although often the state attempted to control cultural representations of its nationalism, intellectuals and poorer minorities did not let this happen easily. Borges refused to accept a simple, patriotic nationalism, thereby brilliantly revealing the paradoxes underlying such a concept; in Vargas' Brazil, the government was in a constant state of war with its poets and authors on the left, while former slaves of the favelas proved to be able to provide Brazil's most uniting and lasting national identity. Perhaps the Mexican state can be said to have succeeded best at imposing an official picture of nationalism. Their corridos and murals invoked a revolutionary-ideological Mexicanidad, party-style; and it would only be by the 1990s that Mexican natives started to wage war on that identity. Nevertheless, the successes in Mexico also knew their limits. The Catholic religion proved impossible to curb, while student protests in the 1970s already revealed the violent antagonism underlying the picture of Mexico that the government offered its people. Representations of national identity, then, are inherently linked with political ideology and power. Brazilian authors with tendencies toward communism provide the example; they employ a highly politicised image of the 'true Brazilian' proletariat. In Argentina, such representations resulted in two opposing views, the gauchoesque patriot, modelled after Martin Fierro stood in contrast to the liberal modernist 'rational' Argentine inspired by Sarmiento's Facundo. When tango, the voice of the minorities, and Borges came into the equation, a mono-cultured Argentine ideal became more complicated and troublesome. Similar in each of the three countries discussed, however, are the influences of 'lower' cultural forms. Tango, initially despised by all respectable layers of society, became the epitome of Argentinidad and proud export product after its success in the Parisian salons, while the samba of the Brazilian blacks became a national virtue. In Mexico, the corrido and new popular film both served the ideal of praising the spiritual idealism that belonged inherently to the Mexican man and woman, as opposed to the violent pragmatism of Northern American men, and the dehumanised star beauty of the Hollywood actress. Nationalism in all countries discussed, became increasingly represented in popular ways, reaching out to a mass audience. The constructors of such nationalistic forms sought to reach out to an unprecedented number of people, instilling upon them these new forms of nationalism. In Brazil, chanchadas united the country despite its immense differences in dialect, while in Mexico the party did their best to involve nationalism in every possible cultural and educational aspect. In Argentina it culminated in the decade of Peron's popular dictatorship. It can thus be said, that across these nations, the 1930s transformed nationalism into a popular and far-reaching construct.","The publication of The Bell Curve in 1994, the issues it discussed and conclusions it drew have triggered an enormous controversy over the years after its publication (Keenan 1999). This comes mostly from its assumed political incorrectness, claimed implications for public policy, and doubts from the scientific community as to its scientific claims (Keenan 1999). In this essay I shall attempt to take a closer look at some sociologically significant conclusions reached in the book. It seems, however, impossible to review the book without references to the sharp criticism it sparked upon publication. I shall therefore occasionally refer to critics or related academic work. The book sets out in an ambitious tone. In its introductory note, the authors forebode sensational conclusions, expecting 'that many people will have a ""This can't possibly be true"" reaction' (p. xix). It soon becomes clear that the book is intended for a wide audience, and throughout the book the authors continually insinuate this presumption. It is a point that is well worth noting, as we shall later see. The intention of the book, however, is not very clear at the outset. Vague notions such as 'for the last thirty years, the concept of intelligence has been the pariah in the world of ideas' (p. 1), and further, 'received wisdom in the media is roughly 180 degrees opposite from each of the six points [that we assume as a basis]' (p. 23) leave the reader with the idea that the authors are to set straight a grave misunderstanding. The six points which the authors write about are of great importance throughout the book. They are six assumptions from the classicist school about intelligence (or cognitive ability, two terms that are used interchangeably in the book) and the place IQ testing takes in expressing a person's intelligence. Especially claim five and six, respectively, that properly administered IQ tests are not biased, and that intelligence is 'substantially heritable [...] no less that 40 percent and no more than 80 percent' (p. 23) are facts that the authors lean on particularly heavily. According to critics, the problem is that these claims involve more complication and nuance than an average reader may understand, and that the consensus among psychometricians that the authors mention is not as broad as their own claims. A critic of the book, professor in psychology Robert Sternberg reacts as follows: 'I think that there is definitely some heritability of intelligence in the White population. Almost every psychologist believes there is some heritability of IQ and I agree. But the public may not understand just what that means. If you accept the use of the heritability statistic, about .5 [50%] is probably right' (Miele 1995). Throughout the book, moderate claims moulded into more radical views and outcomes are commonplace. This becomes especially controversial and fragile as topics around intelligence difference between races are being discussed (Frisby 1995). The virulent attack on the most controversial matters discussed in the book, according to Murray, 'often read like an unintentional confirmation of our view of the ""cognitive elite"" as a new caste', while 'when the Sturm und Drang has subsided, nothing important in The Bell Curve will have been overturned. I say this not because Herrnstein and I were especially far-sighted, but because our conclusions are so cautiously phrased and our findings anchored so securely in the middle of the scientific road' (Upstream). To put these claims in context, it is necessary to look at some of the criticism of the book. The Bell Curve has been accused of presenting conclusions on a pseudo-scientific basis. As the book is quite substantial, and the authors continually moderate the argument besides cross-referencing chapters, it is difficult to either speak completely for or against the type of science that The Bell Curve presents the reader with. However, the idea that Sternberg brings up is worth noting. He says: 'The way that book is written is to, I think, say X on page 605 in sentence 8, with an appropriate caution, and then invite the reader to a somewhat more extreme conclusion elsewhere' (Miele 1995). In line with this claim, we find the authors introducing the idea of a possible constant deviation in intelligence between ethnic groups in a casual way: 'Americans have watched the spectacular economic success of the Pacific rim nations' (p. 272), as if this were a logical consequence of a population's higher IQ average. In the unfolding chapter 13, which deals with variations of cognitive ability between ethnic groups, the book continues to provide the evidence for trends in intelligence difference between 'Asians' and 'Whites'. For this evidence, the book leans heavily upon the work of Lynn. A key claim, that IQ differences in the results are not generally found in culturally biased assignments, is here backed by only one source, Jensen. The work of this scholar, however, is certainly not uncontroversial and may even be racially motivated (Keenan 1999). Thus, where Murray and Herrnstein claim that 'there is no longer an important technical debate' (p. 282), Sternberg counters: 'To the extent that there is a consensus it is certainly not Herrnstein's and Murray's' (Miele 1995). Nonetheless, it is no more than fair to put forward Murray's response in the afterword of the book: 'Never mind that The Bell Curve draws its evidence from more than a thousand scholars. Never mind that among [the accused scholars] are some of the most respected psychologists of our time' (p. 564). Possibly the complication with the data used lies not so much in its collectors as in the theory behind it. In order to further their argument, the authors take a great and careless leap beyond their use of the concepts of 'race' and 'ethnicity', concepts which, according to Frisby, are 'best left to specialists' (Frisby 1995), while a UNESCO report argues that 'The term [race] at best is at present time not really allowable on any score in man' (Miles 1982). How then do the authors of The Bell Curve justify their use of ethnicity (which, again, may be taken as interchangeable in the book)? Only a paragraph seems enough: 'The studies of ""blacks"" or ""Latinos"" or ""Asians"" who live in America generally denote people who say they are black, Latino or Asian - no more, no less' (p. 271), and further they write 'race differences are varied and complex - and they make the human species more adaptable and more interesting' (p. 272). In this way, the authors appear to draw a direct connection between personally perceived ethnic differences and the distribution of cognitive abilities for different human groupings. It can, however, conversely be argued that, like all over the world humans are expected to be born with two arms and two legs, the distribution of intelligence may be expected to fit into a similar predictable model worldwide. Opinions are still so divided about the issue that different conclusions are reached from the same data. As an example, the conclusions of Flynn are refuted by the authors as follows: 'He also says that Asian-Americans actually have the same non-verbal intelligence as whites', but 'Lynn disagrees and concludes from the same data [my italics] that there is an ethnic difference in overall IQ as well' (p. 273). Perhaps the contrast cannot be presented starker than through the following example. Whilst the authors argue that 'tests results are matched by analyses of occupational and scientific attainment by Jews, which consistently show their disproportionate level of success' (p.275), Keenan (1999) insists: 'The Jews provide another example of the blending of racial distinctions. They are all presumably derived from one population [...] Gene frequency studies, however, show that now Jewish populations in different parts of the world tend to resemble their surrounding populations at least as much, if not more, than they do each other'. The book consists of four parts comprising some 500 pages of textual body and several appendices providing more background. Part III, nonetheless, presents the most important and controversial findings. The authors recognise this, claiming, 'that many readers have turned first to this chapter indicates how sensitive the issue has become' (p. 270). It is difficult to accuse the authors of racist motives. Placing possibly controversial outcomes into a thought exercise, the authors state 'some people would interpret the news as a license for treating all whites as intellectually superior to all blacks [but it becomes] obvious how illogical [...] such reactions would be' (p. 312). It can, however, easily be said that the authors jump to conclusions about racial differences too soon, to go on suggesting changes in public policy in part IV. The problem with this book thus lies with its scientific inconsistency. It sets out with no clearly stated objective, while the use of scholarly caution and polite integrity is broken soon after the authors bring it up. While the authors promise in the introduction to 'employ the more neutral term cognitive ability as often as possible' (p. 22), synonyms such as 'smart' (p. 127), 'very smart... Very dumb...' (p. 133) that occasionally crop up nearly indicate the carelessness with which the authors draw their conclusions. Frisby claims that 'to parade facts about racial differences before the general public is akin to putting a lit match to gasoline' (Frisby 1995). The authors of The Bell Curve are eager to draw conclusions and that it is what lends this work its importance, but also its legions of hostile critics. In the words of Sternberg: 'If you were to ask, ""What inference do Herrnstein and Murray invite their readers to draw?"" they go beyond what they know' (Miele 1995). It was in 1903 that Emmeline Pankhurst and her daughter Christabel founded the Women's Social and Political Union (WSPU), following around forty years of organised campaign for female suffrage organisations in Britain (Banks 1981). After another fifteen years of campaign, interrupted by the First World War, women were finally granted the vote in 1918 through the Representation of People Act. It remains, however, a controversial matter as to whether the militant campaigns of the WSPU actually helped to quicken the vote for women, or not. In order to get a clearer picture of the tactics of the suffragettes, it is worthwhile to take a closer look at their use of the female body in violent, unconventional and often illegal ways, to draw attention to their cause. The connection of British femininity with a high morality, and the idea of gender equalities through historical argumentation were common ground arguments for the vote in the late nineteenth century. Although arguments of the 'constitutionalists' always sought to stay within the boundaries of middle class respectability, they certainly incorporated argumentation based upon the female body (Holton 1998). The most evident examples of this can be found in racist theories. Female authors attempted to present an image of a superior British race, of which women, had, by necessity, always been part. Charlotte Carmichael Stopes', in her book British Freewomen of 1894, argued that women's right to political participation originated from the ancient Britons. 'The moral codes, sexual equality in physical height [my italics]' were, in her book presented as arguments for women's suffrage (Holton 1998: 158). Constitutionalist feminists increasingly began to make use of racial reasoning to support their campaign for the vote (Holton 1998). This provided the movement with a legal means of enhancing female respectability and high morale in a way that was compatible and in harmony with society. However, after forty years of such campaigns, the women's vote was still nearly as far away as it had been at the outset. This realisation caused the WSPU to seek to pressurise the government, for they were responsible for the problem (Pugh 2000). From a harmony model, the dominant suffrage campaigns thence shifted to a model of conflict, bringing the movement into a new phase (Banks 1981, Holton 1998). The suffragettes, as the WSPU activists came to be known, sought to cut right through to the core of the problem by addressing the government directly. They sought to point out the inherent contradictions of the political system as it was: the partial inclusion of women into an essentially male-dominated environment (Lawrence 2000). From insisting on politicians' support in public meetings, the suffragettes soon radicalised (Vicinus 1985). They felt that the suffrage question was not dealt with seriously, and from there the WSPU leader Christabel Pankhurst set out to phrase the problem more directly: '[i]f Mr Asquith [PM] would not let her vote, she was not prepared to let him talk' (Lawrence 2001). This meant a great leap away from the Victorian feminist movement; suffragettes sought to replace the passive, homely housewife with a campaigning activist, a political being. In the words of the prominent suffragette Emmeline Pethick-Lawrence: 'Not the Vote only, but what the Vote means - the moral, the mental, economic, the spiritual enfranchisement of Womanhood, the release of women...' was of vital importance to women (Vicinus 1985: 249). A new stage of interrupting meetings started. Suffragettes continually disrupted parliamentary debates, by shouting out against the speaker. Increasingly coordinated, the suffragettes were sometimes able to spoil a complete speech, by disrupting the speaker in turns. In 1908, a speech by Lloyd George was continually interrupted for two hours, with fifty women carried out (Pugh 2000). Although this obstruction of the political process was arguably playing into the hands anti-suffragists, such forceful, physical practices of politics can be said to have been part of masculine politics as it was conducted by male politicians (Lawrence 2001). Indeed, when members of the audience could be forcefully removed from a public political meeting, this might be interpreted as a threat to the civil liberties (Pugh 2000). This meant a moral victory for the suffragettes, especially since gentlemanliness towards women was expected of politicians. Mass assault and arrests of women were not an uncommon sight any longer, which the particularly violent events of 'Black Friday' in November 1910 highlighted (Vicinus 1985). Hence, the suffragettes increasingly highlighted this state brutality by a number of means. Hunger striking was one of them, first begun on the initiative of Marion Wallace Dunlop in July 1909 (Pugh 2000). Protesting against the government's refusal to grant the suffragettes the status of political prisoners, the WSPU soon managed to place the campaign for female suffrage on a moral high ground, as the government had to face the issue of the prisoners' treatment. The WSPU brilliantly publicised this moral strength, speaking of 'moral right triumphing over physical force' (Vicinus 1985). Forcible feeding of hunger striking suffragettes soon received criticism from doctors (Pugh 2000) and graphic representation presented a shocking picture of the treatment of women in prisons. The problem continued, and leaving even the parliament divided (John 2001). In 1913, the Cat and Mouse Act was thus passed, which dismissed the policy of forcible feeding and was aimed at avoiding the negative publicity deriving from it. To some extent, the act succeeded in doing so, as many people argued that their suffering was 'self-imposed and their martyrdom as in some sense staged' (Harrison 1978: 180). This loss of sympathy and moral and intellectual high ground was however enhanced by the suffragettes increasing radicalisation and alienation from sympathisers (Pugh 2000). The suffragettes showed a radical impatience and determination that eventually led them to virtually abandon any techniques of persuasion (Pugh 2000). The years in the run towards World War I resulted in the most passionate outbursts of the suffragettes attack upon male domination of the political system (Vicinus 1985). A first systematic window breaking campaign had been undertaken in 1909, of which Sylvia Pankhurst said, 'let it be the windows of the Government, not the bodies of women which shall be broken' (Lawrence 2001: 222). A near concession of the parliament in 1910, which would have resulted in a limited vote for women, could not be passed by a much divided parliament. Winston Churchill, usually a sympathiser of the women's suffrage cause, voted against the bill in resentment of the WSPU's violent tactics (Harrison 1978). However, this failure so outraged the rank-and-file WSPU members, that another window breaking campaign was started by the end of 1911 (Vicinus 1985). Popular support for the WSPU was on the decline. The violence of the suffragettes' campaign made it possible to argue that women did not demonstrate the capability to participate in politics due to their rash and unreasonable behaviour. Criticism of the Union's leaders, Emmeline and Christabel Pankhurst, even enhanced this view. 'They are not calm women,' Teresa Billington-Greig claimed, 'but irritable, yielding to sudden tempest' (Harrison 1978: 176). And whilst, 'the police, in early stages [...] avoided heavy-handed treatment of the women' (Pugh 2000: 193), they still insisted on continued provocation and imprisonment (Pugh 2000). In 1912 Christabel Pankhurst fled to Paris after the police raided the WSPU headquarters in London. As the movement was losing grip of its popular support, so did Christabel break with some of the high-ranking campaigners, among them the Pethick-Lawrences whom had up until then financed the several branches of the organisation (Pugh 2000). The publication of the 'The Freewoman', a periodical by a number of suffragettes, between 1911 and 1913, 'reawakened the free love fears that had haunted the feminist movements since its beginning' (Banks 1981: 132). The Pankhursts, however, had always insisted on dressing in feminine and respectable manner to disarm opponents from criticism (Pugh 2000). And despite arson, window breaking and other violent behaviour of WSPU movements, the organised anti-suffragist movement was cautiously following the skill with which the suffragettes sought publicity. As correspondence of October 1913 shows, the anti-suffragist movement was well aware of the press attention that the suffragettes managed to obtain. '[P]ublicity in the Press', an executive committee member wrote, 'is our greatest need and our opponents' chief advantage over us' (Harrison 1978: 175). It was this sparked public debate, and the attitude of directness and briskness in contrast to the vagueness and eventual moral weakness of the anti-suffragists over the real debate of women's suffrage, that eventually led the parliament to extend the vote to women over thirty. The suffragettes' efforts can be said to be characterised by a curious mixture of Victorian respectability and morality blended with a continual and controversial insistence on their rights. An immense confidence in the female gender to the extent of superior feelings (Vicinus 1985) combined with a strong sisterhood and sense of sacrifice for the cause led Sylvia Pankhurst to burst out even in 1954 against anti-suffragist Violet Markham: 'that foul traitor - who, while the suffragettes were hunger striking, appeared on the Albert Hall platform, [...] protesting against women having the vote' (Harrison 1978: 13). Resultantly, at all times, the suffragettes used their bodies as a symbol of sovereignty and moral superiority over the established political power, and this to point out the flaws in the anti-suffrage argumentation.",False
16,"Rousseau has always been typified as an intellectual outsider; it was, however, only in the twentieth century that his socio-political theory came to be seen as a threat to the individual's liberty. Emphasis thereby is often laid on his concept of a 'general will', through which personal liberty is handed over to the collective. 'The citizens having but one interest', Rousseau argues, 'the people [have] but a single will'. And although Rousseau favours the ballot vote, he argues for the infallibility of its ruling: 'when the opinion contrary to my own prevails, it proves nothing more than that I made a mistake, and that what I took to be the general will was not'. From the point of view of Rousseau, most importantly protection should be rendered against the emergence of a minority rule, or 'particular will', rather than against the state, which, after all, represents the sovereignty of the lawgiver as it corresponds with the community's best interest, the general will. The question I will look at here is thus to what extent the critical assertion holds truth that 'the more safeguards he established against despotism, the more weapons he forged for tyranny', within Rousseau's system of the social contract. Iain Hampsher-Monk, 'Rousseau and totalitarianism: with hindsight?' in Robert Wokler (ed), Rousseau and Liberty (Manchester/New York, 1995), p. 268. Jean-Jacques Rousseau, The Social Contract and Discourses (London/New York, 1952), p. 87. Christopher Bertram, Routledge Philosophy Guidebook to Rousseau and The Social Contract (London, 2004), p. 120. Hampsher-Monk, 'Rousseau and totalitarianism: with hindsight? ', p. 271. Recent criticism has argued that the Cold War split has caused intellectuals to make an over-simplified distinction between 'individualistic liberalism and state collectivism', stating that Rousseau was on the totalitarian side of the argument. Rousseau, however, from the outset entertains an awareness of the problem of new forms of subjugation that hide behind a powerful state, and thus sets out to construct a model for a 'form of association [...] in which each, while uniting himself with all, may still obey himself alone, and remain free as before'. In this sense, Rousseau undertakes to find a form in which freedom can be exercised on a public, political level. The important distinction that has to be made here though, is the type of freedom at stake. Arguing that pre-modern, pre-social man exercised an individual sovereignty free from social obligation or commitment, knew a natural, brutal kind of liberty. When people come to regular interaction, they become interdependent, which goes accompanied by fierce passions. It is only within society that morality comes to exist, through virtue and pitié, a form of compassion which counters man's corruption by society. What is then needed is the construction of an association in which passions are tempered and virtue is encouraged. In this sense, envisaging a social contract is merely a theoretical exercise, to find a form in which such an exercise of sovereignty can be best expressed, in other words, 'Rousseau's purpose is to discover a rule which has moral validity'. Hampsher-Monk, 'Rousseau and totalitarianism: with hindsight? ', p. 268. Rousseau, The Social Contract, p. 12. Patrick Neal, Liberalism and Its Discontents (London, 1997), p. 61. Rousseau, The Social Contract, p. 5. Maurizio Viroli, Jean-Jacques Rousseau and the 'well-ordered society' (Cambridge, 1988), p. 132. Christopher Bertram, Routledge Philosophy Guidebook to Rousseau and The Social Contract (London, 2004), p. 19. Viroli, The 'well-ordered society', p. 121. Another point of view insists on the juxtaposition of democracy and liberalism, positioning Rousseau in a radically democratic framework. Certainly, this view depends greatly on the view one takes of liberalism. Liberalism can be taken at an individual level, stressing the removal of any constraint on human action (negative liberty), and placing theory in wider context, that of the body politic (positive liberty). Rousseau is quite explicit about what form of liberty we are to seek. 'Man is born free; and everywhere he is in chains', he argues, 'One thinks himself the master of others, and still remains a greater slave than they'. It is therefore clear that Rousseau seeks a wider answer to the question of liberty, one for which 'it is necessary to ensure that the conditions in which [people] live do not pose a threat to their survival'. This condition can be brought about by the realisation that it is the best interest of all citizens to care for other's interest and well-being. Here we find the distinction that Rousseau makes between a narrow form of personal interest and a wider, more coherent form of enlightened interest, being the best for the community, and thus, indirectly, each citizen's actual will, as it secures the best-considered proposal and enjoys general support of the citizenry. Subordination to the law subsequently only becomes an exercise of one's liberty. Carol Blum, Rousseau and the Republic of Virtue. The Language of Politics in the French Revolution (Ithaca/London, 1986), p. 32. Hampsher-Monk, 'Rousseau and totalitarianism: with hindsight? ', p. 271. Rousseau, The Social Contract, p. 3. Viroli, The 'well-ordered society', p. 112. Bertram, Rousseau and The Social Contract, p. 44. Exercising one's freedom through abiding by the law is only reasonable if lawgiver and subject are the same person. This is where Rousseau takes a radically democratic stance, criticising Kant's very open assertion that law can still be legitimate only by the requirement that the requirement 'that the proposal could have been agreed to had the citizens been consulted and were they fully rational'. Hence Rousseau's claim that 'power indeed may be transmitted, but not the will': for the general will to function, it requires the individual standpoints of each citizen. Unanimity is thereby not needed; it is in fact a sign of an unhealthy body politic. The only law that requires unanimity of all citizens is the social pact, since by this each individual hands over their individual sovereignty to the higher expression of the community. Within the body politic, the general will aims to protect rather than to attack its participants, and 'it is not', therefore, 'to be assumed that any of the parties then expects to get hanged'. Likewise, peace with neighbouring communities is required by natural law. It has therefore been claimed that the general will represents a model for the well-being of all, and thus reflects 'each individual's good or real will'. Neal, Liberalism, p. 63. Rousseau, The Social Contract, p. 20. Ibid, pp. 87-88. Ibid, p. 28. Hampsher-Monk, 'Rousseau and totalitarianism: with hindsight? ', p. 270. Ibid, p. 270. The emphasis that Rousseau lays on the collective, however, is today regarded with mistrust. After all, the functioning of the general will depends entirely upon the full dedication of the citizenry to the social contract, and we find 'no restriction of principle on what it may command in that form'. It appears at times as though Rousseau puts all individual risks at right for an ideologically unified community. As soon as all sovereignty passes from the individual to the state, a greater power comes to exist which '[man] cannot use without the help of others'. Contrarily, there is no 'safeguards for the individual against the state'. The Rousseauean state results in collective socialisation, only rendered legitimate through the consent of its participants. In a wider interpretation of the social contract as merely a thought experiment, this socialisation obtains a somewhat enforced character. Bertram, Rousseau and The Social Contract, p. 193. Blum, Republic of Virtue, p. 111. Hampsher-Monk, 'Rousseau and totalitarianism: with hindsight? ', p. 272. Ibid, p. 268. One form of criticism of the general will draws parallels with the Jacobin rule during the French revolution. Both reveal traces of absolutist totalitarianism: 'It is absolutely necessary that evil come from others and from them alone'. Despite the validity of a critique of Rousseau's reassertion that the general will is infallible (which indeed shows traces of a majority intolerance), it is difficult if not altogether impossible to find examples of the Rousseauean state in practice. Eastern block socialism, which has been suggested as imposing some for of a state-directed will upon its citizens, has been severely criticised. Also the Jacobin adoption of Rousseau's general will has been seen as of symbolic value rather than a precise influence of his thought. Lacking everywhere has been the binding together of lawgiver and subject, which according to Rousseau results only in the imposition a particular will of a state governed by a political elite. Jean Starobinski, 'La Mise en accusation de la Société', cited in Carol Blum, Rousseau and the Republic of Virtue. The Language of Politics in the French Revolution (Ithaca/London, 1986), p. 218. Blum, Republic of Virtue, p. 32. Hampsher-Monk, 'Rousseau and totalitarianism: with hindsight? ', p. 272. It is true that Rousseau is most concerned with the organisation of social order based upon the exercise of virtue, rather than on preserving the individual's relative freedom of action. It is therefore perhaps surprising that Rousseau leans entirely on the exercise of a general will in ensuring the best for all citizens. Through this construct, Rousseau categorically denies the individual any sovereignty except for a participatory role in the community. Effectively, the state attains a strongly fortified position in which ' [t]he more the natural strengths are dead and destroyed, the more the acquired ones are great and durable, and the more solid and perfect this institution is'. Such a radical step toward an active, creative form of liberalism challenges liberals into the debate of the equalisation of the citizenry, since 'serious economic inequalities destroy liberty'. Nonetheless, it does not suffice to, as a consequence of this conclusion, leave all individual property and grounds for action within the confines of the community's sovereignty. Viroli, The 'well-ordered society', p. 113. Blum, Republic of Virtue, p. 111. Hampsher-Monk, 'Rousseau and totalitarianism: with hindsight? ', p. 275. One of Rousseau's ideas of human nature was that the strongest of human passions was amour the soi (love of self), of which virtue and piti éwere manifestations within society. 'No-one has any interest in making them [the conditions of association] burdensome to others' This may be seen as a problem for the protection of the individual's rights: Rousseau does not expect the intrusion of state into the individual's life and thus includes no precautions to safeguard certain domains of private life. 'If this leads to intervention in 'areas which are perceived as illiberal, such as the social and economic conditions of citizen's lives and with their beliefs and values', this is seen by Rousseau as a decision of the general will in order to secure the necessary conditions for socio-political liberty. To some extent the modern welfare state can be seen as a moderate outcome of Rousseau's general will, be it on a representative basis. Executive and lawmaking government of the welfare state is now generally expected to create policy that benefits the community in a broad social sense. Taxation, health security and economic intervention are put in place to equalise conditions and ensure a minimum standard of welfare for each citizen, whereas participation in elections for government positions is open to each citizen. This form of moderate liberal welfare state, however, fails to pass the test, since in this model the law does not come from all. Bertram, Rousseau and The Social Contract, p. 24. Hampsher-Monk, 'Rousseau and totalitarianism: with hindsight? ', p. 281. Ibid, p, 275. Bertram, Rousseau and The Social Contract, p. 157. The dichotomy between modern, individualist liberalism and the Rousseauean state lies predominantly in their different interpretations of liberty. Whereas the former stress the lack of protection for the individual in Rousseau's social contract, Rousseau in turn entertains concerns with the crude, unsocial kind of liberty that individualists profess. Rather does he see liberty in a community best exercised at the highest level, thus enhancing liberty in an ordered way. For order to exist, there needs to be unity based on a reasonable principle; the social contract offers just that. As liberty exercised only on the most individual, narrow level only furthers contradiction in interests, Rousseau proposes the pursuit of community interest, which, if carried out fully, secures the welfare and safety of all its citizens. What Rousseau does not display enough attention to, however, was the fact that 'there is never a perfect fit between moral requirements of our rational nature and requirements of citizenship'. This results in a lack of consideration of the nature of the general will, and its proper domain. Rousseau, on the contrary, believes in the stabilising role of the general will, and the inalienability of the will in general. As the general will represent the only genuine method of determining the sovereign will of the citizenry, parallels drawn between totalitarian ideological states and Rousseau's general will are, strictly theoretical, incorrect. The problem, nonetheless, lies with the radical solution Rousseau's general will poses. Asserting that '[t]o be governed by appetite alone is slavery, while obedience to a law one prescribes to oneself is freedom', leaves very little space for individual action. Thus, a community taking its legislative task to the extreme, might indeed force its citizens into abide by the law. The question becomes, then, not to what extent Rousseau can be used for totalitarian ends, which he explicitly sets out to prevent, but rather how a feasible system may be envisaged in which particular interpretations of the general good can obtain a stable construct in which the individual is both more protected and better positioned to criticise the state's pursuit of the general good. Viroli, The 'well-ordered society', p. 38. Christopher Bertram, Routledge Philosophy Guidebook to Rousseau and The Social Contract (London, 2004), p. 33. Rousseau, The Social Contract, p. 20. Neal, Liberalism, p. 60. Christopher Bertram, Routledge Philosophy Guidebook to Rousseau and The Social Contract (London, 2004), p. 194. IntroductionWhen we talk about the 1930s in terms of social history, we often refer to the strong emergence of ideology into the public domain. Moreover, for Latin American countries, the first half of the twentieth century marked the coming of 'modernity'. Other factors, however, such as immigration and a rumbling social hierarchy in the aftermath of slavery, tended to influence society in more informal ways. Self-representation through culture was usually closest at hand for the lower classes, and often provided a colourful means of communicating a multi-faceted message. In this essay we shall look at such cultural representations in Argentina, Brazil and Mexico, and assess the extent to which they contributed to the construction of a national identity. Naturally, art forms such as dance and music were, even though powerful, not the only way of expressing one's national sentiments. The first films with sound managed to reach a mass audience and demonstrated what it was like to be a true Argentine, or Brazilian, in a light package. Moreover, the aftermath of key political events (Argentina, the 'infamous decade' after the fall of Yrigoyen; Brazil, the 1930 coup of Vargas; Mexico, the heritage of the revolution) and the subsequent split among the people brought a new literature of thinkers focussing with urgency upon 'national character and the metaphysics of the Mexican, Argentinian or Brazilian 'being''. The assessment of each country's culture climate thus provides interesting insights into the attempts made at a culturally constructed nationhood. Edwin Williamson, The Penguin History of Latin America, (London, 1992), p. 513. Gerald Martin, Journeys through the Labyrinth. Latin American Fiction in the Twentieth Century, (London/New York, 1989), p. 67. Argentina: 'a land of exiles' Jorge Luis Borges, cited in in Gabriel Nouzeilles and Graciela Montaldo (eds. ), The Argentina Reader. History, Culture, Politics, (Durham, London, 2002), p. 1.The Argentina of the 1930s saw increasing modernisation and a changing political climate. This did not leave the notion of nationalism untouched; and, after the social positivism of the Irigoyen government that was overthrown in 1930, cultural representations of the Argentine nation became increasingly ambiguous. The 'Década Infame' brought a reconsideration of Argentine patriotism and the essence of 'Argentineness'. A new definition of the Argentine identity proved, however, not quite unproblematic. A nation of immigrants, Argentina had many faces and was, with the coming of further modernity and political maturing of the masses, thrust into a further set of cultural disparities. These were based upon dichotomies as varied as those of machismo versus feminism, the European-orientated metropolitan versus the patriotic gaucho, and that of the definition of a high culture versus that of commercially formed mass culture. Essentially, thus, the concept of argentinidad was politically and culturally utilised in a multitude of ideological representations: 'The references to a collective ideal of the argentinidad were continuous [...], although this ideal was defined in the most imprecise way and was basically the product of the whims and wishful thinking of the ideologues who wrote on the subject, more than an ideal that could be identified empirically as a trait of Argentina's culture'. Carlos Escudé, 'The Case of Argentina',  URL , (2000), viewed 2 May 2005. An early elaboration upon Argentine culture and its forms can be found in Sarmiento's Facundo of 1845. Sarmiento, Argentina's president between 1868 and 1874, regarded the countryside, inhabited by the mestizo gaucho Argentine, as uncivilized, chaotic - even barbarous: 'dirty children covered in rags, living amid packs of dogs; men stretched out on the ground, in utter inactivity; filth and poverty everywhere'. The country's biggest cities, on the contrary, contained every aspect 'that characterizes cultured people'. Later, the popular poem about Martín Fierro created a distinction between the good, heroic and patriotic gaucho, and the barbarous and violent bad gaucho. The early twentieth century thereafter saw a complete rehabilitation of the gaucho icon, when it was presented as an example of argentinidad in an increasingly immigrant-dominated society. This was, however, merely a constructed and idealised identity, and the by the end of the twenties, the plurality of immigrant cultures were gaining space from more traditional social and cultural dance clubs with names like 'The Patriotic Gauchos'. In its stead came a new form of dance, less courteous and more fitting in the context of the big metropolis Buenos Aires: tango. Domingo Faustino Sarmiento, Facundo. Civilization and Barbarism, (London, 2003), p. 51. Ibid, p. 52. William Rowe and Vivian Schelling, Memory and Modernity. Popular Culture in Latin America, (London, New York, 1991), p. 34. Leopoldo Logunes, 'National Identity in a Cosmopolitan Society' in The Argentina Reader, p. 209. William Rowe and Vivian Schelling, Memory and Modernity. Popular Culture in Latin America, (London, New York, 1991), p. 35. The thirties in Argentina were marked by modernising development from above and from below. The tango started its ascending popularity from below. A dance of immigrants, it was originally popular in the packed, miserable and crime-ridden suburbs of Buenos Aires. Its association with knife-fighting and gangs was famously put into words in Borges' short story 'Streetcorner Man' of 1933. In it, two gang leaders head into confrontation on an illegal tango party. Ibid, p. 35. 'The only reason I don't carve you up is because you sicken me,' the Butcher said to Rosendo, threatening to strike him. That same moment, La Lujanera threw her arms around the Butcher's neck, fixing those eyes of hers on him, and said in a fury, 'Let him alone - making us think he was a man.' For a minute of so [the Butcher] was bewildered. Then, wrapping his arms around La Lujanera, he called to the musicians to play loud and strong, and he ordered the rest of us to dance. Jorge Luis Borges, The Aleph and Other Stories, (London, 1973), p. 27. These aspects - machismo, violence, illegitimacy, made dancing the tango into a rebellious act, associated with characters on the edge of society, devoid of patriotic virtue or morality. This was soon to change, as the tango was brought to Europe and became popular in the salons of avant-garde Paris. Before this shift, the tango only gained marginal recognition as a truly Argentine metropolitan dance, as in the poem 'Milonga' by Oliverio Girondo. Rowe and Schelling, Memory and Modernity. Popular Culture in Latin America, (London, New York, 1991), p. 36. Males whose bodies rupture in a ritual court, their heads sunk low between their shoulders, their lips thick with coarse remarks. Females with their nervous rumps, bits of foam at their armpits and their eyes looking much too oily.Oliverio Girondo, 'Poems to Be Read on a Trolley Car' in The Argentina Reader, p. 252-53. A second exponent of tango, nevertheless, soon found through tango a new formula for political criticism, woven into poetical lyrics. The song 'Cambalache' by Enrique Santos Discépolo, written in the early thirties, counterposed the liberal positivistic nationalism with a gloomy, destructive view of the new Argentina. 'Those that don't cry don't feed', Santos sang, 'and those that don't steal are fools'. Enrique Santos Discépolo, 'Cambalache', in The Argentina Reader, p. 267. Despite this loathing with politics, the middle class adopted the tango as an aspect of the new, dynamic and modern culture that Argentina was now adopting. In this, the cinema played a big, yet not unambiguous role. Argentina was steadily adopting the Northern American cinema culture. In 1930, around a thousand film screens could be found throughout the country. However, already by 1926 an estimated 95 percent of screen time was filled by films from the United States. Tango reached a mass audience when sound was added to cinema films, and a number of glamorous films with appealing tunes lent this formula its great success. Unfortunately this also caused tango to become a mass cultural phenomenon, alienated from its former basis of charismatic metropolitanism. By the late thirties, fifty such films were produced a year, also serving as a charming and successful export product for other Latin American countries. John King, Magic Reels. A History of Cinema in Latin America, (London, New York, 2000), p. 9. Ibid, p. 11. Ibid, p. 38. While the metropolitan immigrant gained commercial ground and mass appeal, other cultural forces were working from above, posing questions at a more political and intellectual level. X-Ray of the Pampa was published in 1933, melancholically referring back to inner country Argentina as a real source of Argentine national identity. In the essay, Ezequiel Martínez Estrada accused the central government in the metropolitan centre of cultivating city values at the cost of the 'countryside, which is full of truth and life'. Minority immigrant groups, faced by the dominance and lack of understanding of the overwhelmingly Catholic Hispanic cultural majority, attempted to secure themselves a place within the Argentine identity. Jews, who were often particularly victim of racism, attempted, with relative success, to bring the issue to attention in the theatre. A number of Jewish playwrights wrote plays on Argentine Jewish families and their cultural problems, with strategies varying from the complete abandonment of any distinctive Jewish identity', to the incorporation [...] of Argentina's many immigrant groups, their concerns and their idioms'. The playwright César Tiempo in 1937 even won a prestigious theatre prize for one such play with a light-hearted tone, Pan criollo [Native Bread]. Ezequiel Martínez Estrada, 'X-Ray of the Pampa' in The Argentina Reader, p. 262. David William Foster, Cultural Diversity in Latin American Literature (Albuquerque, 1994), p. 97. Foster, Cultural Diversity, pp. 110-11. Ibid, p. 111. The literature of Jorge Luis Borges forms a totally separate category in Argentina's intellectual life of the thirties. Whilst his outlook towards nationalism was complicated and peculiar, both the political left and right came to agree 'that what the fictions display is mastery'. Initially, Borges became part of the capital's avant-garde poetry circle, whose most famous representative was Oliverio Girondo, and his first published poems still breathe the excitement of metropolitan life. By the 1930s, however, Borges came to reject Martín Fierro's nativistic idea of argentinidad as well as the avant-garde as 'infantile disorders'. Perhaps due to some drastic changes in Borges' life over the second half of the thirties, among which the death of his father, a short period of serious illness and a mindless job as a librarian, Borges increasingly came to see all experiences as individual. Increasingly, Borges disapproved of simplistic, mono-cultured concepts of Argentine nationalism, in which some groups were downplayed in favour of others. Instead, he proposed that Jean Franco, Critical Passions: Selected Essays (Durham/London, 1999), p. 328. Martin, Journeys through the Labyrinth, p. 153. Franco, Critical Passions, p. 331. Martin, Journeys through the Labyrint, p. 153. we cannot limit ourselves to purely Argentine subjects in order to be Argentine; for either being Argentine is an inescapable act of fate - and in that case we shall be so in all events - or being Argentine is a mere affectation, a mask.Jorge Luis Borges, cited in Martin, Journeys through the Labyrinth, p. 155. This deconstructionist view of argentinidad cut short all aspirations toward an inward-looking, 'authentic' national identity, and hence, Borges later came to be regarded as an early exponent of postmodernism. Throughout the 1930s, Borges was still very much in search of new values, comparing the Argentine with the Jew, whom, according to him both had no 'particular national tradition', while the entrenchment in a narrow set of national values was 'negative, or, at the very least, uncreative'. Soon, he would move on to a path of displacing, often surprising literary creations, revealing elements of both ironic genius as elitism. Franco, Critical Passions, p. 327. Ibid, pp. 330-31. Williamson, The Penguin History, p. 516. Martin, Journeys through the Labyrint, p. 159. Brazil: 'All hail! This samba's going to end in jail'  Patrícia Galvão, 'Where They about Rosa Luxemburg', in The Brazil Reader, p. 166.The 1930s as a definable decade for Brazil started in October 1930, when a group of young army officers overthrew the long-standing São Paulo oligarchy and brought GetúlioVargas to power - a dictatorship that was to last until 1954. This inaugurated an era of modernisation to the country, and a new search for a national identity. Gradually, art forms and cultural expressions of the lowest classes (in Brazil this mainly meant the former slaves) became accepted by a wider public, and eventually even representative as national symbols. Dances like the samba and capoeira and the carnival of Rio de Janeiro - though often in commercialised and regularised forms- now became celebrated aspects of the Brazilian national identity. The film genre of the chanchada came hereby to play an important role, as it familiarised people from the cities with the cultural life of the favelas, the neighbourhoods of the poor black people, revealing 'its misery and at the same time its tremendous beauty'. Literature and poetry, on the contrary, became increasingly politicised throughout the thirties. Already in 1931, many of the writers in the Modernist movement were aware of the ideological clash that the Vargas government was driving the cultural community into. This drove a number of intellectuals to the Communists' side and often into imprisonment, as they presented a 'proletarian nation' in their work. The most lasting national cultural legacy, nonetheless, comes from the bambas of the favelas. Williamson, The Penguin History, pp. 416-22. Robert Stam, Tropical Multiculturalism. A Comparative History of Race in Brazilian Cinema and Culture, (London, 1997), p. 80. Ibid, p. 82. Mike Gonzales and David Treece, The Gathering of Voices. The Twentieth-Century Poetry of Latin America, (London, New York, 1992), p. 154. Martin, Journeys through the Labyrinth, pp. 69-72. While in the early decades of the twentieth century public concerts of samba were out of question, the 1930s marked a slow shift toward toleration of the music and dance. Whereas previously lovers of samba 'had to sing it far from the police', gradually a case was being made for acceptance of ""black"" art forms. Amidst the still prevalent racism in Brazil, Gilberto Freyes presented his theory of 'racial democracy' in 1933 with the publishing of his book The Masters and the Slaves. In it, Freyes argued that the contribution of Afro Brazilians added to the rich cultural mix in which Brazilian tradition was embedded. It has been argued, in fact, that consumption of Afro Brazilian food and music were already prevalent among all classes in a discrete sense since the nineteenth century. However, now supported by the state, who saw in the lower classes an important element for the maintenance of the status quo, the samba culture increasingly gained space in the public sphere. The gatherings and parties held in the house of Tia Ciata have been said to serve as a microcosmic example of Brazilian society. Each room in her house had different musical genre performances, with in the front rooms -near to the street- respectable dances such as the polka, and toward the back samba and percussive, ""ethnic"" rhythms. Over time, as the samba gained momentum and respectability, it started to refine and develop in a multitude of exponents. Fusing African rhythms and European tonal scales into a more accessible style, the samba now became acceptable for the wealthier white classes. Stam, Tropical Multiculturalism, p. 80. Ibid, p. 79. Paul Austerlitz, 'Review of The Mystery of Samba',  URL , (1999), viewed 10 May 2005. Rowe and Schelling, Memory and Modernity, p. 129. Austerlitz, 'Review of The Mystery of Samba'. Central to the samba scene in the favelas were the batuqueiros, or bambas. These dancers performed the capoeira, a self-defence technique disguised as a dance, which had been developed by runaway slaves. In street carnivals, where performers paraded through the streets, the bambas were singled out for their particular talent for improvised dancing. As the carnival was becoming bigger, it became also more organised. So-called Escolas de Samba (samba schools) started to predominate the street carnival, now participating in officially organised competitions for monetary prizes and social recognition. Thus, gradually, the samba culture came to serve as a means for increasing respectability and social mobility, with the samba schools as benefactors of neighbourhoods, bringing education and increasing unity in Rio de Janeiro. Rowe and Schelling, Memory and Modernity, p. 129. Ibid, p. 136. W.W. Norton, 'Samba: Mass Culture from the Bottom Up',  URL  (2002), viewed 10 May 2005. Whether empowering the black lower classes, or simply reducing their contribution to 'the picturesque and the folkloric', sure is that samba contributed to Brazilian nation-building as it transcended the regional differences and immensely varying dialects. Over time, the samba thus became a national symbol. The production of Brazilian films brought a major contribution in the transformation of samba into a mass culture, and thus to some extent in the emancipation of black Brazilians. With the coming of sound in films, the most popular film genre became, as had been the case in Argentina, a light story incorporating popular songs. The Brazilian version of such films was the chanchada, which, although never entirely unambiguous, has been described in its widest sense as 'the generic name given to all the comedies and musical comedies, with popular pretensions'. As could be expected, Rio de Janeiro emerged as the centre of Brazilian cinema in the 1930s, closely linked to the samba schools and the carnivals, in the sense that chanchada films embraced similar values and were often released around the time of carnival. In films such as Alô Alô Brasil (1935), featuring Brazilian film star Carmen Miranda, and Alô Alô Carnaval (1936), featuring her sister Aurora, blacks played minor roles as musicians in the songs. The presence of blacks in chanchadas, however marginal, were rather significant as they meant the open incorporation of ""black culture"" in Brazilian national culture. Stam, Tropical Multiculturalism, p. 80. Norton, 'Samba: Mass Culture from the Bottom Up'. Rowe and Schelling, Memory and Modernity, p. 131. King, Magical Reels, p. 55. Ibid, p. 55. Stam, Tropical Multiculturalism, p. 83. Ibid, p. 81. The extent to which the Vargas government influenced the incorporation of samba culture is blurred. Although Vargas showed some minor active support for the movement by founding the National Institute of Educational Film (INCE) in 1937, which was to serve national integration, this institute was actually short-lived, while the number of long feature films decreased toward the end of the decade. Moreover, the culture of the favelas brought with it the 'dialectic of malandragem', which was a concept of using the established order for illegal interests and to avoid 'the punishment of work'. Malandragem attempted to function alongside the order of society, thus obscuring the brisk line between order and disorder. It is therefore likely that these cultural shifts were rather a wave that accompanied the emancipation of the former slave classes than a conscious attempt at nation-building for the Estado Novo. King, Magical Reels, p. 56. Rowe and Schelling, Memory and Modernity, p. 128. Traditional high culture in Brazil's thirties was preoccupied with another struggle, that against the right-wing dictatorship. Much of the intelligentsia and artistic scene, united in the Popular Front, chose the direction of regionalism. Upper class painter Tarsila do Amaral, after having explored aspects in Brazilian nativism in her work, now started to focus on representing traditional every-day scenes of the countryside. According to Gilberto Freyes, the 'soul of Brazil' was to be sought in the rural north, which was struck by poverty and where many sugar plantations were situated. The regionalist movement, however, incorporated a wide range of ideologies opposed to the Vargas regime. Raquel de Queirós, in 1930 published Back in '15, a novel on the plight of the northern region during the famine period of 1915. In 1937 she was imprisoned for her radical Trotskyist political outlook. Regionalism thus often served to point class inequalities in a radical leftist outlook of Brazilian society. A less covered ideological viewpoint is provided in Patrícia Galvão's 'proletarian novel' Industrial Park (1933), where she focuses on the injustice of the Brazilian system. Elements of nationalism are obviously subjugated for the purpose of class struggle, as in the following passage. Gonzales and Treece, The Gathering of Voices, p. 152. Carol Damian and Christina Mehrtens, 'Tarsila and the 1920s', in Robert M. Levine and John J. Crocitti (eds. ), The Brazil Reader. History, Culture, Politics, (London, 1999) p. 315. Martin, Journeys through the Labyrinth, p. 66. Ibid, p. 69. Alfredo tries to like the simple and poorly prepared food. He feels happy. He doesn't find Brazil abhorrent, as before. He doesn't need to drown his individualistic irritability in any picturesque scenes, neither in the ovens of the Sahara nor in theglacial Arctic Ocean. He wants them to leave him in Braz. Eating that revolutionary food. Without longing for Cairo hotels or French wines.Galvão, 'Where They about Rosa Luxemburg', in The Brazil Reader, p. 169. The way in which this passage points out any Brazilian national identity is clearly negative; it argues in favour of a 'simple', social Brazil instead of an internationally incorporated, capitalist nation. Elements of Brazilian grassroots national identity are ironically referred to as expressions of a repressed class:  All Hail! All Hail! This samba's Going to land in jail.Ibid, in The Brazil Reader, p. 166. Other poetry, although not as explicitly communist as Galvão (who, for this reason, spent four years in prison as a political prisoner), focussed on the rough and oppressive facets of the regime through from a national perspective. The liberal Mário de Andrade in his poem 'The Wagon of Misery' ridicules the regime with deliberate spelling mistakes. Bump! The Wagon of Misery' s here From the Eyetalian carnival! Old Ma Misery is dressed in honour The colour of the brass of our age Gathering spittle behind her. The colonil the generul the kaptin The pure the heroic the well-meaning Childrin of the Brazilyun factory Colombus's people are swinging. Gonzales and Treece, The Gathering of Voices, p. 157. Brazilian literature and poetry of the modernist movement revealed thus revealed a highly politicised, polemic style, much embedded in the opposition of the dictatorship and united through this opposition rather than through a shared outlook of Brazilian nationalism proper. Throughout the thirties, Brazil remained still very much in search of a form of national cohesion. Martin, Journeys through the Labyrinth, p. 68. Mexico: 'A dark Indian, grateful to the party'In Mexico, the revolution proved to be the 'defining event', shaping the nation for the remainder of the century. For many, the success of the 1910s revolution was the culmination of revolutionary nationalism: the yoke of a century of oppression had now been thrown off. In an attempt to create a ""new man"" and ""new woman"", the new government turned to art, popular culture and education to create a new sense of nationhood for the Mexicans. The new Mexican nation now came to incorporate and celebrate a new group of inhabitants that had been previously been seen as passive and inferior: the Mexican Indian. The art of the 1920s and 1930s is thus marked by a fresh interest in Aztec art and architecture, and a celebration of the 'Indian race' can be found in nationalistic essays. Gilbert M. Joseph and Timothy J. Henderson (eds. ), The Mexico Reader. History, Culture, Politics, (Durham, London, 2002), p. 333. Alan Knight, 'Popular Culture and the Revolutionary State in Mexico, 1910-1940', Hispanic American Historical Review, 74:3 (1994), p. 395. Ibid, p. 395. Alexander S. Dawson, 'From Models for the Nation to Model Citizens: Indigenismo and the 'Revindication' of the Mexican Indian, 1920-1940', Journal of Latin American Studies, 30 (1998), p. 285. By the thirties, a new political machine, the National Revolutionary Party, had been put in place and the work of myth-making of Mexican heroes of the revolution started. This period has been marked the post-revolutionary period, as legitimising the state, as well as instilling a paradoxical blend of conservative revolutionary sentiments in Mexicans. This new ideological stream, indigenismo, was reflected in the mural paintings of Mexico's most prominent modern artists, in the work of intellectuals and in the number of epic novels about the revolution that were published in the twenties and thirties. It has, however, been held that for the government the aim was not 'to 'indigenize' Mexico, but to 'Mexicanize' the Indian'. Knight, 'Popular Culture and the Revolutionary State', p. 393. Martin, Journeys through the Labyrinth, p. 45. Rowe and Schelling, Memory and Modernity, p. 184. Indiginismo involved the reinterpretation of both the Indian and the nation. It involved, on the one hand, the creation of the myth of the radically social and progressive new Mexican nation, and on the other hand, a glorification of the Mexican Indian and their past. By the 1930s, however, the emphasis was shifted from race to culture. An interesting example can be found in the issue of bank notes. The depiction of a 'native woman, wearing traditional costume, standing before the Aztec calendar stone' was never circulated, and instead the images of Zaragoza and Madero were preferred. Now, notions of 'literacy, nationalism, notions of citizenship, sobriety, hygiene, and hard work' became part of the ideological mores; the Indians were believed to possess those qualities to an exceptional extent. They were, in fact, 'rational political actors with modern sensibilities'. To fit this singular representation, a new kind of Indian peasant emerged in literature: Dawson, 'Indigenismo and the 'Revindication' of the Mexican Indian', p. 282. Ibid, p. 285. Ibid, p. 294. Joseph M. Galloy, 'Symbols of Identity and Nationalism in Mexican and Central-American Currency',  URL  , (2000), viewed 11 May 2005. Knight, 'Popular Culture and the Revolutionary State', p. 395. Dawson, 'Indigenismo and the 'Revindication' of the Mexican Indian', p. 279. A dark Indian dressed in white, who wants to learn to read and write in Spanish and is grateful to the party; his children are healthy and can choose whether to stay in the rancho or migrate to the new life in the cities. Martin, Journeys through the Labyrinth, p. 46. Another aspect of this indigenismo were the rural ballads, corridos. They are transformed to be used on mass scale, into three-minute songs to be played on the radio Agustín Lara became the icon of this typically Mexican musical exponent. Often in appraisal of revolutionary heroes, corridos with their simple rhyme were easy to remember and thus accessible to considerable percentage of illiterate Mexicans. The following is an example of a corrido describing the death of Francisco Villa. Rowe and Schelling, Memory and Modernity, p. 100. Poor Pancho Villa! His destiny was very sad; To die in an ambush And in the middle of the way. He went, leaving Parral handling his horse, the brave general author of ""La Cucaracha"" [the cockroach] [own translation] Armando de Maria y Campos, La Revolucion Mexicana a Traves de los Corridos Populares, (Mexico City 1962), p. 245. Yet another way in which the revolutionaries were eternalised and epitomised as heroes of the Mexican nation was through the paintings of Diego Rivera, Jose Orozco and David Alfaro Siqueiros. These modern artists - 'Los Tres Grandes'- had started in the twenties to experiment with mural painting, a style which they thereafter used effectively for the depiction of Mexico's indigenous history, scenes of the revolution and of the modern, industrialised, progressive state. Supported and often commissioned by the government, these painters fulfilled 'the visual component of [the] need to create that Mexican citizen necessary for the survival of the post-Revolutionary state'. Martin, Journeys through the Labyrinth, p. 46. Anthony W. Lee, 'Mural Painting and Social Revolution in Mexico: 1920-1940, Art of the New Order - Review',  URL , (1999) viewed 11 May 2005. Ibid. In terms of literature, the 1930s saw a continuation of the style employed in the twenties. Epic works with 'the man on horseback', the archetypical revolutionary also depicted in the mural paintings of the post-revolutionaries. Where Guzmán had in 1926 recounted his memories in the revolutionary army of Villa with The Eagle and the Serpent, Gregorio López y Fuentes in 1932 created an epic hero out of Zapata. His novel The Land provided the sense of heroic Mexican leadership that legitimised the Mexican revolution. Martin, Journeys through the Labyrinth, p. 38. It is passed on in whispers, in huddles, by men at the plough and women at the well. He has been seen. So it's true he's not dead... 'Do you know who saw him? Old Albina. [...] This is what she told me: ""[...] I recognised him straight off. Hat pushed back, like always, them same trousers with the buttons, and that moustache you could tell a mile away. [...]"" ' Ibid, p. 45. There were, of course, also many writers opposed to this mythical image. In fact, the Mexican revolution brought no generation of poets of any significance; most of them were to be sought with the defeated reactionaries. Gómez Marín, who lived in London at the time wrote in a letter: 'From here, Mexico is something dark and bloody'. Moreover, the corrido, typical Mexican icon, could naturally also be used for other purposes than the glorification of the revolution. The following extract comes from a corrido praising the bravery of a christian Cristero rebellion. Gonzales and Treece, The Gathering of Voices, p. 29. Franco, Critical Passions, p. 456. Then the general said: ""I am prepared to grant you a pardon If you will tell me Where I might find the local priest."" Valentín promptly answered him: ""That I cannot say; I'd rather you kill me Than give up a friend.""... Before they shot him, Before he went up the hill, Valentín cried: ""O mother of Guadalupe! For your religion they will kill me. ""...Anonynous, 'The Ballad of Valentín of the Sierra', in Gilbert M. Joseph and Timothy J. Henderson (eds. ), The Mexico Reader. History, Culture, Politics, (Durham, London, 2002), p. 420. Arguably, illiteracy rates stood in the way of effective nation-building through literature. Some have thus asserted that 'it [was] film, not writing, which create[d] [...] the 'imagined community' of the nation'. Indeed, when the first film with sound was made in 1931, it was sponsored by state funds. Such films, which like in the case of Argentina and Brazil, were capable of reaching as mass audience, dealt with popular themes such as destitute prostitutes (which, of course, fuelled the macho discourse of the frail woman in need of protection) and went accompanied by popular tunes, most notably those of Agustín Lara's soundtracks. The state was quite involved in the ideological message such films conveyed. Thus when the Soviet filmmaker Eisenstein, visiting Mexico, planned to make a film on the history of Mexico, the government withdrew their subsidies for fear that such a film might cause an outrage amongst the oligarchic hacienda owners. King, Magical Reels, p. 99. Ibid, p. 42. Ibid, p. 42. Ibid, p. 44. In the aftermath of the revolutionary decade (1910-1920) in Mexico, the new authorities then energetically set about constructing the archetype of a new ""ideal Mexican"". This was based upon a renewed enthusiasm about the Indian, who was henceforth to be considered as a part of the new Mexican state. Indigenismo and, from the late 1920s into the thirties, revolutionary myth making played a big role in such representations. Nationalism was hence instilled through popular means: mural painting, the corrido ballad, film, and to some extent, epic literature. By actively interfering and negotiating these art forms, the state sought to channel the multitude of opinions into a glorification of the new Mexicanidad. Conclusion Nationhood is a fluid concept. It can be manipulated to incorporate whatever ideological ideas, and exclude or obscure what one does not find appropriate to it. Argentina and Brazil in the 1930s saw the beginning of a great transformation of nationhood, while Mexico had already begun that project directly after the Mexican revolution. Although often the state attempted to control cultural representations of its nationalism, intellectuals and poorer minorities did not let this happen easily. Borges refused to accept a simple, patriotic nationalism, thereby brilliantly revealing the paradoxes underlying such a concept; in Vargas' Brazil, the government was in a constant state of war with its poets and authors on the left, while former slaves of the favelas proved to be able to provide Brazil's most uniting and lasting national identity. Perhaps the Mexican state can be said to have succeeded best at imposing an official picture of nationalism. Their corridos and murals invoked a revolutionary-ideological Mexicanidad, party-style; and it would only be by the 1990s that Mexican natives started to wage war on that identity. Nevertheless, the successes in Mexico also knew their limits. The Catholic religion proved impossible to curb, while student protests in the 1970s already revealed the violent antagonism underlying the picture of Mexico that the government offered its people. Representations of national identity, then, are inherently linked with political ideology and power. Brazilian authors with tendencies toward communism provide the example; they employ a highly politicised image of the 'true Brazilian' proletariat. In Argentina, such representations resulted in two opposing views, the gauchoesque patriot, modelled after Martin Fierro stood in contrast to the liberal modernist 'rational' Argentine inspired by Sarmiento's Facundo. When tango, the voice of the minorities, and Borges came into the equation, a mono-cultured Argentine ideal became more complicated and troublesome. Similar in each of the three countries discussed, however, are the influences of 'lower' cultural forms. Tango, initially despised by all respectable layers of society, became the epitome of Argentinidad and proud export product after its success in the Parisian salons, while the samba of the Brazilian blacks became a national virtue. In Mexico, the corrido and new popular film both served the ideal of praising the spiritual idealism that belonged inherently to the Mexican man and woman, as opposed to the violent pragmatism of Northern American men, and the dehumanised star beauty of the Hollywood actress. Nationalism in all countries discussed, became increasingly represented in popular ways, reaching out to a mass audience. The constructors of such nationalistic forms sought to reach out to an unprecedented number of people, instilling upon them these new forms of nationalism. In Brazil, chanchadas united the country despite its immense differences in dialect, while in Mexico the party did their best to involve nationalism in every possible cultural and educational aspect. In Argentina it culminated in the decade of Peron's popular dictatorship. It can thus be said, that across these nations, the 1930s transformed nationalism into a popular and far-reaching construct.","At the eve of the Versailles conference, promoters of the Polish cause immediately realised the international precariousness of their aims to forge the rebirth of an independent Polish state. Moving with caution, Piłsudski, Head of State and Supreme Commander of the Polish troops, in December 1918 even went as far as addressing his archrival Dmowski of the Polish National Committee in Paris as 'My Dear Roman' in an attempt to reconcile the Polish feuding factions. From the outset, it appeared that Poland's independence 'would depend almost exclusively on the endeavours of the Poles themselves', however, soon after the official acknowledgement of Poland's Second Republic, the political struggle grew bitter. It was only the realisation of Soviet hostility toward their new western neighbour that forced the antagonistic political forces to work together in a somewhat efficient war cabinet, though even then with the greatest difficulty and resentment. At nearly the same time that peace was concluded with the Russians in the Treaty of Riga, the endeks, as Dmowski's National Democrats popularly came to be know, managed to pass an overly liberal constitution which left virtually no freedom to the president of the new Polish Republic and much procedural liberty with the endek-dominated parliament. It was immediately recognised as 'a monument to anti- Piłsudskiism', and left Piłsudski bereft of a meaningful role within Polish politics, despite his nationwide fame as Poland's liberator gained through his successful military operations against the Soviet troops in the war of 1919-21. The seed was thus planted for Piłsudski's initial decision to overthrow the government in 1926 by the National Democrats' uncompromising political attitude and wish for political domination. While the direct cause for Piłsudski's coup d'etat can be said to have sprung from his 'personal ambition and intolerance', and obsession 'with the idea of Poland's unity and greatness', it must be added that Piłsudski's frustrations were partly validated by the sheer incapability of the multitude of Polish governments quickly succeeding each other of dealing with a great number of urgent and pressing matters. Thus, the reasons for the collapse of parliamentary democracy in Poland and the subsequent usurpation of power by the Nazi aggressors thereafter can be brought back to a number of reasons, of which Poland's 'own fundamental weaknesses, the instability of its institutions, and its irresponsible governments, as well as to the active and passive faults of the Great Powers' are the most important and general ones. Antony Polonsky, Politics in Independent Poland, 1921-1939. The Crisis of Constitutional Government (Oxford, 1972), p. 97. Peter D. Stachura, 'The Battle of Warsaw, August 1920, and the Development of the Second Polish Republic' in Stachura, Peter D. (ed. ), Poland between the Wars, 1918-1939 (New York, 1998), p. 43. Polonsky, Politics in Independent Poland, p. 99. R.J. Crampton, Eastern Europe in the Twentieth Century - And After (London/New York, 2005), p. 42. Hugh Seton-Watson, The East European Revolution (London, 1950), p. 47. Alexandra Piłsudska, Pilsudski. A Biography by His Wife (New York, 1971), p. 327. Joseph Rothschild, Return to Diversity. A Political History of East Central Europe Since World War II (New York, 1993), p. 8. The list of problems and grievances that the young Polish state was thrust into from the early twenties was indeed impressive. While basic issues, such as the development of a political system and the role of minorities in the new state still had to be addressed, Poland was first confronted with 'a veritable fight for survival'. Peter D. Stachura, 'The Second Republic in Historiographical Outline' in Stachura, Peter D. (ed. ), Poland between the Wars, 1918-1939 (New York, 1998), p. 2. National unity was far to be sought. The dominion claimed by Poland included vast numbers of minorities with loyalties elsewhere. Of twenty-seven million inhabitants not even 70 per cent were Polish, while 14.3 per cent were of other Slavic decent and 7.8 per cent of the population were Jewish, often not speaking Polish and many opposed to the establishment of a Polish nation. The same went for the small German minority of 3.9 per cent, whom, by conducing themselves 'in manner that was at once provocative and calculated to give notice that it regarded its status in Poland, and Poland herself, as temporary', outraged the Polish majority. A long struggle between Poland and her minorities persisted throughout the interbellum period. While it has been noted that '[n]othing the Poles did was ever right, as far as [the Jews] were concerned', there was little effort on the government's side to draw the Jews into appreciated and meaningful participation of national life. On the contrary, when Poland's first president, Gabriel Narutowicz, came to be elected with support of the bloc of minority parties, he was murdered within days after his election by the Sejm. Despite the endeks' initial condemnation of the murderer, whom saw the election of a minority-dependent president as a 'symbol of shame', he was later celebrated as a Polish nationalistic hero. Generally, the antagonistic sense existed, in which when the minorities, with the little representation they had in parliament, exercised their rightful power, it was seen as invalid and unjustified by a wide range of centre-right and rightist Polonocentric parties, thus blocking the way for meaningful political participation of the minorities. Crampton, Eastern Europe, p. 41. Peter D. Stachura, 'National Identity and the Ethnic Minorities in Early Inter-War Poland' in Stachura, Peter D. (ed. ), Poland between the Wars, 1918-1939 (New York, 1998), p. 71. Ibid, p. 77. Hans Roos, A History of Modern Poland. From the Foundation of the State in the First World War to the Present Day (London, 1966), p. 103. Ibid, p. 100. While, to some degree, all political factions agreed with Piłsudski's insistence on 'complete independence for Poland, [...] sceptical of the value of co-operation with any Russians', little was done to involve the Ukranian and Belorussian minorities in decision-making of their lot, even though Poland's border with Russia came to run 'far to the east of that originally suggested by Lord Curzon' during the Versailles conference. Piłsudski's ideal of a federal model incorporating largely autonomous minority factions were soon defeated; instead there existed 'acrimony and distrust, a sort of internal war'. Moreover, while the minorities did little to contribute to a 'modus vivendi', a sense of community spirit, which might have brought more peaceful social relations, many Poles themselves were unrealistic and intolerant about the formal and informal shape Polish society must take. This had not only racial, but also religious, economic and political reasons. Pre-war conditions of many Poles created a resentful attitude toward minorities and this made it hard to deal with the minorities in a fair way. Seton-Watson, The East European Revolution, p. 37. Crampton, Eastern Europe in the Twentieth Century, p. 39. Stachura, 'National Identity and the Ethnic Minorities', p. 78. Ibid, p. 78. Polonsky, Politics in Independent Poland, p. 100. Politically and economically antagonistic views were from the onset a mutually enforcing problem, which caused great obstacles for the creation of a healthily functioning government. Additionally, the multitude of Polish independence parties which had formerly existed, maintained their air of secrecy and 'conspiratorial character' even when government bodies had been created, which had disastrous results in the conflict between Dmowski and Piłsudski, which took its toll on Poland's national interest. The passing of an overly liberal constitution based on that of France at the end of the Polish-Russian war in 1921 resulted in extreme factionism; by 1926, there existed more than 90 parties, of which around 30 parties were at times represented in the Sejm. As parliament held most power within the political structure, governments found it very difficult to pass reforms. The economy, in particular, proved a disastrously antagonising force. Extreme inflation, such as in Warsaw, where food prices with a unitary value of ""100"" in January 1921 rose to ""1298.2"" in December 1922, brought several governments to fall, while rising unemployment caused the Polish Socialist Party, the PPS, to organise strikes throughout 1924 and 1925. Dmowski and his endecja became increasingly entangled in their bid for power. While land reform was the way out for a country too poor to introduce any significantly contributing industrialisation, the National Democrats, with many landowning members, were reluctant to land redistribution of any real importance, which in turn caused the anomosity of the peasant parties. This, in all, caused land reforms to be rather ineffective and certainly insufficient. A sense of lingering problems thus left a general discontent throughout all layers of society. Ibid, p. 99. Stachura, 'The Battle of Warsaw', p. 52. Stachura, 'Historiographical Outline', p. 5. Polonsky, Politics in Independent Poland, 1921-1939, p. 105. Roos, A History of Modern Poland, p. 109. Crampton, Eastern Europe, p. 45-6. Roos, A History of Modern Poland, p. 105-7. Additional to internal problems, came the issue of Poland's national security. Weimar Germany continued a revisionist policy of regaining its former Prussian areas and southeastern Silesia from Poland. This caused a particular as Poland had no natural defensive barrier and the western powers, although they ratified Poland's borders to the east,  took little affirmative action against Germany's revisionist campaign, which lent its support to Germans in Poznania and Pomerania in a number of ways, thus destabilising the nation through what was essentially a very small percentage of its population. Rothschild, Return to Diversity, p. 9. Crampton, Eastern Europe, p. 40. Stachura, 'National Identity and the Ethnic Minorities', p. 71. Another form of support to the new independent state which was pressingly lacking was financial aid. Having half of its roads, railways and public buildings destroyed during the Great War, and a continuing expenditure of over 50 per cent of the state budget on defense and the railway system, the government lacked the essential funds to modernise industry and thus create employment. The absence of foreign loans and apparent disinterest in creating political stability from the western powers might have done much to help Poland establish a parliamentary democratic routine. Thus, when the British foreign office sneeringly contradicted the Soviet Union's allegation of involvement in Piłsudksi's coup d'etat in a telegram to its ambassador in Moscow, arguing that '[the Russians] are of less consequence to us than they suppose', this revealed much of its indifferent attitude towards Polish politics as well. Roos, A History of Modern Poland, p. 107. Ibid, p. 105. Polonsky, Politics in Independent Poland, p. 515. The exigent situation that Poland was in did not go unnoticed. Piłsudksi, embittered by the course of events since 1921, grew increasingly convinced that Polish 'parliamentarianism had [...] opened the door to general corruption'. But not just Piłsudksi was displeased with the situation. Much of the PPS-cadre, frustrated with years of ineffective government and endek domination, now turned their hopes to Piłsudksi, which prompted PPS theorist Feliks Perl to foreshadowingly argue in 1924: 'He cannot be made into a left-wing Mussolini'. PPS leader Daszyński argued that political immaturity of the state was the main cause for political feuding, but, in the beginning of 1926, also urged for immediate constitutional reforms empowering the presidential post in order to contain parliamentary quarreling. Roos, A History of Modern Poland, p. 110. Ibid, p. 111. Polonsky, Politics in Independent Poland, p. 147. Not just the opposition of the left had their second thoughts about the functioning of the state. In December 1925, Dmowski wrote 'if we could create even half the organization like the Fascists... I would willingly agree to a dictatorship in Poland'. Discontents with the democratic procedure and democracy as such proved different degrees of the same slippery slope, as it soon turned out. University students radicalised under the prospect of unemployment and resented the apparent greater success of their Jewish peers; they formed the leadership of fascist movements that were to increase their power-base throughout the 1930s. Antony Polonsky, Politics in Independent Poland, 1921-1939. The Crisis of Constitutional Government (Oxford, 1972), p. 149. Seton-Watson, The East European Revolution, p. 44. Overwhelmed by problems of such dimensions, historians have argued, 'some action [...] seems in retrospect to have been almost inevitable' within the Polish political landscape. This seems, however, an unlikely viewpoint and it is worthwhile considering what other course of events politics might have taken. An internal conflict in Piłsudski's course of action appears quite clearly, as when in November 1925, when Piłsudski is requested to take over the rule of the country by factions of the army. He claimed he 'would never be a cause of trouble and dissention in the State'. It seems, from this, to have been rather a passionate, naturalistic sense of justice that drove him to carry out the coup d'etat than a positivistic, legal course of action. Already earlier in the twenties had Piłsudski shown this duality in his character. By declining a bid for the presidency, he proudly refused 'to be enclosed in a 'gilded cage'', while apparently seeing no value in attempting reforms through coalition in the Sejm. Rather than acknowledging his objections to National Democratic domination, he slid further back into the trappings of ""moral dictatorship"", hereby only united the body politic in their opposition against his rule. Thus, the peasant parties united in the Sejm to resist political persecution, while his own rule alienated moderate supporters of his ""parliamentary dictatorship"". While open confrontation with the initially supportive socialists became ever more discernible, Piłsudski still drew a Hobbesian legitimacy from the backing of the Sejm's members, which became, however, increasingly difficult to forge. When, in the end of 1929, he personally came to reopen the Sejm accompanied by a hundred armed officers, Daszyński refused to open the session 'under the threat of bayonets, guns and sabres', Piłsudski abstained from openly breaching the constitution by forcefully opening the session. The failure of Piłsudski's coup must thus been assessed within his own framework of legitimacy, which was to ""cleanse"" the parliamentary body politic. 'It is the basis of democracy, for which there is always place in Poland', he still argued in 1926. By late 1930, a number of oppositionist politicians were arrested, held captive and tortured. When eventually a number of politicians chose exile rather than imprisonment, Piłsudski was as far as ever from his ideal of a federalist republic. His cabinets governed for months 'simpy by repeatedly adjourning the Sejm'. Growing older, Piłsudski became an increasingly mistrustful statesman, believing in the last months before his death that 'the nation had reached real unity at last'. In reality, the new 1935 Constitution 'formally put an end to democracy', calling in a period known as the ""rule of the colonels"" which incorporated a mixture of nationalism and fascism. Polonsky, Politics in Independent Poland, p. 100. Piłsudska, Pilsudski, p. 329. Roos, A History of Modern Poland, p. 103. Seton-Watson, The East European Revolution, p. 32. Joshua Cohen, 'Protection for Obedience', MIT,  URL :P_SGvaU6aDYJ:web.mit.edu/polisci/research/cohen/protection_for_obediance.pdf+cohen+obedience+and+protection&hl=nl&ie=UTF-8 (30 October 2005) Roos, A History of Modern Poland, p. 119. Polonsky, Politics in Independent Poland, p. 189. Crampton, Eastern Europe, pp 50-1. Roos, A History of Modern Poland, p. 120. Ibid, p. 114. Piłsudska, Pilsudski, p. 341. Seton-Watson, The East European Revolution, p. 47. Crampton, Eastern Europe, p. 53. By carrying out a coup d'etat, Piłsudski essentially interfered forcefully with the democratic institutions. While initially hesitant, the sanacja's breeches of the constitution and civil rights were increasingly necessary for Piłsudski to maintain power. This, however, had detrimental effects on the democratic institutions and increasingly alienated many parts of society from his government. Being insufficiently successful in curbing further economic regression into the 1930s and failing to integrate increasingly militant groups of minorities, Piłsudski's failures played in the hand of the right, which made a comeback in radical form, while his sanacja leadership was increasingly chosen from a clique of military cronies. Piłsudski's coup d'etat of 1926 proved, thus, not only to be ineffective on theoretical grounds, but also on practical grounds, eventually breaking with Piłsudski's personal visions on good statesmanship. Polonsky, Politics in Independent Poland, p. 280. Crampton, Eastern Europe, p. 51.",True
17,"At the eve of the Versailles conference, promoters of the Polish cause immediately realised the international precariousness of their aims to forge the rebirth of an independent Polish state. Moving with caution, Piłsudski, Head of State and Supreme Commander of the Polish troops, in December 1918 even went as far as addressing his archrival Dmowski of the Polish National Committee in Paris as 'My Dear Roman' in an attempt to reconcile the Polish feuding factions. From the outset, it appeared that Poland's independence 'would depend almost exclusively on the endeavours of the Poles themselves', however, soon after the official acknowledgement of Poland's Second Republic, the political struggle grew bitter. It was only the realisation of Soviet hostility toward their new western neighbour that forced the antagonistic political forces to work together in a somewhat efficient war cabinet, though even then with the greatest difficulty and resentment. At nearly the same time that peace was concluded with the Russians in the Treaty of Riga, the endeks, as Dmowski's National Democrats popularly came to be know, managed to pass an overly liberal constitution which left virtually no freedom to the president of the new Polish Republic and much procedural liberty with the endek-dominated parliament. It was immediately recognised as 'a monument to anti- Piłsudskiism', and left Piłsudski bereft of a meaningful role within Polish politics, despite his nationwide fame as Poland's liberator gained through his successful military operations against the Soviet troops in the war of 1919-21. The seed was thus planted for Piłsudski's initial decision to overthrow the government in 1926 by the National Democrats' uncompromising political attitude and wish for political domination. While the direct cause for Piłsudski's coup d'etat can be said to have sprung from his 'personal ambition and intolerance', and obsession 'with the idea of Poland's unity and greatness', it must be added that Piłsudski's frustrations were partly validated by the sheer incapability of the multitude of Polish governments quickly succeeding each other of dealing with a great number of urgent and pressing matters. Thus, the reasons for the collapse of parliamentary democracy in Poland and the subsequent usurpation of power by the Nazi aggressors thereafter can be brought back to a number of reasons, of which Poland's 'own fundamental weaknesses, the instability of its institutions, and its irresponsible governments, as well as to the active and passive faults of the Great Powers' are the most important and general ones. Antony Polonsky, Politics in Independent Poland, 1921-1939. The Crisis of Constitutional Government (Oxford, 1972), p. 97. Peter D. Stachura, 'The Battle of Warsaw, August 1920, and the Development of the Second Polish Republic' in Stachura, Peter D. (ed. ), Poland between the Wars, 1918-1939 (New York, 1998), p. 43. Polonsky, Politics in Independent Poland, p. 99. R.J. Crampton, Eastern Europe in the Twentieth Century - And After (London/New York, 2005), p. 42. Hugh Seton-Watson, The East European Revolution (London, 1950), p. 47. Alexandra Piłsudska, Pilsudski. A Biography by His Wife (New York, 1971), p. 327. Joseph Rothschild, Return to Diversity. A Political History of East Central Europe Since World War II (New York, 1993), p. 8. The list of problems and grievances that the young Polish state was thrust into from the early twenties was indeed impressive. While basic issues, such as the development of a political system and the role of minorities in the new state still had to be addressed, Poland was first confronted with 'a veritable fight for survival'. Peter D. Stachura, 'The Second Republic in Historiographical Outline' in Stachura, Peter D. (ed. ), Poland between the Wars, 1918-1939 (New York, 1998), p. 2. National unity was far to be sought. The dominion claimed by Poland included vast numbers of minorities with loyalties elsewhere. Of twenty-seven million inhabitants not even 70 per cent were Polish, while 14.3 per cent were of other Slavic decent and 7.8 per cent of the population were Jewish, often not speaking Polish and many opposed to the establishment of a Polish nation. The same went for the small German minority of 3.9 per cent, whom, by conducing themselves 'in manner that was at once provocative and calculated to give notice that it regarded its status in Poland, and Poland herself, as temporary', outraged the Polish majority. A long struggle between Poland and her minorities persisted throughout the interbellum period. While it has been noted that '[n]othing the Poles did was ever right, as far as [the Jews] were concerned', there was little effort on the government's side to draw the Jews into appreciated and meaningful participation of national life. On the contrary, when Poland's first president, Gabriel Narutowicz, came to be elected with support of the bloc of minority parties, he was murdered within days after his election by the Sejm. Despite the endeks' initial condemnation of the murderer, whom saw the election of a minority-dependent president as a 'symbol of shame', he was later celebrated as a Polish nationalistic hero. Generally, the antagonistic sense existed, in which when the minorities, with the little representation they had in parliament, exercised their rightful power, it was seen as invalid and unjustified by a wide range of centre-right and rightist Polonocentric parties, thus blocking the way for meaningful political participation of the minorities. Crampton, Eastern Europe, p. 41. Peter D. Stachura, 'National Identity and the Ethnic Minorities in Early Inter-War Poland' in Stachura, Peter D. (ed. ), Poland between the Wars, 1918-1939 (New York, 1998), p. 71. Ibid, p. 77. Hans Roos, A History of Modern Poland. From the Foundation of the State in the First World War to the Present Day (London, 1966), p. 103. Ibid, p. 100. While, to some degree, all political factions agreed with Piłsudski's insistence on 'complete independence for Poland, [...] sceptical of the value of co-operation with any Russians', little was done to involve the Ukranian and Belorussian minorities in decision-making of their lot, even though Poland's border with Russia came to run 'far to the east of that originally suggested by Lord Curzon' during the Versailles conference. Piłsudski's ideal of a federal model incorporating largely autonomous minority factions were soon defeated; instead there existed 'acrimony and distrust, a sort of internal war'. Moreover, while the minorities did little to contribute to a 'modus vivendi', a sense of community spirit, which might have brought more peaceful social relations, many Poles themselves were unrealistic and intolerant about the formal and informal shape Polish society must take. This had not only racial, but also religious, economic and political reasons. Pre-war conditions of many Poles created a resentful attitude toward minorities and this made it hard to deal with the minorities in a fair way. Seton-Watson, The East European Revolution, p. 37. Crampton, Eastern Europe in the Twentieth Century, p. 39. Stachura, 'National Identity and the Ethnic Minorities', p. 78. Ibid, p. 78. Polonsky, Politics in Independent Poland, p. 100. Politically and economically antagonistic views were from the onset a mutually enforcing problem, which caused great obstacles for the creation of a healthily functioning government. Additionally, the multitude of Polish independence parties which had formerly existed, maintained their air of secrecy and 'conspiratorial character' even when government bodies had been created, which had disastrous results in the conflict between Dmowski and Piłsudski, which took its toll on Poland's national interest. The passing of an overly liberal constitution based on that of France at the end of the Polish-Russian war in 1921 resulted in extreme factionism; by 1926, there existed more than 90 parties, of which around 30 parties were at times represented in the Sejm. As parliament held most power within the political structure, governments found it very difficult to pass reforms. The economy, in particular, proved a disastrously antagonising force. Extreme inflation, such as in Warsaw, where food prices with a unitary value of ""100"" in January 1921 rose to ""1298.2"" in December 1922, brought several governments to fall, while rising unemployment caused the Polish Socialist Party, the PPS, to organise strikes throughout 1924 and 1925. Dmowski and his endecja became increasingly entangled in their bid for power. While land reform was the way out for a country too poor to introduce any significantly contributing industrialisation, the National Democrats, with many landowning members, were reluctant to land redistribution of any real importance, which in turn caused the anomosity of the peasant parties. This, in all, caused land reforms to be rather ineffective and certainly insufficient. A sense of lingering problems thus left a general discontent throughout all layers of society. Ibid, p. 99. Stachura, 'The Battle of Warsaw', p. 52. Stachura, 'Historiographical Outline', p. 5. Polonsky, Politics in Independent Poland, 1921-1939, p. 105. Roos, A History of Modern Poland, p. 109. Crampton, Eastern Europe, p. 45-6. Roos, A History of Modern Poland, p. 105-7. Additional to internal problems, came the issue of Poland's national security. Weimar Germany continued a revisionist policy of regaining its former Prussian areas and southeastern Silesia from Poland. This caused a particular as Poland had no natural defensive barrier and the western powers, although they ratified Poland's borders to the east,  took little affirmative action against Germany's revisionist campaign, which lent its support to Germans in Poznania and Pomerania in a number of ways, thus destabilising the nation through what was essentially a very small percentage of its population. Rothschild, Return to Diversity, p. 9. Crampton, Eastern Europe, p. 40. Stachura, 'National Identity and the Ethnic Minorities', p. 71. Another form of support to the new independent state which was pressingly lacking was financial aid. Having half of its roads, railways and public buildings destroyed during the Great War, and a continuing expenditure of over 50 per cent of the state budget on defense and the railway system, the government lacked the essential funds to modernise industry and thus create employment. The absence of foreign loans and apparent disinterest in creating political stability from the western powers might have done much to help Poland establish a parliamentary democratic routine. Thus, when the British foreign office sneeringly contradicted the Soviet Union's allegation of involvement in Piłsudksi's coup d'etat in a telegram to its ambassador in Moscow, arguing that '[the Russians] are of less consequence to us than they suppose', this revealed much of its indifferent attitude towards Polish politics as well. Roos, A History of Modern Poland, p. 107. Ibid, p. 105. Polonsky, Politics in Independent Poland, p. 515. The exigent situation that Poland was in did not go unnoticed. Piłsudksi, embittered by the course of events since 1921, grew increasingly convinced that Polish 'parliamentarianism had [...] opened the door to general corruption'. But not just Piłsudksi was displeased with the situation. Much of the PPS-cadre, frustrated with years of ineffective government and endek domination, now turned their hopes to Piłsudksi, which prompted PPS theorist Feliks Perl to foreshadowingly argue in 1924: 'He cannot be made into a left-wing Mussolini'. PPS leader Daszyński argued that political immaturity of the state was the main cause for political feuding, but, in the beginning of 1926, also urged for immediate constitutional reforms empowering the presidential post in order to contain parliamentary quarreling. Roos, A History of Modern Poland, p. 110. Ibid, p. 111. Polonsky, Politics in Independent Poland, p. 147. Not just the opposition of the left had their second thoughts about the functioning of the state. In December 1925, Dmowski wrote 'if we could create even half the organization like the Fascists... I would willingly agree to a dictatorship in Poland'. Discontents with the democratic procedure and democracy as such proved different degrees of the same slippery slope, as it soon turned out. University students radicalised under the prospect of unemployment and resented the apparent greater success of their Jewish peers; they formed the leadership of fascist movements that were to increase their power-base throughout the 1930s. Antony Polonsky, Politics in Independent Poland, 1921-1939. The Crisis of Constitutional Government (Oxford, 1972), p. 149. Seton-Watson, The East European Revolution, p. 44. Overwhelmed by problems of such dimensions, historians have argued, 'some action [...] seems in retrospect to have been almost inevitable' within the Polish political landscape. This seems, however, an unlikely viewpoint and it is worthwhile considering what other course of events politics might have taken. An internal conflict in Piłsudski's course of action appears quite clearly, as when in November 1925, when Piłsudski is requested to take over the rule of the country by factions of the army. He claimed he 'would never be a cause of trouble and dissention in the State'. It seems, from this, to have been rather a passionate, naturalistic sense of justice that drove him to carry out the coup d'etat than a positivistic, legal course of action. Already earlier in the twenties had Piłsudski shown this duality in his character. By declining a bid for the presidency, he proudly refused 'to be enclosed in a 'gilded cage'', while apparently seeing no value in attempting reforms through coalition in the Sejm. Rather than acknowledging his objections to National Democratic domination, he slid further back into the trappings of ""moral dictatorship"", hereby only united the body politic in their opposition against his rule. Thus, the peasant parties united in the Sejm to resist political persecution, while his own rule alienated moderate supporters of his ""parliamentary dictatorship"". While open confrontation with the initially supportive socialists became ever more discernible, Piłsudski still drew a Hobbesian legitimacy from the backing of the Sejm's members, which became, however, increasingly difficult to forge. When, in the end of 1929, he personally came to reopen the Sejm accompanied by a hundred armed officers, Daszyński refused to open the session 'under the threat of bayonets, guns and sabres', Piłsudski abstained from openly breaching the constitution by forcefully opening the session. The failure of Piłsudski's coup must thus been assessed within his own framework of legitimacy, which was to ""cleanse"" the parliamentary body politic. 'It is the basis of democracy, for which there is always place in Poland', he still argued in 1926. By late 1930, a number of oppositionist politicians were arrested, held captive and tortured. When eventually a number of politicians chose exile rather than imprisonment, Piłsudski was as far as ever from his ideal of a federalist republic. His cabinets governed for months 'simpy by repeatedly adjourning the Sejm'. Growing older, Piłsudski became an increasingly mistrustful statesman, believing in the last months before his death that 'the nation had reached real unity at last'. In reality, the new 1935 Constitution 'formally put an end to democracy', calling in a period known as the ""rule of the colonels"" which incorporated a mixture of nationalism and fascism. Polonsky, Politics in Independent Poland, p. 100. Piłsudska, Pilsudski, p. 329. Roos, A History of Modern Poland, p. 103. Seton-Watson, The East European Revolution, p. 32. Joshua Cohen, 'Protection for Obedience', MIT,  URL :P_SGvaU6aDYJ:web.mit.edu/polisci/research/cohen/protection_for_obediance.pdf+cohen+obedience+and+protection&hl=nl&ie=UTF-8 (30 October 2005) Roos, A History of Modern Poland, p. 119. Polonsky, Politics in Independent Poland, p. 189. Crampton, Eastern Europe, pp 50-1. Roos, A History of Modern Poland, p. 120. Ibid, p. 114. Piłsudska, Pilsudski, p. 341. Seton-Watson, The East European Revolution, p. 47. Crampton, Eastern Europe, p. 53. By carrying out a coup d'etat, Piłsudski essentially interfered forcefully with the democratic institutions. While initially hesitant, the sanacja's breeches of the constitution and civil rights were increasingly necessary for Piłsudski to maintain power. This, however, had detrimental effects on the democratic institutions and increasingly alienated many parts of society from his government. Being insufficiently successful in curbing further economic regression into the 1930s and failing to integrate increasingly militant groups of minorities, Piłsudski's failures played in the hand of the right, which made a comeback in radical form, while his sanacja leadership was increasingly chosen from a clique of military cronies. Piłsudski's coup d'etat of 1926 proved, thus, not only to be ineffective on theoretical grounds, but also on practical grounds, eventually breaking with Piłsudski's personal visions on good statesmanship. Polonsky, Politics in Independent Poland, p. 280. Crampton, Eastern Europe, p. 51.","Rousseau has always been typified as an intellectual outsider; it was, however, only in the twentieth century that his socio-political theory came to be seen as a threat to the individual's liberty. Emphasis thereby is often laid on his concept of a 'general will', through which personal liberty is handed over to the collective. 'The citizens having but one interest', Rousseau argues, 'the people [have] but a single will'. And although Rousseau favours the ballot vote, he argues for the infallibility of its ruling: 'when the opinion contrary to my own prevails, it proves nothing more than that I made a mistake, and that what I took to be the general will was not'. From the point of view of Rousseau, most importantly protection should be rendered against the emergence of a minority rule, or 'particular will', rather than against the state, which, after all, represents the sovereignty of the lawgiver as it corresponds with the community's best interest, the general will. The question I will look at here is thus to what extent the critical assertion holds truth that 'the more safeguards he established against despotism, the more weapons he forged for tyranny', within Rousseau's system of the social contract. Iain Hampsher-Monk, 'Rousseau and totalitarianism: with hindsight?' in Robert Wokler (ed), Rousseau and Liberty (Manchester/New York, 1995), p. 268. Jean-Jacques Rousseau, The Social Contract and Discourses (London/New York, 1952), p. 87. Christopher Bertram, Routledge Philosophy Guidebook to Rousseau and The Social Contract (London, 2004), p. 120. Hampsher-Monk, 'Rousseau and totalitarianism: with hindsight? ', p. 271. Recent criticism has argued that the Cold War split has caused intellectuals to make an over-simplified distinction between 'individualistic liberalism and state collectivism', stating that Rousseau was on the totalitarian side of the argument. Rousseau, however, from the outset entertains an awareness of the problem of new forms of subjugation that hide behind a powerful state, and thus sets out to construct a model for a 'form of association [...] in which each, while uniting himself with all, may still obey himself alone, and remain free as before'. In this sense, Rousseau undertakes to find a form in which freedom can be exercised on a public, political level. The important distinction that has to be made here though, is the type of freedom at stake. Arguing that pre-modern, pre-social man exercised an individual sovereignty free from social obligation or commitment, knew a natural, brutal kind of liberty. When people come to regular interaction, they become interdependent, which goes accompanied by fierce passions. It is only within society that morality comes to exist, through virtue and pitié, a form of compassion which counters man's corruption by society. What is then needed is the construction of an association in which passions are tempered and virtue is encouraged. In this sense, envisaging a social contract is merely a theoretical exercise, to find a form in which such an exercise of sovereignty can be best expressed, in other words, 'Rousseau's purpose is to discover a rule which has moral validity'. Hampsher-Monk, 'Rousseau and totalitarianism: with hindsight? ', p. 268. Rousseau, The Social Contract, p. 12. Patrick Neal, Liberalism and Its Discontents (London, 1997), p. 61. Rousseau, The Social Contract, p. 5. Maurizio Viroli, Jean-Jacques Rousseau and the 'well-ordered society' (Cambridge, 1988), p. 132. Christopher Bertram, Routledge Philosophy Guidebook to Rousseau and The Social Contract (London, 2004), p. 19. Viroli, The 'well-ordered society', p. 121. Another point of view insists on the juxtaposition of democracy and liberalism, positioning Rousseau in a radically democratic framework. Certainly, this view depends greatly on the view one takes of liberalism. Liberalism can be taken at an individual level, stressing the removal of any constraint on human action (negative liberty), and placing theory in wider context, that of the body politic (positive liberty). Rousseau is quite explicit about what form of liberty we are to seek. 'Man is born free; and everywhere he is in chains', he argues, 'One thinks himself the master of others, and still remains a greater slave than they'. It is therefore clear that Rousseau seeks a wider answer to the question of liberty, one for which 'it is necessary to ensure that the conditions in which [people] live do not pose a threat to their survival'. This condition can be brought about by the realisation that it is the best interest of all citizens to care for other's interest and well-being. Here we find the distinction that Rousseau makes between a narrow form of personal interest and a wider, more coherent form of enlightened interest, being the best for the community, and thus, indirectly, each citizen's actual will, as it secures the best-considered proposal and enjoys general support of the citizenry. Subordination to the law subsequently only becomes an exercise of one's liberty. Carol Blum, Rousseau and the Republic of Virtue. The Language of Politics in the French Revolution (Ithaca/London, 1986), p. 32. Hampsher-Monk, 'Rousseau and totalitarianism: with hindsight? ', p. 271. Rousseau, The Social Contract, p. 3. Viroli, The 'well-ordered society', p. 112. Bertram, Rousseau and The Social Contract, p. 44. Exercising one's freedom through abiding by the law is only reasonable if lawgiver and subject are the same person. This is where Rousseau takes a radically democratic stance, criticising Kant's very open assertion that law can still be legitimate only by the requirement that the requirement 'that the proposal could have been agreed to had the citizens been consulted and were they fully rational'. Hence Rousseau's claim that 'power indeed may be transmitted, but not the will': for the general will to function, it requires the individual standpoints of each citizen. Unanimity is thereby not needed; it is in fact a sign of an unhealthy body politic. The only law that requires unanimity of all citizens is the social pact, since by this each individual hands over their individual sovereignty to the higher expression of the community. Within the body politic, the general will aims to protect rather than to attack its participants, and 'it is not', therefore, 'to be assumed that any of the parties then expects to get hanged'. Likewise, peace with neighbouring communities is required by natural law. It has therefore been claimed that the general will represents a model for the well-being of all, and thus reflects 'each individual's good or real will'. Neal, Liberalism, p. 63. Rousseau, The Social Contract, p. 20. Ibid, pp. 87-88. Ibid, p. 28. Hampsher-Monk, 'Rousseau and totalitarianism: with hindsight? ', p. 270. Ibid, p. 270. The emphasis that Rousseau lays on the collective, however, is today regarded with mistrust. After all, the functioning of the general will depends entirely upon the full dedication of the citizenry to the social contract, and we find 'no restriction of principle on what it may command in that form'. It appears at times as though Rousseau puts all individual risks at right for an ideologically unified community. As soon as all sovereignty passes from the individual to the state, a greater power comes to exist which '[man] cannot use without the help of others'. Contrarily, there is no 'safeguards for the individual against the state'. The Rousseauean state results in collective socialisation, only rendered legitimate through the consent of its participants. In a wider interpretation of the social contract as merely a thought experiment, this socialisation obtains a somewhat enforced character. Bertram, Rousseau and The Social Contract, p. 193. Blum, Republic of Virtue, p. 111. Hampsher-Monk, 'Rousseau and totalitarianism: with hindsight? ', p. 272. Ibid, p. 268. One form of criticism of the general will draws parallels with the Jacobin rule during the French revolution. Both reveal traces of absolutist totalitarianism: 'It is absolutely necessary that evil come from others and from them alone'. Despite the validity of a critique of Rousseau's reassertion that the general will is infallible (which indeed shows traces of a majority intolerance), it is difficult if not altogether impossible to find examples of the Rousseauean state in practice. Eastern block socialism, which has been suggested as imposing some for of a state-directed will upon its citizens, has been severely criticised. Also the Jacobin adoption of Rousseau's general will has been seen as of symbolic value rather than a precise influence of his thought. Lacking everywhere has been the binding together of lawgiver and subject, which according to Rousseau results only in the imposition a particular will of a state governed by a political elite. Jean Starobinski, 'La Mise en accusation de la Société', cited in Carol Blum, Rousseau and the Republic of Virtue. The Language of Politics in the French Revolution (Ithaca/London, 1986), p. 218. Blum, Republic of Virtue, p. 32. Hampsher-Monk, 'Rousseau and totalitarianism: with hindsight? ', p. 272. It is true that Rousseau is most concerned with the organisation of social order based upon the exercise of virtue, rather than on preserving the individual's relative freedom of action. It is therefore perhaps surprising that Rousseau leans entirely on the exercise of a general will in ensuring the best for all citizens. Through this construct, Rousseau categorically denies the individual any sovereignty except for a participatory role in the community. Effectively, the state attains a strongly fortified position in which ' [t]he more the natural strengths are dead and destroyed, the more the acquired ones are great and durable, and the more solid and perfect this institution is'. Such a radical step toward an active, creative form of liberalism challenges liberals into the debate of the equalisation of the citizenry, since 'serious economic inequalities destroy liberty'. Nonetheless, it does not suffice to, as a consequence of this conclusion, leave all individual property and grounds for action within the confines of the community's sovereignty. Viroli, The 'well-ordered society', p. 113. Blum, Republic of Virtue, p. 111. Hampsher-Monk, 'Rousseau and totalitarianism: with hindsight? ', p. 275. One of Rousseau's ideas of human nature was that the strongest of human passions was amour the soi (love of self), of which virtue and piti éwere manifestations within society. 'No-one has any interest in making them [the conditions of association] burdensome to others' This may be seen as a problem for the protection of the individual's rights: Rousseau does not expect the intrusion of state into the individual's life and thus includes no precautions to safeguard certain domains of private life. 'If this leads to intervention in 'areas which are perceived as illiberal, such as the social and economic conditions of citizen's lives and with their beliefs and values', this is seen by Rousseau as a decision of the general will in order to secure the necessary conditions for socio-political liberty. To some extent the modern welfare state can be seen as a moderate outcome of Rousseau's general will, be it on a representative basis. Executive and lawmaking government of the welfare state is now generally expected to create policy that benefits the community in a broad social sense. Taxation, health security and economic intervention are put in place to equalise conditions and ensure a minimum standard of welfare for each citizen, whereas participation in elections for government positions is open to each citizen. This form of moderate liberal welfare state, however, fails to pass the test, since in this model the law does not come from all. Bertram, Rousseau and The Social Contract, p. 24. Hampsher-Monk, 'Rousseau and totalitarianism: with hindsight? ', p. 281. Ibid, p, 275. Bertram, Rousseau and The Social Contract, p. 157. The dichotomy between modern, individualist liberalism and the Rousseauean state lies predominantly in their different interpretations of liberty. Whereas the former stress the lack of protection for the individual in Rousseau's social contract, Rousseau in turn entertains concerns with the crude, unsocial kind of liberty that individualists profess. Rather does he see liberty in a community best exercised at the highest level, thus enhancing liberty in an ordered way. For order to exist, there needs to be unity based on a reasonable principle; the social contract offers just that. As liberty exercised only on the most individual, narrow level only furthers contradiction in interests, Rousseau proposes the pursuit of community interest, which, if carried out fully, secures the welfare and safety of all its citizens. What Rousseau does not display enough attention to, however, was the fact that 'there is never a perfect fit between moral requirements of our rational nature and requirements of citizenship'. This results in a lack of consideration of the nature of the general will, and its proper domain. Rousseau, on the contrary, believes in the stabilising role of the general will, and the inalienability of the will in general. As the general will represent the only genuine method of determining the sovereign will of the citizenry, parallels drawn between totalitarian ideological states and Rousseau's general will are, strictly theoretical, incorrect. The problem, nonetheless, lies with the radical solution Rousseau's general will poses. Asserting that '[t]o be governed by appetite alone is slavery, while obedience to a law one prescribes to oneself is freedom', leaves very little space for individual action. Thus, a community taking its legislative task to the extreme, might indeed force its citizens into abide by the law. The question becomes, then, not to what extent Rousseau can be used for totalitarian ends, which he explicitly sets out to prevent, but rather how a feasible system may be envisaged in which particular interpretations of the general good can obtain a stable construct in which the individual is both more protected and better positioned to criticise the state's pursuit of the general good. Viroli, The 'well-ordered society', p. 38. Christopher Bertram, Routledge Philosophy Guidebook to Rousseau and The Social Contract (London, 2004), p. 33. Rousseau, The Social Contract, p. 20. Neal, Liberalism, p. 60. Christopher Bertram, Routledge Philosophy Guidebook to Rousseau and The Social Contract (London, 2004), p. 194. IntroductionWhen we talk about the 1930s in terms of social history, we often refer to the strong emergence of ideology into the public domain. Moreover, for Latin American countries, the first half of the twentieth century marked the coming of 'modernity'. Other factors, however, such as immigration and a rumbling social hierarchy in the aftermath of slavery, tended to influence society in more informal ways. Self-representation through culture was usually closest at hand for the lower classes, and often provided a colourful means of communicating a multi-faceted message. In this essay we shall look at such cultural representations in Argentina, Brazil and Mexico, and assess the extent to which they contributed to the construction of a national identity. Naturally, art forms such as dance and music were, even though powerful, not the only way of expressing one's national sentiments. The first films with sound managed to reach a mass audience and demonstrated what it was like to be a true Argentine, or Brazilian, in a light package. Moreover, the aftermath of key political events (Argentina, the 'infamous decade' after the fall of Yrigoyen; Brazil, the 1930 coup of Vargas; Mexico, the heritage of the revolution) and the subsequent split among the people brought a new literature of thinkers focussing with urgency upon 'national character and the metaphysics of the Mexican, Argentinian or Brazilian 'being''. The assessment of each country's culture climate thus provides interesting insights into the attempts made at a culturally constructed nationhood. Edwin Williamson, The Penguin History of Latin America, (London, 1992), p. 513. Gerald Martin, Journeys through the Labyrinth. Latin American Fiction in the Twentieth Century, (London/New York, 1989), p. 67. Argentina: 'a land of exiles' Jorge Luis Borges, cited in in Gabriel Nouzeilles and Graciela Montaldo (eds. ), The Argentina Reader. History, Culture, Politics, (Durham, London, 2002), p. 1.The Argentina of the 1930s saw increasing modernisation and a changing political climate. This did not leave the notion of nationalism untouched; and, after the social positivism of the Irigoyen government that was overthrown in 1930, cultural representations of the Argentine nation became increasingly ambiguous. The 'Década Infame' brought a reconsideration of Argentine patriotism and the essence of 'Argentineness'. A new definition of the Argentine identity proved, however, not quite unproblematic. A nation of immigrants, Argentina had many faces and was, with the coming of further modernity and political maturing of the masses, thrust into a further set of cultural disparities. These were based upon dichotomies as varied as those of machismo versus feminism, the European-orientated metropolitan versus the patriotic gaucho, and that of the definition of a high culture versus that of commercially formed mass culture. Essentially, thus, the concept of argentinidad was politically and culturally utilised in a multitude of ideological representations: 'The references to a collective ideal of the argentinidad were continuous [...], although this ideal was defined in the most imprecise way and was basically the product of the whims and wishful thinking of the ideologues who wrote on the subject, more than an ideal that could be identified empirically as a trait of Argentina's culture'. Carlos Escudé, 'The Case of Argentina',  URL , (2000), viewed 2 May 2005. An early elaboration upon Argentine culture and its forms can be found in Sarmiento's Facundo of 1845. Sarmiento, Argentina's president between 1868 and 1874, regarded the countryside, inhabited by the mestizo gaucho Argentine, as uncivilized, chaotic - even barbarous: 'dirty children covered in rags, living amid packs of dogs; men stretched out on the ground, in utter inactivity; filth and poverty everywhere'. The country's biggest cities, on the contrary, contained every aspect 'that characterizes cultured people'. Later, the popular poem about Martín Fierro created a distinction between the good, heroic and patriotic gaucho, and the barbarous and violent bad gaucho. The early twentieth century thereafter saw a complete rehabilitation of the gaucho icon, when it was presented as an example of argentinidad in an increasingly immigrant-dominated society. This was, however, merely a constructed and idealised identity, and the by the end of the twenties, the plurality of immigrant cultures were gaining space from more traditional social and cultural dance clubs with names like 'The Patriotic Gauchos'. In its stead came a new form of dance, less courteous and more fitting in the context of the big metropolis Buenos Aires: tango. Domingo Faustino Sarmiento, Facundo. Civilization and Barbarism, (London, 2003), p. 51. Ibid, p. 52. William Rowe and Vivian Schelling, Memory and Modernity. Popular Culture in Latin America, (London, New York, 1991), p. 34. Leopoldo Logunes, 'National Identity in a Cosmopolitan Society' in The Argentina Reader, p. 209. William Rowe and Vivian Schelling, Memory and Modernity. Popular Culture in Latin America, (London, New York, 1991), p. 35. The thirties in Argentina were marked by modernising development from above and from below. The tango started its ascending popularity from below. A dance of immigrants, it was originally popular in the packed, miserable and crime-ridden suburbs of Buenos Aires. Its association with knife-fighting and gangs was famously put into words in Borges' short story 'Streetcorner Man' of 1933. In it, two gang leaders head into confrontation on an illegal tango party. Ibid, p. 35. 'The only reason I don't carve you up is because you sicken me,' the Butcher said to Rosendo, threatening to strike him. That same moment, La Lujanera threw her arms around the Butcher's neck, fixing those eyes of hers on him, and said in a fury, 'Let him alone - making us think he was a man.' For a minute of so [the Butcher] was bewildered. Then, wrapping his arms around La Lujanera, he called to the musicians to play loud and strong, and he ordered the rest of us to dance. Jorge Luis Borges, The Aleph and Other Stories, (London, 1973), p. 27. These aspects - machismo, violence, illegitimacy, made dancing the tango into a rebellious act, associated with characters on the edge of society, devoid of patriotic virtue or morality. This was soon to change, as the tango was brought to Europe and became popular in the salons of avant-garde Paris. Before this shift, the tango only gained marginal recognition as a truly Argentine metropolitan dance, as in the poem 'Milonga' by Oliverio Girondo. Rowe and Schelling, Memory and Modernity. Popular Culture in Latin America, (London, New York, 1991), p. 36. Males whose bodies rupture in a ritual court, their heads sunk low between their shoulders, their lips thick with coarse remarks. Females with their nervous rumps, bits of foam at their armpits and their eyes looking much too oily.Oliverio Girondo, 'Poems to Be Read on a Trolley Car' in The Argentina Reader, p. 252-53. A second exponent of tango, nevertheless, soon found through tango a new formula for political criticism, woven into poetical lyrics. The song 'Cambalache' by Enrique Santos Discépolo, written in the early thirties, counterposed the liberal positivistic nationalism with a gloomy, destructive view of the new Argentina. 'Those that don't cry don't feed', Santos sang, 'and those that don't steal are fools'. Enrique Santos Discépolo, 'Cambalache', in The Argentina Reader, p. 267. Despite this loathing with politics, the middle class adopted the tango as an aspect of the new, dynamic and modern culture that Argentina was now adopting. In this, the cinema played a big, yet not unambiguous role. Argentina was steadily adopting the Northern American cinema culture. In 1930, around a thousand film screens could be found throughout the country. However, already by 1926 an estimated 95 percent of screen time was filled by films from the United States. Tango reached a mass audience when sound was added to cinema films, and a number of glamorous films with appealing tunes lent this formula its great success. Unfortunately this also caused tango to become a mass cultural phenomenon, alienated from its former basis of charismatic metropolitanism. By the late thirties, fifty such films were produced a year, also serving as a charming and successful export product for other Latin American countries. John King, Magic Reels. A History of Cinema in Latin America, (London, New York, 2000), p. 9. Ibid, p. 11. Ibid, p. 38. While the metropolitan immigrant gained commercial ground and mass appeal, other cultural forces were working from above, posing questions at a more political and intellectual level. X-Ray of the Pampa was published in 1933, melancholically referring back to inner country Argentina as a real source of Argentine national identity. In the essay, Ezequiel Martínez Estrada accused the central government in the metropolitan centre of cultivating city values at the cost of the 'countryside, which is full of truth and life'. Minority immigrant groups, faced by the dominance and lack of understanding of the overwhelmingly Catholic Hispanic cultural majority, attempted to secure themselves a place within the Argentine identity. Jews, who were often particularly victim of racism, attempted, with relative success, to bring the issue to attention in the theatre. A number of Jewish playwrights wrote plays on Argentine Jewish families and their cultural problems, with strategies varying from the complete abandonment of any distinctive Jewish identity', to the incorporation [...] of Argentina's many immigrant groups, their concerns and their idioms'. The playwright César Tiempo in 1937 even won a prestigious theatre prize for one such play with a light-hearted tone, Pan criollo [Native Bread]. Ezequiel Martínez Estrada, 'X-Ray of the Pampa' in The Argentina Reader, p. 262. David William Foster, Cultural Diversity in Latin American Literature (Albuquerque, 1994), p. 97. Foster, Cultural Diversity, pp. 110-11. Ibid, p. 111. The literature of Jorge Luis Borges forms a totally separate category in Argentina's intellectual life of the thirties. Whilst his outlook towards nationalism was complicated and peculiar, both the political left and right came to agree 'that what the fictions display is mastery'. Initially, Borges became part of the capital's avant-garde poetry circle, whose most famous representative was Oliverio Girondo, and his first published poems still breathe the excitement of metropolitan life. By the 1930s, however, Borges came to reject Martín Fierro's nativistic idea of argentinidad as well as the avant-garde as 'infantile disorders'. Perhaps due to some drastic changes in Borges' life over the second half of the thirties, among which the death of his father, a short period of serious illness and a mindless job as a librarian, Borges increasingly came to see all experiences as individual. Increasingly, Borges disapproved of simplistic, mono-cultured concepts of Argentine nationalism, in which some groups were downplayed in favour of others. Instead, he proposed that Jean Franco, Critical Passions: Selected Essays (Durham/London, 1999), p. 328. Martin, Journeys through the Labyrinth, p. 153. Franco, Critical Passions, p. 331. Martin, Journeys through the Labyrint, p. 153. we cannot limit ourselves to purely Argentine subjects in order to be Argentine; for either being Argentine is an inescapable act of fate - and in that case we shall be so in all events - or being Argentine is a mere affectation, a mask.Jorge Luis Borges, cited in Martin, Journeys through the Labyrinth, p. 155. This deconstructionist view of argentinidad cut short all aspirations toward an inward-looking, 'authentic' national identity, and hence, Borges later came to be regarded as an early exponent of postmodernism. Throughout the 1930s, Borges was still very much in search of new values, comparing the Argentine with the Jew, whom, according to him both had no 'particular national tradition', while the entrenchment in a narrow set of national values was 'negative, or, at the very least, uncreative'. Soon, he would move on to a path of displacing, often surprising literary creations, revealing elements of both ironic genius as elitism. Franco, Critical Passions, p. 327. Ibid, pp. 330-31. Williamson, The Penguin History, p. 516. Martin, Journeys through the Labyrint, p. 159. Brazil: 'All hail! This samba's going to end in jail'  Patrícia Galvão, 'Where They about Rosa Luxemburg', in The Brazil Reader, p. 166.The 1930s as a definable decade for Brazil started in October 1930, when a group of young army officers overthrew the long-standing São Paulo oligarchy and brought GetúlioVargas to power - a dictatorship that was to last until 1954. This inaugurated an era of modernisation to the country, and a new search for a national identity. Gradually, art forms and cultural expressions of the lowest classes (in Brazil this mainly meant the former slaves) became accepted by a wider public, and eventually even representative as national symbols. Dances like the samba and capoeira and the carnival of Rio de Janeiro - though often in commercialised and regularised forms- now became celebrated aspects of the Brazilian national identity. The film genre of the chanchada came hereby to play an important role, as it familiarised people from the cities with the cultural life of the favelas, the neighbourhoods of the poor black people, revealing 'its misery and at the same time its tremendous beauty'. Literature and poetry, on the contrary, became increasingly politicised throughout the thirties. Already in 1931, many of the writers in the Modernist movement were aware of the ideological clash that the Vargas government was driving the cultural community into. This drove a number of intellectuals to the Communists' side and often into imprisonment, as they presented a 'proletarian nation' in their work. The most lasting national cultural legacy, nonetheless, comes from the bambas of the favelas. Williamson, The Penguin History, pp. 416-22. Robert Stam, Tropical Multiculturalism. A Comparative History of Race in Brazilian Cinema and Culture, (London, 1997), p. 80. Ibid, p. 82. Mike Gonzales and David Treece, The Gathering of Voices. The Twentieth-Century Poetry of Latin America, (London, New York, 1992), p. 154. Martin, Journeys through the Labyrinth, pp. 69-72. While in the early decades of the twentieth century public concerts of samba were out of question, the 1930s marked a slow shift toward toleration of the music and dance. Whereas previously lovers of samba 'had to sing it far from the police', gradually a case was being made for acceptance of ""black"" art forms. Amidst the still prevalent racism in Brazil, Gilberto Freyes presented his theory of 'racial democracy' in 1933 with the publishing of his book The Masters and the Slaves. In it, Freyes argued that the contribution of Afro Brazilians added to the rich cultural mix in which Brazilian tradition was embedded. It has been argued, in fact, that consumption of Afro Brazilian food and music were already prevalent among all classes in a discrete sense since the nineteenth century. However, now supported by the state, who saw in the lower classes an important element for the maintenance of the status quo, the samba culture increasingly gained space in the public sphere. The gatherings and parties held in the house of Tia Ciata have been said to serve as a microcosmic example of Brazilian society. Each room in her house had different musical genre performances, with in the front rooms -near to the street- respectable dances such as the polka, and toward the back samba and percussive, ""ethnic"" rhythms. Over time, as the samba gained momentum and respectability, it started to refine and develop in a multitude of exponents. Fusing African rhythms and European tonal scales into a more accessible style, the samba now became acceptable for the wealthier white classes. Stam, Tropical Multiculturalism, p. 80. Ibid, p. 79. Paul Austerlitz, 'Review of The Mystery of Samba',  URL , (1999), viewed 10 May 2005. Rowe and Schelling, Memory and Modernity, p. 129. Austerlitz, 'Review of The Mystery of Samba'. Central to the samba scene in the favelas were the batuqueiros, or bambas. These dancers performed the capoeira, a self-defence technique disguised as a dance, which had been developed by runaway slaves. In street carnivals, where performers paraded through the streets, the bambas were singled out for their particular talent for improvised dancing. As the carnival was becoming bigger, it became also more organised. So-called Escolas de Samba (samba schools) started to predominate the street carnival, now participating in officially organised competitions for monetary prizes and social recognition. Thus, gradually, the samba culture came to serve as a means for increasing respectability and social mobility, with the samba schools as benefactors of neighbourhoods, bringing education and increasing unity in Rio de Janeiro. Rowe and Schelling, Memory and Modernity, p. 129. Ibid, p. 136. W.W. Norton, 'Samba: Mass Culture from the Bottom Up',  URL  (2002), viewed 10 May 2005. Whether empowering the black lower classes, or simply reducing their contribution to 'the picturesque and the folkloric', sure is that samba contributed to Brazilian nation-building as it transcended the regional differences and immensely varying dialects. Over time, the samba thus became a national symbol. The production of Brazilian films brought a major contribution in the transformation of samba into a mass culture, and thus to some extent in the emancipation of black Brazilians. With the coming of sound in films, the most popular film genre became, as had been the case in Argentina, a light story incorporating popular songs. The Brazilian version of such films was the chanchada, which, although never entirely unambiguous, has been described in its widest sense as 'the generic name given to all the comedies and musical comedies, with popular pretensions'. As could be expected, Rio de Janeiro emerged as the centre of Brazilian cinema in the 1930s, closely linked to the samba schools and the carnivals, in the sense that chanchada films embraced similar values and were often released around the time of carnival. In films such as Alô Alô Brasil (1935), featuring Brazilian film star Carmen Miranda, and Alô Alô Carnaval (1936), featuring her sister Aurora, blacks played minor roles as musicians in the songs. The presence of blacks in chanchadas, however marginal, were rather significant as they meant the open incorporation of ""black culture"" in Brazilian national culture. Stam, Tropical Multiculturalism, p. 80. Norton, 'Samba: Mass Culture from the Bottom Up'. Rowe and Schelling, Memory and Modernity, p. 131. King, Magical Reels, p. 55. Ibid, p. 55. Stam, Tropical Multiculturalism, p. 83. Ibid, p. 81. The extent to which the Vargas government influenced the incorporation of samba culture is blurred. Although Vargas showed some minor active support for the movement by founding the National Institute of Educational Film (INCE) in 1937, which was to serve national integration, this institute was actually short-lived, while the number of long feature films decreased toward the end of the decade. Moreover, the culture of the favelas brought with it the 'dialectic of malandragem', which was a concept of using the established order for illegal interests and to avoid 'the punishment of work'. Malandragem attempted to function alongside the order of society, thus obscuring the brisk line between order and disorder. It is therefore likely that these cultural shifts were rather a wave that accompanied the emancipation of the former slave classes than a conscious attempt at nation-building for the Estado Novo. King, Magical Reels, p. 56. Rowe and Schelling, Memory and Modernity, p. 128. Traditional high culture in Brazil's thirties was preoccupied with another struggle, that against the right-wing dictatorship. Much of the intelligentsia and artistic scene, united in the Popular Front, chose the direction of regionalism. Upper class painter Tarsila do Amaral, after having explored aspects in Brazilian nativism in her work, now started to focus on representing traditional every-day scenes of the countryside. According to Gilberto Freyes, the 'soul of Brazil' was to be sought in the rural north, which was struck by poverty and where many sugar plantations were situated. The regionalist movement, however, incorporated a wide range of ideologies opposed to the Vargas regime. Raquel de Queirós, in 1930 published Back in '15, a novel on the plight of the northern region during the famine period of 1915. In 1937 she was imprisoned for her radical Trotskyist political outlook. Regionalism thus often served to point class inequalities in a radical leftist outlook of Brazilian society. A less covered ideological viewpoint is provided in Patrícia Galvão's 'proletarian novel' Industrial Park (1933), where she focuses on the injustice of the Brazilian system. Elements of nationalism are obviously subjugated for the purpose of class struggle, as in the following passage. Gonzales and Treece, The Gathering of Voices, p. 152. Carol Damian and Christina Mehrtens, 'Tarsila and the 1920s', in Robert M. Levine and John J. Crocitti (eds. ), The Brazil Reader. History, Culture, Politics, (London, 1999) p. 315. Martin, Journeys through the Labyrinth, p. 66. Ibid, p. 69. Alfredo tries to like the simple and poorly prepared food. He feels happy. He doesn't find Brazil abhorrent, as before. He doesn't need to drown his individualistic irritability in any picturesque scenes, neither in the ovens of the Sahara nor in theglacial Arctic Ocean. He wants them to leave him in Braz. Eating that revolutionary food. Without longing for Cairo hotels or French wines.Galvão, 'Where They about Rosa Luxemburg', in The Brazil Reader, p. 169. The way in which this passage points out any Brazilian national identity is clearly negative; it argues in favour of a 'simple', social Brazil instead of an internationally incorporated, capitalist nation. Elements of Brazilian grassroots national identity are ironically referred to as expressions of a repressed class:  All Hail! All Hail! This samba's Going to land in jail.Ibid, in The Brazil Reader, p. 166. Other poetry, although not as explicitly communist as Galvão (who, for this reason, spent four years in prison as a political prisoner), focussed on the rough and oppressive facets of the regime through from a national perspective. The liberal Mário de Andrade in his poem 'The Wagon of Misery' ridicules the regime with deliberate spelling mistakes. Bump! The Wagon of Misery' s here From the Eyetalian carnival! Old Ma Misery is dressed in honour The colour of the brass of our age Gathering spittle behind her. The colonil the generul the kaptin The pure the heroic the well-meaning Childrin of the Brazilyun factory Colombus's people are swinging. Gonzales and Treece, The Gathering of Voices, p. 157. Brazilian literature and poetry of the modernist movement revealed thus revealed a highly politicised, polemic style, much embedded in the opposition of the dictatorship and united through this opposition rather than through a shared outlook of Brazilian nationalism proper. Throughout the thirties, Brazil remained still very much in search of a form of national cohesion. Martin, Journeys through the Labyrinth, p. 68. Mexico: 'A dark Indian, grateful to the party'In Mexico, the revolution proved to be the 'defining event', shaping the nation for the remainder of the century. For many, the success of the 1910s revolution was the culmination of revolutionary nationalism: the yoke of a century of oppression had now been thrown off. In an attempt to create a ""new man"" and ""new woman"", the new government turned to art, popular culture and education to create a new sense of nationhood for the Mexicans. The new Mexican nation now came to incorporate and celebrate a new group of inhabitants that had been previously been seen as passive and inferior: the Mexican Indian. The art of the 1920s and 1930s is thus marked by a fresh interest in Aztec art and architecture, and a celebration of the 'Indian race' can be found in nationalistic essays. Gilbert M. Joseph and Timothy J. Henderson (eds. ), The Mexico Reader. History, Culture, Politics, (Durham, London, 2002), p. 333. Alan Knight, 'Popular Culture and the Revolutionary State in Mexico, 1910-1940', Hispanic American Historical Review, 74:3 (1994), p. 395. Ibid, p. 395. Alexander S. Dawson, 'From Models for the Nation to Model Citizens: Indigenismo and the 'Revindication' of the Mexican Indian, 1920-1940', Journal of Latin American Studies, 30 (1998), p. 285. By the thirties, a new political machine, the National Revolutionary Party, had been put in place and the work of myth-making of Mexican heroes of the revolution started. This period has been marked the post-revolutionary period, as legitimising the state, as well as instilling a paradoxical blend of conservative revolutionary sentiments in Mexicans. This new ideological stream, indigenismo, was reflected in the mural paintings of Mexico's most prominent modern artists, in the work of intellectuals and in the number of epic novels about the revolution that were published in the twenties and thirties. It has, however, been held that for the government the aim was not 'to 'indigenize' Mexico, but to 'Mexicanize' the Indian'. Knight, 'Popular Culture and the Revolutionary State', p. 393. Martin, Journeys through the Labyrinth, p. 45. Rowe and Schelling, Memory and Modernity, p. 184. Indiginismo involved the reinterpretation of both the Indian and the nation. It involved, on the one hand, the creation of the myth of the radically social and progressive new Mexican nation, and on the other hand, a glorification of the Mexican Indian and their past. By the 1930s, however, the emphasis was shifted from race to culture. An interesting example can be found in the issue of bank notes. The depiction of a 'native woman, wearing traditional costume, standing before the Aztec calendar stone' was never circulated, and instead the images of Zaragoza and Madero were preferred. Now, notions of 'literacy, nationalism, notions of citizenship, sobriety, hygiene, and hard work' became part of the ideological mores; the Indians were believed to possess those qualities to an exceptional extent. They were, in fact, 'rational political actors with modern sensibilities'. To fit this singular representation, a new kind of Indian peasant emerged in literature: Dawson, 'Indigenismo and the 'Revindication' of the Mexican Indian', p. 282. Ibid, p. 285. Ibid, p. 294. Joseph M. Galloy, 'Symbols of Identity and Nationalism in Mexican and Central-American Currency',  URL  , (2000), viewed 11 May 2005. Knight, 'Popular Culture and the Revolutionary State', p. 395. Dawson, 'Indigenismo and the 'Revindication' of the Mexican Indian', p. 279. A dark Indian dressed in white, who wants to learn to read and write in Spanish and is grateful to the party; his children are healthy and can choose whether to stay in the rancho or migrate to the new life in the cities. Martin, Journeys through the Labyrinth, p. 46. Another aspect of this indigenismo were the rural ballads, corridos. They are transformed to be used on mass scale, into three-minute songs to be played on the radio Agustín Lara became the icon of this typically Mexican musical exponent. Often in appraisal of revolutionary heroes, corridos with their simple rhyme were easy to remember and thus accessible to considerable percentage of illiterate Mexicans. The following is an example of a corrido describing the death of Francisco Villa. Rowe and Schelling, Memory and Modernity, p. 100. Poor Pancho Villa! His destiny was very sad; To die in an ambush And in the middle of the way. He went, leaving Parral handling his horse, the brave general author of ""La Cucaracha"" [the cockroach] [own translation] Armando de Maria y Campos, La Revolucion Mexicana a Traves de los Corridos Populares, (Mexico City 1962), p. 245. Yet another way in which the revolutionaries were eternalised and epitomised as heroes of the Mexican nation was through the paintings of Diego Rivera, Jose Orozco and David Alfaro Siqueiros. These modern artists - 'Los Tres Grandes'- had started in the twenties to experiment with mural painting, a style which they thereafter used effectively for the depiction of Mexico's indigenous history, scenes of the revolution and of the modern, industrialised, progressive state. Supported and often commissioned by the government, these painters fulfilled 'the visual component of [the] need to create that Mexican citizen necessary for the survival of the post-Revolutionary state'. Martin, Journeys through the Labyrinth, p. 46. Anthony W. Lee, 'Mural Painting and Social Revolution in Mexico: 1920-1940, Art of the New Order - Review',  URL , (1999) viewed 11 May 2005. Ibid. In terms of literature, the 1930s saw a continuation of the style employed in the twenties. Epic works with 'the man on horseback', the archetypical revolutionary also depicted in the mural paintings of the post-revolutionaries. Where Guzmán had in 1926 recounted his memories in the revolutionary army of Villa with The Eagle and the Serpent, Gregorio López y Fuentes in 1932 created an epic hero out of Zapata. His novel The Land provided the sense of heroic Mexican leadership that legitimised the Mexican revolution. Martin, Journeys through the Labyrinth, p. 38. It is passed on in whispers, in huddles, by men at the plough and women at the well. He has been seen. So it's true he's not dead... 'Do you know who saw him? Old Albina. [...] This is what she told me: ""[...] I recognised him straight off. Hat pushed back, like always, them same trousers with the buttons, and that moustache you could tell a mile away. [...]"" ' Ibid, p. 45. There were, of course, also many writers opposed to this mythical image. In fact, the Mexican revolution brought no generation of poets of any significance; most of them were to be sought with the defeated reactionaries. Gómez Marín, who lived in London at the time wrote in a letter: 'From here, Mexico is something dark and bloody'. Moreover, the corrido, typical Mexican icon, could naturally also be used for other purposes than the glorification of the revolution. The following extract comes from a corrido praising the bravery of a christian Cristero rebellion. Gonzales and Treece, The Gathering of Voices, p. 29. Franco, Critical Passions, p. 456. Then the general said: ""I am prepared to grant you a pardon If you will tell me Where I might find the local priest."" Valentín promptly answered him: ""That I cannot say; I'd rather you kill me Than give up a friend.""... Before they shot him, Before he went up the hill, Valentín cried: ""O mother of Guadalupe! For your religion they will kill me. ""...Anonynous, 'The Ballad of Valentín of the Sierra', in Gilbert M. Joseph and Timothy J. Henderson (eds. ), The Mexico Reader. History, Culture, Politics, (Durham, London, 2002), p. 420. Arguably, illiteracy rates stood in the way of effective nation-building through literature. Some have thus asserted that 'it [was] film, not writing, which create[d] [...] the 'imagined community' of the nation'. Indeed, when the first film with sound was made in 1931, it was sponsored by state funds. Such films, which like in the case of Argentina and Brazil, were capable of reaching as mass audience, dealt with popular themes such as destitute prostitutes (which, of course, fuelled the macho discourse of the frail woman in need of protection) and went accompanied by popular tunes, most notably those of Agustín Lara's soundtracks. The state was quite involved in the ideological message such films conveyed. Thus when the Soviet filmmaker Eisenstein, visiting Mexico, planned to make a film on the history of Mexico, the government withdrew their subsidies for fear that such a film might cause an outrage amongst the oligarchic hacienda owners. King, Magical Reels, p. 99. Ibid, p. 42. Ibid, p. 42. Ibid, p. 44. In the aftermath of the revolutionary decade (1910-1920) in Mexico, the new authorities then energetically set about constructing the archetype of a new ""ideal Mexican"". This was based upon a renewed enthusiasm about the Indian, who was henceforth to be considered as a part of the new Mexican state. Indigenismo and, from the late 1920s into the thirties, revolutionary myth making played a big role in such representations. Nationalism was hence instilled through popular means: mural painting, the corrido ballad, film, and to some extent, epic literature. By actively interfering and negotiating these art forms, the state sought to channel the multitude of opinions into a glorification of the new Mexicanidad. Conclusion Nationhood is a fluid concept. It can be manipulated to incorporate whatever ideological ideas, and exclude or obscure what one does not find appropriate to it. Argentina and Brazil in the 1930s saw the beginning of a great transformation of nationhood, while Mexico had already begun that project directly after the Mexican revolution. Although often the state attempted to control cultural representations of its nationalism, intellectuals and poorer minorities did not let this happen easily. Borges refused to accept a simple, patriotic nationalism, thereby brilliantly revealing the paradoxes underlying such a concept; in Vargas' Brazil, the government was in a constant state of war with its poets and authors on the left, while former slaves of the favelas proved to be able to provide Brazil's most uniting and lasting national identity. Perhaps the Mexican state can be said to have succeeded best at imposing an official picture of nationalism. Their corridos and murals invoked a revolutionary-ideological Mexicanidad, party-style; and it would only be by the 1990s that Mexican natives started to wage war on that identity. Nevertheless, the successes in Mexico also knew their limits. The Catholic religion proved impossible to curb, while student protests in the 1970s already revealed the violent antagonism underlying the picture of Mexico that the government offered its people. Representations of national identity, then, are inherently linked with political ideology and power. Brazilian authors with tendencies toward communism provide the example; they employ a highly politicised image of the 'true Brazilian' proletariat. In Argentina, such representations resulted in two opposing views, the gauchoesque patriot, modelled after Martin Fierro stood in contrast to the liberal modernist 'rational' Argentine inspired by Sarmiento's Facundo. When tango, the voice of the minorities, and Borges came into the equation, a mono-cultured Argentine ideal became more complicated and troublesome. Similar in each of the three countries discussed, however, are the influences of 'lower' cultural forms. Tango, initially despised by all respectable layers of society, became the epitome of Argentinidad and proud export product after its success in the Parisian salons, while the samba of the Brazilian blacks became a national virtue. In Mexico, the corrido and new popular film both served the ideal of praising the spiritual idealism that belonged inherently to the Mexican man and woman, as opposed to the violent pragmatism of Northern American men, and the dehumanised star beauty of the Hollywood actress. Nationalism in all countries discussed, became increasingly represented in popular ways, reaching out to a mass audience. The constructors of such nationalistic forms sought to reach out to an unprecedented number of people, instilling upon them these new forms of nationalism. In Brazil, chanchadas united the country despite its immense differences in dialect, while in Mexico the party did their best to involve nationalism in every possible cultural and educational aspect. In Argentina it culminated in the decade of Peron's popular dictatorship. It can thus be said, that across these nations, the 1930s transformed nationalism into a popular and far-reaching construct.",False
18,"At the eve of the Versailles conference, promoters of the Polish cause immediately realised the international precariousness of their aims to forge the rebirth of an independent Polish state. Moving with caution, Piłsudski, Head of State and Supreme Commander of the Polish troops, in December 1918 even went as far as addressing his archrival Dmowski of the Polish National Committee in Paris as 'My Dear Roman' in an attempt to reconcile the Polish feuding factions. From the outset, it appeared that Poland's independence 'would depend almost exclusively on the endeavours of the Poles themselves', however, soon after the official acknowledgement of Poland's Second Republic, the political struggle grew bitter. It was only the realisation of Soviet hostility toward their new western neighbour that forced the antagonistic political forces to work together in a somewhat efficient war cabinet, though even then with the greatest difficulty and resentment. At nearly the same time that peace was concluded with the Russians in the Treaty of Riga, the endeks, as Dmowski's National Democrats popularly came to be know, managed to pass an overly liberal constitution which left virtually no freedom to the president of the new Polish Republic and much procedural liberty with the endek-dominated parliament. It was immediately recognised as 'a monument to anti- Piłsudskiism', and left Piłsudski bereft of a meaningful role within Polish politics, despite his nationwide fame as Poland's liberator gained through his successful military operations against the Soviet troops in the war of 1919-21. The seed was thus planted for Piłsudski's initial decision to overthrow the government in 1926 by the National Democrats' uncompromising political attitude and wish for political domination. While the direct cause for Piłsudski's coup d'etat can be said to have sprung from his 'personal ambition and intolerance', and obsession 'with the idea of Poland's unity and greatness', it must be added that Piłsudski's frustrations were partly validated by the sheer incapability of the multitude of Polish governments quickly succeeding each other of dealing with a great number of urgent and pressing matters. Thus, the reasons for the collapse of parliamentary democracy in Poland and the subsequent usurpation of power by the Nazi aggressors thereafter can be brought back to a number of reasons, of which Poland's 'own fundamental weaknesses, the instability of its institutions, and its irresponsible governments, as well as to the active and passive faults of the Great Powers' are the most important and general ones. Antony Polonsky, Politics in Independent Poland, 1921-1939. The Crisis of Constitutional Government (Oxford, 1972), p. 97. Peter D. Stachura, 'The Battle of Warsaw, August 1920, and the Development of the Second Polish Republic' in Stachura, Peter D. (ed. ), Poland between the Wars, 1918-1939 (New York, 1998), p. 43. Polonsky, Politics in Independent Poland, p. 99. R.J. Crampton, Eastern Europe in the Twentieth Century - And After (London/New York, 2005), p. 42. Hugh Seton-Watson, The East European Revolution (London, 1950), p. 47. Alexandra Piłsudska, Pilsudski. A Biography by His Wife (New York, 1971), p. 327. Joseph Rothschild, Return to Diversity. A Political History of East Central Europe Since World War II (New York, 1993), p. 8. The list of problems and grievances that the young Polish state was thrust into from the early twenties was indeed impressive. While basic issues, such as the development of a political system and the role of minorities in the new state still had to be addressed, Poland was first confronted with 'a veritable fight for survival'. Peter D. Stachura, 'The Second Republic in Historiographical Outline' in Stachura, Peter D. (ed. ), Poland between the Wars, 1918-1939 (New York, 1998), p. 2. National unity was far to be sought. The dominion claimed by Poland included vast numbers of minorities with loyalties elsewhere. Of twenty-seven million inhabitants not even 70 per cent were Polish, while 14.3 per cent were of other Slavic decent and 7.8 per cent of the population were Jewish, often not speaking Polish and many opposed to the establishment of a Polish nation. The same went for the small German minority of 3.9 per cent, whom, by conducing themselves 'in manner that was at once provocative and calculated to give notice that it regarded its status in Poland, and Poland herself, as temporary', outraged the Polish majority. A long struggle between Poland and her minorities persisted throughout the interbellum period. While it has been noted that '[n]othing the Poles did was ever right, as far as [the Jews] were concerned', there was little effort on the government's side to draw the Jews into appreciated and meaningful participation of national life. On the contrary, when Poland's first president, Gabriel Narutowicz, came to be elected with support of the bloc of minority parties, he was murdered within days after his election by the Sejm. Despite the endeks' initial condemnation of the murderer, whom saw the election of a minority-dependent president as a 'symbol of shame', he was later celebrated as a Polish nationalistic hero. Generally, the antagonistic sense existed, in which when the minorities, with the little representation they had in parliament, exercised their rightful power, it was seen as invalid and unjustified by a wide range of centre-right and rightist Polonocentric parties, thus blocking the way for meaningful political participation of the minorities. Crampton, Eastern Europe, p. 41. Peter D. Stachura, 'National Identity and the Ethnic Minorities in Early Inter-War Poland' in Stachura, Peter D. (ed. ), Poland between the Wars, 1918-1939 (New York, 1998), p. 71. Ibid, p. 77. Hans Roos, A History of Modern Poland. From the Foundation of the State in the First World War to the Present Day (London, 1966), p. 103. Ibid, p. 100. While, to some degree, all political factions agreed with Piłsudski's insistence on 'complete independence for Poland, [...] sceptical of the value of co-operation with any Russians', little was done to involve the Ukranian and Belorussian minorities in decision-making of their lot, even though Poland's border with Russia came to run 'far to the east of that originally suggested by Lord Curzon' during the Versailles conference. Piłsudski's ideal of a federal model incorporating largely autonomous minority factions were soon defeated; instead there existed 'acrimony and distrust, a sort of internal war'. Moreover, while the minorities did little to contribute to a 'modus vivendi', a sense of community spirit, which might have brought more peaceful social relations, many Poles themselves were unrealistic and intolerant about the formal and informal shape Polish society must take. This had not only racial, but also religious, economic and political reasons. Pre-war conditions of many Poles created a resentful attitude toward minorities and this made it hard to deal with the minorities in a fair way. Seton-Watson, The East European Revolution, p. 37. Crampton, Eastern Europe in the Twentieth Century, p. 39. Stachura, 'National Identity and the Ethnic Minorities', p. 78. Ibid, p. 78. Polonsky, Politics in Independent Poland, p. 100. Politically and economically antagonistic views were from the onset a mutually enforcing problem, which caused great obstacles for the creation of a healthily functioning government. Additionally, the multitude of Polish independence parties which had formerly existed, maintained their air of secrecy and 'conspiratorial character' even when government bodies had been created, which had disastrous results in the conflict between Dmowski and Piłsudski, which took its toll on Poland's national interest. The passing of an overly liberal constitution based on that of France at the end of the Polish-Russian war in 1921 resulted in extreme factionism; by 1926, there existed more than 90 parties, of which around 30 parties were at times represented in the Sejm. As parliament held most power within the political structure, governments found it very difficult to pass reforms. The economy, in particular, proved a disastrously antagonising force. Extreme inflation, such as in Warsaw, where food prices with a unitary value of ""100"" in January 1921 rose to ""1298.2"" in December 1922, brought several governments to fall, while rising unemployment caused the Polish Socialist Party, the PPS, to organise strikes throughout 1924 and 1925. Dmowski and his endecja became increasingly entangled in their bid for power. While land reform was the way out for a country too poor to introduce any significantly contributing industrialisation, the National Democrats, with many landowning members, were reluctant to land redistribution of any real importance, which in turn caused the anomosity of the peasant parties. This, in all, caused land reforms to be rather ineffective and certainly insufficient. A sense of lingering problems thus left a general discontent throughout all layers of society. Ibid, p. 99. Stachura, 'The Battle of Warsaw', p. 52. Stachura, 'Historiographical Outline', p. 5. Polonsky, Politics in Independent Poland, 1921-1939, p. 105. Roos, A History of Modern Poland, p. 109. Crampton, Eastern Europe, p. 45-6. Roos, A History of Modern Poland, p. 105-7. Additional to internal problems, came the issue of Poland's national security. Weimar Germany continued a revisionist policy of regaining its former Prussian areas and southeastern Silesia from Poland. This caused a particular as Poland had no natural defensive barrier and the western powers, although they ratified Poland's borders to the east,  took little affirmative action against Germany's revisionist campaign, which lent its support to Germans in Poznania and Pomerania in a number of ways, thus destabilising the nation through what was essentially a very small percentage of its population. Rothschild, Return to Diversity, p. 9. Crampton, Eastern Europe, p. 40. Stachura, 'National Identity and the Ethnic Minorities', p. 71. Another form of support to the new independent state which was pressingly lacking was financial aid. Having half of its roads, railways and public buildings destroyed during the Great War, and a continuing expenditure of over 50 per cent of the state budget on defense and the railway system, the government lacked the essential funds to modernise industry and thus create employment. The absence of foreign loans and apparent disinterest in creating political stability from the western powers might have done much to help Poland establish a parliamentary democratic routine. Thus, when the British foreign office sneeringly contradicted the Soviet Union's allegation of involvement in Piłsudksi's coup d'etat in a telegram to its ambassador in Moscow, arguing that '[the Russians] are of less consequence to us than they suppose', this revealed much of its indifferent attitude towards Polish politics as well. Roos, A History of Modern Poland, p. 107. Ibid, p. 105. Polonsky, Politics in Independent Poland, p. 515. The exigent situation that Poland was in did not go unnoticed. Piłsudksi, embittered by the course of events since 1921, grew increasingly convinced that Polish 'parliamentarianism had [...] opened the door to general corruption'. But not just Piłsudksi was displeased with the situation. Much of the PPS-cadre, frustrated with years of ineffective government and endek domination, now turned their hopes to Piłsudksi, which prompted PPS theorist Feliks Perl to foreshadowingly argue in 1924: 'He cannot be made into a left-wing Mussolini'. PPS leader Daszyński argued that political immaturity of the state was the main cause for political feuding, but, in the beginning of 1926, also urged for immediate constitutional reforms empowering the presidential post in order to contain parliamentary quarreling. Roos, A History of Modern Poland, p. 110. Ibid, p. 111. Polonsky, Politics in Independent Poland, p. 147. Not just the opposition of the left had their second thoughts about the functioning of the state. In December 1925, Dmowski wrote 'if we could create even half the organization like the Fascists... I would willingly agree to a dictatorship in Poland'. Discontents with the democratic procedure and democracy as such proved different degrees of the same slippery slope, as it soon turned out. University students radicalised under the prospect of unemployment and resented the apparent greater success of their Jewish peers; they formed the leadership of fascist movements that were to increase their power-base throughout the 1930s. Antony Polonsky, Politics in Independent Poland, 1921-1939. The Crisis of Constitutional Government (Oxford, 1972), p. 149. Seton-Watson, The East European Revolution, p. 44. Overwhelmed by problems of such dimensions, historians have argued, 'some action [...] seems in retrospect to have been almost inevitable' within the Polish political landscape. This seems, however, an unlikely viewpoint and it is worthwhile considering what other course of events politics might have taken. An internal conflict in Piłsudski's course of action appears quite clearly, as when in November 1925, when Piłsudski is requested to take over the rule of the country by factions of the army. He claimed he 'would never be a cause of trouble and dissention in the State'. It seems, from this, to have been rather a passionate, naturalistic sense of justice that drove him to carry out the coup d'etat than a positivistic, legal course of action. Already earlier in the twenties had Piłsudski shown this duality in his character. By declining a bid for the presidency, he proudly refused 'to be enclosed in a 'gilded cage'', while apparently seeing no value in attempting reforms through coalition in the Sejm. Rather than acknowledging his objections to National Democratic domination, he slid further back into the trappings of ""moral dictatorship"", hereby only united the body politic in their opposition against his rule. Thus, the peasant parties united in the Sejm to resist political persecution, while his own rule alienated moderate supporters of his ""parliamentary dictatorship"". While open confrontation with the initially supportive socialists became ever more discernible, Piłsudski still drew a Hobbesian legitimacy from the backing of the Sejm's members, which became, however, increasingly difficult to forge. When, in the end of 1929, he personally came to reopen the Sejm accompanied by a hundred armed officers, Daszyński refused to open the session 'under the threat of bayonets, guns and sabres', Piłsudski abstained from openly breaching the constitution by forcefully opening the session. The failure of Piłsudski's coup must thus been assessed within his own framework of legitimacy, which was to ""cleanse"" the parliamentary body politic. 'It is the basis of democracy, for which there is always place in Poland', he still argued in 1926. By late 1930, a number of oppositionist politicians were arrested, held captive and tortured. When eventually a number of politicians chose exile rather than imprisonment, Piłsudski was as far as ever from his ideal of a federalist republic. His cabinets governed for months 'simpy by repeatedly adjourning the Sejm'. Growing older, Piłsudski became an increasingly mistrustful statesman, believing in the last months before his death that 'the nation had reached real unity at last'. In reality, the new 1935 Constitution 'formally put an end to democracy', calling in a period known as the ""rule of the colonels"" which incorporated a mixture of nationalism and fascism. Polonsky, Politics in Independent Poland, p. 100. Piłsudska, Pilsudski, p. 329. Roos, A History of Modern Poland, p. 103. Seton-Watson, The East European Revolution, p. 32. Joshua Cohen, 'Protection for Obedience', MIT,  URL :P_SGvaU6aDYJ:web.mit.edu/polisci/research/cohen/protection_for_obediance.pdf+cohen+obedience+and+protection&hl=nl&ie=UTF-8 (30 October 2005) Roos, A History of Modern Poland, p. 119. Polonsky, Politics in Independent Poland, p. 189. Crampton, Eastern Europe, pp 50-1. Roos, A History of Modern Poland, p. 120. Ibid, p. 114. Piłsudska, Pilsudski, p. 341. Seton-Watson, The East European Revolution, p. 47. Crampton, Eastern Europe, p. 53. By carrying out a coup d'etat, Piłsudski essentially interfered forcefully with the democratic institutions. While initially hesitant, the sanacja's breeches of the constitution and civil rights were increasingly necessary for Piłsudski to maintain power. This, however, had detrimental effects on the democratic institutions and increasingly alienated many parts of society from his government. Being insufficiently successful in curbing further economic regression into the 1930s and failing to integrate increasingly militant groups of minorities, Piłsudski's failures played in the hand of the right, which made a comeback in radical form, while his sanacja leadership was increasingly chosen from a clique of military cronies. Piłsudski's coup d'etat of 1926 proved, thus, not only to be ineffective on theoretical grounds, but also on practical grounds, eventually breaking with Piłsudski's personal visions on good statesmanship. Polonsky, Politics in Independent Poland, p. 280. Crampton, Eastern Europe, p. 51.","The writing of Simmel on the discipline of sociology is characterised by a desire to reduce the study of society to its essence. Instead of viewing society as Durkheim did, that is, as a generic force working externally of the individual and reifying itself through organisational structures and artefacts, Simmel was rather concerned with giving a 'serious [...]analysis of the social bond' (Nisbit 1959: 479), which can be described as the reciprocally working relation between two or more social actors. He denounced the view that a society ""as such"" could exist outside individuals (Simmel 1971a: 27), preferring to build a sociological system from the individual's standpoint. In the first place, Simmel represented social interaction as the dynamic attempt to reconcile the opposite drives within each human being - 'for example, the individual and the group, the need for conformity and for individuation, the need for stability and for flexibility' (Abel 1959: 478). Despite the theoretical character of these opposites, Simmel believed all ""forms-of-sociation"" are to some extent conditioned by the tension between them. Secondly, Simmel sought to separate the forms these conflicting ways of behaving could take, and their socio-historically dependent content. The form thereby attains a certain ""objective"" quality, acting as a generally discernable trait in a variety of social circumstances. As an illustration of this position, Simmel considered the phenomenon of secrecy to be a social form which could, therefore, not be judged as such, as it 'stands in neutrality above the value functions of its content' (Simmel 1950: 331). Furthermore, the greater the number of links through sociation, the more the social sphere becomes ""society"". Critics have recognised Simmel's breach with the nineteenth century approach of ""doing"" sociology: constructing a model of society and then coming up with solutions to its problems. It has been argued that this way of theorising society created laws and principles 'too vague and too sweeping to be applicable in practical circumstances' (Abel 1959: 475). Hence, it was exactly 'the microsociological character' of Simmel's approach to sociology which won his writing its original quality over other sociologists (Nisbit 1959: 480). The extent of strict reductionism in Simmel's writing, however, must not be exaggerated. Simmel did recognise that, while only forms, forms-of-sociation could take different shapes (contents), with, at times, such strong social repercussions that the social sphere acquired a nearly metaphysical quality. Consequently, he wrote of keeping a secret that it 'produces an immense enlargement of life: [...] the possibility of a second world alongside the manifest world; and the latter is decisively influenced by the former' (Simmel 1950: 330). Much of Simmel's writing on approaches to sociological studies is related to the role of abstraction, and its balance in relation to concrete situations. He stressed the value of a division between a broader, psychological approach to social drives, and the shapes these drives take in interacting with others, whom allow or resist these drives in varying degrees, and simultaneously in their turn attempt to give shape to their personal drives. Hence, 'the further development of every relation is determined by the ratio of persevering and yielding energies which are contained in the relation' (Simmel 1950: 334). Because of his view of society as the result of the constant reciprocal interaction and mutual pressure of concretely realised social forms Simmel is widely recognised as primarily 'a sociologist of forms' (Nisbit 1959: 479). The role of forms-of-sociation, however, was of even greater importance to him. In Simmel's writings on how sociology ought to approach the social question, he proposed that the detachment of the intention of a social action (form) from the actual deed was the fundamental solution to gain a greater insight in the working of the social sphere. Indeed, Simmel went even further and proposed that '[t]o separate, by scientific abstraction, these two factors of form and content which are in reality inseparably united; [...] seems to me the basis for the only, as well as the entire, possibility of a special science of society as such' (Simmel 1971a: 25). To illustrate the position of the sociologist within such a framework of thinking, Simmel drew an analogy between society and language. While both convey a message, they express this in particular way, embedded in the context of their users. This particularity, in both cases, is conditioned by economical standing, and historical and cultural forces. Still, we can trace a binding element between all users across time and social position. The shape the social takes is conditioned by the intention that the individual may have. Thus, social forms are like grammar, which 'isolates the forms of languages from their contents' beyond the 'heterogeneity of content and purposes which life reveals' (Nisbit 1959: 480), while the sociologist attempts to atomise and describe this ""social grammar"". The separation of form and content automatically assumes the individual as the basis for a social interaction. It thereby recognises the role of psychology in determining the origins of social behaviour (Simmel 1971a). Simmel was, however, much concerned with defining the proper territory of sociology, arguing that 'we are not interested in the psychological processes that occur in each of the two individuals but in their subsumption under the categories of union and discord' (Simmel 1971a: 34). All we can properly and legitimately call society is 'a mode of existence which concretely expresses itself in the reciprocal relations of human beings' (Abel 1959: 475). The social grid of forms-of-sociation owes to Kant's virtue ethics in that it separates intention from action (Craib 1997: 56); Simmel, however, was not concerned with adding a value judgment but argued that the sociologist should solely attempt to determine the ""grammar"" of society by example. A point of criticism in the use of forms as social elements separate from the real may be that it creates an extent of theoretical abstraction. For Simmel, however, this practice only has a function as a theoretical exercise. 'In any given social phenomenon, content and societal form constitute on reality' (Simmel 1971a: 24), he argued; the separation of form is to serve as a tool for interpretation of case study. In fact, in this, Simmel went even further than Weber, whom, with his use of the ""ideal types"" provided the two theoretical ends of a scale on which a particular society can then be placed. Simmel, on the contrary, insisted that the use of forms must always relate to 'a piece of history, either past or present' (Abel 1959: 479). As a result, all Simmelian sociological concepts such as 'status, Gemeinschaft, relative deprivation, and solidarity' maintain a degree concreteness despite their traits of universality (Abel 1959: 478). Due to the position, moreover, that '[f]orm and content cannot, in practice, be separated' (Nisbit 1959: 480), much of Simmel's writing did in fact not maintain such a theoretical strictness. He recognised this fact, but saw this rather as a natural process of knowledge-gathering, which, in sociology, can only be done in concrete terms: '[t]hrough [...] a study, of course, these forms would lose in applicability what they would gain in definiteness' (Simmel 1971a: 28). The use of the term ""form"" must, therefore, not be seen as an equivalent of ""social law"", since a conception of law may exist independently from the individual, while ""form"" must always be filled in by ""content"", like a geometrical shape needs a material to exist at all (Simmel 1971a). Another point of criticism that has been brought to the fore, is the way in which sociology, in Simmel's conception, occupies a place merely as the study of the microsocial, which comes out of the distinction between and form and content as the only purely human element in social relations (Abel 1959: 476). It is, furthermore, a point of inconsistency between Simmel's writing on what sociological studies should consist of, and his actual sociological treatises. For example, when Simmel describes the social form ""secrecy"" as 'a formal means of boasting and of subordinating the others' (Simmel 1950: 332), this definition is much narrower than his further expansion on its role in associations, and the physical-spatial element that secrecy often involves. Hence, it can be said that the Simmel's descriptions of the content often go beyond what he himself characterised as the elements that make up a society. By focussing on the microsocial element of society, Simmel failed to appropriate the external force of society over the individual, arguing, contrarily to Durkheim, that 'with every growth of new synthesising phenomena [...], the same group becomes ""more society"" than it was before' (Simmel 1971a: 27); that is - society is the sum-total of all microsocial interaction. Since, as mentioned above, 'the investigation of forms-of-sociation, requested by Simmel, involves the determination of their functions with regard to social life' (Abel 1959: 478), it may be of value to assess Simmel's application of the use of forms in one of his own investigations. In his essay 'The Metropolis and Mental Life', Simmel draws a parallel with the emergence of conglomerations of people in the metropolis on the one hand, and the coming into existence of new ways of social interaction or the absence of interaction due to a growth of individualism (Simmel 1971b: 325). In this model, the metropolis may be seen as the expression, or content, of the various forms-of-sociation that we associate with modernity and the emergence of individuality. Thence, life in the metropolis sees a dual, antagonistic development: on the one hand, the vast increase of daily moments of interaction between people, while on the other hand, individuals 'retreat from excessive external stimuli [which are social content]' (Frisby 1992: 66). According to Simmel, the foremost conflict in metropolitan society can be observed in the tension between the individual, rationalistic, and the weight of the continuing flux of impressions upon the emotive, subjective side of each person. This creates two types of extreme social reactions. On the one hand, '[t]he purely intellectualistic person [who] is indifferent to all things personal' (Simmel 1971b: 326) becomes wholly disinterested and impatient in interaction with others. For him, or her, '[t]he meaning and value of the distinction between things, and therewith of the things themselves, are experienced as meaningless' (Simmel 1971b: 330), solely out of a vast overflow of impressions in everyday life. On the other hand, maintaining a superficial, rational approach toward others may result in nervous breakdowns and agoraphobia, which Simmel regarded as 'pathological characteristics of modern cities' (Vidler 1991: 36). This view, despite providing an interesting model for the social effects of modern life, however shows some ambiguities in definition and theoretical inconsistency, while it steps over a few essential elements without touching upon them. For, when Simmel remarks that 'the metropolis [...] has outgrown every personal element' (Vidler 1991: 38), he does not satisfactorily substantiate this view in the terms that he set out as sociological tools. It remains unclear, for example whether the metropolis sees an ""increase"" or ""decrease"" of the ""thing"" society. For, while numbers of contact moments, which are essentially opportunities to add content to form vastly increase, they simultaneously become infinitely more fragmented, single-minded and disinterested. To add to this problem of definition, it can be argued that each individual 'derives [these] values and improvements from this collective' (Wolff 1950: 54). Here we arrive at the fundamental complexity of form as a strictly separate theoretical concept. From Simmel, we may read that the form of 'the secret is a first-rate element of individualization' (Simmel 1950: 334), while principally it is a social form which, despite the fact that it must, per definition, involve more than one social actor, on the other hand consist of not sharing something with others. This makes the distinction between the individual and the social a very troubled one, from which one can conclude that 'a person is never merely a collective being just as he is never merely an individual being' (Levine 1971: 261). As '[t]he social is determined, but it is also determining' (Abel 1959: 476), it is necessary to provide a strong definition of the forms-of-sociation before anything can be said of its relation to forms. While people engage in relationships with each other 'to satisfy their mutual and individual needs' (Craib 1997: 56), this does, however, not provide us with a very firm tool to determine the every-day content of such forms: we might think of the relationship between two individuals and something as abstract and indirect as the state. Moreover, by leaving out physical aspects of a model of society, Simmel narrows the understanding that sociology may generate of society. Thus, while Simmel acknowledges that '[s]ome social forms, such as the state, manifest themselves in a unique and localized space that excludes the possibility of forms inhabiting the same space' (Vidler 1991: 39), he fails to appropriate this fact in a societal sense. Yet, when a person gets a prison sentence, this is not the direct result of his interaction with merely the judge, but rather does this spring from a relation within a more collective set of concepts such as ""the law"" and ""the state"". Interpreting this example in purely microsocial, individual relations means missing the impersonal and continuous aspects of social organisation. In conclusion, the separation between form and content is important in that it stresses 'the ""humanistic coefficient""' (Abel 1959: 479) in social relations. Whereas it allows a certain degree of generalisation of human drives within a heterogeneity of social circumstances, Simmel's theory stresses that we should not lose the 'element of irriducible humanism' (Nisbit 1959: 481) in society. It also stresses the direct reciprocal influence of individuals over each other in every-day social relations: '[t]he significance of [...] interactions among men lies in the fact that it is because of them that the individuals, in whom these driving impulses and purposes are lodged, form a unity, that is, a society.' (Simmel 1971a: 23). Thus, Simmel's reductionist sociology reveals a certain immediacy and existentialism to it. This approach to sociology, however, brings up a number of problems which Simmel sought to address by simply placing all socio-historical biases and influences inside the person-to-person relationships which gave content to universal, timeless forms. This brings about an antagonism in Simmel's system of thought, in that it reveals a disparity between the apparent behaviour (or action) of an individual's reaction to another, while this particular social form can only be analysed in a wider, macrosocial context. Especially more continuous, bigger forms of organisation have a greater sense of impersonality to them, making the underlying form (essentially a drive springing from individual actors) hard to define and ambiguous. The separation of form and content is thus a very useful too in the study of microsocial relations, but cannot be seen to contend any notion of a ""society in its entirety"".",True
19,"The writing of Simmel on the discipline of sociology is characterised by a desire to reduce the study of society to its essence. Instead of viewing society as Durkheim did, that is, as a generic force working externally of the individual and reifying itself through organisational structures and artefacts, Simmel was rather concerned with giving a 'serious [...]analysis of the social bond' (Nisbit 1959: 479), which can be described as the reciprocally working relation between two or more social actors. He denounced the view that a society ""as such"" could exist outside individuals (Simmel 1971a: 27), preferring to build a sociological system from the individual's standpoint. In the first place, Simmel represented social interaction as the dynamic attempt to reconcile the opposite drives within each human being - 'for example, the individual and the group, the need for conformity and for individuation, the need for stability and for flexibility' (Abel 1959: 478). Despite the theoretical character of these opposites, Simmel believed all ""forms-of-sociation"" are to some extent conditioned by the tension between them. Secondly, Simmel sought to separate the forms these conflicting ways of behaving could take, and their socio-historically dependent content. The form thereby attains a certain ""objective"" quality, acting as a generally discernable trait in a variety of social circumstances. As an illustration of this position, Simmel considered the phenomenon of secrecy to be a social form which could, therefore, not be judged as such, as it 'stands in neutrality above the value functions of its content' (Simmel 1950: 331). Furthermore, the greater the number of links through sociation, the more the social sphere becomes ""society"". Critics have recognised Simmel's breach with the nineteenth century approach of ""doing"" sociology: constructing a model of society and then coming up with solutions to its problems. It has been argued that this way of theorising society created laws and principles 'too vague and too sweeping to be applicable in practical circumstances' (Abel 1959: 475). Hence, it was exactly 'the microsociological character' of Simmel's approach to sociology which won his writing its original quality over other sociologists (Nisbit 1959: 480). The extent of strict reductionism in Simmel's writing, however, must not be exaggerated. Simmel did recognise that, while only forms, forms-of-sociation could take different shapes (contents), with, at times, such strong social repercussions that the social sphere acquired a nearly metaphysical quality. Consequently, he wrote of keeping a secret that it 'produces an immense enlargement of life: [...] the possibility of a second world alongside the manifest world; and the latter is decisively influenced by the former' (Simmel 1950: 330). Much of Simmel's writing on approaches to sociological studies is related to the role of abstraction, and its balance in relation to concrete situations. He stressed the value of a division between a broader, psychological approach to social drives, and the shapes these drives take in interacting with others, whom allow or resist these drives in varying degrees, and simultaneously in their turn attempt to give shape to their personal drives. Hence, 'the further development of every relation is determined by the ratio of persevering and yielding energies which are contained in the relation' (Simmel 1950: 334). Because of his view of society as the result of the constant reciprocal interaction and mutual pressure of concretely realised social forms Simmel is widely recognised as primarily 'a sociologist of forms' (Nisbit 1959: 479). The role of forms-of-sociation, however, was of even greater importance to him. In Simmel's writings on how sociology ought to approach the social question, he proposed that the detachment of the intention of a social action (form) from the actual deed was the fundamental solution to gain a greater insight in the working of the social sphere. Indeed, Simmel went even further and proposed that '[t]o separate, by scientific abstraction, these two factors of form and content which are in reality inseparably united; [...] seems to me the basis for the only, as well as the entire, possibility of a special science of society as such' (Simmel 1971a: 25). To illustrate the position of the sociologist within such a framework of thinking, Simmel drew an analogy between society and language. While both convey a message, they express this in particular way, embedded in the context of their users. This particularity, in both cases, is conditioned by economical standing, and historical and cultural forces. Still, we can trace a binding element between all users across time and social position. The shape the social takes is conditioned by the intention that the individual may have. Thus, social forms are like grammar, which 'isolates the forms of languages from their contents' beyond the 'heterogeneity of content and purposes which life reveals' (Nisbit 1959: 480), while the sociologist attempts to atomise and describe this ""social grammar"". The separation of form and content automatically assumes the individual as the basis for a social interaction. It thereby recognises the role of psychology in determining the origins of social behaviour (Simmel 1971a). Simmel was, however, much concerned with defining the proper territory of sociology, arguing that 'we are not interested in the psychological processes that occur in each of the two individuals but in their subsumption under the categories of union and discord' (Simmel 1971a: 34). All we can properly and legitimately call society is 'a mode of existence which concretely expresses itself in the reciprocal relations of human beings' (Abel 1959: 475). The social grid of forms-of-sociation owes to Kant's virtue ethics in that it separates intention from action (Craib 1997: 56); Simmel, however, was not concerned with adding a value judgment but argued that the sociologist should solely attempt to determine the ""grammar"" of society by example. A point of criticism in the use of forms as social elements separate from the real may be that it creates an extent of theoretical abstraction. For Simmel, however, this practice only has a function as a theoretical exercise. 'In any given social phenomenon, content and societal form constitute on reality' (Simmel 1971a: 24), he argued; the separation of form is to serve as a tool for interpretation of case study. In fact, in this, Simmel went even further than Weber, whom, with his use of the ""ideal types"" provided the two theoretical ends of a scale on which a particular society can then be placed. Simmel, on the contrary, insisted that the use of forms must always relate to 'a piece of history, either past or present' (Abel 1959: 479). As a result, all Simmelian sociological concepts such as 'status, Gemeinschaft, relative deprivation, and solidarity' maintain a degree concreteness despite their traits of universality (Abel 1959: 478). Due to the position, moreover, that '[f]orm and content cannot, in practice, be separated' (Nisbit 1959: 480), much of Simmel's writing did in fact not maintain such a theoretical strictness. He recognised this fact, but saw this rather as a natural process of knowledge-gathering, which, in sociology, can only be done in concrete terms: '[t]hrough [...] a study, of course, these forms would lose in applicability what they would gain in definiteness' (Simmel 1971a: 28). The use of the term ""form"" must, therefore, not be seen as an equivalent of ""social law"", since a conception of law may exist independently from the individual, while ""form"" must always be filled in by ""content"", like a geometrical shape needs a material to exist at all (Simmel 1971a). Another point of criticism that has been brought to the fore, is the way in which sociology, in Simmel's conception, occupies a place merely as the study of the microsocial, which comes out of the distinction between and form and content as the only purely human element in social relations (Abel 1959: 476). It is, furthermore, a point of inconsistency between Simmel's writing on what sociological studies should consist of, and his actual sociological treatises. For example, when Simmel describes the social form ""secrecy"" as 'a formal means of boasting and of subordinating the others' (Simmel 1950: 332), this definition is much narrower than his further expansion on its role in associations, and the physical-spatial element that secrecy often involves. Hence, it can be said that the Simmel's descriptions of the content often go beyond what he himself characterised as the elements that make up a society. By focussing on the microsocial element of society, Simmel failed to appropriate the external force of society over the individual, arguing, contrarily to Durkheim, that 'with every growth of new synthesising phenomena [...], the same group becomes ""more society"" than it was before' (Simmel 1971a: 27); that is - society is the sum-total of all microsocial interaction. Since, as mentioned above, 'the investigation of forms-of-sociation, requested by Simmel, involves the determination of their functions with regard to social life' (Abel 1959: 478), it may be of value to assess Simmel's application of the use of forms in one of his own investigations. In his essay 'The Metropolis and Mental Life', Simmel draws a parallel with the emergence of conglomerations of people in the metropolis on the one hand, and the coming into existence of new ways of social interaction or the absence of interaction due to a growth of individualism (Simmel 1971b: 325). In this model, the metropolis may be seen as the expression, or content, of the various forms-of-sociation that we associate with modernity and the emergence of individuality. Thence, life in the metropolis sees a dual, antagonistic development: on the one hand, the vast increase of daily moments of interaction between people, while on the other hand, individuals 'retreat from excessive external stimuli [which are social content]' (Frisby 1992: 66). According to Simmel, the foremost conflict in metropolitan society can be observed in the tension between the individual, rationalistic, and the weight of the continuing flux of impressions upon the emotive, subjective side of each person. This creates two types of extreme social reactions. On the one hand, '[t]he purely intellectualistic person [who] is indifferent to all things personal' (Simmel 1971b: 326) becomes wholly disinterested and impatient in interaction with others. For him, or her, '[t]he meaning and value of the distinction between things, and therewith of the things themselves, are experienced as meaningless' (Simmel 1971b: 330), solely out of a vast overflow of impressions in everyday life. On the other hand, maintaining a superficial, rational approach toward others may result in nervous breakdowns and agoraphobia, which Simmel regarded as 'pathological characteristics of modern cities' (Vidler 1991: 36). This view, despite providing an interesting model for the social effects of modern life, however shows some ambiguities in definition and theoretical inconsistency, while it steps over a few essential elements without touching upon them. For, when Simmel remarks that 'the metropolis [...] has outgrown every personal element' (Vidler 1991: 38), he does not satisfactorily substantiate this view in the terms that he set out as sociological tools. It remains unclear, for example whether the metropolis sees an ""increase"" or ""decrease"" of the ""thing"" society. For, while numbers of contact moments, which are essentially opportunities to add content to form vastly increase, they simultaneously become infinitely more fragmented, single-minded and disinterested. To add to this problem of definition, it can be argued that each individual 'derives [these] values and improvements from this collective' (Wolff 1950: 54). Here we arrive at the fundamental complexity of form as a strictly separate theoretical concept. From Simmel, we may read that the form of 'the secret is a first-rate element of individualization' (Simmel 1950: 334), while principally it is a social form which, despite the fact that it must, per definition, involve more than one social actor, on the other hand consist of not sharing something with others. This makes the distinction between the individual and the social a very troubled one, from which one can conclude that 'a person is never merely a collective being just as he is never merely an individual being' (Levine 1971: 261). As '[t]he social is determined, but it is also determining' (Abel 1959: 476), it is necessary to provide a strong definition of the forms-of-sociation before anything can be said of its relation to forms. While people engage in relationships with each other 'to satisfy their mutual and individual needs' (Craib 1997: 56), this does, however, not provide us with a very firm tool to determine the every-day content of such forms: we might think of the relationship between two individuals and something as abstract and indirect as the state. Moreover, by leaving out physical aspects of a model of society, Simmel narrows the understanding that sociology may generate of society. Thus, while Simmel acknowledges that '[s]ome social forms, such as the state, manifest themselves in a unique and localized space that excludes the possibility of forms inhabiting the same space' (Vidler 1991: 39), he fails to appropriate this fact in a societal sense. Yet, when a person gets a prison sentence, this is not the direct result of his interaction with merely the judge, but rather does this spring from a relation within a more collective set of concepts such as ""the law"" and ""the state"". Interpreting this example in purely microsocial, individual relations means missing the impersonal and continuous aspects of social organisation. In conclusion, the separation between form and content is important in that it stresses 'the ""humanistic coefficient""' (Abel 1959: 479) in social relations. Whereas it allows a certain degree of generalisation of human drives within a heterogeneity of social circumstances, Simmel's theory stresses that we should not lose the 'element of irriducible humanism' (Nisbit 1959: 481) in society. It also stresses the direct reciprocal influence of individuals over each other in every-day social relations: '[t]he significance of [...] interactions among men lies in the fact that it is because of them that the individuals, in whom these driving impulses and purposes are lodged, form a unity, that is, a society.' (Simmel 1971a: 23). Thus, Simmel's reductionist sociology reveals a certain immediacy and existentialism to it. This approach to sociology, however, brings up a number of problems which Simmel sought to address by simply placing all socio-historical biases and influences inside the person-to-person relationships which gave content to universal, timeless forms. This brings about an antagonism in Simmel's system of thought, in that it reveals a disparity between the apparent behaviour (or action) of an individual's reaction to another, while this particular social form can only be analysed in a wider, macrosocial context. Especially more continuous, bigger forms of organisation have a greater sense of impersonality to them, making the underlying form (essentially a drive springing from individual actors) hard to define and ambiguous. The separation of form and content is thus a very useful too in the study of microsocial relations, but cannot be seen to contend any notion of a ""society in its entirety"".","At the eve of the Versailles conference, promoters of the Polish cause immediately realised the international precariousness of their aims to forge the rebirth of an independent Polish state. Moving with caution, Piłsudski, Head of State and Supreme Commander of the Polish troops, in December 1918 even went as far as addressing his archrival Dmowski of the Polish National Committee in Paris as 'My Dear Roman' in an attempt to reconcile the Polish feuding factions. From the outset, it appeared that Poland's independence 'would depend almost exclusively on the endeavours of the Poles themselves', however, soon after the official acknowledgement of Poland's Second Republic, the political struggle grew bitter. It was only the realisation of Soviet hostility toward their new western neighbour that forced the antagonistic political forces to work together in a somewhat efficient war cabinet, though even then with the greatest difficulty and resentment. At nearly the same time that peace was concluded with the Russians in the Treaty of Riga, the endeks, as Dmowski's National Democrats popularly came to be know, managed to pass an overly liberal constitution which left virtually no freedom to the president of the new Polish Republic and much procedural liberty with the endek-dominated parliament. It was immediately recognised as 'a monument to anti- Piłsudskiism', and left Piłsudski bereft of a meaningful role within Polish politics, despite his nationwide fame as Poland's liberator gained through his successful military operations against the Soviet troops in the war of 1919-21. The seed was thus planted for Piłsudski's initial decision to overthrow the government in 1926 by the National Democrats' uncompromising political attitude and wish for political domination. While the direct cause for Piłsudski's coup d'etat can be said to have sprung from his 'personal ambition and intolerance', and obsession 'with the idea of Poland's unity and greatness', it must be added that Piłsudski's frustrations were partly validated by the sheer incapability of the multitude of Polish governments quickly succeeding each other of dealing with a great number of urgent and pressing matters. Thus, the reasons for the collapse of parliamentary democracy in Poland and the subsequent usurpation of power by the Nazi aggressors thereafter can be brought back to a number of reasons, of which Poland's 'own fundamental weaknesses, the instability of its institutions, and its irresponsible governments, as well as to the active and passive faults of the Great Powers' are the most important and general ones. Antony Polonsky, Politics in Independent Poland, 1921-1939. The Crisis of Constitutional Government (Oxford, 1972), p. 97. Peter D. Stachura, 'The Battle of Warsaw, August 1920, and the Development of the Second Polish Republic' in Stachura, Peter D. (ed. ), Poland between the Wars, 1918-1939 (New York, 1998), p. 43. Polonsky, Politics in Independent Poland, p. 99. R.J. Crampton, Eastern Europe in the Twentieth Century - And After (London/New York, 2005), p. 42. Hugh Seton-Watson, The East European Revolution (London, 1950), p. 47. Alexandra Piłsudska, Pilsudski. A Biography by His Wife (New York, 1971), p. 327. Joseph Rothschild, Return to Diversity. A Political History of East Central Europe Since World War II (New York, 1993), p. 8. The list of problems and grievances that the young Polish state was thrust into from the early twenties was indeed impressive. While basic issues, such as the development of a political system and the role of minorities in the new state still had to be addressed, Poland was first confronted with 'a veritable fight for survival'. Peter D. Stachura, 'The Second Republic in Historiographical Outline' in Stachura, Peter D. (ed. ), Poland between the Wars, 1918-1939 (New York, 1998), p. 2. National unity was far to be sought. The dominion claimed by Poland included vast numbers of minorities with loyalties elsewhere. Of twenty-seven million inhabitants not even 70 per cent were Polish, while 14.3 per cent were of other Slavic decent and 7.8 per cent of the population were Jewish, often not speaking Polish and many opposed to the establishment of a Polish nation. The same went for the small German minority of 3.9 per cent, whom, by conducing themselves 'in manner that was at once provocative and calculated to give notice that it regarded its status in Poland, and Poland herself, as temporary', outraged the Polish majority. A long struggle between Poland and her minorities persisted throughout the interbellum period. While it has been noted that '[n]othing the Poles did was ever right, as far as [the Jews] were concerned', there was little effort on the government's side to draw the Jews into appreciated and meaningful participation of national life. On the contrary, when Poland's first president, Gabriel Narutowicz, came to be elected with support of the bloc of minority parties, he was murdered within days after his election by the Sejm. Despite the endeks' initial condemnation of the murderer, whom saw the election of a minority-dependent president as a 'symbol of shame', he was later celebrated as a Polish nationalistic hero. Generally, the antagonistic sense existed, in which when the minorities, with the little representation they had in parliament, exercised their rightful power, it was seen as invalid and unjustified by a wide range of centre-right and rightist Polonocentric parties, thus blocking the way for meaningful political participation of the minorities. Crampton, Eastern Europe, p. 41. Peter D. Stachura, 'National Identity and the Ethnic Minorities in Early Inter-War Poland' in Stachura, Peter D. (ed. ), Poland between the Wars, 1918-1939 (New York, 1998), p. 71. Ibid, p. 77. Hans Roos, A History of Modern Poland. From the Foundation of the State in the First World War to the Present Day (London, 1966), p. 103. Ibid, p. 100. While, to some degree, all political factions agreed with Piłsudski's insistence on 'complete independence for Poland, [...] sceptical of the value of co-operation with any Russians', little was done to involve the Ukranian and Belorussian minorities in decision-making of their lot, even though Poland's border with Russia came to run 'far to the east of that originally suggested by Lord Curzon' during the Versailles conference. Piłsudski's ideal of a federal model incorporating largely autonomous minority factions were soon defeated; instead there existed 'acrimony and distrust, a sort of internal war'. Moreover, while the minorities did little to contribute to a 'modus vivendi', a sense of community spirit, which might have brought more peaceful social relations, many Poles themselves were unrealistic and intolerant about the formal and informal shape Polish society must take. This had not only racial, but also religious, economic and political reasons. Pre-war conditions of many Poles created a resentful attitude toward minorities and this made it hard to deal with the minorities in a fair way. Seton-Watson, The East European Revolution, p. 37. Crampton, Eastern Europe in the Twentieth Century, p. 39. Stachura, 'National Identity and the Ethnic Minorities', p. 78. Ibid, p. 78. Polonsky, Politics in Independent Poland, p. 100. Politically and economically antagonistic views were from the onset a mutually enforcing problem, which caused great obstacles for the creation of a healthily functioning government. Additionally, the multitude of Polish independence parties which had formerly existed, maintained their air of secrecy and 'conspiratorial character' even when government bodies had been created, which had disastrous results in the conflict between Dmowski and Piłsudski, which took its toll on Poland's national interest. The passing of an overly liberal constitution based on that of France at the end of the Polish-Russian war in 1921 resulted in extreme factionism; by 1926, there existed more than 90 parties, of which around 30 parties were at times represented in the Sejm. As parliament held most power within the political structure, governments found it very difficult to pass reforms. The economy, in particular, proved a disastrously antagonising force. Extreme inflation, such as in Warsaw, where food prices with a unitary value of ""100"" in January 1921 rose to ""1298.2"" in December 1922, brought several governments to fall, while rising unemployment caused the Polish Socialist Party, the PPS, to organise strikes throughout 1924 and 1925. Dmowski and his endecja became increasingly entangled in their bid for power. While land reform was the way out for a country too poor to introduce any significantly contributing industrialisation, the National Democrats, with many landowning members, were reluctant to land redistribution of any real importance, which in turn caused the anomosity of the peasant parties. This, in all, caused land reforms to be rather ineffective and certainly insufficient. A sense of lingering problems thus left a general discontent throughout all layers of society. Ibid, p. 99. Stachura, 'The Battle of Warsaw', p. 52. Stachura, 'Historiographical Outline', p. 5. Polonsky, Politics in Independent Poland, 1921-1939, p. 105. Roos, A History of Modern Poland, p. 109. Crampton, Eastern Europe, p. 45-6. Roos, A History of Modern Poland, p. 105-7. Additional to internal problems, came the issue of Poland's national security. Weimar Germany continued a revisionist policy of regaining its former Prussian areas and southeastern Silesia from Poland. This caused a particular as Poland had no natural defensive barrier and the western powers, although they ratified Poland's borders to the east,  took little affirmative action against Germany's revisionist campaign, which lent its support to Germans in Poznania and Pomerania in a number of ways, thus destabilising the nation through what was essentially a very small percentage of its population. Rothschild, Return to Diversity, p. 9. Crampton, Eastern Europe, p. 40. Stachura, 'National Identity and the Ethnic Minorities', p. 71. Another form of support to the new independent state which was pressingly lacking was financial aid. Having half of its roads, railways and public buildings destroyed during the Great War, and a continuing expenditure of over 50 per cent of the state budget on defense and the railway system, the government lacked the essential funds to modernise industry and thus create employment. The absence of foreign loans and apparent disinterest in creating political stability from the western powers might have done much to help Poland establish a parliamentary democratic routine. Thus, when the British foreign office sneeringly contradicted the Soviet Union's allegation of involvement in Piłsudksi's coup d'etat in a telegram to its ambassador in Moscow, arguing that '[the Russians] are of less consequence to us than they suppose', this revealed much of its indifferent attitude towards Polish politics as well. Roos, A History of Modern Poland, p. 107. Ibid, p. 105. Polonsky, Politics in Independent Poland, p. 515. The exigent situation that Poland was in did not go unnoticed. Piłsudksi, embittered by the course of events since 1921, grew increasingly convinced that Polish 'parliamentarianism had [...] opened the door to general corruption'. But not just Piłsudksi was displeased with the situation. Much of the PPS-cadre, frustrated with years of ineffective government and endek domination, now turned their hopes to Piłsudksi, which prompted PPS theorist Feliks Perl to foreshadowingly argue in 1924: 'He cannot be made into a left-wing Mussolini'. PPS leader Daszyński argued that political immaturity of the state was the main cause for political feuding, but, in the beginning of 1926, also urged for immediate constitutional reforms empowering the presidential post in order to contain parliamentary quarreling. Roos, A History of Modern Poland, p. 110. Ibid, p. 111. Polonsky, Politics in Independent Poland, p. 147. Not just the opposition of the left had their second thoughts about the functioning of the state. In December 1925, Dmowski wrote 'if we could create even half the organization like the Fascists... I would willingly agree to a dictatorship in Poland'. Discontents with the democratic procedure and democracy as such proved different degrees of the same slippery slope, as it soon turned out. University students radicalised under the prospect of unemployment and resented the apparent greater success of their Jewish peers; they formed the leadership of fascist movements that were to increase their power-base throughout the 1930s. Antony Polonsky, Politics in Independent Poland, 1921-1939. The Crisis of Constitutional Government (Oxford, 1972), p. 149. Seton-Watson, The East European Revolution, p. 44. Overwhelmed by problems of such dimensions, historians have argued, 'some action [...] seems in retrospect to have been almost inevitable' within the Polish political landscape. This seems, however, an unlikely viewpoint and it is worthwhile considering what other course of events politics might have taken. An internal conflict in Piłsudski's course of action appears quite clearly, as when in November 1925, when Piłsudski is requested to take over the rule of the country by factions of the army. He claimed he 'would never be a cause of trouble and dissention in the State'. It seems, from this, to have been rather a passionate, naturalistic sense of justice that drove him to carry out the coup d'etat than a positivistic, legal course of action. Already earlier in the twenties had Piłsudski shown this duality in his character. By declining a bid for the presidency, he proudly refused 'to be enclosed in a 'gilded cage'', while apparently seeing no value in attempting reforms through coalition in the Sejm. Rather than acknowledging his objections to National Democratic domination, he slid further back into the trappings of ""moral dictatorship"", hereby only united the body politic in their opposition against his rule. Thus, the peasant parties united in the Sejm to resist political persecution, while his own rule alienated moderate supporters of his ""parliamentary dictatorship"". While open confrontation with the initially supportive socialists became ever more discernible, Piłsudski still drew a Hobbesian legitimacy from the backing of the Sejm's members, which became, however, increasingly difficult to forge. When, in the end of 1929, he personally came to reopen the Sejm accompanied by a hundred armed officers, Daszyński refused to open the session 'under the threat of bayonets, guns and sabres', Piłsudski abstained from openly breaching the constitution by forcefully opening the session. The failure of Piłsudski's coup must thus been assessed within his own framework of legitimacy, which was to ""cleanse"" the parliamentary body politic. 'It is the basis of democracy, for which there is always place in Poland', he still argued in 1926. By late 1930, a number of oppositionist politicians were arrested, held captive and tortured. When eventually a number of politicians chose exile rather than imprisonment, Piłsudski was as far as ever from his ideal of a federalist republic. His cabinets governed for months 'simpy by repeatedly adjourning the Sejm'. Growing older, Piłsudski became an increasingly mistrustful statesman, believing in the last months before his death that 'the nation had reached real unity at last'. In reality, the new 1935 Constitution 'formally put an end to democracy', calling in a period known as the ""rule of the colonels"" which incorporated a mixture of nationalism and fascism. Polonsky, Politics in Independent Poland, p. 100. Piłsudska, Pilsudski, p. 329. Roos, A History of Modern Poland, p. 103. Seton-Watson, The East European Revolution, p. 32. Joshua Cohen, 'Protection for Obedience', MIT,  URL :P_SGvaU6aDYJ:web.mit.edu/polisci/research/cohen/protection_for_obediance.pdf+cohen+obedience+and+protection&hl=nl&ie=UTF-8 (30 October 2005) Roos, A History of Modern Poland, p. 119. Polonsky, Politics in Independent Poland, p. 189. Crampton, Eastern Europe, pp 50-1. Roos, A History of Modern Poland, p. 120. Ibid, p. 114. Piłsudska, Pilsudski, p. 341. Seton-Watson, The East European Revolution, p. 47. Crampton, Eastern Europe, p. 53. By carrying out a coup d'etat, Piłsudski essentially interfered forcefully with the democratic institutions. While initially hesitant, the sanacja's breeches of the constitution and civil rights were increasingly necessary for Piłsudski to maintain power. This, however, had detrimental effects on the democratic institutions and increasingly alienated many parts of society from his government. Being insufficiently successful in curbing further economic regression into the 1930s and failing to integrate increasingly militant groups of minorities, Piłsudski's failures played in the hand of the right, which made a comeback in radical form, while his sanacja leadership was increasingly chosen from a clique of military cronies. Piłsudski's coup d'etat of 1926 proved, thus, not only to be ineffective on theoretical grounds, but also on practical grounds, eventually breaking with Piłsudski's personal visions on good statesmanship. Polonsky, Politics in Independent Poland, p. 280. Crampton, Eastern Europe, p. 51.",False
20,"The writing of Simmel on the discipline of sociology is characterised by a desire to reduce the study of society to its essence. Instead of viewing society as Durkheim did, that is, as a generic force working externally of the individual and reifying itself through organisational structures and artefacts, Simmel was rather concerned with giving a 'serious [...]analysis of the social bond' (Nisbit 1959: 479), which can be described as the reciprocally working relation between two or more social actors. He denounced the view that a society ""as such"" could exist outside individuals (Simmel 1971a: 27), preferring to build a sociological system from the individual's standpoint. In the first place, Simmel represented social interaction as the dynamic attempt to reconcile the opposite drives within each human being - 'for example, the individual and the group, the need for conformity and for individuation, the need for stability and for flexibility' (Abel 1959: 478). Despite the theoretical character of these opposites, Simmel believed all ""forms-of-sociation"" are to some extent conditioned by the tension between them. Secondly, Simmel sought to separate the forms these conflicting ways of behaving could take, and their socio-historically dependent content. The form thereby attains a certain ""objective"" quality, acting as a generally discernable trait in a variety of social circumstances. As an illustration of this position, Simmel considered the phenomenon of secrecy to be a social form which could, therefore, not be judged as such, as it 'stands in neutrality above the value functions of its content' (Simmel 1950: 331). Furthermore, the greater the number of links through sociation, the more the social sphere becomes ""society"". Critics have recognised Simmel's breach with the nineteenth century approach of ""doing"" sociology: constructing a model of society and then coming up with solutions to its problems. It has been argued that this way of theorising society created laws and principles 'too vague and too sweeping to be applicable in practical circumstances' (Abel 1959: 475). Hence, it was exactly 'the microsociological character' of Simmel's approach to sociology which won his writing its original quality over other sociologists (Nisbit 1959: 480). The extent of strict reductionism in Simmel's writing, however, must not be exaggerated. Simmel did recognise that, while only forms, forms-of-sociation could take different shapes (contents), with, at times, such strong social repercussions that the social sphere acquired a nearly metaphysical quality. Consequently, he wrote of keeping a secret that it 'produces an immense enlargement of life: [...] the possibility of a second world alongside the manifest world; and the latter is decisively influenced by the former' (Simmel 1950: 330). Much of Simmel's writing on approaches to sociological studies is related to the role of abstraction, and its balance in relation to concrete situations. He stressed the value of a division between a broader, psychological approach to social drives, and the shapes these drives take in interacting with others, whom allow or resist these drives in varying degrees, and simultaneously in their turn attempt to give shape to their personal drives. Hence, 'the further development of every relation is determined by the ratio of persevering and yielding energies which are contained in the relation' (Simmel 1950: 334). Because of his view of society as the result of the constant reciprocal interaction and mutual pressure of concretely realised social forms Simmel is widely recognised as primarily 'a sociologist of forms' (Nisbit 1959: 479). The role of forms-of-sociation, however, was of even greater importance to him. In Simmel's writings on how sociology ought to approach the social question, he proposed that the detachment of the intention of a social action (form) from the actual deed was the fundamental solution to gain a greater insight in the working of the social sphere. Indeed, Simmel went even further and proposed that '[t]o separate, by scientific abstraction, these two factors of form and content which are in reality inseparably united; [...] seems to me the basis for the only, as well as the entire, possibility of a special science of society as such' (Simmel 1971a: 25). To illustrate the position of the sociologist within such a framework of thinking, Simmel drew an analogy between society and language. While both convey a message, they express this in particular way, embedded in the context of their users. This particularity, in both cases, is conditioned by economical standing, and historical and cultural forces. Still, we can trace a binding element between all users across time and social position. The shape the social takes is conditioned by the intention that the individual may have. Thus, social forms are like grammar, which 'isolates the forms of languages from their contents' beyond the 'heterogeneity of content and purposes which life reveals' (Nisbit 1959: 480), while the sociologist attempts to atomise and describe this ""social grammar"". The separation of form and content automatically assumes the individual as the basis for a social interaction. It thereby recognises the role of psychology in determining the origins of social behaviour (Simmel 1971a). Simmel was, however, much concerned with defining the proper territory of sociology, arguing that 'we are not interested in the psychological processes that occur in each of the two individuals but in their subsumption under the categories of union and discord' (Simmel 1971a: 34). All we can properly and legitimately call society is 'a mode of existence which concretely expresses itself in the reciprocal relations of human beings' (Abel 1959: 475). The social grid of forms-of-sociation owes to Kant's virtue ethics in that it separates intention from action (Craib 1997: 56); Simmel, however, was not concerned with adding a value judgment but argued that the sociologist should solely attempt to determine the ""grammar"" of society by example. A point of criticism in the use of forms as social elements separate from the real may be that it creates an extent of theoretical abstraction. For Simmel, however, this practice only has a function as a theoretical exercise. 'In any given social phenomenon, content and societal form constitute on reality' (Simmel 1971a: 24), he argued; the separation of form is to serve as a tool for interpretation of case study. In fact, in this, Simmel went even further than Weber, whom, with his use of the ""ideal types"" provided the two theoretical ends of a scale on which a particular society can then be placed. Simmel, on the contrary, insisted that the use of forms must always relate to 'a piece of history, either past or present' (Abel 1959: 479). As a result, all Simmelian sociological concepts such as 'status, Gemeinschaft, relative deprivation, and solidarity' maintain a degree concreteness despite their traits of universality (Abel 1959: 478). Due to the position, moreover, that '[f]orm and content cannot, in practice, be separated' (Nisbit 1959: 480), much of Simmel's writing did in fact not maintain such a theoretical strictness. He recognised this fact, but saw this rather as a natural process of knowledge-gathering, which, in sociology, can only be done in concrete terms: '[t]hrough [...] a study, of course, these forms would lose in applicability what they would gain in definiteness' (Simmel 1971a: 28). The use of the term ""form"" must, therefore, not be seen as an equivalent of ""social law"", since a conception of law may exist independently from the individual, while ""form"" must always be filled in by ""content"", like a geometrical shape needs a material to exist at all (Simmel 1971a). Another point of criticism that has been brought to the fore, is the way in which sociology, in Simmel's conception, occupies a place merely as the study of the microsocial, which comes out of the distinction between and form and content as the only purely human element in social relations (Abel 1959: 476). It is, furthermore, a point of inconsistency between Simmel's writing on what sociological studies should consist of, and his actual sociological treatises. For example, when Simmel describes the social form ""secrecy"" as 'a formal means of boasting and of subordinating the others' (Simmel 1950: 332), this definition is much narrower than his further expansion on its role in associations, and the physical-spatial element that secrecy often involves. Hence, it can be said that the Simmel's descriptions of the content often go beyond what he himself characterised as the elements that make up a society. By focussing on the microsocial element of society, Simmel failed to appropriate the external force of society over the individual, arguing, contrarily to Durkheim, that 'with every growth of new synthesising phenomena [...], the same group becomes ""more society"" than it was before' (Simmel 1971a: 27); that is - society is the sum-total of all microsocial interaction. Since, as mentioned above, 'the investigation of forms-of-sociation, requested by Simmel, involves the determination of their functions with regard to social life' (Abel 1959: 478), it may be of value to assess Simmel's application of the use of forms in one of his own investigations. In his essay 'The Metropolis and Mental Life', Simmel draws a parallel with the emergence of conglomerations of people in the metropolis on the one hand, and the coming into existence of new ways of social interaction or the absence of interaction due to a growth of individualism (Simmel 1971b: 325). In this model, the metropolis may be seen as the expression, or content, of the various forms-of-sociation that we associate with modernity and the emergence of individuality. Thence, life in the metropolis sees a dual, antagonistic development: on the one hand, the vast increase of daily moments of interaction between people, while on the other hand, individuals 'retreat from excessive external stimuli [which are social content]' (Frisby 1992: 66). According to Simmel, the foremost conflict in metropolitan society can be observed in the tension between the individual, rationalistic, and the weight of the continuing flux of impressions upon the emotive, subjective side of each person. This creates two types of extreme social reactions. On the one hand, '[t]he purely intellectualistic person [who] is indifferent to all things personal' (Simmel 1971b: 326) becomes wholly disinterested and impatient in interaction with others. For him, or her, '[t]he meaning and value of the distinction between things, and therewith of the things themselves, are experienced as meaningless' (Simmel 1971b: 330), solely out of a vast overflow of impressions in everyday life. On the other hand, maintaining a superficial, rational approach toward others may result in nervous breakdowns and agoraphobia, which Simmel regarded as 'pathological characteristics of modern cities' (Vidler 1991: 36). This view, despite providing an interesting model for the social effects of modern life, however shows some ambiguities in definition and theoretical inconsistency, while it steps over a few essential elements without touching upon them. For, when Simmel remarks that 'the metropolis [...] has outgrown every personal element' (Vidler 1991: 38), he does not satisfactorily substantiate this view in the terms that he set out as sociological tools. It remains unclear, for example whether the metropolis sees an ""increase"" or ""decrease"" of the ""thing"" society. For, while numbers of contact moments, which are essentially opportunities to add content to form vastly increase, they simultaneously become infinitely more fragmented, single-minded and disinterested. To add to this problem of definition, it can be argued that each individual 'derives [these] values and improvements from this collective' (Wolff 1950: 54). Here we arrive at the fundamental complexity of form as a strictly separate theoretical concept. From Simmel, we may read that the form of 'the secret is a first-rate element of individualization' (Simmel 1950: 334), while principally it is a social form which, despite the fact that it must, per definition, involve more than one social actor, on the other hand consist of not sharing something with others. This makes the distinction between the individual and the social a very troubled one, from which one can conclude that 'a person is never merely a collective being just as he is never merely an individual being' (Levine 1971: 261). As '[t]he social is determined, but it is also determining' (Abel 1959: 476), it is necessary to provide a strong definition of the forms-of-sociation before anything can be said of its relation to forms. While people engage in relationships with each other 'to satisfy their mutual and individual needs' (Craib 1997: 56), this does, however, not provide us with a very firm tool to determine the every-day content of such forms: we might think of the relationship between two individuals and something as abstract and indirect as the state. Moreover, by leaving out physical aspects of a model of society, Simmel narrows the understanding that sociology may generate of society. Thus, while Simmel acknowledges that '[s]ome social forms, such as the state, manifest themselves in a unique and localized space that excludes the possibility of forms inhabiting the same space' (Vidler 1991: 39), he fails to appropriate this fact in a societal sense. Yet, when a person gets a prison sentence, this is not the direct result of his interaction with merely the judge, but rather does this spring from a relation within a more collective set of concepts such as ""the law"" and ""the state"". Interpreting this example in purely microsocial, individual relations means missing the impersonal and continuous aspects of social organisation. In conclusion, the separation between form and content is important in that it stresses 'the ""humanistic coefficient""' (Abel 1959: 479) in social relations. Whereas it allows a certain degree of generalisation of human drives within a heterogeneity of social circumstances, Simmel's theory stresses that we should not lose the 'element of irriducible humanism' (Nisbit 1959: 481) in society. It also stresses the direct reciprocal influence of individuals over each other in every-day social relations: '[t]he significance of [...] interactions among men lies in the fact that it is because of them that the individuals, in whom these driving impulses and purposes are lodged, form a unity, that is, a society.' (Simmel 1971a: 23). Thus, Simmel's reductionist sociology reveals a certain immediacy and existentialism to it. This approach to sociology, however, brings up a number of problems which Simmel sought to address by simply placing all socio-historical biases and influences inside the person-to-person relationships which gave content to universal, timeless forms. This brings about an antagonism in Simmel's system of thought, in that it reveals a disparity between the apparent behaviour (or action) of an individual's reaction to another, while this particular social form can only be analysed in a wider, macrosocial context. Especially more continuous, bigger forms of organisation have a greater sense of impersonality to them, making the underlying form (essentially a drive springing from individual actors) hard to define and ambiguous. The separation of form and content is thus a very useful too in the study of microsocial relations, but cannot be seen to contend any notion of a ""society in its entirety"".","In negotiating the post-war political landscape of Eastern Europe, the western powers revealed an ambiguous attitude. Although historians still disagree on whether diplomatic misunderstandings between the allies of the east and west eventually caused the emergence of a cold war, or whether a gradual loss of influence in Eastern European countries was the reason for its outbreak, the half-hearted commitment to their self-proclaimed defense of liberal democratic principles such as democracy and regional stability prompted disillusioned contemporaries to see the allied settlement of Eastern Europe as again one of unprincipled compromise and appeasement. As former Polish Prime Minister in exile Mikołajczyk wrote a few years later: 'we were sacrificed by our allies, the United States and Great Britain'. R.J. Crampton, Eastern Europe in the Twentieth Century - And After (London/New York, 2005), p. 211. Stanisław Mikołajczyk, The Rape of Poland. Patterns of Soviet Agression (New York, 1973), p. viii. Nonetheless, the United States had full intentions to prevent the re-emergence of any great power on the European continent. The reasons for the western allies' failure to prevent the communisation of Eastern Europe must be sought in the complex and antagonistic diplomatic relations between the two camps of the grand alliance. Whereas the Soviets mistrusted Anglo-American intentions, Roosevelt believed until the end in the possibility of a peaceful settlement with regard to Soviet claims to ""national security"" in Eastern Europe. By pursuing this policy, Roosevelt downplayed the ideological difficulties underlying the matter. Optimistically, he believed in the compatibility of free and democratic elections in the Soviet Union's neighbouring states, and the natural emergence of 'governments which would maintain friendly relations with the Soviet Union'. The incompatibility of these two principles caused the Americans to deal with the ideological justification of a possible settlement in somewhat abstract terms, believing a favourable outcome would eventually justify such agreements as the Declaration on Liberated Europe made with the Soviets. Roosevelt's underestimation of Soviet expansionism ultimately rendered an agreement on Poland made at the Yalta conference 'so elastic that the Russians can stretch it all the way from Yalta to Washington without technically breaking it'. John Lewis Gaddis, We Know Now. Rethinking Cold War History (Oxford/New York, 1998), p. 12. Ibid, p. 23. Martin McCauley, The Origins of the Cold War 1941-1949 (London/New York, 1995), p. 54. Caroline Kennedy-Pipe, Stalin's Cold War. Soviet strategies in Europe, 1943 to 1956 (Manchester/New York, 1995), pp. 49-50. Walter Lafeber, America, Russia, and the Cold War, 1945-2002 (Boston, 2004), p. 13. Britain, too, found itself in a difficult moral position between its mid-war pledges to the Polish government that '""His Majesty's Government do not recognize any territorial changes which have been effected in Poland since August, 1939""', and the maintenance of allied unity, which eventually involved some concessions to the Soviet Union as to territorial and political claims in Poland. The Soviet's insistence on territorial expansion in East Prussia, Bessarabia, northern Bukovina and Ruthenia, besides the incorporation of eastern Poland up to the Curzon line were thereafter verified at Yalta in February 1945 and at Potsdam in July of that year. Most likely, Churcill's infamous percentages deal with Stalin in October 1944 only reaffirmed Stalin's conviction that he was allowed a free hand in Eastern European territories based merely on unprincipled Realpolitik. However, opposition to these concessions grew. Mikołajczyk, who had all along protested 'that we were only asking for a return to the prewar status quo' resigned, while in the United States Senator Arthur Vandenberg, in 1945, 'looked at the Soviet Union through the lens of Poland and did not like what he saw'. Thus, the Soviet Union and the western powers gradually maneuvered into a bipolar power structure. Mikołajczyk, The Rape of Poland, p. 16-7. Kennedy-Pipe, Stalin's Cold War, p. 39. Lewis Gaddis, We Know Now, p. 16. Crampton, Eastern Europe, p. 212. Kennedy-Pipe, Stalin's Cold War, p. 46. Mikołajczyk, The Rape of Poland, p. 16. McCauley, The Origins of the Cold War, p. 60. Despite the apparently systematic usurpation of power in Eastern European countries by the Soviet Union, it must be borne in mind that Stalin hardly foresaw the full course of events. Often, he acted upon situations as he was confronted with them. It has, therefore, been argued that 'Stalinisation was a process, not a plan'. Intrinsically mistrustful, Stalin concealed much of his intentions, convincing the Americans for a long time that the Soviet Union subscribed to a free, unified Europe. The disparities between open and concealed policy at times took immense and grotesque proportions. Hence, when the Nazis, in 1943, unveiled a mass grave of 15,000 Polish officers at Katyn, murdered upon Stalin's orders, he denied all allegation and seized the opportunity to break diplomatic relations with the Polish government-in-exile upon inquiries from President Sikorski. Moreover, the Soviet-sponsored Polish provisional government in Lublin was ordered to break its links with the loyalist Polish underground Home Army (AK). Crampton, Eastern Europe, p. 212. Lafeber, America, Russia, and the Cold War, p. 11. Anthony Polansky and Bolesław Drukier, The Beginnings of Communist Rule in Poland (London, 1980), p. 8. Acting from a conviction that any liberated territory immediately became of concern solely for the liberating nation, Moscow eventually resorted to unilateral action to reinforce its sphere of influence. However, '[t]he brutality and cynicism with which [Stalin] handled these matters did more than anything to exhaust the goodwill the Soviet war effort had accumulated in the West'. The replacement of the Radescu government in Romania by the Soviet-supported, communist Groza government, but more strongly, ongoing practices to consolidate Soviet power in Poland in the beginning of 1945 appeared 'to London and Washington deliberately to violate the Yalta agreement'. Cynicism about the possibility of a settlement with Moscow eventually caused the alliance to break before the end the war. Lewis Gaddis, We Know Now, p. 14. McCauley, The Origins of the Cold War, p. 58. Lewis Gaddis, We Know Now, p. 19. Kennedy-Pipe, Stalin's Cold War, p. 57. McCauley, The Origins of the Cold War, p. 57. Stalin's interpretation of 'governmental authorities broadly representative of all democratic elements' was, from the western viewpoint, indeed a curious one. Toward the end of the war, Moscow, defying the ""London Poles"", backed the establishment of a new interim government, while between July 1944 and July 1945, Stalin started to build for Poland 'a security apparatus which was already showing disturbing signs of independence from the Polish government and which was, in fact, effectively controlled by the Soviet 'advisers' within it'. Kennedy-Pipe, Stalin's Cold War, p. 49. Polansky and Drukier, The Beginnings, p. 10. Ibid, p. 2. In the middle of 1944, however, the Lublin claims to governmental supremacy went by no means uncontested. The loyalist Home Army was still Poland's largest organised resistance movement, and Stalin, uncertain about the Lublin committee's loyalty, made a separate agreement with another Polish communist faction based in Moscow, the Polish Committee of National Liberation, which he, according to Mikołajczyk, 'had organised earlier as an instrument of his plans for postwar Poland'. Later that year, the balance of power shifted further in favour of the Soviet Union, by the failing Warsaw uprising, which meant 'the end of the independent Polish left-wing alternative to the Communists'. Mikołajczyk, The Rape of Poland, p. 70. Anita J. Prazmowska, 'The Soviet Liberation of Poland and the Polish Left, 1943-5', in Francesca Gori and Silvio Pons (eds. ), The Soviet Union and Europe in the Cold War, 1943-53 (London/New York, 1996), p. 84. With the crushing of 'the leadership of the historic political forces and their military infrastructure' in Poland, communisation now became only a matter of time. In March 1945 the Soviet Union signed a treaty of friendship with Lublin Committee; in the same month a delegation of sixteen prominent politicians, among whom the government delegate of the Polish government in exile, and the commander of the Polish Home Army (Armia Krajowa) were trapped in a meeting and deported to the Soviet Union to face accusations of conspiring against the Red Army. Crampton, Eastern Europe, p. 217. McCauley, The Origins of the Cold War, p. 58. Crampton, Eastern Europe, p. 218. These developments were signs of the establishment of a Polish interim government existing solely by the grace of the Soviet Union. At this time, however, Poland was still by no means a neatly communised, country, nor was it accepted to be so by the western powers. The post-war political programmes of the Lublin committee and the Polish Committee of National Liberation still revealed rather divergent political agendas. Moreover, despite a domination of Lublin committee ministers in the cabinet, the communists still saw a necessity of including oppositionists in the government. The return to Poland of Mikołajczyk in late 1944 had halted, at least for the time being, an effective total usurpation of the body politic by pro-Soviet communists. Even by 1946, Mikołajczyk's Polish Peasant Party was, with its 600,000 members, Poland's largest political party. Polansky and Drukier, The Beginnings, p. 13. Crampton, Eastern Europe, pp. 218-9. Up to this point, we have primarily looked at the situation in Poland, which, for many western politicians, had become 'the test case of Soviet intentions'. Nevertheless, the communisation of other Eastern European countries happened quite differently, depending on the internal political situation as well as the national state of affairs in the international context at the close of the war. A basis for further negotiation was laid in October 1944 with Churchill's percentages deal with Stalin, in which, among other things, the balance of influence in Yugoslavia was to be divided on a fifty-fifty basis. However, too much has been made of these negotiations. As has been pointed out, in discussions following up on these divisions, British and Soviet foreign ministers Eden and Molotov were unable to reach an eventual agreement which satisfied both demands. As mentioned before, Stalin saw communisation much as a process of opportunities, and in Yugoslavia, the influence of Moscow was 'paramount from the outset'. Paradoxically, however, it was Stalin's caution to avoid alarming the western powers that saw the construction of a compromise government, although on favourable terms for the Yugoslavian communists. In March 1945, an acting regency was agreed, consisting of 'three members, one Croat, one Serb, and one Slovene [who], though non-communists, were all nominees of Tito'. Stalin even managed to convince Churchill to add British support to Tito's provisional government, despite its earlier overt hostilities to 'the English imperialists' back in 1941. The British support helped the Yugoslavian communists to strengthen their power base at the expense of the exile government. With the help of the Red Army, they 'dispersed, disarmed, or forced the exodus of Mihailović's forces from Serbia', thus gaining access to actual power in the body politic. Moreover, 'the communists, though they might share government [in a coalition], would not share power'. Although it was now 'but a short step to the formalisation of the communist takeover', this meant by no means that this happened firmly under Stalin's direction. The Red Army had retreated from Yugoslavian territory, and Tito, though deferential to Stalin, 'had formed views of his own and had no hesitation in expressing them, even when they contradicted those of Stalin himself'. Lafeber, America, Russia, and the Cold War, p. 14. Crampton, Eastern Europe, p. 213. Stephen Clissold (ed. ), Yugoslavia and the Soviet Union 1939-1973. A documentary survey (London, New York, Toronto,1975), p. 34. Kennedy-Pipe, Stalin's Cold War, p. 46. Clissold (ed. ), Yugoslavia and the Soviet Union, p. 34. Crampton, Eastern Europe, p. 216. Clissold (ed. ), Yugoslavia and the Soviet Union, p. 35. Ivan Avakumovic, History of the Communist Party of Yugoslavia. Volume one (Aberdeen, 1967), p. 177. Clissold (ed. ), Yugoslavia and the Soviet Union, p. 40. Crampton, Eastern Europe, p. 216. Ibid., p. 217. Clissold (ed. ), Yugoslavia and the Soviet Union, p. 38. The postwar Czechoslovak dealings with the Soviet Union were quite different. Although it has been argued that 'Stalin as well as Roosevelt and Churchill miscalculated when they assumed that there could be friendly states along an expanded Soviet periphery', Czech leader in exile Beneš certainly tried, and temporarily managed, to come to some agreement with the Soviet Union. He 'spoke openly of a ""Czech solution"" that would exchange internal autonomy for Soviet control over foreign and military policy'. Beneš was not unjustified in his calculations of the postwar situation in the area. Although Churchill had repeatedly argued for a speeded advance eastwards beyond the western zone of occupation to liberate Prague and prevent Czechoslovakia from falling within the Soviet sphere of influence, general Eisenhower declined, arguing that he 'refused to risk casualties for 'purely political reasons''. Directly upon the liberation of Prague in early May, Soviet interests were secured. The communists within the country were thereafter 'in a strong position for a seizure of power', but Stalin told their leader Gottwald to '""work with Beneš, reach an agreement with him, accept him as president.""'. Fearing that an immediate communist takeover would only work counterproductively by concentrating anti-communist factions around the democratic person of Beneš, they decided to negotiate the terms of a new government on a broader base until the right time would appear to assume power. Beneš, from his side, hoped that the inclusion of communist factions in the cabinet would result in a strengthened democratic system, 'with the blessing of the Soviet Union'. Lewis Gaddis, We Know Now, p. 18. Ibid., p. 17. Kennedy-Pipe, Stalin's Cold War, p. 66. Ibid., p. 66. Crampton, Eastern Europe, p. 236. Josef Korbel, Twentieth-Century Czechoslovakia. The Meaning of Its History (New York, 1977), p. 195. Crampton, Eastern Europe, p. 236. Korbel, Twentieth-Century Czechoslovakia, p. 197. This policy turned out to be only one of buying time. Despite the relative moderateness and tolerance of Gottwald's communists, the Soviet Union continued to press for further advances to communisation. Due to an initial underestimation of Soviet interests in Czechoslovakia by the Americans, the military means of interference were lost when the communists eventually seized power in 1948 without any direct justification. Crampton, Eastern Europe, p. 237. Ibid., pp. 238-9. The various examples of Eastern European reveal a diverse range of political situations in the period running up to May 1945. In the Polish case, Stalin's hopes for a post-war accommodation with the Sikorski government were abandoned already in 1942, while in the Czechoslovak case, Beneš' immense popularity prevented the Soviets from bluntly by-passing its exiled government. The Yugoslavian situation was yet different, with a political climate in which the communist party directly and almost naturally dominated the new government. Nonetheless, Tito's power of government was not immediately and unconditionally recognised by Moscow, which is telling of Stalin's opportunism in his efforts to shape Eastern Europe into a system in which communisation dominated. Polansky and Drukier, The Beginnings, p. 5. Crampton, Eastern Europe, p. 216. Meanwhile, goodwill for Soviet intentions in the west gradually deteriorated until the alliance eventually collapsed. A turning point in Soviet diplomacy directed toward the west can be noted around March and April 1945, while an increasing number of American liberals were becoming disenchanted with the Soviet Union and its attitude. After an initial willingness to cooperate, a number of unbridgeable disagreements increasingly turned Soviet-Western relations into an ideological conflict, epitomised by Trumans words to Molotov in their first meeting, 'Carry out your agreements and you won't get talked to like that'. Kennedy-Pipe, Stalin's Cold War, p. 64. Lafeber, America, Russia, and the Cold War, p. 5. McCauley, The Origins of the Cold War, p. 61. May 1945 saw the beginnings of a realisation that Soviet unilateral activities in Eastern Europe had to be confronted. However, whilst Stalin had already been active with his communisation plans as territories were being liberated, the Truman administration still resisted the harsher anti-Soviet line held by the British around May 1945. The communisation of Eastern Europe happened step by step; yet most countries, in May 1945, faced a deteriorating, yet not hopeless democratic climate, with strong oppositions counterweighing against communist aspirations. The late recognition and reluctance to take responsibility of the Americans may thus have been the deciding factor in the eventual communisation of the countries in Eastern Europe. Kennedy-Pipe, Stalin's Cold War, p. 63.",True
21,"In negotiating the post-war political landscape of Eastern Europe, the western powers revealed an ambiguous attitude. Although historians still disagree on whether diplomatic misunderstandings between the allies of the east and west eventually caused the emergence of a cold war, or whether a gradual loss of influence in Eastern European countries was the reason for its outbreak, the half-hearted commitment to their self-proclaimed defense of liberal democratic principles such as democracy and regional stability prompted disillusioned contemporaries to see the allied settlement of Eastern Europe as again one of unprincipled compromise and appeasement. As former Polish Prime Minister in exile Mikołajczyk wrote a few years later: 'we were sacrificed by our allies, the United States and Great Britain'. R.J. Crampton, Eastern Europe in the Twentieth Century - And After (London/New York, 2005), p. 211. Stanisław Mikołajczyk, The Rape of Poland. Patterns of Soviet Agression (New York, 1973), p. viii. Nonetheless, the United States had full intentions to prevent the re-emergence of any great power on the European continent. The reasons for the western allies' failure to prevent the communisation of Eastern Europe must be sought in the complex and antagonistic diplomatic relations between the two camps of the grand alliance. Whereas the Soviets mistrusted Anglo-American intentions, Roosevelt believed until the end in the possibility of a peaceful settlement with regard to Soviet claims to ""national security"" in Eastern Europe. By pursuing this policy, Roosevelt downplayed the ideological difficulties underlying the matter. Optimistically, he believed in the compatibility of free and democratic elections in the Soviet Union's neighbouring states, and the natural emergence of 'governments which would maintain friendly relations with the Soviet Union'. The incompatibility of these two principles caused the Americans to deal with the ideological justification of a possible settlement in somewhat abstract terms, believing a favourable outcome would eventually justify such agreements as the Declaration on Liberated Europe made with the Soviets. Roosevelt's underestimation of Soviet expansionism ultimately rendered an agreement on Poland made at the Yalta conference 'so elastic that the Russians can stretch it all the way from Yalta to Washington without technically breaking it'. John Lewis Gaddis, We Know Now. Rethinking Cold War History (Oxford/New York, 1998), p. 12. Ibid, p. 23. Martin McCauley, The Origins of the Cold War 1941-1949 (London/New York, 1995), p. 54. Caroline Kennedy-Pipe, Stalin's Cold War. Soviet strategies in Europe, 1943 to 1956 (Manchester/New York, 1995), pp. 49-50. Walter Lafeber, America, Russia, and the Cold War, 1945-2002 (Boston, 2004), p. 13. Britain, too, found itself in a difficult moral position between its mid-war pledges to the Polish government that '""His Majesty's Government do not recognize any territorial changes which have been effected in Poland since August, 1939""', and the maintenance of allied unity, which eventually involved some concessions to the Soviet Union as to territorial and political claims in Poland. The Soviet's insistence on territorial expansion in East Prussia, Bessarabia, northern Bukovina and Ruthenia, besides the incorporation of eastern Poland up to the Curzon line were thereafter verified at Yalta in February 1945 and at Potsdam in July of that year. Most likely, Churcill's infamous percentages deal with Stalin in October 1944 only reaffirmed Stalin's conviction that he was allowed a free hand in Eastern European territories based merely on unprincipled Realpolitik. However, opposition to these concessions grew. Mikołajczyk, who had all along protested 'that we were only asking for a return to the prewar status quo' resigned, while in the United States Senator Arthur Vandenberg, in 1945, 'looked at the Soviet Union through the lens of Poland and did not like what he saw'. Thus, the Soviet Union and the western powers gradually maneuvered into a bipolar power structure. Mikołajczyk, The Rape of Poland, p. 16-7. Kennedy-Pipe, Stalin's Cold War, p. 39. Lewis Gaddis, We Know Now, p. 16. Crampton, Eastern Europe, p. 212. Kennedy-Pipe, Stalin's Cold War, p. 46. Mikołajczyk, The Rape of Poland, p. 16. McCauley, The Origins of the Cold War, p. 60. Despite the apparently systematic usurpation of power in Eastern European countries by the Soviet Union, it must be borne in mind that Stalin hardly foresaw the full course of events. Often, he acted upon situations as he was confronted with them. It has, therefore, been argued that 'Stalinisation was a process, not a plan'. Intrinsically mistrustful, Stalin concealed much of his intentions, convincing the Americans for a long time that the Soviet Union subscribed to a free, unified Europe. The disparities between open and concealed policy at times took immense and grotesque proportions. Hence, when the Nazis, in 1943, unveiled a mass grave of 15,000 Polish officers at Katyn, murdered upon Stalin's orders, he denied all allegation and seized the opportunity to break diplomatic relations with the Polish government-in-exile upon inquiries from President Sikorski. Moreover, the Soviet-sponsored Polish provisional government in Lublin was ordered to break its links with the loyalist Polish underground Home Army (AK). Crampton, Eastern Europe, p. 212. Lafeber, America, Russia, and the Cold War, p. 11. Anthony Polansky and Bolesław Drukier, The Beginnings of Communist Rule in Poland (London, 1980), p. 8. Acting from a conviction that any liberated territory immediately became of concern solely for the liberating nation, Moscow eventually resorted to unilateral action to reinforce its sphere of influence. However, '[t]he brutality and cynicism with which [Stalin] handled these matters did more than anything to exhaust the goodwill the Soviet war effort had accumulated in the West'. The replacement of the Radescu government in Romania by the Soviet-supported, communist Groza government, but more strongly, ongoing practices to consolidate Soviet power in Poland in the beginning of 1945 appeared 'to London and Washington deliberately to violate the Yalta agreement'. Cynicism about the possibility of a settlement with Moscow eventually caused the alliance to break before the end the war. Lewis Gaddis, We Know Now, p. 14. McCauley, The Origins of the Cold War, p. 58. Lewis Gaddis, We Know Now, p. 19. Kennedy-Pipe, Stalin's Cold War, p. 57. McCauley, The Origins of the Cold War, p. 57. Stalin's interpretation of 'governmental authorities broadly representative of all democratic elements' was, from the western viewpoint, indeed a curious one. Toward the end of the war, Moscow, defying the ""London Poles"", backed the establishment of a new interim government, while between July 1944 and July 1945, Stalin started to build for Poland 'a security apparatus which was already showing disturbing signs of independence from the Polish government and which was, in fact, effectively controlled by the Soviet 'advisers' within it'. Kennedy-Pipe, Stalin's Cold War, p. 49. Polansky and Drukier, The Beginnings, p. 10. Ibid, p. 2. In the middle of 1944, however, the Lublin claims to governmental supremacy went by no means uncontested. The loyalist Home Army was still Poland's largest organised resistance movement, and Stalin, uncertain about the Lublin committee's loyalty, made a separate agreement with another Polish communist faction based in Moscow, the Polish Committee of National Liberation, which he, according to Mikołajczyk, 'had organised earlier as an instrument of his plans for postwar Poland'. Later that year, the balance of power shifted further in favour of the Soviet Union, by the failing Warsaw uprising, which meant 'the end of the independent Polish left-wing alternative to the Communists'. Mikołajczyk, The Rape of Poland, p. 70. Anita J. Prazmowska, 'The Soviet Liberation of Poland and the Polish Left, 1943-5', in Francesca Gori and Silvio Pons (eds. ), The Soviet Union and Europe in the Cold War, 1943-53 (London/New York, 1996), p. 84. With the crushing of 'the leadership of the historic political forces and their military infrastructure' in Poland, communisation now became only a matter of time. In March 1945 the Soviet Union signed a treaty of friendship with Lublin Committee; in the same month a delegation of sixteen prominent politicians, among whom the government delegate of the Polish government in exile, and the commander of the Polish Home Army (Armia Krajowa) were trapped in a meeting and deported to the Soviet Union to face accusations of conspiring against the Red Army. Crampton, Eastern Europe, p. 217. McCauley, The Origins of the Cold War, p. 58. Crampton, Eastern Europe, p. 218. These developments were signs of the establishment of a Polish interim government existing solely by the grace of the Soviet Union. At this time, however, Poland was still by no means a neatly communised, country, nor was it accepted to be so by the western powers. The post-war political programmes of the Lublin committee and the Polish Committee of National Liberation still revealed rather divergent political agendas. Moreover, despite a domination of Lublin committee ministers in the cabinet, the communists still saw a necessity of including oppositionists in the government. The return to Poland of Mikołajczyk in late 1944 had halted, at least for the time being, an effective total usurpation of the body politic by pro-Soviet communists. Even by 1946, Mikołajczyk's Polish Peasant Party was, with its 600,000 members, Poland's largest political party. Polansky and Drukier, The Beginnings, p. 13. Crampton, Eastern Europe, pp. 218-9. Up to this point, we have primarily looked at the situation in Poland, which, for many western politicians, had become 'the test case of Soviet intentions'. Nevertheless, the communisation of other Eastern European countries happened quite differently, depending on the internal political situation as well as the national state of affairs in the international context at the close of the war. A basis for further negotiation was laid in October 1944 with Churchill's percentages deal with Stalin, in which, among other things, the balance of influence in Yugoslavia was to be divided on a fifty-fifty basis. However, too much has been made of these negotiations. As has been pointed out, in discussions following up on these divisions, British and Soviet foreign ministers Eden and Molotov were unable to reach an eventual agreement which satisfied both demands. As mentioned before, Stalin saw communisation much as a process of opportunities, and in Yugoslavia, the influence of Moscow was 'paramount from the outset'. Paradoxically, however, it was Stalin's caution to avoid alarming the western powers that saw the construction of a compromise government, although on favourable terms for the Yugoslavian communists. In March 1945, an acting regency was agreed, consisting of 'three members, one Croat, one Serb, and one Slovene [who], though non-communists, were all nominees of Tito'. Stalin even managed to convince Churchill to add British support to Tito's provisional government, despite its earlier overt hostilities to 'the English imperialists' back in 1941. The British support helped the Yugoslavian communists to strengthen their power base at the expense of the exile government. With the help of the Red Army, they 'dispersed, disarmed, or forced the exodus of Mihailović's forces from Serbia', thus gaining access to actual power in the body politic. Moreover, 'the communists, though they might share government [in a coalition], would not share power'. Although it was now 'but a short step to the formalisation of the communist takeover', this meant by no means that this happened firmly under Stalin's direction. The Red Army had retreated from Yugoslavian territory, and Tito, though deferential to Stalin, 'had formed views of his own and had no hesitation in expressing them, even when they contradicted those of Stalin himself'. Lafeber, America, Russia, and the Cold War, p. 14. Crampton, Eastern Europe, p. 213. Stephen Clissold (ed. ), Yugoslavia and the Soviet Union 1939-1973. A documentary survey (London, New York, Toronto,1975), p. 34. Kennedy-Pipe, Stalin's Cold War, p. 46. Clissold (ed. ), Yugoslavia and the Soviet Union, p. 34. Crampton, Eastern Europe, p. 216. Clissold (ed. ), Yugoslavia and the Soviet Union, p. 35. Ivan Avakumovic, History of the Communist Party of Yugoslavia. Volume one (Aberdeen, 1967), p. 177. Clissold (ed. ), Yugoslavia and the Soviet Union, p. 40. Crampton, Eastern Europe, p. 216. Ibid., p. 217. Clissold (ed. ), Yugoslavia and the Soviet Union, p. 38. The postwar Czechoslovak dealings with the Soviet Union were quite different. Although it has been argued that 'Stalin as well as Roosevelt and Churchill miscalculated when they assumed that there could be friendly states along an expanded Soviet periphery', Czech leader in exile Beneš certainly tried, and temporarily managed, to come to some agreement with the Soviet Union. He 'spoke openly of a ""Czech solution"" that would exchange internal autonomy for Soviet control over foreign and military policy'. Beneš was not unjustified in his calculations of the postwar situation in the area. Although Churchill had repeatedly argued for a speeded advance eastwards beyond the western zone of occupation to liberate Prague and prevent Czechoslovakia from falling within the Soviet sphere of influence, general Eisenhower declined, arguing that he 'refused to risk casualties for 'purely political reasons''. Directly upon the liberation of Prague in early May, Soviet interests were secured. The communists within the country were thereafter 'in a strong position for a seizure of power', but Stalin told their leader Gottwald to '""work with Beneš, reach an agreement with him, accept him as president.""'. Fearing that an immediate communist takeover would only work counterproductively by concentrating anti-communist factions around the democratic person of Beneš, they decided to negotiate the terms of a new government on a broader base until the right time would appear to assume power. Beneš, from his side, hoped that the inclusion of communist factions in the cabinet would result in a strengthened democratic system, 'with the blessing of the Soviet Union'. Lewis Gaddis, We Know Now, p. 18. Ibid., p. 17. Kennedy-Pipe, Stalin's Cold War, p. 66. Ibid., p. 66. Crampton, Eastern Europe, p. 236. Josef Korbel, Twentieth-Century Czechoslovakia. The Meaning of Its History (New York, 1977), p. 195. Crampton, Eastern Europe, p. 236. Korbel, Twentieth-Century Czechoslovakia, p. 197. This policy turned out to be only one of buying time. Despite the relative moderateness and tolerance of Gottwald's communists, the Soviet Union continued to press for further advances to communisation. Due to an initial underestimation of Soviet interests in Czechoslovakia by the Americans, the military means of interference were lost when the communists eventually seized power in 1948 without any direct justification. Crampton, Eastern Europe, p. 237. Ibid., pp. 238-9. The various examples of Eastern European reveal a diverse range of political situations in the period running up to May 1945. In the Polish case, Stalin's hopes for a post-war accommodation with the Sikorski government were abandoned already in 1942, while in the Czechoslovak case, Beneš' immense popularity prevented the Soviets from bluntly by-passing its exiled government. The Yugoslavian situation was yet different, with a political climate in which the communist party directly and almost naturally dominated the new government. Nonetheless, Tito's power of government was not immediately and unconditionally recognised by Moscow, which is telling of Stalin's opportunism in his efforts to shape Eastern Europe into a system in which communisation dominated. Polansky and Drukier, The Beginnings, p. 5. Crampton, Eastern Europe, p. 216. Meanwhile, goodwill for Soviet intentions in the west gradually deteriorated until the alliance eventually collapsed. A turning point in Soviet diplomacy directed toward the west can be noted around March and April 1945, while an increasing number of American liberals were becoming disenchanted with the Soviet Union and its attitude. After an initial willingness to cooperate, a number of unbridgeable disagreements increasingly turned Soviet-Western relations into an ideological conflict, epitomised by Trumans words to Molotov in their first meeting, 'Carry out your agreements and you won't get talked to like that'. Kennedy-Pipe, Stalin's Cold War, p. 64. Lafeber, America, Russia, and the Cold War, p. 5. McCauley, The Origins of the Cold War, p. 61. May 1945 saw the beginnings of a realisation that Soviet unilateral activities in Eastern Europe had to be confronted. However, whilst Stalin had already been active with his communisation plans as territories were being liberated, the Truman administration still resisted the harsher anti-Soviet line held by the British around May 1945. The communisation of Eastern Europe happened step by step; yet most countries, in May 1945, faced a deteriorating, yet not hopeless democratic climate, with strong oppositions counterweighing against communist aspirations. The late recognition and reluctance to take responsibility of the Americans may thus have been the deciding factor in the eventual communisation of the countries in Eastern Europe. Kennedy-Pipe, Stalin's Cold War, p. 63.","The writing of Simmel on the discipline of sociology is characterised by a desire to reduce the study of society to its essence. Instead of viewing society as Durkheim did, that is, as a generic force working externally of the individual and reifying itself through organisational structures and artefacts, Simmel was rather concerned with giving a 'serious [...]analysis of the social bond' (Nisbit 1959: 479), which can be described as the reciprocally working relation between two or more social actors. He denounced the view that a society ""as such"" could exist outside individuals (Simmel 1971a: 27), preferring to build a sociological system from the individual's standpoint. In the first place, Simmel represented social interaction as the dynamic attempt to reconcile the opposite drives within each human being - 'for example, the individual and the group, the need for conformity and for individuation, the need for stability and for flexibility' (Abel 1959: 478). Despite the theoretical character of these opposites, Simmel believed all ""forms-of-sociation"" are to some extent conditioned by the tension between them. Secondly, Simmel sought to separate the forms these conflicting ways of behaving could take, and their socio-historically dependent content. The form thereby attains a certain ""objective"" quality, acting as a generally discernable trait in a variety of social circumstances. As an illustration of this position, Simmel considered the phenomenon of secrecy to be a social form which could, therefore, not be judged as such, as it 'stands in neutrality above the value functions of its content' (Simmel 1950: 331). Furthermore, the greater the number of links through sociation, the more the social sphere becomes ""society"". Critics have recognised Simmel's breach with the nineteenth century approach of ""doing"" sociology: constructing a model of society and then coming up with solutions to its problems. It has been argued that this way of theorising society created laws and principles 'too vague and too sweeping to be applicable in practical circumstances' (Abel 1959: 475). Hence, it was exactly 'the microsociological character' of Simmel's approach to sociology which won his writing its original quality over other sociologists (Nisbit 1959: 480). The extent of strict reductionism in Simmel's writing, however, must not be exaggerated. Simmel did recognise that, while only forms, forms-of-sociation could take different shapes (contents), with, at times, such strong social repercussions that the social sphere acquired a nearly metaphysical quality. Consequently, he wrote of keeping a secret that it 'produces an immense enlargement of life: [...] the possibility of a second world alongside the manifest world; and the latter is decisively influenced by the former' (Simmel 1950: 330). Much of Simmel's writing on approaches to sociological studies is related to the role of abstraction, and its balance in relation to concrete situations. He stressed the value of a division between a broader, psychological approach to social drives, and the shapes these drives take in interacting with others, whom allow or resist these drives in varying degrees, and simultaneously in their turn attempt to give shape to their personal drives. Hence, 'the further development of every relation is determined by the ratio of persevering and yielding energies which are contained in the relation' (Simmel 1950: 334). Because of his view of society as the result of the constant reciprocal interaction and mutual pressure of concretely realised social forms Simmel is widely recognised as primarily 'a sociologist of forms' (Nisbit 1959: 479). The role of forms-of-sociation, however, was of even greater importance to him. In Simmel's writings on how sociology ought to approach the social question, he proposed that the detachment of the intention of a social action (form) from the actual deed was the fundamental solution to gain a greater insight in the working of the social sphere. Indeed, Simmel went even further and proposed that '[t]o separate, by scientific abstraction, these two factors of form and content which are in reality inseparably united; [...] seems to me the basis for the only, as well as the entire, possibility of a special science of society as such' (Simmel 1971a: 25). To illustrate the position of the sociologist within such a framework of thinking, Simmel drew an analogy between society and language. While both convey a message, they express this in particular way, embedded in the context of their users. This particularity, in both cases, is conditioned by economical standing, and historical and cultural forces. Still, we can trace a binding element between all users across time and social position. The shape the social takes is conditioned by the intention that the individual may have. Thus, social forms are like grammar, which 'isolates the forms of languages from their contents' beyond the 'heterogeneity of content and purposes which life reveals' (Nisbit 1959: 480), while the sociologist attempts to atomise and describe this ""social grammar"". The separation of form and content automatically assumes the individual as the basis for a social interaction. It thereby recognises the role of psychology in determining the origins of social behaviour (Simmel 1971a). Simmel was, however, much concerned with defining the proper territory of sociology, arguing that 'we are not interested in the psychological processes that occur in each of the two individuals but in their subsumption under the categories of union and discord' (Simmel 1971a: 34). All we can properly and legitimately call society is 'a mode of existence which concretely expresses itself in the reciprocal relations of human beings' (Abel 1959: 475). The social grid of forms-of-sociation owes to Kant's virtue ethics in that it separates intention from action (Craib 1997: 56); Simmel, however, was not concerned with adding a value judgment but argued that the sociologist should solely attempt to determine the ""grammar"" of society by example. A point of criticism in the use of forms as social elements separate from the real may be that it creates an extent of theoretical abstraction. For Simmel, however, this practice only has a function as a theoretical exercise. 'In any given social phenomenon, content and societal form constitute on reality' (Simmel 1971a: 24), he argued; the separation of form is to serve as a tool for interpretation of case study. In fact, in this, Simmel went even further than Weber, whom, with his use of the ""ideal types"" provided the two theoretical ends of a scale on which a particular society can then be placed. Simmel, on the contrary, insisted that the use of forms must always relate to 'a piece of history, either past or present' (Abel 1959: 479). As a result, all Simmelian sociological concepts such as 'status, Gemeinschaft, relative deprivation, and solidarity' maintain a degree concreteness despite their traits of universality (Abel 1959: 478). Due to the position, moreover, that '[f]orm and content cannot, in practice, be separated' (Nisbit 1959: 480), much of Simmel's writing did in fact not maintain such a theoretical strictness. He recognised this fact, but saw this rather as a natural process of knowledge-gathering, which, in sociology, can only be done in concrete terms: '[t]hrough [...] a study, of course, these forms would lose in applicability what they would gain in definiteness' (Simmel 1971a: 28). The use of the term ""form"" must, therefore, not be seen as an equivalent of ""social law"", since a conception of law may exist independently from the individual, while ""form"" must always be filled in by ""content"", like a geometrical shape needs a material to exist at all (Simmel 1971a). Another point of criticism that has been brought to the fore, is the way in which sociology, in Simmel's conception, occupies a place merely as the study of the microsocial, which comes out of the distinction between and form and content as the only purely human element in social relations (Abel 1959: 476). It is, furthermore, a point of inconsistency between Simmel's writing on what sociological studies should consist of, and his actual sociological treatises. For example, when Simmel describes the social form ""secrecy"" as 'a formal means of boasting and of subordinating the others' (Simmel 1950: 332), this definition is much narrower than his further expansion on its role in associations, and the physical-spatial element that secrecy often involves. Hence, it can be said that the Simmel's descriptions of the content often go beyond what he himself characterised as the elements that make up a society. By focussing on the microsocial element of society, Simmel failed to appropriate the external force of society over the individual, arguing, contrarily to Durkheim, that 'with every growth of new synthesising phenomena [...], the same group becomes ""more society"" than it was before' (Simmel 1971a: 27); that is - society is the sum-total of all microsocial interaction. Since, as mentioned above, 'the investigation of forms-of-sociation, requested by Simmel, involves the determination of their functions with regard to social life' (Abel 1959: 478), it may be of value to assess Simmel's application of the use of forms in one of his own investigations. In his essay 'The Metropolis and Mental Life', Simmel draws a parallel with the emergence of conglomerations of people in the metropolis on the one hand, and the coming into existence of new ways of social interaction or the absence of interaction due to a growth of individualism (Simmel 1971b: 325). In this model, the metropolis may be seen as the expression, or content, of the various forms-of-sociation that we associate with modernity and the emergence of individuality. Thence, life in the metropolis sees a dual, antagonistic development: on the one hand, the vast increase of daily moments of interaction between people, while on the other hand, individuals 'retreat from excessive external stimuli [which are social content]' (Frisby 1992: 66). According to Simmel, the foremost conflict in metropolitan society can be observed in the tension between the individual, rationalistic, and the weight of the continuing flux of impressions upon the emotive, subjective side of each person. This creates two types of extreme social reactions. On the one hand, '[t]he purely intellectualistic person [who] is indifferent to all things personal' (Simmel 1971b: 326) becomes wholly disinterested and impatient in interaction with others. For him, or her, '[t]he meaning and value of the distinction between things, and therewith of the things themselves, are experienced as meaningless' (Simmel 1971b: 330), solely out of a vast overflow of impressions in everyday life. On the other hand, maintaining a superficial, rational approach toward others may result in nervous breakdowns and agoraphobia, which Simmel regarded as 'pathological characteristics of modern cities' (Vidler 1991: 36). This view, despite providing an interesting model for the social effects of modern life, however shows some ambiguities in definition and theoretical inconsistency, while it steps over a few essential elements without touching upon them. For, when Simmel remarks that 'the metropolis [...] has outgrown every personal element' (Vidler 1991: 38), he does not satisfactorily substantiate this view in the terms that he set out as sociological tools. It remains unclear, for example whether the metropolis sees an ""increase"" or ""decrease"" of the ""thing"" society. For, while numbers of contact moments, which are essentially opportunities to add content to form vastly increase, they simultaneously become infinitely more fragmented, single-minded and disinterested. To add to this problem of definition, it can be argued that each individual 'derives [these] values and improvements from this collective' (Wolff 1950: 54). Here we arrive at the fundamental complexity of form as a strictly separate theoretical concept. From Simmel, we may read that the form of 'the secret is a first-rate element of individualization' (Simmel 1950: 334), while principally it is a social form which, despite the fact that it must, per definition, involve more than one social actor, on the other hand consist of not sharing something with others. This makes the distinction between the individual and the social a very troubled one, from which one can conclude that 'a person is never merely a collective being just as he is never merely an individual being' (Levine 1971: 261). As '[t]he social is determined, but it is also determining' (Abel 1959: 476), it is necessary to provide a strong definition of the forms-of-sociation before anything can be said of its relation to forms. While people engage in relationships with each other 'to satisfy their mutual and individual needs' (Craib 1997: 56), this does, however, not provide us with a very firm tool to determine the every-day content of such forms: we might think of the relationship between two individuals and something as abstract and indirect as the state. Moreover, by leaving out physical aspects of a model of society, Simmel narrows the understanding that sociology may generate of society. Thus, while Simmel acknowledges that '[s]ome social forms, such as the state, manifest themselves in a unique and localized space that excludes the possibility of forms inhabiting the same space' (Vidler 1991: 39), he fails to appropriate this fact in a societal sense. Yet, when a person gets a prison sentence, this is not the direct result of his interaction with merely the judge, but rather does this spring from a relation within a more collective set of concepts such as ""the law"" and ""the state"". Interpreting this example in purely microsocial, individual relations means missing the impersonal and continuous aspects of social organisation. In conclusion, the separation between form and content is important in that it stresses 'the ""humanistic coefficient""' (Abel 1959: 479) in social relations. Whereas it allows a certain degree of generalisation of human drives within a heterogeneity of social circumstances, Simmel's theory stresses that we should not lose the 'element of irriducible humanism' (Nisbit 1959: 481) in society. It also stresses the direct reciprocal influence of individuals over each other in every-day social relations: '[t]he significance of [...] interactions among men lies in the fact that it is because of them that the individuals, in whom these driving impulses and purposes are lodged, form a unity, that is, a society.' (Simmel 1971a: 23). Thus, Simmel's reductionist sociology reveals a certain immediacy and existentialism to it. This approach to sociology, however, brings up a number of problems which Simmel sought to address by simply placing all socio-historical biases and influences inside the person-to-person relationships which gave content to universal, timeless forms. This brings about an antagonism in Simmel's system of thought, in that it reveals a disparity between the apparent behaviour (or action) of an individual's reaction to another, while this particular social form can only be analysed in a wider, macrosocial context. Especially more continuous, bigger forms of organisation have a greater sense of impersonality to them, making the underlying form (essentially a drive springing from individual actors) hard to define and ambiguous. The separation of form and content is thus a very useful too in the study of microsocial relations, but cannot be seen to contend any notion of a ""society in its entirety"".",False
22,"In negotiating the post-war political landscape of Eastern Europe, the western powers revealed an ambiguous attitude. Although historians still disagree on whether diplomatic misunderstandings between the allies of the east and west eventually caused the emergence of a cold war, or whether a gradual loss of influence in Eastern European countries was the reason for its outbreak, the half-hearted commitment to their self-proclaimed defense of liberal democratic principles such as democracy and regional stability prompted disillusioned contemporaries to see the allied settlement of Eastern Europe as again one of unprincipled compromise and appeasement. As former Polish Prime Minister in exile Mikołajczyk wrote a few years later: 'we were sacrificed by our allies, the United States and Great Britain'. R.J. Crampton, Eastern Europe in the Twentieth Century - And After (London/New York, 2005), p. 211. Stanisław Mikołajczyk, The Rape of Poland. Patterns of Soviet Agression (New York, 1973), p. viii. Nonetheless, the United States had full intentions to prevent the re-emergence of any great power on the European continent. The reasons for the western allies' failure to prevent the communisation of Eastern Europe must be sought in the complex and antagonistic diplomatic relations between the two camps of the grand alliance. Whereas the Soviets mistrusted Anglo-American intentions, Roosevelt believed until the end in the possibility of a peaceful settlement with regard to Soviet claims to ""national security"" in Eastern Europe. By pursuing this policy, Roosevelt downplayed the ideological difficulties underlying the matter. Optimistically, he believed in the compatibility of free and democratic elections in the Soviet Union's neighbouring states, and the natural emergence of 'governments which would maintain friendly relations with the Soviet Union'. The incompatibility of these two principles caused the Americans to deal with the ideological justification of a possible settlement in somewhat abstract terms, believing a favourable outcome would eventually justify such agreements as the Declaration on Liberated Europe made with the Soviets. Roosevelt's underestimation of Soviet expansionism ultimately rendered an agreement on Poland made at the Yalta conference 'so elastic that the Russians can stretch it all the way from Yalta to Washington without technically breaking it'. John Lewis Gaddis, We Know Now. Rethinking Cold War History (Oxford/New York, 1998), p. 12. Ibid, p. 23. Martin McCauley, The Origins of the Cold War 1941-1949 (London/New York, 1995), p. 54. Caroline Kennedy-Pipe, Stalin's Cold War. Soviet strategies in Europe, 1943 to 1956 (Manchester/New York, 1995), pp. 49-50. Walter Lafeber, America, Russia, and the Cold War, 1945-2002 (Boston, 2004), p. 13. Britain, too, found itself in a difficult moral position between its mid-war pledges to the Polish government that '""His Majesty's Government do not recognize any territorial changes which have been effected in Poland since August, 1939""', and the maintenance of allied unity, which eventually involved some concessions to the Soviet Union as to territorial and political claims in Poland. The Soviet's insistence on territorial expansion in East Prussia, Bessarabia, northern Bukovina and Ruthenia, besides the incorporation of eastern Poland up to the Curzon line were thereafter verified at Yalta in February 1945 and at Potsdam in July of that year. Most likely, Churcill's infamous percentages deal with Stalin in October 1944 only reaffirmed Stalin's conviction that he was allowed a free hand in Eastern European territories based merely on unprincipled Realpolitik. However, opposition to these concessions grew. Mikołajczyk, who had all along protested 'that we were only asking for a return to the prewar status quo' resigned, while in the United States Senator Arthur Vandenberg, in 1945, 'looked at the Soviet Union through the lens of Poland and did not like what he saw'. Thus, the Soviet Union and the western powers gradually maneuvered into a bipolar power structure. Mikołajczyk, The Rape of Poland, p. 16-7. Kennedy-Pipe, Stalin's Cold War, p. 39. Lewis Gaddis, We Know Now, p. 16. Crampton, Eastern Europe, p. 212. Kennedy-Pipe, Stalin's Cold War, p. 46. Mikołajczyk, The Rape of Poland, p. 16. McCauley, The Origins of the Cold War, p. 60. Despite the apparently systematic usurpation of power in Eastern European countries by the Soviet Union, it must be borne in mind that Stalin hardly foresaw the full course of events. Often, he acted upon situations as he was confronted with them. It has, therefore, been argued that 'Stalinisation was a process, not a plan'. Intrinsically mistrustful, Stalin concealed much of his intentions, convincing the Americans for a long time that the Soviet Union subscribed to a free, unified Europe. The disparities between open and concealed policy at times took immense and grotesque proportions. Hence, when the Nazis, in 1943, unveiled a mass grave of 15,000 Polish officers at Katyn, murdered upon Stalin's orders, he denied all allegation and seized the opportunity to break diplomatic relations with the Polish government-in-exile upon inquiries from President Sikorski. Moreover, the Soviet-sponsored Polish provisional government in Lublin was ordered to break its links with the loyalist Polish underground Home Army (AK). Crampton, Eastern Europe, p. 212. Lafeber, America, Russia, and the Cold War, p. 11. Anthony Polansky and Bolesław Drukier, The Beginnings of Communist Rule in Poland (London, 1980), p. 8. Acting from a conviction that any liberated territory immediately became of concern solely for the liberating nation, Moscow eventually resorted to unilateral action to reinforce its sphere of influence. However, '[t]he brutality and cynicism with which [Stalin] handled these matters did more than anything to exhaust the goodwill the Soviet war effort had accumulated in the West'. The replacement of the Radescu government in Romania by the Soviet-supported, communist Groza government, but more strongly, ongoing practices to consolidate Soviet power in Poland in the beginning of 1945 appeared 'to London and Washington deliberately to violate the Yalta agreement'. Cynicism about the possibility of a settlement with Moscow eventually caused the alliance to break before the end the war. Lewis Gaddis, We Know Now, p. 14. McCauley, The Origins of the Cold War, p. 58. Lewis Gaddis, We Know Now, p. 19. Kennedy-Pipe, Stalin's Cold War, p. 57. McCauley, The Origins of the Cold War, p. 57. Stalin's interpretation of 'governmental authorities broadly representative of all democratic elements' was, from the western viewpoint, indeed a curious one. Toward the end of the war, Moscow, defying the ""London Poles"", backed the establishment of a new interim government, while between July 1944 and July 1945, Stalin started to build for Poland 'a security apparatus which was already showing disturbing signs of independence from the Polish government and which was, in fact, effectively controlled by the Soviet 'advisers' within it'. Kennedy-Pipe, Stalin's Cold War, p. 49. Polansky and Drukier, The Beginnings, p. 10. Ibid, p. 2. In the middle of 1944, however, the Lublin claims to governmental supremacy went by no means uncontested. The loyalist Home Army was still Poland's largest organised resistance movement, and Stalin, uncertain about the Lublin committee's loyalty, made a separate agreement with another Polish communist faction based in Moscow, the Polish Committee of National Liberation, which he, according to Mikołajczyk, 'had organised earlier as an instrument of his plans for postwar Poland'. Later that year, the balance of power shifted further in favour of the Soviet Union, by the failing Warsaw uprising, which meant 'the end of the independent Polish left-wing alternative to the Communists'. Mikołajczyk, The Rape of Poland, p. 70. Anita J. Prazmowska, 'The Soviet Liberation of Poland and the Polish Left, 1943-5', in Francesca Gori and Silvio Pons (eds. ), The Soviet Union and Europe in the Cold War, 1943-53 (London/New York, 1996), p. 84. With the crushing of 'the leadership of the historic political forces and their military infrastructure' in Poland, communisation now became only a matter of time. In March 1945 the Soviet Union signed a treaty of friendship with Lublin Committee; in the same month a delegation of sixteen prominent politicians, among whom the government delegate of the Polish government in exile, and the commander of the Polish Home Army (Armia Krajowa) were trapped in a meeting and deported to the Soviet Union to face accusations of conspiring against the Red Army. Crampton, Eastern Europe, p. 217. McCauley, The Origins of the Cold War, p. 58. Crampton, Eastern Europe, p. 218. These developments were signs of the establishment of a Polish interim government existing solely by the grace of the Soviet Union. At this time, however, Poland was still by no means a neatly communised, country, nor was it accepted to be so by the western powers. The post-war political programmes of the Lublin committee and the Polish Committee of National Liberation still revealed rather divergent political agendas. Moreover, despite a domination of Lublin committee ministers in the cabinet, the communists still saw a necessity of including oppositionists in the government. The return to Poland of Mikołajczyk in late 1944 had halted, at least for the time being, an effective total usurpation of the body politic by pro-Soviet communists. Even by 1946, Mikołajczyk's Polish Peasant Party was, with its 600,000 members, Poland's largest political party. Polansky and Drukier, The Beginnings, p. 13. Crampton, Eastern Europe, pp. 218-9. Up to this point, we have primarily looked at the situation in Poland, which, for many western politicians, had become 'the test case of Soviet intentions'. Nevertheless, the communisation of other Eastern European countries happened quite differently, depending on the internal political situation as well as the national state of affairs in the international context at the close of the war. A basis for further negotiation was laid in October 1944 with Churchill's percentages deal with Stalin, in which, among other things, the balance of influence in Yugoslavia was to be divided on a fifty-fifty basis. However, too much has been made of these negotiations. As has been pointed out, in discussions following up on these divisions, British and Soviet foreign ministers Eden and Molotov were unable to reach an eventual agreement which satisfied both demands. As mentioned before, Stalin saw communisation much as a process of opportunities, and in Yugoslavia, the influence of Moscow was 'paramount from the outset'. Paradoxically, however, it was Stalin's caution to avoid alarming the western powers that saw the construction of a compromise government, although on favourable terms for the Yugoslavian communists. In March 1945, an acting regency was agreed, consisting of 'three members, one Croat, one Serb, and one Slovene [who], though non-communists, were all nominees of Tito'. Stalin even managed to convince Churchill to add British support to Tito's provisional government, despite its earlier overt hostilities to 'the English imperialists' back in 1941. The British support helped the Yugoslavian communists to strengthen their power base at the expense of the exile government. With the help of the Red Army, they 'dispersed, disarmed, or forced the exodus of Mihailović's forces from Serbia', thus gaining access to actual power in the body politic. Moreover, 'the communists, though they might share government [in a coalition], would not share power'. Although it was now 'but a short step to the formalisation of the communist takeover', this meant by no means that this happened firmly under Stalin's direction. The Red Army had retreated from Yugoslavian territory, and Tito, though deferential to Stalin, 'had formed views of his own and had no hesitation in expressing them, even when they contradicted those of Stalin himself'. Lafeber, America, Russia, and the Cold War, p. 14. Crampton, Eastern Europe, p. 213. Stephen Clissold (ed. ), Yugoslavia and the Soviet Union 1939-1973. A documentary survey (London, New York, Toronto,1975), p. 34. Kennedy-Pipe, Stalin's Cold War, p. 46. Clissold (ed. ), Yugoslavia and the Soviet Union, p. 34. Crampton, Eastern Europe, p. 216. Clissold (ed. ), Yugoslavia and the Soviet Union, p. 35. Ivan Avakumovic, History of the Communist Party of Yugoslavia. Volume one (Aberdeen, 1967), p. 177. Clissold (ed. ), Yugoslavia and the Soviet Union, p. 40. Crampton, Eastern Europe, p. 216. Ibid., p. 217. Clissold (ed. ), Yugoslavia and the Soviet Union, p. 38. The postwar Czechoslovak dealings with the Soviet Union were quite different. Although it has been argued that 'Stalin as well as Roosevelt and Churchill miscalculated when they assumed that there could be friendly states along an expanded Soviet periphery', Czech leader in exile Beneš certainly tried, and temporarily managed, to come to some agreement with the Soviet Union. He 'spoke openly of a ""Czech solution"" that would exchange internal autonomy for Soviet control over foreign and military policy'. Beneš was not unjustified in his calculations of the postwar situation in the area. Although Churchill had repeatedly argued for a speeded advance eastwards beyond the western zone of occupation to liberate Prague and prevent Czechoslovakia from falling within the Soviet sphere of influence, general Eisenhower declined, arguing that he 'refused to risk casualties for 'purely political reasons''. Directly upon the liberation of Prague in early May, Soviet interests were secured. The communists within the country were thereafter 'in a strong position for a seizure of power', but Stalin told their leader Gottwald to '""work with Beneš, reach an agreement with him, accept him as president.""'. Fearing that an immediate communist takeover would only work counterproductively by concentrating anti-communist factions around the democratic person of Beneš, they decided to negotiate the terms of a new government on a broader base until the right time would appear to assume power. Beneš, from his side, hoped that the inclusion of communist factions in the cabinet would result in a strengthened democratic system, 'with the blessing of the Soviet Union'. Lewis Gaddis, We Know Now, p. 18. Ibid., p. 17. Kennedy-Pipe, Stalin's Cold War, p. 66. Ibid., p. 66. Crampton, Eastern Europe, p. 236. Josef Korbel, Twentieth-Century Czechoslovakia. The Meaning of Its History (New York, 1977), p. 195. Crampton, Eastern Europe, p. 236. Korbel, Twentieth-Century Czechoslovakia, p. 197. This policy turned out to be only one of buying time. Despite the relative moderateness and tolerance of Gottwald's communists, the Soviet Union continued to press for further advances to communisation. Due to an initial underestimation of Soviet interests in Czechoslovakia by the Americans, the military means of interference were lost when the communists eventually seized power in 1948 without any direct justification. Crampton, Eastern Europe, p. 237. Ibid., pp. 238-9. The various examples of Eastern European reveal a diverse range of political situations in the period running up to May 1945. In the Polish case, Stalin's hopes for a post-war accommodation with the Sikorski government were abandoned already in 1942, while in the Czechoslovak case, Beneš' immense popularity prevented the Soviets from bluntly by-passing its exiled government. The Yugoslavian situation was yet different, with a political climate in which the communist party directly and almost naturally dominated the new government. Nonetheless, Tito's power of government was not immediately and unconditionally recognised by Moscow, which is telling of Stalin's opportunism in his efforts to shape Eastern Europe into a system in which communisation dominated. Polansky and Drukier, The Beginnings, p. 5. Crampton, Eastern Europe, p. 216. Meanwhile, goodwill for Soviet intentions in the west gradually deteriorated until the alliance eventually collapsed. A turning point in Soviet diplomacy directed toward the west can be noted around March and April 1945, while an increasing number of American liberals were becoming disenchanted with the Soviet Union and its attitude. After an initial willingness to cooperate, a number of unbridgeable disagreements increasingly turned Soviet-Western relations into an ideological conflict, epitomised by Trumans words to Molotov in their first meeting, 'Carry out your agreements and you won't get talked to like that'. Kennedy-Pipe, Stalin's Cold War, p. 64. Lafeber, America, Russia, and the Cold War, p. 5. McCauley, The Origins of the Cold War, p. 61. May 1945 saw the beginnings of a realisation that Soviet unilateral activities in Eastern Europe had to be confronted. However, whilst Stalin had already been active with his communisation plans as territories were being liberated, the Truman administration still resisted the harsher anti-Soviet line held by the British around May 1945. The communisation of Eastern Europe happened step by step; yet most countries, in May 1945, faced a deteriorating, yet not hopeless democratic climate, with strong oppositions counterweighing against communist aspirations. The late recognition and reluctance to take responsibility of the Americans may thus have been the deciding factor in the eventual communisation of the countries in Eastern Europe. Kennedy-Pipe, Stalin's Cold War, p. 63.","The emergence of the sociological school of ethnomethodology in the early 1960s created a shock wave across the sociological establishment. Its revision of social norms and their relation to the social action springing from individual cases of social interaction was indeed deemed so radical that stories exist of 'ethnomethodologists being fired from their jobs simply because they were ethnomethodologists' (Craib 1992: 102). However, what may at first glance appear a drastically individualistic approach to social relations theory must be seen as embedded within a wider philosophical and social scientific critique of the value of conventional types of knowledge (Alexander 1987). Ethnomethodology asserts that functionalist theory lays an exaggerated amount of emphasis on collectivity within social relations, crystallised in organisations and norms of interaction that are derived therefrom. By first theorising a consistent system of norms, subsequent data gathered from social observation are then ""made to fit"" within this system. These data are, as traditional sociology has it, 'as it were, readily waiting' (Silverman 1972: 173) to be categorised. This, however, naturally creates a tension, as 'the meaning employed by those we study - their norms, values, attitudes and beliefs; the rules which govern their conduct - are treated as if their meaning were unproblematic' (Craib 1992: 103). This can even be observed to be the case in highly advanced social theories, as the overarching theoretical grid still requires the classification of a highly contestable and unstable social reality ""out there"": 'To employ such 'operational' or 'theory-laden' categories is not to escape the ascriptive or imputative work that ordinary action-concept usages carry with them' (Coulter 1979: 11). By failing to observe the complication characteristic of such norm-attribution, sociologists fall into the trap of taking sets of typification for granted, thereby tacitly avoiding the 'problem of concealed, commonsense commitments' (Coulter 1979: 15) to which all social actors involved conform. Instead, according to ethnomethodologists, the only way to avoid adding to such a body of affirmative ""folk sociology"" is to 'seek to understand the process of experiencing' (Silverman 1972: 166), by searching for the underlying motivation of action of each social actor. Such a methodology abstains from explaining social phenomena in immediate relation to abstract, theoretical norms, deriving actual interaction from alternative, everyday commonsensical approaches to social interaction, which have also been described as ""second-order typifications"", 'typifications of our common-sense typifications which order the social world in a rational way' (Craib 1992: 99-100). Instead of taking methods of social ordering for granted, the sociologist may then concern himself or herself with 'the manner in which members use rules to do the work of defining and interpreting actions' (Silverman 1972: 179). In this essay I shall discuss the D. Lawrence Wieder's case study of ""the code"", a semi-formal normative body maintained as the set of social rules by which residents in a 'rehabilitative facility for narcotic-addict felons on parole' (Wieder 1974: 144) claim to measure the validity of their social action. This ethnomethodological ethnographic study may provide interesting insights into how, for all actors involved in any organisational structure, 'talk and action are produced and understood as indexical displays of the everyday world' (Cicourel 1973: 99). The problematic relation Wieder discovers between the norms as he has them explained to him, and as he sees those norms put in practice reveals the usefulness for, 'not a commonsense sociology but a sociology of commonsense' (Silverman 1972: 170), an often arbitrary and precarious battlefield for social power negotiation. I shall thereafter also look at the limitations of such an approach to sociological studies. In his analysis of the role of ""telling the code"", Wieder begins by looking more closely at the way in which he personally internalised its meaning. As he received information on the content of ""the code"" piecemeal, it became necessary to create an internal schema for understanding it. In this way, it became possible to interpret further bits of information, while simultaneously elaborating the internal system into a generic and constantly expanding body of understanding. 'Each newly encountered 'piece' of talk was simultaneously rendered sensible by interpreting it in terms of the developing relevancies of the code and was, at the same time, more evidence of the existence of that code' (Wieder 1974: 161). Hereby, Wieder points out the indexical functioning of the ""code system"": the various norms at the core serve to interpret behaviour which is said to be based on it. However, this ""indexing"" of behaviour was not quite an unproblematic or effortless method. Behavioural significance was rendered only 'meaningful in the ways that it was said socially-in-a-context' (Wieder 1974: 163). While Wieder initially observed that he related residents' uncooperative attitudes quite naturally to the norms described in the convict code, he found that such categorisation could at times be rather arbitrary, resulting in situations in which  '...either chosen alternative would be equally plausible interpretations in terms of the same rule, even though the alternatives propose opposite actions. [...] Instead of 'predicting' behavior, the rule is actually employed as an interpretative device' (Wieder 1974: 168).According to Wieder, all of residents' behaviour was, in the end, placed, and thereafter rendered rational, in the light of the convict code. It created, for the observer, a method in which social '...order and orderliness is accomplished - that is, it involves interpretive ""work""' (Silverman 1972: 175). In the light of such formalistic social meta-theories as Durkheim's, the position of residents is understandable in terms of the social force of the convict code weighing upon them as an ""institutionalised"" norms of interaction. This position, however, fails to see the manipulative, indefinite character of the convict code, using instead 'the fact of norm compliance to assume that [the residents'] action has a compliant character, that it is passive and conformistic rather than active and constructing' (Alexander 1987: 274). In opposition to this, ethnomethodology, rather taking an individualistic stance, argues that in the enactment of norms (in this case described as ""telling the code""), 'there is negotiation vis-à-vis [the meaning of these] rules'. In a similar vein, Cicourel has argued that 'this [...] environment of objects cannot be tied easily to notions like status, role and norms, employed by actors in less structured or controlled everyday situations' (Cicourel 1973: 15). In other words, norms only derive meaning from their enactment, which is continually negotiated and whose stability depends on the degree of acceptance which the various social actors concede to the meaning contained within the enactment. Under normal circumstances, the process of norm negotiation takes place without great complication, as 'we are frequently engaged in making connections in our descriptions' (Coulter 1979: 15). In this process of ""making sense"" of the social stimuli around us, we draw upon previous experience, thus arriving at 'a series of commonsense constructs [which we] have pre-selected and pre-interpreted [from] this world which [we] experience as the reality of their daily lives' (Schutz in Coulter 1979: 9). Beyond this, the analogy of ""making sense"" extends further in the sense of serving the purpose of creating structural coherence and predictability in our perception of the world that we experience. A convincing set of behavioural rules such as the convict code is necessary to order the residents' forms of social interaction, which is, in fact, 'knowledge [that] is itself inherently unstable, something which is created anew in each encounter' (Craib 1992: 102). The precariousness of resident-staff interaction is pointed out by its strong dependence on contextuality. With ""the code"" in mind as an interpretative tool, all actors involved (which in this case may involve the residents, members of staff and the observer), continually scan incoming social stimuli for recognisable modes of behaviour which may contextually seem logical. In doing this, variables as the particular social actors, the setting and the timing play an indispensable role for practices of indexing, i.e., ""sense-making"" (Cicourel 1973: 101, Wieder 1974). In this context, staff's reaction to residents' 'problematic acts' were not confronted as subversive behaviour; rather was their character now converted 'into instances of a familiar pattern' (Wieder 1974: 151). In this way, the facility's staff utilised the convicts' code in order 'to formulate a recognizable coherent story, standard, typical, cogent, uniform, planful, i.e., a professionally defensible, and thereby, for members, a recognizably rational account' (Garfinkel in Alexander 1987: 262-3). Unsurprisingly, Wieder sees ""telling the code"" in the first place as a means of justification of the chosen course of action. By ""telling the code"", residents explain and justify why they will not cooperate in certain situations or answer certain questions posed by staff. By accepting the code, staff, in turn, account for residents' uncooperativeness in activities and rehabilitative procedures. ''Telling the code' provided staff with a useful way of talking about residents and themselves which portrayed both teams as more or less reasonable and more or less helpless to change the character of the relationship between the teams, because of the social-fact character (in Durkheim's sense) of the regularities made available by 'telling the code' which were none of anybody's specific doing or responsibility' (Wieder 1974: 151)Interesting is Wieder's reference to Durkheim: in ""telling the code"", all social actors in the facility render types of social interaction institutionalised, thereby attempting to (though not actually) reifying them (Durkheim's ""social facts""). With this, however, the door is only just opened to further and more penetrating interpretations of action-motivation. Ethnomethodology points out that, in order to gain real understanding that means something, 'intentions and the actor's views are always potentially relevant and must be taken into account' (Pitkin in Coulter 1979: 12-3). Indeed, by assuming interpretation ends at normative causality, sociologists inevitably miss out on the politics underlying social action (Coulter 1979: 11, Silverman 1972: 175). The darker, more precarious side of social interaction lies in the tensions of power underneath its surface. Ethnomethodology does not deny the existence of a ""common culture"", and this would be an absurd claim to make (Alexander 1987: 262); however, the precariousness of social interaction is exactly that what this ""common culture"" is not: individuals may challenge the existing order by either directly confronting it, or, more subtly, by manipulating its general traits to their advantage. Whether this challenge is pursued consciously or not is, strictly spoken, not relevant; as Pitkin asserts, 'One may have to intend to lie in order to lie, but one need not intend to deceive in order to deceive' (Pitkin in Coulter 1979: 12). An analogy may here be found in the functioning of penal codes, in that 'the fit is 'managed' through negotiating socially organized activities' (Cicourel 1973: 13) in which both hidden and open methods of manipulation are used. The position Wieder takes in his account of the role of the convict code draws on the traditions of both radically challenging all knowledge in a systematic way, and emphasising the inescapable burdens of responsibility that social action brings with it. It is therefore no surprise that 'phenomenological sociology, existential sociology, [and] the sociology of everyday life' (Craib 1992: 97) are often mentioned in conjunction with one another. The indefiniteness of society described by such a sociology stresses a few characteristics. In the first place, we can observe that social ""roles"" have a dynamic character that extends beyond the immediately observable social surface. It can be observed that 'the dynamic aspect of status or office and such is always influenced by factors other than the stipulations of the position itself' (David in Cicourel 1973: 14). From this follows, secondly, that the situational placement of social interaction takes an important place in determining its meaning, i.e. it is not just about what actors speak, but more importantly about how communication takes place (Garfinkel in Alexander 1987: 270). Thirdly, the constant change in interpretation of social norms results in an extreme degree of relativism invoked by ethnomethodology (Craib 1992: 102). In assuming the '""adequate"" or ""appropriate"" operational definitions' (Silverman 1972: 177), staff of the ""halfway house"" avoided facing up to the existential question of where this appropriateness was derived from. Thus, Wieder can ultimately only conclude one thing: ''Telling the code' masked what was 'really going on' at the halfway house - what was 'really going on' was residents' pursuing their immediate interests by fabricating instances of 'telling the code' which deceived me and the staff' (Wieder 1974: 169). Naturally, Wieder's approach is not free from criticism. It can, in the first place, be said that it gives a rather cynical, or at least extremely sceptical account the natural course that social interaction takes within the example of his case study. While underlying forces may be at work in social interaction, it is entirely possible that even the social actors involved are not aware of them. Residents in the facility may, in fact, feel that behaviour represents a genuine reflection of the loyalty and distrust that the convict code at varying times demands of him. Likewise, it is not unrealistic to think that staff generally recognises the ""realness"" of the code and in instances acts in accordance with it to protect certain individual residents. In other words, the convict code is not entirely without justified grounded. This problem brings under discussion the extent to which the freedom to negotiate socials norms actually exists. Regarded as a whole, society can be said to be rather more stable than the Wieder claims from within the ethnomethodological perspective. Thus, while Wieder's approach may be laudably seen to attempt to incorporate 'a genuine humanism in an ideological sense', it can be asked '[w]hether this moral humanism is theoretically justified' (Alexander 1987: 280). In his account of how the convict code is to be interpreted, Wieder emphasises how it obtains a indexical status. Rather like the coding system of books in a library, its norms serve to interpret rather than to describe it. Thus, norms serve as a tacit justification of real events, for both social actors and the sociologist. A truer account of events, is, however, more complicated. By accepting norms as motivation and authentic drives of social actors, the convict code came to serve as a tool of rendering behaviour logical and coherent. For staff, this tool justified an acceptance of nonconformist behaviour from the residents' side, while the residents often successfully manipulated it to their direct advantage. This underlying differentiation reveals the political and arbitrary nature of social norms, however developed and ""theory-laden"". In his approach to sociology, Wieder, and the wider school of ethnomethodologists, build upon the work of phenomenology and existentialism to show the inescapability of situational variables in the everyday context of social interaction. Thus, the ""what"" of social norms must separated from the ""how"" of interpretation, and subsequently analysed at a different level. This approach, however, has not remained uncriticised. Many sociologists deem the extent of manipulative power that ethnomethodologists award the individual with unrealistic. According to them, behaviour is more context-bound and can be seen to be derived from this context. In that case, Wieder's account of the convict code serves, foremostly, as an existential reminder of the forces underlying our everyday commonsense social interaction. .",True
23,"The emergence of the sociological school of ethnomethodology in the early 1960s created a shock wave across the sociological establishment. Its revision of social norms and their relation to the social action springing from individual cases of social interaction was indeed deemed so radical that stories exist of 'ethnomethodologists being fired from their jobs simply because they were ethnomethodologists' (Craib 1992: 102). However, what may at first glance appear a drastically individualistic approach to social relations theory must be seen as embedded within a wider philosophical and social scientific critique of the value of conventional types of knowledge (Alexander 1987). Ethnomethodology asserts that functionalist theory lays an exaggerated amount of emphasis on collectivity within social relations, crystallised in organisations and norms of interaction that are derived therefrom. By first theorising a consistent system of norms, subsequent data gathered from social observation are then ""made to fit"" within this system. These data are, as traditional sociology has it, 'as it were, readily waiting' (Silverman 1972: 173) to be categorised. This, however, naturally creates a tension, as 'the meaning employed by those we study - their norms, values, attitudes and beliefs; the rules which govern their conduct - are treated as if their meaning were unproblematic' (Craib 1992: 103). This can even be observed to be the case in highly advanced social theories, as the overarching theoretical grid still requires the classification of a highly contestable and unstable social reality ""out there"": 'To employ such 'operational' or 'theory-laden' categories is not to escape the ascriptive or imputative work that ordinary action-concept usages carry with them' (Coulter 1979: 11). By failing to observe the complication characteristic of such norm-attribution, sociologists fall into the trap of taking sets of typification for granted, thereby tacitly avoiding the 'problem of concealed, commonsense commitments' (Coulter 1979: 15) to which all social actors involved conform. Instead, according to ethnomethodologists, the only way to avoid adding to such a body of affirmative ""folk sociology"" is to 'seek to understand the process of experiencing' (Silverman 1972: 166), by searching for the underlying motivation of action of each social actor. Such a methodology abstains from explaining social phenomena in immediate relation to abstract, theoretical norms, deriving actual interaction from alternative, everyday commonsensical approaches to social interaction, which have also been described as ""second-order typifications"", 'typifications of our common-sense typifications which order the social world in a rational way' (Craib 1992: 99-100). Instead of taking methods of social ordering for granted, the sociologist may then concern himself or herself with 'the manner in which members use rules to do the work of defining and interpreting actions' (Silverman 1972: 179). In this essay I shall discuss the D. Lawrence Wieder's case study of ""the code"", a semi-formal normative body maintained as the set of social rules by which residents in a 'rehabilitative facility for narcotic-addict felons on parole' (Wieder 1974: 144) claim to measure the validity of their social action. This ethnomethodological ethnographic study may provide interesting insights into how, for all actors involved in any organisational structure, 'talk and action are produced and understood as indexical displays of the everyday world' (Cicourel 1973: 99). The problematic relation Wieder discovers between the norms as he has them explained to him, and as he sees those norms put in practice reveals the usefulness for, 'not a commonsense sociology but a sociology of commonsense' (Silverman 1972: 170), an often arbitrary and precarious battlefield for social power negotiation. I shall thereafter also look at the limitations of such an approach to sociological studies. In his analysis of the role of ""telling the code"", Wieder begins by looking more closely at the way in which he personally internalised its meaning. As he received information on the content of ""the code"" piecemeal, it became necessary to create an internal schema for understanding it. In this way, it became possible to interpret further bits of information, while simultaneously elaborating the internal system into a generic and constantly expanding body of understanding. 'Each newly encountered 'piece' of talk was simultaneously rendered sensible by interpreting it in terms of the developing relevancies of the code and was, at the same time, more evidence of the existence of that code' (Wieder 1974: 161). Hereby, Wieder points out the indexical functioning of the ""code system"": the various norms at the core serve to interpret behaviour which is said to be based on it. However, this ""indexing"" of behaviour was not quite an unproblematic or effortless method. Behavioural significance was rendered only 'meaningful in the ways that it was said socially-in-a-context' (Wieder 1974: 163). While Wieder initially observed that he related residents' uncooperative attitudes quite naturally to the norms described in the convict code, he found that such categorisation could at times be rather arbitrary, resulting in situations in which  '...either chosen alternative would be equally plausible interpretations in terms of the same rule, even though the alternatives propose opposite actions. [...] Instead of 'predicting' behavior, the rule is actually employed as an interpretative device' (Wieder 1974: 168).According to Wieder, all of residents' behaviour was, in the end, placed, and thereafter rendered rational, in the light of the convict code. It created, for the observer, a method in which social '...order and orderliness is accomplished - that is, it involves interpretive ""work""' (Silverman 1972: 175). In the light of such formalistic social meta-theories as Durkheim's, the position of residents is understandable in terms of the social force of the convict code weighing upon them as an ""institutionalised"" norms of interaction. This position, however, fails to see the manipulative, indefinite character of the convict code, using instead 'the fact of norm compliance to assume that [the residents'] action has a compliant character, that it is passive and conformistic rather than active and constructing' (Alexander 1987: 274). In opposition to this, ethnomethodology, rather taking an individualistic stance, argues that in the enactment of norms (in this case described as ""telling the code""), 'there is negotiation vis-à-vis [the meaning of these] rules'. In a similar vein, Cicourel has argued that 'this [...] environment of objects cannot be tied easily to notions like status, role and norms, employed by actors in less structured or controlled everyday situations' (Cicourel 1973: 15). In other words, norms only derive meaning from their enactment, which is continually negotiated and whose stability depends on the degree of acceptance which the various social actors concede to the meaning contained within the enactment. Under normal circumstances, the process of norm negotiation takes place without great complication, as 'we are frequently engaged in making connections in our descriptions' (Coulter 1979: 15). In this process of ""making sense"" of the social stimuli around us, we draw upon previous experience, thus arriving at 'a series of commonsense constructs [which we] have pre-selected and pre-interpreted [from] this world which [we] experience as the reality of their daily lives' (Schutz in Coulter 1979: 9). Beyond this, the analogy of ""making sense"" extends further in the sense of serving the purpose of creating structural coherence and predictability in our perception of the world that we experience. A convincing set of behavioural rules such as the convict code is necessary to order the residents' forms of social interaction, which is, in fact, 'knowledge [that] is itself inherently unstable, something which is created anew in each encounter' (Craib 1992: 102). The precariousness of resident-staff interaction is pointed out by its strong dependence on contextuality. With ""the code"" in mind as an interpretative tool, all actors involved (which in this case may involve the residents, members of staff and the observer), continually scan incoming social stimuli for recognisable modes of behaviour which may contextually seem logical. In doing this, variables as the particular social actors, the setting and the timing play an indispensable role for practices of indexing, i.e., ""sense-making"" (Cicourel 1973: 101, Wieder 1974). In this context, staff's reaction to residents' 'problematic acts' were not confronted as subversive behaviour; rather was their character now converted 'into instances of a familiar pattern' (Wieder 1974: 151). In this way, the facility's staff utilised the convicts' code in order 'to formulate a recognizable coherent story, standard, typical, cogent, uniform, planful, i.e., a professionally defensible, and thereby, for members, a recognizably rational account' (Garfinkel in Alexander 1987: 262-3). Unsurprisingly, Wieder sees ""telling the code"" in the first place as a means of justification of the chosen course of action. By ""telling the code"", residents explain and justify why they will not cooperate in certain situations or answer certain questions posed by staff. By accepting the code, staff, in turn, account for residents' uncooperativeness in activities and rehabilitative procedures. ''Telling the code' provided staff with a useful way of talking about residents and themselves which portrayed both teams as more or less reasonable and more or less helpless to change the character of the relationship between the teams, because of the social-fact character (in Durkheim's sense) of the regularities made available by 'telling the code' which were none of anybody's specific doing or responsibility' (Wieder 1974: 151)Interesting is Wieder's reference to Durkheim: in ""telling the code"", all social actors in the facility render types of social interaction institutionalised, thereby attempting to (though not actually) reifying them (Durkheim's ""social facts""). With this, however, the door is only just opened to further and more penetrating interpretations of action-motivation. Ethnomethodology points out that, in order to gain real understanding that means something, 'intentions and the actor's views are always potentially relevant and must be taken into account' (Pitkin in Coulter 1979: 12-3). Indeed, by assuming interpretation ends at normative causality, sociologists inevitably miss out on the politics underlying social action (Coulter 1979: 11, Silverman 1972: 175). The darker, more precarious side of social interaction lies in the tensions of power underneath its surface. Ethnomethodology does not deny the existence of a ""common culture"", and this would be an absurd claim to make (Alexander 1987: 262); however, the precariousness of social interaction is exactly that what this ""common culture"" is not: individuals may challenge the existing order by either directly confronting it, or, more subtly, by manipulating its general traits to their advantage. Whether this challenge is pursued consciously or not is, strictly spoken, not relevant; as Pitkin asserts, 'One may have to intend to lie in order to lie, but one need not intend to deceive in order to deceive' (Pitkin in Coulter 1979: 12). An analogy may here be found in the functioning of penal codes, in that 'the fit is 'managed' through negotiating socially organized activities' (Cicourel 1973: 13) in which both hidden and open methods of manipulation are used. The position Wieder takes in his account of the role of the convict code draws on the traditions of both radically challenging all knowledge in a systematic way, and emphasising the inescapable burdens of responsibility that social action brings with it. It is therefore no surprise that 'phenomenological sociology, existential sociology, [and] the sociology of everyday life' (Craib 1992: 97) are often mentioned in conjunction with one another. The indefiniteness of society described by such a sociology stresses a few characteristics. In the first place, we can observe that social ""roles"" have a dynamic character that extends beyond the immediately observable social surface. It can be observed that 'the dynamic aspect of status or office and such is always influenced by factors other than the stipulations of the position itself' (David in Cicourel 1973: 14). From this follows, secondly, that the situational placement of social interaction takes an important place in determining its meaning, i.e. it is not just about what actors speak, but more importantly about how communication takes place (Garfinkel in Alexander 1987: 270). Thirdly, the constant change in interpretation of social norms results in an extreme degree of relativism invoked by ethnomethodology (Craib 1992: 102). In assuming the '""adequate"" or ""appropriate"" operational definitions' (Silverman 1972: 177), staff of the ""halfway house"" avoided facing up to the existential question of where this appropriateness was derived from. Thus, Wieder can ultimately only conclude one thing: ''Telling the code' masked what was 'really going on' at the halfway house - what was 'really going on' was residents' pursuing their immediate interests by fabricating instances of 'telling the code' which deceived me and the staff' (Wieder 1974: 169). Naturally, Wieder's approach is not free from criticism. It can, in the first place, be said that it gives a rather cynical, or at least extremely sceptical account the natural course that social interaction takes within the example of his case study. While underlying forces may be at work in social interaction, it is entirely possible that even the social actors involved are not aware of them. Residents in the facility may, in fact, feel that behaviour represents a genuine reflection of the loyalty and distrust that the convict code at varying times demands of him. Likewise, it is not unrealistic to think that staff generally recognises the ""realness"" of the code and in instances acts in accordance with it to protect certain individual residents. In other words, the convict code is not entirely without justified grounded. This problem brings under discussion the extent to which the freedom to negotiate socials norms actually exists. Regarded as a whole, society can be said to be rather more stable than the Wieder claims from within the ethnomethodological perspective. Thus, while Wieder's approach may be laudably seen to attempt to incorporate 'a genuine humanism in an ideological sense', it can be asked '[w]hether this moral humanism is theoretically justified' (Alexander 1987: 280). In his account of how the convict code is to be interpreted, Wieder emphasises how it obtains a indexical status. Rather like the coding system of books in a library, its norms serve to interpret rather than to describe it. Thus, norms serve as a tacit justification of real events, for both social actors and the sociologist. A truer account of events, is, however, more complicated. By accepting norms as motivation and authentic drives of social actors, the convict code came to serve as a tool of rendering behaviour logical and coherent. For staff, this tool justified an acceptance of nonconformist behaviour from the residents' side, while the residents often successfully manipulated it to their direct advantage. This underlying differentiation reveals the political and arbitrary nature of social norms, however developed and ""theory-laden"". In his approach to sociology, Wieder, and the wider school of ethnomethodologists, build upon the work of phenomenology and existentialism to show the inescapability of situational variables in the everyday context of social interaction. Thus, the ""what"" of social norms must separated from the ""how"" of interpretation, and subsequently analysed at a different level. This approach, however, has not remained uncriticised. Many sociologists deem the extent of manipulative power that ethnomethodologists award the individual with unrealistic. According to them, behaviour is more context-bound and can be seen to be derived from this context. In that case, Wieder's account of the convict code serves, foremostly, as an existential reminder of the forces underlying our everyday commonsense social interaction. .","In negotiating the post-war political landscape of Eastern Europe, the western powers revealed an ambiguous attitude. Although historians still disagree on whether diplomatic misunderstandings between the allies of the east and west eventually caused the emergence of a cold war, or whether a gradual loss of influence in Eastern European countries was the reason for its outbreak, the half-hearted commitment to their self-proclaimed defense of liberal democratic principles such as democracy and regional stability prompted disillusioned contemporaries to see the allied settlement of Eastern Europe as again one of unprincipled compromise and appeasement. As former Polish Prime Minister in exile Mikołajczyk wrote a few years later: 'we were sacrificed by our allies, the United States and Great Britain'. R.J. Crampton, Eastern Europe in the Twentieth Century - And After (London/New York, 2005), p. 211. Stanisław Mikołajczyk, The Rape of Poland. Patterns of Soviet Agression (New York, 1973), p. viii. Nonetheless, the United States had full intentions to prevent the re-emergence of any great power on the European continent. The reasons for the western allies' failure to prevent the communisation of Eastern Europe must be sought in the complex and antagonistic diplomatic relations between the two camps of the grand alliance. Whereas the Soviets mistrusted Anglo-American intentions, Roosevelt believed until the end in the possibility of a peaceful settlement with regard to Soviet claims to ""national security"" in Eastern Europe. By pursuing this policy, Roosevelt downplayed the ideological difficulties underlying the matter. Optimistically, he believed in the compatibility of free and democratic elections in the Soviet Union's neighbouring states, and the natural emergence of 'governments which would maintain friendly relations with the Soviet Union'. The incompatibility of these two principles caused the Americans to deal with the ideological justification of a possible settlement in somewhat abstract terms, believing a favourable outcome would eventually justify such agreements as the Declaration on Liberated Europe made with the Soviets. Roosevelt's underestimation of Soviet expansionism ultimately rendered an agreement on Poland made at the Yalta conference 'so elastic that the Russians can stretch it all the way from Yalta to Washington without technically breaking it'. John Lewis Gaddis, We Know Now. Rethinking Cold War History (Oxford/New York, 1998), p. 12. Ibid, p. 23. Martin McCauley, The Origins of the Cold War 1941-1949 (London/New York, 1995), p. 54. Caroline Kennedy-Pipe, Stalin's Cold War. Soviet strategies in Europe, 1943 to 1956 (Manchester/New York, 1995), pp. 49-50. Walter Lafeber, America, Russia, and the Cold War, 1945-2002 (Boston, 2004), p. 13. Britain, too, found itself in a difficult moral position between its mid-war pledges to the Polish government that '""His Majesty's Government do not recognize any territorial changes which have been effected in Poland since August, 1939""', and the maintenance of allied unity, which eventually involved some concessions to the Soviet Union as to territorial and political claims in Poland. The Soviet's insistence on territorial expansion in East Prussia, Bessarabia, northern Bukovina and Ruthenia, besides the incorporation of eastern Poland up to the Curzon line were thereafter verified at Yalta in February 1945 and at Potsdam in July of that year. Most likely, Churcill's infamous percentages deal with Stalin in October 1944 only reaffirmed Stalin's conviction that he was allowed a free hand in Eastern European territories based merely on unprincipled Realpolitik. However, opposition to these concessions grew. Mikołajczyk, who had all along protested 'that we were only asking for a return to the prewar status quo' resigned, while in the United States Senator Arthur Vandenberg, in 1945, 'looked at the Soviet Union through the lens of Poland and did not like what he saw'. Thus, the Soviet Union and the western powers gradually maneuvered into a bipolar power structure. Mikołajczyk, The Rape of Poland, p. 16-7. Kennedy-Pipe, Stalin's Cold War, p. 39. Lewis Gaddis, We Know Now, p. 16. Crampton, Eastern Europe, p. 212. Kennedy-Pipe, Stalin's Cold War, p. 46. Mikołajczyk, The Rape of Poland, p. 16. McCauley, The Origins of the Cold War, p. 60. Despite the apparently systematic usurpation of power in Eastern European countries by the Soviet Union, it must be borne in mind that Stalin hardly foresaw the full course of events. Often, he acted upon situations as he was confronted with them. It has, therefore, been argued that 'Stalinisation was a process, not a plan'. Intrinsically mistrustful, Stalin concealed much of his intentions, convincing the Americans for a long time that the Soviet Union subscribed to a free, unified Europe. The disparities between open and concealed policy at times took immense and grotesque proportions. Hence, when the Nazis, in 1943, unveiled a mass grave of 15,000 Polish officers at Katyn, murdered upon Stalin's orders, he denied all allegation and seized the opportunity to break diplomatic relations with the Polish government-in-exile upon inquiries from President Sikorski. Moreover, the Soviet-sponsored Polish provisional government in Lublin was ordered to break its links with the loyalist Polish underground Home Army (AK). Crampton, Eastern Europe, p. 212. Lafeber, America, Russia, and the Cold War, p. 11. Anthony Polansky and Bolesław Drukier, The Beginnings of Communist Rule in Poland (London, 1980), p. 8. Acting from a conviction that any liberated territory immediately became of concern solely for the liberating nation, Moscow eventually resorted to unilateral action to reinforce its sphere of influence. However, '[t]he brutality and cynicism with which [Stalin] handled these matters did more than anything to exhaust the goodwill the Soviet war effort had accumulated in the West'. The replacement of the Radescu government in Romania by the Soviet-supported, communist Groza government, but more strongly, ongoing practices to consolidate Soviet power in Poland in the beginning of 1945 appeared 'to London and Washington deliberately to violate the Yalta agreement'. Cynicism about the possibility of a settlement with Moscow eventually caused the alliance to break before the end the war. Lewis Gaddis, We Know Now, p. 14. McCauley, The Origins of the Cold War, p. 58. Lewis Gaddis, We Know Now, p. 19. Kennedy-Pipe, Stalin's Cold War, p. 57. McCauley, The Origins of the Cold War, p. 57. Stalin's interpretation of 'governmental authorities broadly representative of all democratic elements' was, from the western viewpoint, indeed a curious one. Toward the end of the war, Moscow, defying the ""London Poles"", backed the establishment of a new interim government, while between July 1944 and July 1945, Stalin started to build for Poland 'a security apparatus which was already showing disturbing signs of independence from the Polish government and which was, in fact, effectively controlled by the Soviet 'advisers' within it'. Kennedy-Pipe, Stalin's Cold War, p. 49. Polansky and Drukier, The Beginnings, p. 10. Ibid, p. 2. In the middle of 1944, however, the Lublin claims to governmental supremacy went by no means uncontested. The loyalist Home Army was still Poland's largest organised resistance movement, and Stalin, uncertain about the Lublin committee's loyalty, made a separate agreement with another Polish communist faction based in Moscow, the Polish Committee of National Liberation, which he, according to Mikołajczyk, 'had organised earlier as an instrument of his plans for postwar Poland'. Later that year, the balance of power shifted further in favour of the Soviet Union, by the failing Warsaw uprising, which meant 'the end of the independent Polish left-wing alternative to the Communists'. Mikołajczyk, The Rape of Poland, p. 70. Anita J. Prazmowska, 'The Soviet Liberation of Poland and the Polish Left, 1943-5', in Francesca Gori and Silvio Pons (eds. ), The Soviet Union and Europe in the Cold War, 1943-53 (London/New York, 1996), p. 84. With the crushing of 'the leadership of the historic political forces and their military infrastructure' in Poland, communisation now became only a matter of time. In March 1945 the Soviet Union signed a treaty of friendship with Lublin Committee; in the same month a delegation of sixteen prominent politicians, among whom the government delegate of the Polish government in exile, and the commander of the Polish Home Army (Armia Krajowa) were trapped in a meeting and deported to the Soviet Union to face accusations of conspiring against the Red Army. Crampton, Eastern Europe, p. 217. McCauley, The Origins of the Cold War, p. 58. Crampton, Eastern Europe, p. 218. These developments were signs of the establishment of a Polish interim government existing solely by the grace of the Soviet Union. At this time, however, Poland was still by no means a neatly communised, country, nor was it accepted to be so by the western powers. The post-war political programmes of the Lublin committee and the Polish Committee of National Liberation still revealed rather divergent political agendas. Moreover, despite a domination of Lublin committee ministers in the cabinet, the communists still saw a necessity of including oppositionists in the government. The return to Poland of Mikołajczyk in late 1944 had halted, at least for the time being, an effective total usurpation of the body politic by pro-Soviet communists. Even by 1946, Mikołajczyk's Polish Peasant Party was, with its 600,000 members, Poland's largest political party. Polansky and Drukier, The Beginnings, p. 13. Crampton, Eastern Europe, pp. 218-9. Up to this point, we have primarily looked at the situation in Poland, which, for many western politicians, had become 'the test case of Soviet intentions'. Nevertheless, the communisation of other Eastern European countries happened quite differently, depending on the internal political situation as well as the national state of affairs in the international context at the close of the war. A basis for further negotiation was laid in October 1944 with Churchill's percentages deal with Stalin, in which, among other things, the balance of influence in Yugoslavia was to be divided on a fifty-fifty basis. However, too much has been made of these negotiations. As has been pointed out, in discussions following up on these divisions, British and Soviet foreign ministers Eden and Molotov were unable to reach an eventual agreement which satisfied both demands. As mentioned before, Stalin saw communisation much as a process of opportunities, and in Yugoslavia, the influence of Moscow was 'paramount from the outset'. Paradoxically, however, it was Stalin's caution to avoid alarming the western powers that saw the construction of a compromise government, although on favourable terms for the Yugoslavian communists. In March 1945, an acting regency was agreed, consisting of 'three members, one Croat, one Serb, and one Slovene [who], though non-communists, were all nominees of Tito'. Stalin even managed to convince Churchill to add British support to Tito's provisional government, despite its earlier overt hostilities to 'the English imperialists' back in 1941. The British support helped the Yugoslavian communists to strengthen their power base at the expense of the exile government. With the help of the Red Army, they 'dispersed, disarmed, or forced the exodus of Mihailović's forces from Serbia', thus gaining access to actual power in the body politic. Moreover, 'the communists, though they might share government [in a coalition], would not share power'. Although it was now 'but a short step to the formalisation of the communist takeover', this meant by no means that this happened firmly under Stalin's direction. The Red Army had retreated from Yugoslavian territory, and Tito, though deferential to Stalin, 'had formed views of his own and had no hesitation in expressing them, even when they contradicted those of Stalin himself'. Lafeber, America, Russia, and the Cold War, p. 14. Crampton, Eastern Europe, p. 213. Stephen Clissold (ed. ), Yugoslavia and the Soviet Union 1939-1973. A documentary survey (London, New York, Toronto,1975), p. 34. Kennedy-Pipe, Stalin's Cold War, p. 46. Clissold (ed. ), Yugoslavia and the Soviet Union, p. 34. Crampton, Eastern Europe, p. 216. Clissold (ed. ), Yugoslavia and the Soviet Union, p. 35. Ivan Avakumovic, History of the Communist Party of Yugoslavia. Volume one (Aberdeen, 1967), p. 177. Clissold (ed. ), Yugoslavia and the Soviet Union, p. 40. Crampton, Eastern Europe, p. 216. Ibid., p. 217. Clissold (ed. ), Yugoslavia and the Soviet Union, p. 38. The postwar Czechoslovak dealings with the Soviet Union were quite different. Although it has been argued that 'Stalin as well as Roosevelt and Churchill miscalculated when they assumed that there could be friendly states along an expanded Soviet periphery', Czech leader in exile Beneš certainly tried, and temporarily managed, to come to some agreement with the Soviet Union. He 'spoke openly of a ""Czech solution"" that would exchange internal autonomy for Soviet control over foreign and military policy'. Beneš was not unjustified in his calculations of the postwar situation in the area. Although Churchill had repeatedly argued for a speeded advance eastwards beyond the western zone of occupation to liberate Prague and prevent Czechoslovakia from falling within the Soviet sphere of influence, general Eisenhower declined, arguing that he 'refused to risk casualties for 'purely political reasons''. Directly upon the liberation of Prague in early May, Soviet interests were secured. The communists within the country were thereafter 'in a strong position for a seizure of power', but Stalin told their leader Gottwald to '""work with Beneš, reach an agreement with him, accept him as president.""'. Fearing that an immediate communist takeover would only work counterproductively by concentrating anti-communist factions around the democratic person of Beneš, they decided to negotiate the terms of a new government on a broader base until the right time would appear to assume power. Beneš, from his side, hoped that the inclusion of communist factions in the cabinet would result in a strengthened democratic system, 'with the blessing of the Soviet Union'. Lewis Gaddis, We Know Now, p. 18. Ibid., p. 17. Kennedy-Pipe, Stalin's Cold War, p. 66. Ibid., p. 66. Crampton, Eastern Europe, p. 236. Josef Korbel, Twentieth-Century Czechoslovakia. The Meaning of Its History (New York, 1977), p. 195. Crampton, Eastern Europe, p. 236. Korbel, Twentieth-Century Czechoslovakia, p. 197. This policy turned out to be only one of buying time. Despite the relative moderateness and tolerance of Gottwald's communists, the Soviet Union continued to press for further advances to communisation. Due to an initial underestimation of Soviet interests in Czechoslovakia by the Americans, the military means of interference were lost when the communists eventually seized power in 1948 without any direct justification. Crampton, Eastern Europe, p. 237. Ibid., pp. 238-9. The various examples of Eastern European reveal a diverse range of political situations in the period running up to May 1945. In the Polish case, Stalin's hopes for a post-war accommodation with the Sikorski government were abandoned already in 1942, while in the Czechoslovak case, Beneš' immense popularity prevented the Soviets from bluntly by-passing its exiled government. The Yugoslavian situation was yet different, with a political climate in which the communist party directly and almost naturally dominated the new government. Nonetheless, Tito's power of government was not immediately and unconditionally recognised by Moscow, which is telling of Stalin's opportunism in his efforts to shape Eastern Europe into a system in which communisation dominated. Polansky and Drukier, The Beginnings, p. 5. Crampton, Eastern Europe, p. 216. Meanwhile, goodwill for Soviet intentions in the west gradually deteriorated until the alliance eventually collapsed. A turning point in Soviet diplomacy directed toward the west can be noted around March and April 1945, while an increasing number of American liberals were becoming disenchanted with the Soviet Union and its attitude. After an initial willingness to cooperate, a number of unbridgeable disagreements increasingly turned Soviet-Western relations into an ideological conflict, epitomised by Trumans words to Molotov in their first meeting, 'Carry out your agreements and you won't get talked to like that'. Kennedy-Pipe, Stalin's Cold War, p. 64. Lafeber, America, Russia, and the Cold War, p. 5. McCauley, The Origins of the Cold War, p. 61. May 1945 saw the beginnings of a realisation that Soviet unilateral activities in Eastern Europe had to be confronted. However, whilst Stalin had already been active with his communisation plans as territories were being liberated, the Truman administration still resisted the harsher anti-Soviet line held by the British around May 1945. The communisation of Eastern Europe happened step by step; yet most countries, in May 1945, faced a deteriorating, yet not hopeless democratic climate, with strong oppositions counterweighing against communist aspirations. The late recognition and reluctance to take responsibility of the Americans may thus have been the deciding factor in the eventual communisation of the countries in Eastern Europe. Kennedy-Pipe, Stalin's Cold War, p. 63.",False
24,"The emergence of the sociological school of ethnomethodology in the early 1960s created a shock wave across the sociological establishment. Its revision of social norms and their relation to the social action springing from individual cases of social interaction was indeed deemed so radical that stories exist of 'ethnomethodologists being fired from their jobs simply because they were ethnomethodologists' (Craib 1992: 102). However, what may at first glance appear a drastically individualistic approach to social relations theory must be seen as embedded within a wider philosophical and social scientific critique of the value of conventional types of knowledge (Alexander 1987). Ethnomethodology asserts that functionalist theory lays an exaggerated amount of emphasis on collectivity within social relations, crystallised in organisations and norms of interaction that are derived therefrom. By first theorising a consistent system of norms, subsequent data gathered from social observation are then ""made to fit"" within this system. These data are, as traditional sociology has it, 'as it were, readily waiting' (Silverman 1972: 173) to be categorised. This, however, naturally creates a tension, as 'the meaning employed by those we study - their norms, values, attitudes and beliefs; the rules which govern their conduct - are treated as if their meaning were unproblematic' (Craib 1992: 103). This can even be observed to be the case in highly advanced social theories, as the overarching theoretical grid still requires the classification of a highly contestable and unstable social reality ""out there"": 'To employ such 'operational' or 'theory-laden' categories is not to escape the ascriptive or imputative work that ordinary action-concept usages carry with them' (Coulter 1979: 11). By failing to observe the complication characteristic of such norm-attribution, sociologists fall into the trap of taking sets of typification for granted, thereby tacitly avoiding the 'problem of concealed, commonsense commitments' (Coulter 1979: 15) to which all social actors involved conform. Instead, according to ethnomethodologists, the only way to avoid adding to such a body of affirmative ""folk sociology"" is to 'seek to understand the process of experiencing' (Silverman 1972: 166), by searching for the underlying motivation of action of each social actor. Such a methodology abstains from explaining social phenomena in immediate relation to abstract, theoretical norms, deriving actual interaction from alternative, everyday commonsensical approaches to social interaction, which have also been described as ""second-order typifications"", 'typifications of our common-sense typifications which order the social world in a rational way' (Craib 1992: 99-100). Instead of taking methods of social ordering for granted, the sociologist may then concern himself or herself with 'the manner in which members use rules to do the work of defining and interpreting actions' (Silverman 1972: 179). In this essay I shall discuss the D. Lawrence Wieder's case study of ""the code"", a semi-formal normative body maintained as the set of social rules by which residents in a 'rehabilitative facility for narcotic-addict felons on parole' (Wieder 1974: 144) claim to measure the validity of their social action. This ethnomethodological ethnographic study may provide interesting insights into how, for all actors involved in any organisational structure, 'talk and action are produced and understood as indexical displays of the everyday world' (Cicourel 1973: 99). The problematic relation Wieder discovers between the norms as he has them explained to him, and as he sees those norms put in practice reveals the usefulness for, 'not a commonsense sociology but a sociology of commonsense' (Silverman 1972: 170), an often arbitrary and precarious battlefield for social power negotiation. I shall thereafter also look at the limitations of such an approach to sociological studies. In his analysis of the role of ""telling the code"", Wieder begins by looking more closely at the way in which he personally internalised its meaning. As he received information on the content of ""the code"" piecemeal, it became necessary to create an internal schema for understanding it. In this way, it became possible to interpret further bits of information, while simultaneously elaborating the internal system into a generic and constantly expanding body of understanding. 'Each newly encountered 'piece' of talk was simultaneously rendered sensible by interpreting it in terms of the developing relevancies of the code and was, at the same time, more evidence of the existence of that code' (Wieder 1974: 161). Hereby, Wieder points out the indexical functioning of the ""code system"": the various norms at the core serve to interpret behaviour which is said to be based on it. However, this ""indexing"" of behaviour was not quite an unproblematic or effortless method. Behavioural significance was rendered only 'meaningful in the ways that it was said socially-in-a-context' (Wieder 1974: 163). While Wieder initially observed that he related residents' uncooperative attitudes quite naturally to the norms described in the convict code, he found that such categorisation could at times be rather arbitrary, resulting in situations in which  '...either chosen alternative would be equally plausible interpretations in terms of the same rule, even though the alternatives propose opposite actions. [...] Instead of 'predicting' behavior, the rule is actually employed as an interpretative device' (Wieder 1974: 168).According to Wieder, all of residents' behaviour was, in the end, placed, and thereafter rendered rational, in the light of the convict code. It created, for the observer, a method in which social '...order and orderliness is accomplished - that is, it involves interpretive ""work""' (Silverman 1972: 175). In the light of such formalistic social meta-theories as Durkheim's, the position of residents is understandable in terms of the social force of the convict code weighing upon them as an ""institutionalised"" norms of interaction. This position, however, fails to see the manipulative, indefinite character of the convict code, using instead 'the fact of norm compliance to assume that [the residents'] action has a compliant character, that it is passive and conformistic rather than active and constructing' (Alexander 1987: 274). In opposition to this, ethnomethodology, rather taking an individualistic stance, argues that in the enactment of norms (in this case described as ""telling the code""), 'there is negotiation vis-à-vis [the meaning of these] rules'. In a similar vein, Cicourel has argued that 'this [...] environment of objects cannot be tied easily to notions like status, role and norms, employed by actors in less structured or controlled everyday situations' (Cicourel 1973: 15). In other words, norms only derive meaning from their enactment, which is continually negotiated and whose stability depends on the degree of acceptance which the various social actors concede to the meaning contained within the enactment. Under normal circumstances, the process of norm negotiation takes place without great complication, as 'we are frequently engaged in making connections in our descriptions' (Coulter 1979: 15). In this process of ""making sense"" of the social stimuli around us, we draw upon previous experience, thus arriving at 'a series of commonsense constructs [which we] have pre-selected and pre-interpreted [from] this world which [we] experience as the reality of their daily lives' (Schutz in Coulter 1979: 9). Beyond this, the analogy of ""making sense"" extends further in the sense of serving the purpose of creating structural coherence and predictability in our perception of the world that we experience. A convincing set of behavioural rules such as the convict code is necessary to order the residents' forms of social interaction, which is, in fact, 'knowledge [that] is itself inherently unstable, something which is created anew in each encounter' (Craib 1992: 102). The precariousness of resident-staff interaction is pointed out by its strong dependence on contextuality. With ""the code"" in mind as an interpretative tool, all actors involved (which in this case may involve the residents, members of staff and the observer), continually scan incoming social stimuli for recognisable modes of behaviour which may contextually seem logical. In doing this, variables as the particular social actors, the setting and the timing play an indispensable role for practices of indexing, i.e., ""sense-making"" (Cicourel 1973: 101, Wieder 1974). In this context, staff's reaction to residents' 'problematic acts' were not confronted as subversive behaviour; rather was their character now converted 'into instances of a familiar pattern' (Wieder 1974: 151). In this way, the facility's staff utilised the convicts' code in order 'to formulate a recognizable coherent story, standard, typical, cogent, uniform, planful, i.e., a professionally defensible, and thereby, for members, a recognizably rational account' (Garfinkel in Alexander 1987: 262-3). Unsurprisingly, Wieder sees ""telling the code"" in the first place as a means of justification of the chosen course of action. By ""telling the code"", residents explain and justify why they will not cooperate in certain situations or answer certain questions posed by staff. By accepting the code, staff, in turn, account for residents' uncooperativeness in activities and rehabilitative procedures. ''Telling the code' provided staff with a useful way of talking about residents and themselves which portrayed both teams as more or less reasonable and more or less helpless to change the character of the relationship between the teams, because of the social-fact character (in Durkheim's sense) of the regularities made available by 'telling the code' which were none of anybody's specific doing or responsibility' (Wieder 1974: 151)Interesting is Wieder's reference to Durkheim: in ""telling the code"", all social actors in the facility render types of social interaction institutionalised, thereby attempting to (though not actually) reifying them (Durkheim's ""social facts""). With this, however, the door is only just opened to further and more penetrating interpretations of action-motivation. Ethnomethodology points out that, in order to gain real understanding that means something, 'intentions and the actor's views are always potentially relevant and must be taken into account' (Pitkin in Coulter 1979: 12-3). Indeed, by assuming interpretation ends at normative causality, sociologists inevitably miss out on the politics underlying social action (Coulter 1979: 11, Silverman 1972: 175). The darker, more precarious side of social interaction lies in the tensions of power underneath its surface. Ethnomethodology does not deny the existence of a ""common culture"", and this would be an absurd claim to make (Alexander 1987: 262); however, the precariousness of social interaction is exactly that what this ""common culture"" is not: individuals may challenge the existing order by either directly confronting it, or, more subtly, by manipulating its general traits to their advantage. Whether this challenge is pursued consciously or not is, strictly spoken, not relevant; as Pitkin asserts, 'One may have to intend to lie in order to lie, but one need not intend to deceive in order to deceive' (Pitkin in Coulter 1979: 12). An analogy may here be found in the functioning of penal codes, in that 'the fit is 'managed' through negotiating socially organized activities' (Cicourel 1973: 13) in which both hidden and open methods of manipulation are used. The position Wieder takes in his account of the role of the convict code draws on the traditions of both radically challenging all knowledge in a systematic way, and emphasising the inescapable burdens of responsibility that social action brings with it. It is therefore no surprise that 'phenomenological sociology, existential sociology, [and] the sociology of everyday life' (Craib 1992: 97) are often mentioned in conjunction with one another. The indefiniteness of society described by such a sociology stresses a few characteristics. In the first place, we can observe that social ""roles"" have a dynamic character that extends beyond the immediately observable social surface. It can be observed that 'the dynamic aspect of status or office and such is always influenced by factors other than the stipulations of the position itself' (David in Cicourel 1973: 14). From this follows, secondly, that the situational placement of social interaction takes an important place in determining its meaning, i.e. it is not just about what actors speak, but more importantly about how communication takes place (Garfinkel in Alexander 1987: 270). Thirdly, the constant change in interpretation of social norms results in an extreme degree of relativism invoked by ethnomethodology (Craib 1992: 102). In assuming the '""adequate"" or ""appropriate"" operational definitions' (Silverman 1972: 177), staff of the ""halfway house"" avoided facing up to the existential question of where this appropriateness was derived from. Thus, Wieder can ultimately only conclude one thing: ''Telling the code' masked what was 'really going on' at the halfway house - what was 'really going on' was residents' pursuing their immediate interests by fabricating instances of 'telling the code' which deceived me and the staff' (Wieder 1974: 169). Naturally, Wieder's approach is not free from criticism. It can, in the first place, be said that it gives a rather cynical, or at least extremely sceptical account the natural course that social interaction takes within the example of his case study. While underlying forces may be at work in social interaction, it is entirely possible that even the social actors involved are not aware of them. Residents in the facility may, in fact, feel that behaviour represents a genuine reflection of the loyalty and distrust that the convict code at varying times demands of him. Likewise, it is not unrealistic to think that staff generally recognises the ""realness"" of the code and in instances acts in accordance with it to protect certain individual residents. In other words, the convict code is not entirely without justified grounded. This problem brings under discussion the extent to which the freedom to negotiate socials norms actually exists. Regarded as a whole, society can be said to be rather more stable than the Wieder claims from within the ethnomethodological perspective. Thus, while Wieder's approach may be laudably seen to attempt to incorporate 'a genuine humanism in an ideological sense', it can be asked '[w]hether this moral humanism is theoretically justified' (Alexander 1987: 280). In his account of how the convict code is to be interpreted, Wieder emphasises how it obtains a indexical status. Rather like the coding system of books in a library, its norms serve to interpret rather than to describe it. Thus, norms serve as a tacit justification of real events, for both social actors and the sociologist. A truer account of events, is, however, more complicated. By accepting norms as motivation and authentic drives of social actors, the convict code came to serve as a tool of rendering behaviour logical and coherent. For staff, this tool justified an acceptance of nonconformist behaviour from the residents' side, while the residents often successfully manipulated it to their direct advantage. This underlying differentiation reveals the political and arbitrary nature of social norms, however developed and ""theory-laden"". In his approach to sociology, Wieder, and the wider school of ethnomethodologists, build upon the work of phenomenology and existentialism to show the inescapability of situational variables in the everyday context of social interaction. Thus, the ""what"" of social norms must separated from the ""how"" of interpretation, and subsequently analysed at a different level. This approach, however, has not remained uncriticised. Many sociologists deem the extent of manipulative power that ethnomethodologists award the individual with unrealistic. According to them, behaviour is more context-bound and can be seen to be derived from this context. In that case, Wieder's account of the convict code serves, foremostly, as an existential reminder of the forces underlying our everyday commonsense social interaction. .","The advent of the Solidarity strikes and protests halfway 1980 struck a serious blow at the Polish communist regime. Whilst it provided a powerful response of workers to the repressive character and 'determination [of the Polish regime] to maintain [the political and economic] systems, whatever their human costs and lack of legitimacy', an important questions comes to mind. Where from came this powerful articulation of demands, and of 'the most powerful, sophisticated and advanced working-class movement yet seen, certainly in the 'communist' sphere and perhaps anywhere in the world'? To answer this question, it may be fruitful to study the emergence of new class societies in Eastern Europe, paradoxical when matched with Marxist thought. Economic and political developments throughout the 1970s united with a blend of national-historical and wider Eastern European economic grievances to prepare the ferment for 'a new generation of protesting workers with a clearly different way of articulating their grievances [as] a strictly Polish phenomenon'. In this essay I will have a closer look at the development of a class spirit among Polish workers, and argue that it was from their socio-economic situation that discord arose. Walter D. Connor, 'Social Change and Stability in Eastern Europe' in Problems of Communism, vol. 26 (1977), p. 16. Colin Barker, Festival of the Oppressed. Solidarity, Reform and Revolution in Poland 1980-81 (London: 1986), p. 11. Walter D. Connor, 'Social Change and Stability in Eastern Europe' in Problems of Communism, vol. 26 (1977), p. 29. Jadwiga Staniszkis, The Dynamics of the Breakthrough in Eastern Europe. The Polish Experience (Berkeley: 1991), p. 107-8. The prehistory of the Solidarity movement was, most importantly, sparked by an increasing crystallisation of a ""class consciousness"" within the ranks of the Polish work force, many of whom were becoming convinced 'that they [could only] improve their lot collectively'. Despite official state rhetoric, there was an increasingly realistic attitude to the existence of a new dominant class made up of 'the intelligentsia as a whole rather than just the bureaucracy narrowly defined'; a class of highly educated, better-rewarded technocrats and managers, charged with the daily affairs of increasing efficiency of production and economic policy making. While official policy claimed to pursue a policy of egalitarianism, the reality behind these claims was only limited. The producing classes, those of the workers and peasants, found themselves on the same position in the material hierarchy as before, while a decrease in real standards of living were initially concealed only by the relative success of social mobility. However, the stretch of social mobility had its limits and by the 1970s a new generation of workers, unfamiliar with opportunities for social mobility, many of them younger than 25 years of age, began to form an increasingly articulate and assertive sense of collective grievances. While according to the orthodox theory of communism the party was the vanguard of the workers, epitomising its real interest, workers on the work floor reportedly referred to 'management in the third person, reserving ""we"" for themselves'. This clearly meant the beginning of an independently developing working class, at odds with the intelligentsia and a regime increasingly alienated from Poland's new social reality. Walter D. Connor, 'Social Change and Stability in Eastern Europe' in Problems of Communism, vol. 26 (1977), p. 28. Georg Konrád and Ivan Szelényi, The Intellectuals on the Road to Class Power (Brighton: 1979), p. xiv-v. Walter D. Connor, 'Social Change and Stability in Eastern Europe' in Problems of Communism, vol. 26 (1977), p. 17. R.J. Crampton, Eastern Europe in the Twentieth Century - And After (London/New York, 2005), p. 364. Konrád and Szelényi, The Intellectuals, p. 223. Similar to the England of the early nineteenth century, the creation of a new Eastern European working class indeed took place within the rather short timeframe of one or two generations. Over the decades after the Second World War, an average of nearly fifty percent of the younger peasantry came 'to form the new working class, [and] to create the social drama of a new life, a new world'. As their direct socio-economic interests began to reveal themselves in increasing antagonism with the state-directed economic policy, Polish workers, as a class an sich and with an agenda different even from that of the oppositional intelligentsia, more and more often became involved in violent confrontations with the state. Social discord became increasingly apparent, and ''Poles murdered by Poles, workers murdered by a 'Workers' State', became the symbol for all their accumulated grievances'. Connor, 'Social Change and Stability in Eastern Europe', p. 21. Ibid, p. 28. Garton Ash, The Polish Revolution, p. 12. Timothy Garton Ash's assertion that 'December 1970 [was] the single most important date in the pre-history of Solidarity', may provide further insight into the extent that Solidarity was connected to the working class. Its reaction to the government's implementation of food price increases revealed signs of a growing unity and singular purpose of action. When the government resolved to give up its policy of consistent non-unemployment 'aimed at forcing greater productivity through the use of threat of unemployment as an incentive for better and more efficient work', this seemed a direct threat to workers at large. After strikes and violent confrontations throughout January 1971, the government withdrew its economic reform plans, while Gomułka was forced to step down as a consequence of his unpopular repressive measures and number of deaths as a direct consequence thereof. This political outcome was seen by many protesters as a direct result of their protests, which served made workers realise where there strength lay. Throughout the 1970s, many strikes followed, and although the regime 'exacted its revenge on the workers', resulting in many workers being 'sacked, arrested, beaten, jailed', strikers usually got what they wanted. This, and the fact that many protests 'involved a large group which was generally representative of the worker population as a whole', accounts for the growth of an awareness that the economic situation of workers was connected with, and could be influenced by, their political role as an occupational group. Ibid, p. 12. Jan B. de Weydenthal, 'Poland: Workers and Politics' in Jan F. Triska and Charles Gati (eds. ), Blue-Collar Workers in Eastern Europe (London: 1981), p. 193. Crampton, Eastern Europe, p. 359. Weydenthal, 'Poland: Workers and Politics', p. 194. Barker, Festival of the Oppressed, p. 12. Weydenthal, 'Poland: Workers and Politics', p. 195. Yet, what made for the timing of this growing self-awareness? Where from came the momentum that enabled workers to start attempting to influence their economic position through strikes? The Polish communist party had, from the onset, based much of its legitimacy through the reasonably successful initiation of a process of social mobility upward. Workers were recruited from among the peasant ranks, while better-educated workers were needed to support the growing bureaucratic apparatus. These developments enabled many to enter new realities that were hitherto undreamed of, creating a sense of greater equality, if not of material wealth, then of opportunity. It also sparked, however, another process, which would later develop into one of the primary reasons for the antagonism between the workers and the intelligentsia as a whole. The quick development of industries created a culture of technocracy, in which economic status became more directly related to 'a hierarchy of statuses based on diplomas, irrespective of the actual amount of work done'. This put pressures on access to higher education as a direct way of material advancement, as the intelligentsia were protective of their traditionally advantaged position. The attitude of this ""red bourgeoisie"" was becoming an assertion '[t]hat they do and should live better than workers'. This, however, created tension, as these 'unequal patterns of allocation of social rewards' were legitimised by the class that benefited from it, 'despite the traditional systemic commitment to egalitarianism'. With the decreasing flexibility in the Polish labour market, most economists and sociologists began to note a shift from 'phases of system-building to the tasks of system-maintenance'. The loss of revolutionary economic appeal for the masses, however, eventually began to cost the Party members as well; 'between October 1980 and March 1981 the Party lost 216,000 members, bringing total membership below three million'. Connor, 'Social Change and Stability in Eastern Europe', p. 24. Konrád and Szelényi, The Intellectuals, p. 228. Connor, 'Social Change and Stability in Eastern Europe', p. 26. Weydenthal, 'Poland: Workers and Politics', p. 191. Connor, 'Social Change and Stability in Eastern Europe', p. 25. Garton Ash, The Polish Revolution, p. 170. The stagnation of social mobility can be said to have a two-sided effect. While, one the one hand, it formed a ferment for discontent because the state failed to meet the workers' expectations of equality, Eastern European 'regimes [had to], in a sense, deal with the workers they have'. While the legitimating narrative came out of redistribution, 'the technocrats, like the ruling elite itself, [had] an interest in keeping labor in a dispositive position', which entailed maintenance of the status quo. A further source of frustration was found in the fact that, despite repeated pledges for reform from the party leadership, very little change was noticed, while concessions were repeatedly watered down. This was particularly hard to swallow since continually facts were coming to light revealing the decadent and privileged world that the small number of the party and administrative élite' had created for themselves. Contradictions were experienced very starkly when any kind of protest was followed by measures on a worker's economic privileges and 'any sign of active solidarity' was immediately called 'political conspiracy'. Gradually, workers began to connect their economic discontent with a wider, social picture, which thoroughly undermined the legitimacy of the state. As now the argument of economic prosperity was no longer something Gierek could count on, the social and political situation became something of a ticking bomb. Connor, 'Social Change and Stability in Eastern Europe', p. 29. Konrád and Szelényi, The Intellectuals, p. 225. Weydenthal, 'Poland: Workers and Politics', p. 190. Crampton, Eastern Europe, p. 362. Konrád and Szelényi, The Intellectuals, p. 227. Connor, 'Social Change and Stability in Eastern Europe', p. 32. The decade running up to the strikes of saw an increasingly untenable paradoxical situation. According to Marxist thinking, communism provided the means to stop the alienation of the workers; yet, they were becoming increasingly alienated. This was clearly demonstrated by the slow production process of inferior quality produce. By the 1980s, an observer described ""the Polish situation as 'the emergence of workers as a distinct, albeit unorganized, political force and the inability of institutionalized leadership groups to integrate the workers into the existing pattern of systematic operations'. It is important to note, that the origins of this alienation were, to a great extent, class-originated, coming from the specific economic grievances of the workers. Konrád and Szelényi, The Intellectuals, p. 226. Weydenthal, 'Poland: Workers and Politics', p. 187. Economic alienation eventually meant political alienation, which then began to organise itself. The strikers of 1980 were no longer easily reintegrated. They were 'far more disciplined; they were not committed to socialism or the party' any longer, whereas large sections of Poland's industrial workers in 1970 were still prepared to give the regime another chance, in 1980 Lech Wałęsa bitterly reminded his co-workers of how little had been done with the workers' trust and support. If the Party was not prepared to listen to their grievances anymore, the workers would organise themselves outside the state. This essentially foreboded the inability to continue any profitable form of communism. As R.J. Crampton observed, Crampton, Eastern Europe, p. 367. Garton Ash, The Polish Revolution, p. 13. Konrád and Szelényi, The Intellectuals, p. 227. 'Solidarity had shown the bankruptcy of east European communism because the bulk of communism's chosen people, the workers, had looked outside the party for redress of their social grievances'.Crampton, Eastern Europe, p. 376. We have observed that the regime became increasingly inapt to rally the support of the workers for the socialist cause, or to reasonably incorporate them within a plan economy. It is, nonetheless, important to have a closer look at the reasons that caused the process to assume such a radical working class character. Surely, there existed a strong section of ""critical intelligentsia"", yet their position was precarious: whereas they assumed a role as the voice of the working class, they were also 'part of the intellectual class, from which neither its structural position nor its value-system [allowed] it to secede'. It is said, moreover, that moral decay could be observed across the intelligentsia - so much so, that one sociologist spoke of 'the 'semi-feudalisation' of Polish society'. The success propaganda of communist society that western historians are quite familiar with, also seemed to have blinded the regime, rendering it numb to criticism or alternative ideas. This hostile view, however, is only useful to a certain extent, and it is more likely that many members of the intelligentsia simply still '[had] a stake in the status quo and, therefore, support[ed] it'. Konrád and Szelényi, The Intellectuals, p. 221. Barker, Festival of the Oppressed. , p. 13. Ibid, p. 12. Connor, 'Social Change and Stability in Eastern Europe', p. 27. There were, however, also initiatives from the intelligentsia within broad daylight of the Party's political sphere, which provided powerful criticism of the way in which the state treated dissident workers. And while many intellectuals desiring reform continued to trap themselves in 'an iron cage' of 'the limits of possible reform', organisations such as the Committee in Defence of the Workers (KOR) set out in a number of semi-legal ways to create a bond of solidarity between workers and intellectuals, an number of them eventually becoming involved in Solidarity. Examples of selfless and risk-taking initiatives are not hard to find; intellectuals such as Miroslaw Chojecki lost their jobs, continuously spent time in jail and got beaten up, in the meanwhile enabling the publication of 150 illegal books. The problem of intellectualism and political involvement, however, would eventually cause problems by 1989, when, during the round table talks intellectualism gave the negotiations an air of secrecy and 'institutional horse trading'. Crampton, Eastern Europe, p. 364. Barker, Festival of the Oppressed, p. 16. Garton Ash, The Polish Revolution, p. 18. Barker, Festival of the Oppressed, p. 15. Jadwiga Staniszkis, The Dynamics of the Breakthrough in Eastern Europe. The Polish Experience (Berkeley: 1991), p. 143. By 1970, n increasing awareness of the existence of social class was observed in Polish society. This classification was not so much based upon political affiliation as upon material wealth. Rapid industrialisation since World War Two had brought new realities urban workers, a large number of them of young age, in a setting of narrowly confined economic possibilities. When rapid economic and industrial development began to slow down and the initial success story of social mobility began to lose its stretch, problems became observable. The popular official state narrative of egalitarianism became increasingly exposed, whereas the state had little means to provide alternative justifications. In a situation that was becoming more and more complicated, the workers soon discovered the power of strike and protest, something which the regime was rather fearful of. With no alternative means of political participation, the working class was rapidly becoming self-aware and radical, while the regime, with a large number of critical intellectuals torn between the rigor of party policy and the difficulty of reform, proved unable to control economic decline and the social outcry against party elitism and decadence. A state that claimed to hold the interests of workers as its highest principle was rapidly losing support from just that group. The strikers of Solidarity were no longer easily drawn back into the folds of socialist society which had, according to them, structurally failed to improve itself. A number of critical intellectuals, however, come to the aid of the workers. The KOR created effective means of communication between different workers' groups, and bridged the gap of solidarity between intellectuals and workers. Nonetheless, the development of Solidarity must be viewed as a broad expression of workers' grievances with the sympathy and support of a number of critical intellectuals.",True
25,"The advent of the Solidarity strikes and protests halfway 1980 struck a serious blow at the Polish communist regime. Whilst it provided a powerful response of workers to the repressive character and 'determination [of the Polish regime] to maintain [the political and economic] systems, whatever their human costs and lack of legitimacy', an important questions comes to mind. Where from came this powerful articulation of demands, and of 'the most powerful, sophisticated and advanced working-class movement yet seen, certainly in the 'communist' sphere and perhaps anywhere in the world'? To answer this question, it may be fruitful to study the emergence of new class societies in Eastern Europe, paradoxical when matched with Marxist thought. Economic and political developments throughout the 1970s united with a blend of national-historical and wider Eastern European economic grievances to prepare the ferment for 'a new generation of protesting workers with a clearly different way of articulating their grievances [as] a strictly Polish phenomenon'. In this essay I will have a closer look at the development of a class spirit among Polish workers, and argue that it was from their socio-economic situation that discord arose. Walter D. Connor, 'Social Change and Stability in Eastern Europe' in Problems of Communism, vol. 26 (1977), p. 16. Colin Barker, Festival of the Oppressed. Solidarity, Reform and Revolution in Poland 1980-81 (London: 1986), p. 11. Walter D. Connor, 'Social Change and Stability in Eastern Europe' in Problems of Communism, vol. 26 (1977), p. 29. Jadwiga Staniszkis, The Dynamics of the Breakthrough in Eastern Europe. The Polish Experience (Berkeley: 1991), p. 107-8. The prehistory of the Solidarity movement was, most importantly, sparked by an increasing crystallisation of a ""class consciousness"" within the ranks of the Polish work force, many of whom were becoming convinced 'that they [could only] improve their lot collectively'. Despite official state rhetoric, there was an increasingly realistic attitude to the existence of a new dominant class made up of 'the intelligentsia as a whole rather than just the bureaucracy narrowly defined'; a class of highly educated, better-rewarded technocrats and managers, charged with the daily affairs of increasing efficiency of production and economic policy making. While official policy claimed to pursue a policy of egalitarianism, the reality behind these claims was only limited. The producing classes, those of the workers and peasants, found themselves on the same position in the material hierarchy as before, while a decrease in real standards of living were initially concealed only by the relative success of social mobility. However, the stretch of social mobility had its limits and by the 1970s a new generation of workers, unfamiliar with opportunities for social mobility, many of them younger than 25 years of age, began to form an increasingly articulate and assertive sense of collective grievances. While according to the orthodox theory of communism the party was the vanguard of the workers, epitomising its real interest, workers on the work floor reportedly referred to 'management in the third person, reserving ""we"" for themselves'. This clearly meant the beginning of an independently developing working class, at odds with the intelligentsia and a regime increasingly alienated from Poland's new social reality. Walter D. Connor, 'Social Change and Stability in Eastern Europe' in Problems of Communism, vol. 26 (1977), p. 28. Georg Konrád and Ivan Szelényi, The Intellectuals on the Road to Class Power (Brighton: 1979), p. xiv-v. Walter D. Connor, 'Social Change and Stability in Eastern Europe' in Problems of Communism, vol. 26 (1977), p. 17. R.J. Crampton, Eastern Europe in the Twentieth Century - And After (London/New York, 2005), p. 364. Konrád and Szelényi, The Intellectuals, p. 223. Similar to the England of the early nineteenth century, the creation of a new Eastern European working class indeed took place within the rather short timeframe of one or two generations. Over the decades after the Second World War, an average of nearly fifty percent of the younger peasantry came 'to form the new working class, [and] to create the social drama of a new life, a new world'. As their direct socio-economic interests began to reveal themselves in increasing antagonism with the state-directed economic policy, Polish workers, as a class an sich and with an agenda different even from that of the oppositional intelligentsia, more and more often became involved in violent confrontations with the state. Social discord became increasingly apparent, and ''Poles murdered by Poles, workers murdered by a 'Workers' State', became the symbol for all their accumulated grievances'. Connor, 'Social Change and Stability in Eastern Europe', p. 21. Ibid, p. 28. Garton Ash, The Polish Revolution, p. 12. Timothy Garton Ash's assertion that 'December 1970 [was] the single most important date in the pre-history of Solidarity', may provide further insight into the extent that Solidarity was connected to the working class. Its reaction to the government's implementation of food price increases revealed signs of a growing unity and singular purpose of action. When the government resolved to give up its policy of consistent non-unemployment 'aimed at forcing greater productivity through the use of threat of unemployment as an incentive for better and more efficient work', this seemed a direct threat to workers at large. After strikes and violent confrontations throughout January 1971, the government withdrew its economic reform plans, while Gomułka was forced to step down as a consequence of his unpopular repressive measures and number of deaths as a direct consequence thereof. This political outcome was seen by many protesters as a direct result of their protests, which served made workers realise where there strength lay. Throughout the 1970s, many strikes followed, and although the regime 'exacted its revenge on the workers', resulting in many workers being 'sacked, arrested, beaten, jailed', strikers usually got what they wanted. This, and the fact that many protests 'involved a large group which was generally representative of the worker population as a whole', accounts for the growth of an awareness that the economic situation of workers was connected with, and could be influenced by, their political role as an occupational group. Ibid, p. 12. Jan B. de Weydenthal, 'Poland: Workers and Politics' in Jan F. Triska and Charles Gati (eds. ), Blue-Collar Workers in Eastern Europe (London: 1981), p. 193. Crampton, Eastern Europe, p. 359. Weydenthal, 'Poland: Workers and Politics', p. 194. Barker, Festival of the Oppressed, p. 12. Weydenthal, 'Poland: Workers and Politics', p. 195. Yet, what made for the timing of this growing self-awareness? Where from came the momentum that enabled workers to start attempting to influence their economic position through strikes? The Polish communist party had, from the onset, based much of its legitimacy through the reasonably successful initiation of a process of social mobility upward. Workers were recruited from among the peasant ranks, while better-educated workers were needed to support the growing bureaucratic apparatus. These developments enabled many to enter new realities that were hitherto undreamed of, creating a sense of greater equality, if not of material wealth, then of opportunity. It also sparked, however, another process, which would later develop into one of the primary reasons for the antagonism between the workers and the intelligentsia as a whole. The quick development of industries created a culture of technocracy, in which economic status became more directly related to 'a hierarchy of statuses based on diplomas, irrespective of the actual amount of work done'. This put pressures on access to higher education as a direct way of material advancement, as the intelligentsia were protective of their traditionally advantaged position. The attitude of this ""red bourgeoisie"" was becoming an assertion '[t]hat they do and should live better than workers'. This, however, created tension, as these 'unequal patterns of allocation of social rewards' were legitimised by the class that benefited from it, 'despite the traditional systemic commitment to egalitarianism'. With the decreasing flexibility in the Polish labour market, most economists and sociologists began to note a shift from 'phases of system-building to the tasks of system-maintenance'. The loss of revolutionary economic appeal for the masses, however, eventually began to cost the Party members as well; 'between October 1980 and March 1981 the Party lost 216,000 members, bringing total membership below three million'. Connor, 'Social Change and Stability in Eastern Europe', p. 24. Konrád and Szelényi, The Intellectuals, p. 228. Connor, 'Social Change and Stability in Eastern Europe', p. 26. Weydenthal, 'Poland: Workers and Politics', p. 191. Connor, 'Social Change and Stability in Eastern Europe', p. 25. Garton Ash, The Polish Revolution, p. 170. The stagnation of social mobility can be said to have a two-sided effect. While, one the one hand, it formed a ferment for discontent because the state failed to meet the workers' expectations of equality, Eastern European 'regimes [had to], in a sense, deal with the workers they have'. While the legitimating narrative came out of redistribution, 'the technocrats, like the ruling elite itself, [had] an interest in keeping labor in a dispositive position', which entailed maintenance of the status quo. A further source of frustration was found in the fact that, despite repeated pledges for reform from the party leadership, very little change was noticed, while concessions were repeatedly watered down. This was particularly hard to swallow since continually facts were coming to light revealing the decadent and privileged world that the small number of the party and administrative élite' had created for themselves. Contradictions were experienced very starkly when any kind of protest was followed by measures on a worker's economic privileges and 'any sign of active solidarity' was immediately called 'political conspiracy'. Gradually, workers began to connect their economic discontent with a wider, social picture, which thoroughly undermined the legitimacy of the state. As now the argument of economic prosperity was no longer something Gierek could count on, the social and political situation became something of a ticking bomb. Connor, 'Social Change and Stability in Eastern Europe', p. 29. Konrád and Szelényi, The Intellectuals, p. 225. Weydenthal, 'Poland: Workers and Politics', p. 190. Crampton, Eastern Europe, p. 362. Konrád and Szelényi, The Intellectuals, p. 227. Connor, 'Social Change and Stability in Eastern Europe', p. 32. The decade running up to the strikes of saw an increasingly untenable paradoxical situation. According to Marxist thinking, communism provided the means to stop the alienation of the workers; yet, they were becoming increasingly alienated. This was clearly demonstrated by the slow production process of inferior quality produce. By the 1980s, an observer described ""the Polish situation as 'the emergence of workers as a distinct, albeit unorganized, political force and the inability of institutionalized leadership groups to integrate the workers into the existing pattern of systematic operations'. It is important to note, that the origins of this alienation were, to a great extent, class-originated, coming from the specific economic grievances of the workers. Konrád and Szelényi, The Intellectuals, p. 226. Weydenthal, 'Poland: Workers and Politics', p. 187. Economic alienation eventually meant political alienation, which then began to organise itself. The strikers of 1980 were no longer easily reintegrated. They were 'far more disciplined; they were not committed to socialism or the party' any longer, whereas large sections of Poland's industrial workers in 1970 were still prepared to give the regime another chance, in 1980 Lech Wałęsa bitterly reminded his co-workers of how little had been done with the workers' trust and support. If the Party was not prepared to listen to their grievances anymore, the workers would organise themselves outside the state. This essentially foreboded the inability to continue any profitable form of communism. As R.J. Crampton observed, Crampton, Eastern Europe, p. 367. Garton Ash, The Polish Revolution, p. 13. Konrád and Szelényi, The Intellectuals, p. 227. 'Solidarity had shown the bankruptcy of east European communism because the bulk of communism's chosen people, the workers, had looked outside the party for redress of their social grievances'.Crampton, Eastern Europe, p. 376. We have observed that the regime became increasingly inapt to rally the support of the workers for the socialist cause, or to reasonably incorporate them within a plan economy. It is, nonetheless, important to have a closer look at the reasons that caused the process to assume such a radical working class character. Surely, there existed a strong section of ""critical intelligentsia"", yet their position was precarious: whereas they assumed a role as the voice of the working class, they were also 'part of the intellectual class, from which neither its structural position nor its value-system [allowed] it to secede'. It is said, moreover, that moral decay could be observed across the intelligentsia - so much so, that one sociologist spoke of 'the 'semi-feudalisation' of Polish society'. The success propaganda of communist society that western historians are quite familiar with, also seemed to have blinded the regime, rendering it numb to criticism or alternative ideas. This hostile view, however, is only useful to a certain extent, and it is more likely that many members of the intelligentsia simply still '[had] a stake in the status quo and, therefore, support[ed] it'. Konrád and Szelényi, The Intellectuals, p. 221. Barker, Festival of the Oppressed. , p. 13. Ibid, p. 12. Connor, 'Social Change and Stability in Eastern Europe', p. 27. There were, however, also initiatives from the intelligentsia within broad daylight of the Party's political sphere, which provided powerful criticism of the way in which the state treated dissident workers. And while many intellectuals desiring reform continued to trap themselves in 'an iron cage' of 'the limits of possible reform', organisations such as the Committee in Defence of the Workers (KOR) set out in a number of semi-legal ways to create a bond of solidarity between workers and intellectuals, an number of them eventually becoming involved in Solidarity. Examples of selfless and risk-taking initiatives are not hard to find; intellectuals such as Miroslaw Chojecki lost their jobs, continuously spent time in jail and got beaten up, in the meanwhile enabling the publication of 150 illegal books. The problem of intellectualism and political involvement, however, would eventually cause problems by 1989, when, during the round table talks intellectualism gave the negotiations an air of secrecy and 'institutional horse trading'. Crampton, Eastern Europe, p. 364. Barker, Festival of the Oppressed, p. 16. Garton Ash, The Polish Revolution, p. 18. Barker, Festival of the Oppressed, p. 15. Jadwiga Staniszkis, The Dynamics of the Breakthrough in Eastern Europe. The Polish Experience (Berkeley: 1991), p. 143. By 1970, n increasing awareness of the existence of social class was observed in Polish society. This classification was not so much based upon political affiliation as upon material wealth. Rapid industrialisation since World War Two had brought new realities urban workers, a large number of them of young age, in a setting of narrowly confined economic possibilities. When rapid economic and industrial development began to slow down and the initial success story of social mobility began to lose its stretch, problems became observable. The popular official state narrative of egalitarianism became increasingly exposed, whereas the state had little means to provide alternative justifications. In a situation that was becoming more and more complicated, the workers soon discovered the power of strike and protest, something which the regime was rather fearful of. With no alternative means of political participation, the working class was rapidly becoming self-aware and radical, while the regime, with a large number of critical intellectuals torn between the rigor of party policy and the difficulty of reform, proved unable to control economic decline and the social outcry against party elitism and decadence. A state that claimed to hold the interests of workers as its highest principle was rapidly losing support from just that group. The strikers of Solidarity were no longer easily drawn back into the folds of socialist society which had, according to them, structurally failed to improve itself. A number of critical intellectuals, however, come to the aid of the workers. The KOR created effective means of communication between different workers' groups, and bridged the gap of solidarity between intellectuals and workers. Nonetheless, the development of Solidarity must be viewed as a broad expression of workers' grievances with the sympathy and support of a number of critical intellectuals.","The emergence of the sociological school of ethnomethodology in the early 1960s created a shock wave across the sociological establishment. Its revision of social norms and their relation to the social action springing from individual cases of social interaction was indeed deemed so radical that stories exist of 'ethnomethodologists being fired from their jobs simply because they were ethnomethodologists' (Craib 1992: 102). However, what may at first glance appear a drastically individualistic approach to social relations theory must be seen as embedded within a wider philosophical and social scientific critique of the value of conventional types of knowledge (Alexander 1987). Ethnomethodology asserts that functionalist theory lays an exaggerated amount of emphasis on collectivity within social relations, crystallised in organisations and norms of interaction that are derived therefrom. By first theorising a consistent system of norms, subsequent data gathered from social observation are then ""made to fit"" within this system. These data are, as traditional sociology has it, 'as it were, readily waiting' (Silverman 1972: 173) to be categorised. This, however, naturally creates a tension, as 'the meaning employed by those we study - their norms, values, attitudes and beliefs; the rules which govern their conduct - are treated as if their meaning were unproblematic' (Craib 1992: 103). This can even be observed to be the case in highly advanced social theories, as the overarching theoretical grid still requires the classification of a highly contestable and unstable social reality ""out there"": 'To employ such 'operational' or 'theory-laden' categories is not to escape the ascriptive or imputative work that ordinary action-concept usages carry with them' (Coulter 1979: 11). By failing to observe the complication characteristic of such norm-attribution, sociologists fall into the trap of taking sets of typification for granted, thereby tacitly avoiding the 'problem of concealed, commonsense commitments' (Coulter 1979: 15) to which all social actors involved conform. Instead, according to ethnomethodologists, the only way to avoid adding to such a body of affirmative ""folk sociology"" is to 'seek to understand the process of experiencing' (Silverman 1972: 166), by searching for the underlying motivation of action of each social actor. Such a methodology abstains from explaining social phenomena in immediate relation to abstract, theoretical norms, deriving actual interaction from alternative, everyday commonsensical approaches to social interaction, which have also been described as ""second-order typifications"", 'typifications of our common-sense typifications which order the social world in a rational way' (Craib 1992: 99-100). Instead of taking methods of social ordering for granted, the sociologist may then concern himself or herself with 'the manner in which members use rules to do the work of defining and interpreting actions' (Silverman 1972: 179). In this essay I shall discuss the D. Lawrence Wieder's case study of ""the code"", a semi-formal normative body maintained as the set of social rules by which residents in a 'rehabilitative facility for narcotic-addict felons on parole' (Wieder 1974: 144) claim to measure the validity of their social action. This ethnomethodological ethnographic study may provide interesting insights into how, for all actors involved in any organisational structure, 'talk and action are produced and understood as indexical displays of the everyday world' (Cicourel 1973: 99). The problematic relation Wieder discovers between the norms as he has them explained to him, and as he sees those norms put in practice reveals the usefulness for, 'not a commonsense sociology but a sociology of commonsense' (Silverman 1972: 170), an often arbitrary and precarious battlefield for social power negotiation. I shall thereafter also look at the limitations of such an approach to sociological studies. In his analysis of the role of ""telling the code"", Wieder begins by looking more closely at the way in which he personally internalised its meaning. As he received information on the content of ""the code"" piecemeal, it became necessary to create an internal schema for understanding it. In this way, it became possible to interpret further bits of information, while simultaneously elaborating the internal system into a generic and constantly expanding body of understanding. 'Each newly encountered 'piece' of talk was simultaneously rendered sensible by interpreting it in terms of the developing relevancies of the code and was, at the same time, more evidence of the existence of that code' (Wieder 1974: 161). Hereby, Wieder points out the indexical functioning of the ""code system"": the various norms at the core serve to interpret behaviour which is said to be based on it. However, this ""indexing"" of behaviour was not quite an unproblematic or effortless method. Behavioural significance was rendered only 'meaningful in the ways that it was said socially-in-a-context' (Wieder 1974: 163). While Wieder initially observed that he related residents' uncooperative attitudes quite naturally to the norms described in the convict code, he found that such categorisation could at times be rather arbitrary, resulting in situations in which  '...either chosen alternative would be equally plausible interpretations in terms of the same rule, even though the alternatives propose opposite actions. [...] Instead of 'predicting' behavior, the rule is actually employed as an interpretative device' (Wieder 1974: 168).According to Wieder, all of residents' behaviour was, in the end, placed, and thereafter rendered rational, in the light of the convict code. It created, for the observer, a method in which social '...order and orderliness is accomplished - that is, it involves interpretive ""work""' (Silverman 1972: 175). In the light of such formalistic social meta-theories as Durkheim's, the position of residents is understandable in terms of the social force of the convict code weighing upon them as an ""institutionalised"" norms of interaction. This position, however, fails to see the manipulative, indefinite character of the convict code, using instead 'the fact of norm compliance to assume that [the residents'] action has a compliant character, that it is passive and conformistic rather than active and constructing' (Alexander 1987: 274). In opposition to this, ethnomethodology, rather taking an individualistic stance, argues that in the enactment of norms (in this case described as ""telling the code""), 'there is negotiation vis-à-vis [the meaning of these] rules'. In a similar vein, Cicourel has argued that 'this [...] environment of objects cannot be tied easily to notions like status, role and norms, employed by actors in less structured or controlled everyday situations' (Cicourel 1973: 15). In other words, norms only derive meaning from their enactment, which is continually negotiated and whose stability depends on the degree of acceptance which the various social actors concede to the meaning contained within the enactment. Under normal circumstances, the process of norm negotiation takes place without great complication, as 'we are frequently engaged in making connections in our descriptions' (Coulter 1979: 15). In this process of ""making sense"" of the social stimuli around us, we draw upon previous experience, thus arriving at 'a series of commonsense constructs [which we] have pre-selected and pre-interpreted [from] this world which [we] experience as the reality of their daily lives' (Schutz in Coulter 1979: 9). Beyond this, the analogy of ""making sense"" extends further in the sense of serving the purpose of creating structural coherence and predictability in our perception of the world that we experience. A convincing set of behavioural rules such as the convict code is necessary to order the residents' forms of social interaction, which is, in fact, 'knowledge [that] is itself inherently unstable, something which is created anew in each encounter' (Craib 1992: 102). The precariousness of resident-staff interaction is pointed out by its strong dependence on contextuality. With ""the code"" in mind as an interpretative tool, all actors involved (which in this case may involve the residents, members of staff and the observer), continually scan incoming social stimuli for recognisable modes of behaviour which may contextually seem logical. In doing this, variables as the particular social actors, the setting and the timing play an indispensable role for practices of indexing, i.e., ""sense-making"" (Cicourel 1973: 101, Wieder 1974). In this context, staff's reaction to residents' 'problematic acts' were not confronted as subversive behaviour; rather was their character now converted 'into instances of a familiar pattern' (Wieder 1974: 151). In this way, the facility's staff utilised the convicts' code in order 'to formulate a recognizable coherent story, standard, typical, cogent, uniform, planful, i.e., a professionally defensible, and thereby, for members, a recognizably rational account' (Garfinkel in Alexander 1987: 262-3). Unsurprisingly, Wieder sees ""telling the code"" in the first place as a means of justification of the chosen course of action. By ""telling the code"", residents explain and justify why they will not cooperate in certain situations or answer certain questions posed by staff. By accepting the code, staff, in turn, account for residents' uncooperativeness in activities and rehabilitative procedures. ''Telling the code' provided staff with a useful way of talking about residents and themselves which portrayed both teams as more or less reasonable and more or less helpless to change the character of the relationship between the teams, because of the social-fact character (in Durkheim's sense) of the regularities made available by 'telling the code' which were none of anybody's specific doing or responsibility' (Wieder 1974: 151)Interesting is Wieder's reference to Durkheim: in ""telling the code"", all social actors in the facility render types of social interaction institutionalised, thereby attempting to (though not actually) reifying them (Durkheim's ""social facts""). With this, however, the door is only just opened to further and more penetrating interpretations of action-motivation. Ethnomethodology points out that, in order to gain real understanding that means something, 'intentions and the actor's views are always potentially relevant and must be taken into account' (Pitkin in Coulter 1979: 12-3). Indeed, by assuming interpretation ends at normative causality, sociologists inevitably miss out on the politics underlying social action (Coulter 1979: 11, Silverman 1972: 175). The darker, more precarious side of social interaction lies in the tensions of power underneath its surface. Ethnomethodology does not deny the existence of a ""common culture"", and this would be an absurd claim to make (Alexander 1987: 262); however, the precariousness of social interaction is exactly that what this ""common culture"" is not: individuals may challenge the existing order by either directly confronting it, or, more subtly, by manipulating its general traits to their advantage. Whether this challenge is pursued consciously or not is, strictly spoken, not relevant; as Pitkin asserts, 'One may have to intend to lie in order to lie, but one need not intend to deceive in order to deceive' (Pitkin in Coulter 1979: 12). An analogy may here be found in the functioning of penal codes, in that 'the fit is 'managed' through negotiating socially organized activities' (Cicourel 1973: 13) in which both hidden and open methods of manipulation are used. The position Wieder takes in his account of the role of the convict code draws on the traditions of both radically challenging all knowledge in a systematic way, and emphasising the inescapable burdens of responsibility that social action brings with it. It is therefore no surprise that 'phenomenological sociology, existential sociology, [and] the sociology of everyday life' (Craib 1992: 97) are often mentioned in conjunction with one another. The indefiniteness of society described by such a sociology stresses a few characteristics. In the first place, we can observe that social ""roles"" have a dynamic character that extends beyond the immediately observable social surface. It can be observed that 'the dynamic aspect of status or office and such is always influenced by factors other than the stipulations of the position itself' (David in Cicourel 1973: 14). From this follows, secondly, that the situational placement of social interaction takes an important place in determining its meaning, i.e. it is not just about what actors speak, but more importantly about how communication takes place (Garfinkel in Alexander 1987: 270). Thirdly, the constant change in interpretation of social norms results in an extreme degree of relativism invoked by ethnomethodology (Craib 1992: 102). In assuming the '""adequate"" or ""appropriate"" operational definitions' (Silverman 1972: 177), staff of the ""halfway house"" avoided facing up to the existential question of where this appropriateness was derived from. Thus, Wieder can ultimately only conclude one thing: ''Telling the code' masked what was 'really going on' at the halfway house - what was 'really going on' was residents' pursuing their immediate interests by fabricating instances of 'telling the code' which deceived me and the staff' (Wieder 1974: 169). Naturally, Wieder's approach is not free from criticism. It can, in the first place, be said that it gives a rather cynical, or at least extremely sceptical account the natural course that social interaction takes within the example of his case study. While underlying forces may be at work in social interaction, it is entirely possible that even the social actors involved are not aware of them. Residents in the facility may, in fact, feel that behaviour represents a genuine reflection of the loyalty and distrust that the convict code at varying times demands of him. Likewise, it is not unrealistic to think that staff generally recognises the ""realness"" of the code and in instances acts in accordance with it to protect certain individual residents. In other words, the convict code is not entirely without justified grounded. This problem brings under discussion the extent to which the freedom to negotiate socials norms actually exists. Regarded as a whole, society can be said to be rather more stable than the Wieder claims from within the ethnomethodological perspective. Thus, while Wieder's approach may be laudably seen to attempt to incorporate 'a genuine humanism in an ideological sense', it can be asked '[w]hether this moral humanism is theoretically justified' (Alexander 1987: 280). In his account of how the convict code is to be interpreted, Wieder emphasises how it obtains a indexical status. Rather like the coding system of books in a library, its norms serve to interpret rather than to describe it. Thus, norms serve as a tacit justification of real events, for both social actors and the sociologist. A truer account of events, is, however, more complicated. By accepting norms as motivation and authentic drives of social actors, the convict code came to serve as a tool of rendering behaviour logical and coherent. For staff, this tool justified an acceptance of nonconformist behaviour from the residents' side, while the residents often successfully manipulated it to their direct advantage. This underlying differentiation reveals the political and arbitrary nature of social norms, however developed and ""theory-laden"". In his approach to sociology, Wieder, and the wider school of ethnomethodologists, build upon the work of phenomenology and existentialism to show the inescapability of situational variables in the everyday context of social interaction. Thus, the ""what"" of social norms must separated from the ""how"" of interpretation, and subsequently analysed at a different level. This approach, however, has not remained uncriticised. Many sociologists deem the extent of manipulative power that ethnomethodologists award the individual with unrealistic. According to them, behaviour is more context-bound and can be seen to be derived from this context. In that case, Wieder's account of the convict code serves, foremostly, as an existential reminder of the forces underlying our everyday commonsense social interaction. .",False
26,"The advent of the Solidarity strikes and protests halfway 1980 struck a serious blow at the Polish communist regime. Whilst it provided a powerful response of workers to the repressive character and 'determination [of the Polish regime] to maintain [the political and economic] systems, whatever their human costs and lack of legitimacy', an important questions comes to mind. Where from came this powerful articulation of demands, and of 'the most powerful, sophisticated and advanced working-class movement yet seen, certainly in the 'communist' sphere and perhaps anywhere in the world'? To answer this question, it may be fruitful to study the emergence of new class societies in Eastern Europe, paradoxical when matched with Marxist thought. Economic and political developments throughout the 1970s united with a blend of national-historical and wider Eastern European economic grievances to prepare the ferment for 'a new generation of protesting workers with a clearly different way of articulating their grievances [as] a strictly Polish phenomenon'. In this essay I will have a closer look at the development of a class spirit among Polish workers, and argue that it was from their socio-economic situation that discord arose. Walter D. Connor, 'Social Change and Stability in Eastern Europe' in Problems of Communism, vol. 26 (1977), p. 16. Colin Barker, Festival of the Oppressed. Solidarity, Reform and Revolution in Poland 1980-81 (London: 1986), p. 11. Walter D. Connor, 'Social Change and Stability in Eastern Europe' in Problems of Communism, vol. 26 (1977), p. 29. Jadwiga Staniszkis, The Dynamics of the Breakthrough in Eastern Europe. The Polish Experience (Berkeley: 1991), p. 107-8. The prehistory of the Solidarity movement was, most importantly, sparked by an increasing crystallisation of a ""class consciousness"" within the ranks of the Polish work force, many of whom were becoming convinced 'that they [could only] improve their lot collectively'. Despite official state rhetoric, there was an increasingly realistic attitude to the existence of a new dominant class made up of 'the intelligentsia as a whole rather than just the bureaucracy narrowly defined'; a class of highly educated, better-rewarded technocrats and managers, charged with the daily affairs of increasing efficiency of production and economic policy making. While official policy claimed to pursue a policy of egalitarianism, the reality behind these claims was only limited. The producing classes, those of the workers and peasants, found themselves on the same position in the material hierarchy as before, while a decrease in real standards of living were initially concealed only by the relative success of social mobility. However, the stretch of social mobility had its limits and by the 1970s a new generation of workers, unfamiliar with opportunities for social mobility, many of them younger than 25 years of age, began to form an increasingly articulate and assertive sense of collective grievances. While according to the orthodox theory of communism the party was the vanguard of the workers, epitomising its real interest, workers on the work floor reportedly referred to 'management in the third person, reserving ""we"" for themselves'. This clearly meant the beginning of an independently developing working class, at odds with the intelligentsia and a regime increasingly alienated from Poland's new social reality. Walter D. Connor, 'Social Change and Stability in Eastern Europe' in Problems of Communism, vol. 26 (1977), p. 28. Georg Konrád and Ivan Szelényi, The Intellectuals on the Road to Class Power (Brighton: 1979), p. xiv-v. Walter D. Connor, 'Social Change and Stability in Eastern Europe' in Problems of Communism, vol. 26 (1977), p. 17. R.J. Crampton, Eastern Europe in the Twentieth Century - And After (London/New York, 2005), p. 364. Konrád and Szelényi, The Intellectuals, p. 223. Similar to the England of the early nineteenth century, the creation of a new Eastern European working class indeed took place within the rather short timeframe of one or two generations. Over the decades after the Second World War, an average of nearly fifty percent of the younger peasantry came 'to form the new working class, [and] to create the social drama of a new life, a new world'. As their direct socio-economic interests began to reveal themselves in increasing antagonism with the state-directed economic policy, Polish workers, as a class an sich and with an agenda different even from that of the oppositional intelligentsia, more and more often became involved in violent confrontations with the state. Social discord became increasingly apparent, and ''Poles murdered by Poles, workers murdered by a 'Workers' State', became the symbol for all their accumulated grievances'. Connor, 'Social Change and Stability in Eastern Europe', p. 21. Ibid, p. 28. Garton Ash, The Polish Revolution, p. 12. Timothy Garton Ash's assertion that 'December 1970 [was] the single most important date in the pre-history of Solidarity', may provide further insight into the extent that Solidarity was connected to the working class. Its reaction to the government's implementation of food price increases revealed signs of a growing unity and singular purpose of action. When the government resolved to give up its policy of consistent non-unemployment 'aimed at forcing greater productivity through the use of threat of unemployment as an incentive for better and more efficient work', this seemed a direct threat to workers at large. After strikes and violent confrontations throughout January 1971, the government withdrew its economic reform plans, while Gomułka was forced to step down as a consequence of his unpopular repressive measures and number of deaths as a direct consequence thereof. This political outcome was seen by many protesters as a direct result of their protests, which served made workers realise where there strength lay. Throughout the 1970s, many strikes followed, and although the regime 'exacted its revenge on the workers', resulting in many workers being 'sacked, arrested, beaten, jailed', strikers usually got what they wanted. This, and the fact that many protests 'involved a large group which was generally representative of the worker population as a whole', accounts for the growth of an awareness that the economic situation of workers was connected with, and could be influenced by, their political role as an occupational group. Ibid, p. 12. Jan B. de Weydenthal, 'Poland: Workers and Politics' in Jan F. Triska and Charles Gati (eds. ), Blue-Collar Workers in Eastern Europe (London: 1981), p. 193. Crampton, Eastern Europe, p. 359. Weydenthal, 'Poland: Workers and Politics', p. 194. Barker, Festival of the Oppressed, p. 12. Weydenthal, 'Poland: Workers and Politics', p. 195. Yet, what made for the timing of this growing self-awareness? Where from came the momentum that enabled workers to start attempting to influence their economic position through strikes? The Polish communist party had, from the onset, based much of its legitimacy through the reasonably successful initiation of a process of social mobility upward. Workers were recruited from among the peasant ranks, while better-educated workers were needed to support the growing bureaucratic apparatus. These developments enabled many to enter new realities that were hitherto undreamed of, creating a sense of greater equality, if not of material wealth, then of opportunity. It also sparked, however, another process, which would later develop into one of the primary reasons for the antagonism between the workers and the intelligentsia as a whole. The quick development of industries created a culture of technocracy, in which economic status became more directly related to 'a hierarchy of statuses based on diplomas, irrespective of the actual amount of work done'. This put pressures on access to higher education as a direct way of material advancement, as the intelligentsia were protective of their traditionally advantaged position. The attitude of this ""red bourgeoisie"" was becoming an assertion '[t]hat they do and should live better than workers'. This, however, created tension, as these 'unequal patterns of allocation of social rewards' were legitimised by the class that benefited from it, 'despite the traditional systemic commitment to egalitarianism'. With the decreasing flexibility in the Polish labour market, most economists and sociologists began to note a shift from 'phases of system-building to the tasks of system-maintenance'. The loss of revolutionary economic appeal for the masses, however, eventually began to cost the Party members as well; 'between October 1980 and March 1981 the Party lost 216,000 members, bringing total membership below three million'. Connor, 'Social Change and Stability in Eastern Europe', p. 24. Konrád and Szelényi, The Intellectuals, p. 228. Connor, 'Social Change and Stability in Eastern Europe', p. 26. Weydenthal, 'Poland: Workers and Politics', p. 191. Connor, 'Social Change and Stability in Eastern Europe', p. 25. Garton Ash, The Polish Revolution, p. 170. The stagnation of social mobility can be said to have a two-sided effect. While, one the one hand, it formed a ferment for discontent because the state failed to meet the workers' expectations of equality, Eastern European 'regimes [had to], in a sense, deal with the workers they have'. While the legitimating narrative came out of redistribution, 'the technocrats, like the ruling elite itself, [had] an interest in keeping labor in a dispositive position', which entailed maintenance of the status quo. A further source of frustration was found in the fact that, despite repeated pledges for reform from the party leadership, very little change was noticed, while concessions were repeatedly watered down. This was particularly hard to swallow since continually facts were coming to light revealing the decadent and privileged world that the small number of the party and administrative élite' had created for themselves. Contradictions were experienced very starkly when any kind of protest was followed by measures on a worker's economic privileges and 'any sign of active solidarity' was immediately called 'political conspiracy'. Gradually, workers began to connect their economic discontent with a wider, social picture, which thoroughly undermined the legitimacy of the state. As now the argument of economic prosperity was no longer something Gierek could count on, the social and political situation became something of a ticking bomb. Connor, 'Social Change and Stability in Eastern Europe', p. 29. Konrád and Szelényi, The Intellectuals, p. 225. Weydenthal, 'Poland: Workers and Politics', p. 190. Crampton, Eastern Europe, p. 362. Konrád and Szelényi, The Intellectuals, p. 227. Connor, 'Social Change and Stability in Eastern Europe', p. 32. The decade running up to the strikes of saw an increasingly untenable paradoxical situation. According to Marxist thinking, communism provided the means to stop the alienation of the workers; yet, they were becoming increasingly alienated. This was clearly demonstrated by the slow production process of inferior quality produce. By the 1980s, an observer described ""the Polish situation as 'the emergence of workers as a distinct, albeit unorganized, political force and the inability of institutionalized leadership groups to integrate the workers into the existing pattern of systematic operations'. It is important to note, that the origins of this alienation were, to a great extent, class-originated, coming from the specific economic grievances of the workers. Konrád and Szelényi, The Intellectuals, p. 226. Weydenthal, 'Poland: Workers and Politics', p. 187. Economic alienation eventually meant political alienation, which then began to organise itself. The strikers of 1980 were no longer easily reintegrated. They were 'far more disciplined; they were not committed to socialism or the party' any longer, whereas large sections of Poland's industrial workers in 1970 were still prepared to give the regime another chance, in 1980 Lech Wałęsa bitterly reminded his co-workers of how little had been done with the workers' trust and support. If the Party was not prepared to listen to their grievances anymore, the workers would organise themselves outside the state. This essentially foreboded the inability to continue any profitable form of communism. As R.J. Crampton observed, Crampton, Eastern Europe, p. 367. Garton Ash, The Polish Revolution, p. 13. Konrád and Szelényi, The Intellectuals, p. 227. 'Solidarity had shown the bankruptcy of east European communism because the bulk of communism's chosen people, the workers, had looked outside the party for redress of their social grievances'.Crampton, Eastern Europe, p. 376. We have observed that the regime became increasingly inapt to rally the support of the workers for the socialist cause, or to reasonably incorporate them within a plan economy. It is, nonetheless, important to have a closer look at the reasons that caused the process to assume such a radical working class character. Surely, there existed a strong section of ""critical intelligentsia"", yet their position was precarious: whereas they assumed a role as the voice of the working class, they were also 'part of the intellectual class, from which neither its structural position nor its value-system [allowed] it to secede'. It is said, moreover, that moral decay could be observed across the intelligentsia - so much so, that one sociologist spoke of 'the 'semi-feudalisation' of Polish society'. The success propaganda of communist society that western historians are quite familiar with, also seemed to have blinded the regime, rendering it numb to criticism or alternative ideas. This hostile view, however, is only useful to a certain extent, and it is more likely that many members of the intelligentsia simply still '[had] a stake in the status quo and, therefore, support[ed] it'. Konrád and Szelényi, The Intellectuals, p. 221. Barker, Festival of the Oppressed. , p. 13. Ibid, p. 12. Connor, 'Social Change and Stability in Eastern Europe', p. 27. There were, however, also initiatives from the intelligentsia within broad daylight of the Party's political sphere, which provided powerful criticism of the way in which the state treated dissident workers. And while many intellectuals desiring reform continued to trap themselves in 'an iron cage' of 'the limits of possible reform', organisations such as the Committee in Defence of the Workers (KOR) set out in a number of semi-legal ways to create a bond of solidarity between workers and intellectuals, an number of them eventually becoming involved in Solidarity. Examples of selfless and risk-taking initiatives are not hard to find; intellectuals such as Miroslaw Chojecki lost their jobs, continuously spent time in jail and got beaten up, in the meanwhile enabling the publication of 150 illegal books. The problem of intellectualism and political involvement, however, would eventually cause problems by 1989, when, during the round table talks intellectualism gave the negotiations an air of secrecy and 'institutional horse trading'. Crampton, Eastern Europe, p. 364. Barker, Festival of the Oppressed, p. 16. Garton Ash, The Polish Revolution, p. 18. Barker, Festival of the Oppressed, p. 15. Jadwiga Staniszkis, The Dynamics of the Breakthrough in Eastern Europe. The Polish Experience (Berkeley: 1991), p. 143. By 1970, n increasing awareness of the existence of social class was observed in Polish society. This classification was not so much based upon political affiliation as upon material wealth. Rapid industrialisation since World War Two had brought new realities urban workers, a large number of them of young age, in a setting of narrowly confined economic possibilities. When rapid economic and industrial development began to slow down and the initial success story of social mobility began to lose its stretch, problems became observable. The popular official state narrative of egalitarianism became increasingly exposed, whereas the state had little means to provide alternative justifications. In a situation that was becoming more and more complicated, the workers soon discovered the power of strike and protest, something which the regime was rather fearful of. With no alternative means of political participation, the working class was rapidly becoming self-aware and radical, while the regime, with a large number of critical intellectuals torn between the rigor of party policy and the difficulty of reform, proved unable to control economic decline and the social outcry against party elitism and decadence. A state that claimed to hold the interests of workers as its highest principle was rapidly losing support from just that group. The strikers of Solidarity were no longer easily drawn back into the folds of socialist society which had, according to them, structurally failed to improve itself. A number of critical intellectuals, however, come to the aid of the workers. The KOR created effective means of communication between different workers' groups, and bridged the gap of solidarity between intellectuals and workers. Nonetheless, the development of Solidarity must be viewed as a broad expression of workers' grievances with the sympathy and support of a number of critical intellectuals.","While it is certain that the reformation which set off in the twenties of the fifteenth century was preceded by more than a century of humanism challenging, among other things, man's attitude toward theological issues, the extent and influence of the humanist movement are still a matter of debate. While it has been asserted that humanist thought centred around the capabilities of the human as gifts of God, as opposed to the middle age view of man as 'a humble, fallen creature under the permanent curse of original sin', the unity of the humanist agenda has to a large extent been exaggerated, and the various strands of humanism across Europe 'do not appear to have adopted a coherent position on matters of substance'. What does emerge upon careful assessment, though, is a general set of new attitudes and principles, often contrasting those of the medieval scholastic movement which is shared by both the humanist and reformer. Moreover, a careful assessment of the general theological position inherent in most of humanist thought must be contrasted to that of the reformers' movement to come to a fuller appreciation of how humanism came to influence reforming thought and vice versa. Noel L. Brann, 'Humanism in Germany' in Albert Rabil, Jr. (ed. ), Renaissance Humanism. Foundations, Forms and Legacy. Vol. 2, Humanism Beyond Italy (Philadelphia, 1991), pp. 123-4. Eckhard Bernstein, German Humanism (Boston, 1983), p. 4. Alister McGrath, The Intellectual Origins of the European Reformation (Oxford/New York, 1987), p. 34. C. Scott Dixon, The Reformation in Germany (Oxford/Malden, 2002), p. 35. McGrath, The Intellectual Origins, p. 41. The coming of humanism to Germany virtually coincided with the founding of the first German university in Prague. Indeed, the establishment of universities brought a fresh outlook to the studies of theology and morals, entrenching the humanist tradition (studia humanitatis) ever firmer into German scholarly attitudes. By the beginning of the sixteenth century, sixteen university had been founded across the German lands, which in various degrees pursued the methods of a new kind of studies. Professors of the studia humanitatis stressed text-based studies, thereby aiming to gain powerful and valuable insights of Christian religion brought back to a purer interpretation of the biblical text. Their slogan ad fontes signified, most importantly, an attempt to get closer to the true intention of the scriptures. Moreover, the via moderna, as this new interpretation of religion came to be called, also challenged the minutely derived rationalist viewpoints of medieval theologians (known as the via antiqua) by stressing the value of human experience and interpretation. For example, the German philologist and humanist Rudolf Agricola sought to explore this in his writing. Thus, he came to assert: 'Prodigious, immense, and unbelievable is the power of the human mind'. A renewed strong interest with secular Latin and Greek authors derived directly from the humanists' concern with the reform of language into clear uncomplicated formulations so as to arrive at more logical conclusions. Such linguistic reform the humanists found through 'the observation and imitation of the best authors and writers', classics such as 'Virgil, Lucan, Horace [...], Plautus, Terence, Cicero, Sallust, Seneca, and the Roman historian Valerius Maximus'. Bernstein, German Humanism, p. 5. Brann, 'Humanism in Germany', p. 130. Scott Dixon, The Reformation in Germany, 15. McGrath, The Intellectual Origins, p. 40. Scott Dixon, The Reformation in Germany, p. 16. Bernstein, German Humanism, p. 4. Brann, 'Humanism in Germany', p. 132. As considered suitable by humanist Jakob Wimpfeling. See Bernstein, German Humanism, p. 32. It goes without saying that, by permitting themselves such a new, revisionist outlook, the northern European humanists soon came to criticise what they considered unjustified behaviour within the church and among all Christians in general, be it while viewing themselves inside the religious order and thus without calling into doubt the authority of the church. Still, in many respects, humanism undermined the absolute authority of the church organisation, stressing the humble, speculative element of religious life. Erasmus suitably quoted St. Paul in his work Praise of Folly (1509): 'I speak it not after the Lord, but as it were foolishly'. McGrath, The Intellectual Origins, p. 36. Desiderius Erasmus, 'Oration: Theological Dispute' in The Praise of Folly,  URL  (1509) By the time Luther came in conflict with the church, humanism had become well engendered in Germany. But besides the Italian humanist influences of text-based study, German humanists furthermore came to incorporate the creation of a distinct German identity in their writing. Around the turn of the century, several texts were published praising Germany's pious and learned traditions, with at its foundation the rediscovering of Tacitus' Germania in 1455. It was within this tradition that the greatest reformers received their education. The studies of Zwingli in Vienna, the Swiss reformer, coincided with the arrival to that city of Conrad Celtis, Germany's poet laureate, while Luther regarded humanist techniques of philology and the close study of the scriptures as the key to his theological discoveries. Similarly, Calvin, a decade later, would be grounded in the study of bonae litterae. Yet, the position of the reformers went beyond that of the humanists in theological zeal and single-mindedness. While most humanists agreed that '[t]he irreducible particularity of human reality was [...] more suited to historical description, rather than logical analysis', they proposed as a solution simply the reform of education through training in rhetoric and the study of 'ancient philosophers, poets and orators', as Celtis argued already in 1492. The differences in point of view of reformers like Luther and Zwingli initially appeared rather blurred to many humanists, which in effect strengthened their case. It was this 'productive misunderstanding' which made a great controversy out of Luther's theological uneasy with some of the church's practices. The theoretical misunderstanding is to some extent no surprise. Humanists and reformers alike shared many principles which clothed the doctrinal move away from the church in humanist language. Thus, when Luther wrote of his breakthrough in the theory of justification, that '[h]itherto, this epistle has been smothered with comments and all sorts of irrelevances, yet, in essence, it is a brillant light, almost enough to illuminate the whole bible', this certainly bore the semblance of the humanists' study of the scriptures ad fontes. Scott Dixon, The Reformation in Germany, p. 15. McGrath, The Intellectual Origins, Zwingli, p. 45; Calvin, p. 54; Luther, p. 59. Scott Dixon, The Reformation in Germany, pp. 47-8. McGrath, The Intellectual Origins, p. 39. Bernstein, German Humanism, p. 60. McGrath, The Intellectual Origins, p. 41. Martin Luther, 'Preface to the Epistle of St. Paul to the Romans' in John Dillenberger (ed. ), Selections From His Writings (New York, 1962), p. 19. Historians have argued that '[t]o a certain extent, the reformers were simply confirming the advances made by the Humanist agenda, with its stress on ancient languages, its tools of exegesis and its approval to the sources'. This assertion rings a certain truth, in that humanists were quite ready to speak out against any kind of practice which they considered a perversion of the true faith. Consequently, we find that Erasmus freely criticised the incorporation of pagan elements into the Christian faith, and the complicity of the clergy in these practices: 'so it be of ghosts, spirits, goblins, devils, or the like; [...] the further they are from truth, the more readily they are believed and the more do they tickle their itching ears. And these serve not only to pass away time but bring profit, especially to mass priests and pardoners'. Furthermore, he criticised the fact that selling indulgences evaded the simple fact that 'to live well is the way to die well'. While humanist and reformer alike stressed the simple and direct relationship in which the individual stood to God, the period directly preceding the reformation saw the sweeping popularity of so-called sacramentals, small objects that were to have sacred protective powers, turning the religious experience into 'a kind of popular Catholic version of the priesthood of all believers'. Catholic theologians were quick to assert that the working of sacramentals worked, not through physical efficacy; rather, it was spiritual efficacy, based on the prayers of the church', yet, Luther attacked the very authority of the church as an intermediary between the individual and God. 'How much more properly did the apostles call themselves servants of the present Christ', he wrote in an open letter to pope Leo X, 'and not [like the clergy] vicars of an absent Christ?'. Such boldness did certainly lie outside the tradition of pre-reformation humanists, who, despite their criticism, were nearly all 'men who saw themselves as operating within the context of the church'. Scott Dixon, The Reformation in Germany, p. 47. Desiderius Erasmus, 'Oration: Magical Charms' in The Praise of Folly,  URL  (1509) Ibid. R.W. Scribner, Popular Culture and Popular Movements in Reformation Germany (London/Ronceverte, 1987), p. 12. Ibid, p. 10. Martin Luther, 'The Freedom of a Christian' in John Dillenberger (ed. ), Selections From His Writings (New York, 1962), p. 51. McGrath, The Intellectual Origins, p. 36. On a note, however, it must be added that much of the question of the relation between humanism and the reformation depends on definition. Kristeller has rightly pointed out that humanism, taken in a broad definition, 'was not [...] Catholic or Protestant, [...] although it is easy to find [for both of them] a certain number of humanists who favoured them'. More correctly was it a new scholarly attitude, which often had little directly to do with theological questions. Naturally, humanist achievements did much to aid reformist criticism of the church, thereby unintentionally damaging the pope's authority as ultimate arbiter of religious matters. By 1520, Luther felt sufficiently confident to claim that '[t]heir claim that only the pope may interpret scripture is an outrageous facied fable'. Moreover, while many humanists were entangled in a 'productive misunderstanding' of reformation doctrine, similarly 'the response of reformers and traditionalists to humanism was improvised and lacked any clear, programmatic agenda'. Paul Oskar Kristeller, 'Studies on Renaissance Humanism during the last Twenty Years' in Studies in the Renaissance, vol. 9 (1962), p.22. McGrath, The Intellectual Origins, pp. 47-8. Scott Dixon, The Reformation in Germany, p. 47. Patrick Hayden-Roy, 'Review of The Confessionalization of Humanism in Reformation Germany by Erika Rummel' in Renaissance Quarterly, vol. 54, no. 4, part 2 (Winter, 2001), p. 1636. The intellectual life of northern European humanists had brought extensive soil for social development. Influenced by Italian humanists, many German humanists had studied at Italian universities, while extensive correspondence and printed works guaranteed a considerable reading audience to some of the more famous scholars such as Erasmus. This intellectual network now came to serve the reformation twofold. While it came to counter the vices of ""popular religion"", which reformers and humanists alike attacked, it also created a well-publicised stage for a reformed alternative. While religious life in reformed thought experienced a kind of rationalisation, relying only on the scriptures (sola scriptura), all acts in daily life were now to the honour of God. Simultaneously, too, the reformers managed through humanist channels to justify this transformation. Luther's new doctrine of justification now freed Christians from 'an agreed bargain involving a near-legal sense of obligation', thus evading the need for an authoritarian church. Aided by forthright humanists such as the patriotic nobleman Ulrich von Hutten, the church was portrayed as 'an unjust, unkind, and bloodthirsty shepherd'. Thus, the reformation succeeded in countering 'religion in favour of an emotional sensibility too preoccupied with outward appearances', managing to some extent to draw parallels between the opposites religion/magic and Germany/Rome. McGrath, The Intellectual Origins, p. 38. Scribner, Popular Culture and Popular Movements, pp. 17-8. Scott Dixon, The Reformation in Germany, p. 47. Ibid, p. 13. McGrath, The Intellectual Origins, p. 40. Bernstein, German Humanism, p. 125. Scribner, Popular Culture and Popular Movements, p. 14. Ibid, p. 15. The spread of these new ideas came hand in hand. While many humanists, trained and educated in Italy, brought up 'questions that [...] were discussed openly and debated in public', their sojourns in Italy generally brought out 'what was peculiarly Germanic'. Luther's address to the German nobility, then, must also be seen in this light. 'All classes in Christendom, particularly in Germany [my italics], are now oppressed by distress and affliction', Luther wrote, and it is not unlikely that some of the German nobility supported his reformed Christianity for national-political reasons. After all, since Luther refuted the idea that any active works or confession were of any importance in an individual's relationship to God, this discredited the authority of the church as a body of absolute mediation. Scott Dixon, The Reformation in Germany, p. 59. Brann, 'Humanism in Germany', p. 137. Martin Luther, 'An Appeal to the Ruling Class of German Nationality as to the Amelioration of the State of Christendom' in John Dillenberger (ed. ), Selections From His Writings (New York, 1962), p. 405. C. Scott Dixon, The Reformation in Germany (Oxford/Malden, 2002), p. 46. To conclude, the role of humanism in preparing the ground enabling a reformation to take place in Germany and the rest of northern Europe was very great. Despite the fact that many a humanist was, and remained, supportive of the church in Rome, the humanist challenges to education, thereby firmly establishing the studia humanitatis around Europe, was of tremendous importance for the theological and social atmosphere to bring about further-going criticism of the church institutions. Moreover, many of the humanist values were shared by the reformation movement. The emphasis on text-based studies was of key importance to Luther (sola scriptura), while the study of classical Greek and Latin texts enabled Zwingli to successfully challenge the church's position on several matters. Both humanists and reformers criticised the incorporation of pagan elements such as sacramentals, which they considered charms, and the church's trade of indulgences. Indeed, Luther's ideas and style of criticism of the church bore such semblance to humanist traditions, that the first period of clash saw the support of many humanists, which has been described as a 'productive misunderstanding', which, additionally, brought much attention to the Wittenberg disputation through the humanists' extensive correspondence network. McGrath, The Intellectual Origins, p. 47. The exact relation between humanists and reformers has been a matter of dispute among scholars of our time. It depends, firstly, on a definition of humanism. While humanism knew certain general traits, it was not necessarily confined by a certain theological outlook, and the humanist movement was never united on theological matters. In fact, the period after the reformation saw the emergence of a number of prominent Protestant humanists. Thus, we can say humanism and reformation were mutually influencing, the reformer often using humanist tools in his pursuits, while many humanists, often demonstratively Teutonic, admired the reformation movement.",True
27,"While it is certain that the reformation which set off in the twenties of the fifteenth century was preceded by more than a century of humanism challenging, among other things, man's attitude toward theological issues, the extent and influence of the humanist movement are still a matter of debate. While it has been asserted that humanist thought centred around the capabilities of the human as gifts of God, as opposed to the middle age view of man as 'a humble, fallen creature under the permanent curse of original sin', the unity of the humanist agenda has to a large extent been exaggerated, and the various strands of humanism across Europe 'do not appear to have adopted a coherent position on matters of substance'. What does emerge upon careful assessment, though, is a general set of new attitudes and principles, often contrasting those of the medieval scholastic movement which is shared by both the humanist and reformer. Moreover, a careful assessment of the general theological position inherent in most of humanist thought must be contrasted to that of the reformers' movement to come to a fuller appreciation of how humanism came to influence reforming thought and vice versa. Noel L. Brann, 'Humanism in Germany' in Albert Rabil, Jr. (ed. ), Renaissance Humanism. Foundations, Forms and Legacy. Vol. 2, Humanism Beyond Italy (Philadelphia, 1991), pp. 123-4. Eckhard Bernstein, German Humanism (Boston, 1983), p. 4. Alister McGrath, The Intellectual Origins of the European Reformation (Oxford/New York, 1987), p. 34. C. Scott Dixon, The Reformation in Germany (Oxford/Malden, 2002), p. 35. McGrath, The Intellectual Origins, p. 41. The coming of humanism to Germany virtually coincided with the founding of the first German university in Prague. Indeed, the establishment of universities brought a fresh outlook to the studies of theology and morals, entrenching the humanist tradition (studia humanitatis) ever firmer into German scholarly attitudes. By the beginning of the sixteenth century, sixteen university had been founded across the German lands, which in various degrees pursued the methods of a new kind of studies. Professors of the studia humanitatis stressed text-based studies, thereby aiming to gain powerful and valuable insights of Christian religion brought back to a purer interpretation of the biblical text. Their slogan ad fontes signified, most importantly, an attempt to get closer to the true intention of the scriptures. Moreover, the via moderna, as this new interpretation of religion came to be called, also challenged the minutely derived rationalist viewpoints of medieval theologians (known as the via antiqua) by stressing the value of human experience and interpretation. For example, the German philologist and humanist Rudolf Agricola sought to explore this in his writing. Thus, he came to assert: 'Prodigious, immense, and unbelievable is the power of the human mind'. A renewed strong interest with secular Latin and Greek authors derived directly from the humanists' concern with the reform of language into clear uncomplicated formulations so as to arrive at more logical conclusions. Such linguistic reform the humanists found through 'the observation and imitation of the best authors and writers', classics such as 'Virgil, Lucan, Horace [...], Plautus, Terence, Cicero, Sallust, Seneca, and the Roman historian Valerius Maximus'. Bernstein, German Humanism, p. 5. Brann, 'Humanism in Germany', p. 130. Scott Dixon, The Reformation in Germany, 15. McGrath, The Intellectual Origins, p. 40. Scott Dixon, The Reformation in Germany, p. 16. Bernstein, German Humanism, p. 4. Brann, 'Humanism in Germany', p. 132. As considered suitable by humanist Jakob Wimpfeling. See Bernstein, German Humanism, p. 32. It goes without saying that, by permitting themselves such a new, revisionist outlook, the northern European humanists soon came to criticise what they considered unjustified behaviour within the church and among all Christians in general, be it while viewing themselves inside the religious order and thus without calling into doubt the authority of the church. Still, in many respects, humanism undermined the absolute authority of the church organisation, stressing the humble, speculative element of religious life. Erasmus suitably quoted St. Paul in his work Praise of Folly (1509): 'I speak it not after the Lord, but as it were foolishly'. McGrath, The Intellectual Origins, p. 36. Desiderius Erasmus, 'Oration: Theological Dispute' in The Praise of Folly,  URL  (1509) By the time Luther came in conflict with the church, humanism had become well engendered in Germany. But besides the Italian humanist influences of text-based study, German humanists furthermore came to incorporate the creation of a distinct German identity in their writing. Around the turn of the century, several texts were published praising Germany's pious and learned traditions, with at its foundation the rediscovering of Tacitus' Germania in 1455. It was within this tradition that the greatest reformers received their education. The studies of Zwingli in Vienna, the Swiss reformer, coincided with the arrival to that city of Conrad Celtis, Germany's poet laureate, while Luther regarded humanist techniques of philology and the close study of the scriptures as the key to his theological discoveries. Similarly, Calvin, a decade later, would be grounded in the study of bonae litterae. Yet, the position of the reformers went beyond that of the humanists in theological zeal and single-mindedness. While most humanists agreed that '[t]he irreducible particularity of human reality was [...] more suited to historical description, rather than logical analysis', they proposed as a solution simply the reform of education through training in rhetoric and the study of 'ancient philosophers, poets and orators', as Celtis argued already in 1492. The differences in point of view of reformers like Luther and Zwingli initially appeared rather blurred to many humanists, which in effect strengthened their case. It was this 'productive misunderstanding' which made a great controversy out of Luther's theological uneasy with some of the church's practices. The theoretical misunderstanding is to some extent no surprise. Humanists and reformers alike shared many principles which clothed the doctrinal move away from the church in humanist language. Thus, when Luther wrote of his breakthrough in the theory of justification, that '[h]itherto, this epistle has been smothered with comments and all sorts of irrelevances, yet, in essence, it is a brillant light, almost enough to illuminate the whole bible', this certainly bore the semblance of the humanists' study of the scriptures ad fontes. Scott Dixon, The Reformation in Germany, p. 15. McGrath, The Intellectual Origins, Zwingli, p. 45; Calvin, p. 54; Luther, p. 59. Scott Dixon, The Reformation in Germany, pp. 47-8. McGrath, The Intellectual Origins, p. 39. Bernstein, German Humanism, p. 60. McGrath, The Intellectual Origins, p. 41. Martin Luther, 'Preface to the Epistle of St. Paul to the Romans' in John Dillenberger (ed. ), Selections From His Writings (New York, 1962), p. 19. Historians have argued that '[t]o a certain extent, the reformers were simply confirming the advances made by the Humanist agenda, with its stress on ancient languages, its tools of exegesis and its approval to the sources'. This assertion rings a certain truth, in that humanists were quite ready to speak out against any kind of practice which they considered a perversion of the true faith. Consequently, we find that Erasmus freely criticised the incorporation of pagan elements into the Christian faith, and the complicity of the clergy in these practices: 'so it be of ghosts, spirits, goblins, devils, or the like; [...] the further they are from truth, the more readily they are believed and the more do they tickle their itching ears. And these serve not only to pass away time but bring profit, especially to mass priests and pardoners'. Furthermore, he criticised the fact that selling indulgences evaded the simple fact that 'to live well is the way to die well'. While humanist and reformer alike stressed the simple and direct relationship in which the individual stood to God, the period directly preceding the reformation saw the sweeping popularity of so-called sacramentals, small objects that were to have sacred protective powers, turning the religious experience into 'a kind of popular Catholic version of the priesthood of all believers'. Catholic theologians were quick to assert that the working of sacramentals worked, not through physical efficacy; rather, it was spiritual efficacy, based on the prayers of the church', yet, Luther attacked the very authority of the church as an intermediary between the individual and God. 'How much more properly did the apostles call themselves servants of the present Christ', he wrote in an open letter to pope Leo X, 'and not [like the clergy] vicars of an absent Christ?'. Such boldness did certainly lie outside the tradition of pre-reformation humanists, who, despite their criticism, were nearly all 'men who saw themselves as operating within the context of the church'. Scott Dixon, The Reformation in Germany, p. 47. Desiderius Erasmus, 'Oration: Magical Charms' in The Praise of Folly,  URL  (1509) Ibid. R.W. Scribner, Popular Culture and Popular Movements in Reformation Germany (London/Ronceverte, 1987), p. 12. Ibid, p. 10. Martin Luther, 'The Freedom of a Christian' in John Dillenberger (ed. ), Selections From His Writings (New York, 1962), p. 51. McGrath, The Intellectual Origins, p. 36. On a note, however, it must be added that much of the question of the relation between humanism and the reformation depends on definition. Kristeller has rightly pointed out that humanism, taken in a broad definition, 'was not [...] Catholic or Protestant, [...] although it is easy to find [for both of them] a certain number of humanists who favoured them'. More correctly was it a new scholarly attitude, which often had little directly to do with theological questions. Naturally, humanist achievements did much to aid reformist criticism of the church, thereby unintentionally damaging the pope's authority as ultimate arbiter of religious matters. By 1520, Luther felt sufficiently confident to claim that '[t]heir claim that only the pope may interpret scripture is an outrageous facied fable'. Moreover, while many humanists were entangled in a 'productive misunderstanding' of reformation doctrine, similarly 'the response of reformers and traditionalists to humanism was improvised and lacked any clear, programmatic agenda'. Paul Oskar Kristeller, 'Studies on Renaissance Humanism during the last Twenty Years' in Studies in the Renaissance, vol. 9 (1962), p.22. McGrath, The Intellectual Origins, pp. 47-8. Scott Dixon, The Reformation in Germany, p. 47. Patrick Hayden-Roy, 'Review of The Confessionalization of Humanism in Reformation Germany by Erika Rummel' in Renaissance Quarterly, vol. 54, no. 4, part 2 (Winter, 2001), p. 1636. The intellectual life of northern European humanists had brought extensive soil for social development. Influenced by Italian humanists, many German humanists had studied at Italian universities, while extensive correspondence and printed works guaranteed a considerable reading audience to some of the more famous scholars such as Erasmus. This intellectual network now came to serve the reformation twofold. While it came to counter the vices of ""popular religion"", which reformers and humanists alike attacked, it also created a well-publicised stage for a reformed alternative. While religious life in reformed thought experienced a kind of rationalisation, relying only on the scriptures (sola scriptura), all acts in daily life were now to the honour of God. Simultaneously, too, the reformers managed through humanist channels to justify this transformation. Luther's new doctrine of justification now freed Christians from 'an agreed bargain involving a near-legal sense of obligation', thus evading the need for an authoritarian church. Aided by forthright humanists such as the patriotic nobleman Ulrich von Hutten, the church was portrayed as 'an unjust, unkind, and bloodthirsty shepherd'. Thus, the reformation succeeded in countering 'religion in favour of an emotional sensibility too preoccupied with outward appearances', managing to some extent to draw parallels between the opposites religion/magic and Germany/Rome. McGrath, The Intellectual Origins, p. 38. Scribner, Popular Culture and Popular Movements, pp. 17-8. Scott Dixon, The Reformation in Germany, p. 47. Ibid, p. 13. McGrath, The Intellectual Origins, p. 40. Bernstein, German Humanism, p. 125. Scribner, Popular Culture and Popular Movements, p. 14. Ibid, p. 15. The spread of these new ideas came hand in hand. While many humanists, trained and educated in Italy, brought up 'questions that [...] were discussed openly and debated in public', their sojourns in Italy generally brought out 'what was peculiarly Germanic'. Luther's address to the German nobility, then, must also be seen in this light. 'All classes in Christendom, particularly in Germany [my italics], are now oppressed by distress and affliction', Luther wrote, and it is not unlikely that some of the German nobility supported his reformed Christianity for national-political reasons. After all, since Luther refuted the idea that any active works or confession were of any importance in an individual's relationship to God, this discredited the authority of the church as a body of absolute mediation. Scott Dixon, The Reformation in Germany, p. 59. Brann, 'Humanism in Germany', p. 137. Martin Luther, 'An Appeal to the Ruling Class of German Nationality as to the Amelioration of the State of Christendom' in John Dillenberger (ed. ), Selections From His Writings (New York, 1962), p. 405. C. Scott Dixon, The Reformation in Germany (Oxford/Malden, 2002), p. 46. To conclude, the role of humanism in preparing the ground enabling a reformation to take place in Germany and the rest of northern Europe was very great. Despite the fact that many a humanist was, and remained, supportive of the church in Rome, the humanist challenges to education, thereby firmly establishing the studia humanitatis around Europe, was of tremendous importance for the theological and social atmosphere to bring about further-going criticism of the church institutions. Moreover, many of the humanist values were shared by the reformation movement. The emphasis on text-based studies was of key importance to Luther (sola scriptura), while the study of classical Greek and Latin texts enabled Zwingli to successfully challenge the church's position on several matters. Both humanists and reformers criticised the incorporation of pagan elements such as sacramentals, which they considered charms, and the church's trade of indulgences. Indeed, Luther's ideas and style of criticism of the church bore such semblance to humanist traditions, that the first period of clash saw the support of many humanists, which has been described as a 'productive misunderstanding', which, additionally, brought much attention to the Wittenberg disputation through the humanists' extensive correspondence network. McGrath, The Intellectual Origins, p. 47. The exact relation between humanists and reformers has been a matter of dispute among scholars of our time. It depends, firstly, on a definition of humanism. While humanism knew certain general traits, it was not necessarily confined by a certain theological outlook, and the humanist movement was never united on theological matters. In fact, the period after the reformation saw the emergence of a number of prominent Protestant humanists. Thus, we can say humanism and reformation were mutually influencing, the reformer often using humanist tools in his pursuits, while many humanists, often demonstratively Teutonic, admired the reformation movement.","The advent of the Solidarity strikes and protests halfway 1980 struck a serious blow at the Polish communist regime. Whilst it provided a powerful response of workers to the repressive character and 'determination [of the Polish regime] to maintain [the political and economic] systems, whatever their human costs and lack of legitimacy', an important questions comes to mind. Where from came this powerful articulation of demands, and of 'the most powerful, sophisticated and advanced working-class movement yet seen, certainly in the 'communist' sphere and perhaps anywhere in the world'? To answer this question, it may be fruitful to study the emergence of new class societies in Eastern Europe, paradoxical when matched with Marxist thought. Economic and political developments throughout the 1970s united with a blend of national-historical and wider Eastern European economic grievances to prepare the ferment for 'a new generation of protesting workers with a clearly different way of articulating their grievances [as] a strictly Polish phenomenon'. In this essay I will have a closer look at the development of a class spirit among Polish workers, and argue that it was from their socio-economic situation that discord arose. Walter D. Connor, 'Social Change and Stability in Eastern Europe' in Problems of Communism, vol. 26 (1977), p. 16. Colin Barker, Festival of the Oppressed. Solidarity, Reform and Revolution in Poland 1980-81 (London: 1986), p. 11. Walter D. Connor, 'Social Change and Stability in Eastern Europe' in Problems of Communism, vol. 26 (1977), p. 29. Jadwiga Staniszkis, The Dynamics of the Breakthrough in Eastern Europe. The Polish Experience (Berkeley: 1991), p. 107-8. The prehistory of the Solidarity movement was, most importantly, sparked by an increasing crystallisation of a ""class consciousness"" within the ranks of the Polish work force, many of whom were becoming convinced 'that they [could only] improve their lot collectively'. Despite official state rhetoric, there was an increasingly realistic attitude to the existence of a new dominant class made up of 'the intelligentsia as a whole rather than just the bureaucracy narrowly defined'; a class of highly educated, better-rewarded technocrats and managers, charged with the daily affairs of increasing efficiency of production and economic policy making. While official policy claimed to pursue a policy of egalitarianism, the reality behind these claims was only limited. The producing classes, those of the workers and peasants, found themselves on the same position in the material hierarchy as before, while a decrease in real standards of living were initially concealed only by the relative success of social mobility. However, the stretch of social mobility had its limits and by the 1970s a new generation of workers, unfamiliar with opportunities for social mobility, many of them younger than 25 years of age, began to form an increasingly articulate and assertive sense of collective grievances. While according to the orthodox theory of communism the party was the vanguard of the workers, epitomising its real interest, workers on the work floor reportedly referred to 'management in the third person, reserving ""we"" for themselves'. This clearly meant the beginning of an independently developing working class, at odds with the intelligentsia and a regime increasingly alienated from Poland's new social reality. Walter D. Connor, 'Social Change and Stability in Eastern Europe' in Problems of Communism, vol. 26 (1977), p. 28. Georg Konrád and Ivan Szelényi, The Intellectuals on the Road to Class Power (Brighton: 1979), p. xiv-v. Walter D. Connor, 'Social Change and Stability in Eastern Europe' in Problems of Communism, vol. 26 (1977), p. 17. R.J. Crampton, Eastern Europe in the Twentieth Century - And After (London/New York, 2005), p. 364. Konrád and Szelényi, The Intellectuals, p. 223. Similar to the England of the early nineteenth century, the creation of a new Eastern European working class indeed took place within the rather short timeframe of one or two generations. Over the decades after the Second World War, an average of nearly fifty percent of the younger peasantry came 'to form the new working class, [and] to create the social drama of a new life, a new world'. As their direct socio-economic interests began to reveal themselves in increasing antagonism with the state-directed economic policy, Polish workers, as a class an sich and with an agenda different even from that of the oppositional intelligentsia, more and more often became involved in violent confrontations with the state. Social discord became increasingly apparent, and ''Poles murdered by Poles, workers murdered by a 'Workers' State', became the symbol for all their accumulated grievances'. Connor, 'Social Change and Stability in Eastern Europe', p. 21. Ibid, p. 28. Garton Ash, The Polish Revolution, p. 12. Timothy Garton Ash's assertion that 'December 1970 [was] the single most important date in the pre-history of Solidarity', may provide further insight into the extent that Solidarity was connected to the working class. Its reaction to the government's implementation of food price increases revealed signs of a growing unity and singular purpose of action. When the government resolved to give up its policy of consistent non-unemployment 'aimed at forcing greater productivity through the use of threat of unemployment as an incentive for better and more efficient work', this seemed a direct threat to workers at large. After strikes and violent confrontations throughout January 1971, the government withdrew its economic reform plans, while Gomułka was forced to step down as a consequence of his unpopular repressive measures and number of deaths as a direct consequence thereof. This political outcome was seen by many protesters as a direct result of their protests, which served made workers realise where there strength lay. Throughout the 1970s, many strikes followed, and although the regime 'exacted its revenge on the workers', resulting in many workers being 'sacked, arrested, beaten, jailed', strikers usually got what they wanted. This, and the fact that many protests 'involved a large group which was generally representative of the worker population as a whole', accounts for the growth of an awareness that the economic situation of workers was connected with, and could be influenced by, their political role as an occupational group. Ibid, p. 12. Jan B. de Weydenthal, 'Poland: Workers and Politics' in Jan F. Triska and Charles Gati (eds. ), Blue-Collar Workers in Eastern Europe (London: 1981), p. 193. Crampton, Eastern Europe, p. 359. Weydenthal, 'Poland: Workers and Politics', p. 194. Barker, Festival of the Oppressed, p. 12. Weydenthal, 'Poland: Workers and Politics', p. 195. Yet, what made for the timing of this growing self-awareness? Where from came the momentum that enabled workers to start attempting to influence their economic position through strikes? The Polish communist party had, from the onset, based much of its legitimacy through the reasonably successful initiation of a process of social mobility upward. Workers were recruited from among the peasant ranks, while better-educated workers were needed to support the growing bureaucratic apparatus. These developments enabled many to enter new realities that were hitherto undreamed of, creating a sense of greater equality, if not of material wealth, then of opportunity. It also sparked, however, another process, which would later develop into one of the primary reasons for the antagonism between the workers and the intelligentsia as a whole. The quick development of industries created a culture of technocracy, in which economic status became more directly related to 'a hierarchy of statuses based on diplomas, irrespective of the actual amount of work done'. This put pressures on access to higher education as a direct way of material advancement, as the intelligentsia were protective of their traditionally advantaged position. The attitude of this ""red bourgeoisie"" was becoming an assertion '[t]hat they do and should live better than workers'. This, however, created tension, as these 'unequal patterns of allocation of social rewards' were legitimised by the class that benefited from it, 'despite the traditional systemic commitment to egalitarianism'. With the decreasing flexibility in the Polish labour market, most economists and sociologists began to note a shift from 'phases of system-building to the tasks of system-maintenance'. The loss of revolutionary economic appeal for the masses, however, eventually began to cost the Party members as well; 'between October 1980 and March 1981 the Party lost 216,000 members, bringing total membership below three million'. Connor, 'Social Change and Stability in Eastern Europe', p. 24. Konrád and Szelényi, The Intellectuals, p. 228. Connor, 'Social Change and Stability in Eastern Europe', p. 26. Weydenthal, 'Poland: Workers and Politics', p. 191. Connor, 'Social Change and Stability in Eastern Europe', p. 25. Garton Ash, The Polish Revolution, p. 170. The stagnation of social mobility can be said to have a two-sided effect. While, one the one hand, it formed a ferment for discontent because the state failed to meet the workers' expectations of equality, Eastern European 'regimes [had to], in a sense, deal with the workers they have'. While the legitimating narrative came out of redistribution, 'the technocrats, like the ruling elite itself, [had] an interest in keeping labor in a dispositive position', which entailed maintenance of the status quo. A further source of frustration was found in the fact that, despite repeated pledges for reform from the party leadership, very little change was noticed, while concessions were repeatedly watered down. This was particularly hard to swallow since continually facts were coming to light revealing the decadent and privileged world that the small number of the party and administrative élite' had created for themselves. Contradictions were experienced very starkly when any kind of protest was followed by measures on a worker's economic privileges and 'any sign of active solidarity' was immediately called 'political conspiracy'. Gradually, workers began to connect their economic discontent with a wider, social picture, which thoroughly undermined the legitimacy of the state. As now the argument of economic prosperity was no longer something Gierek could count on, the social and political situation became something of a ticking bomb. Connor, 'Social Change and Stability in Eastern Europe', p. 29. Konrád and Szelényi, The Intellectuals, p. 225. Weydenthal, 'Poland: Workers and Politics', p. 190. Crampton, Eastern Europe, p. 362. Konrád and Szelényi, The Intellectuals, p. 227. Connor, 'Social Change and Stability in Eastern Europe', p. 32. The decade running up to the strikes of saw an increasingly untenable paradoxical situation. According to Marxist thinking, communism provided the means to stop the alienation of the workers; yet, they were becoming increasingly alienated. This was clearly demonstrated by the slow production process of inferior quality produce. By the 1980s, an observer described ""the Polish situation as 'the emergence of workers as a distinct, albeit unorganized, political force and the inability of institutionalized leadership groups to integrate the workers into the existing pattern of systematic operations'. It is important to note, that the origins of this alienation were, to a great extent, class-originated, coming from the specific economic grievances of the workers. Konrád and Szelényi, The Intellectuals, p. 226. Weydenthal, 'Poland: Workers and Politics', p. 187. Economic alienation eventually meant political alienation, which then began to organise itself. The strikers of 1980 were no longer easily reintegrated. They were 'far more disciplined; they were not committed to socialism or the party' any longer, whereas large sections of Poland's industrial workers in 1970 were still prepared to give the regime another chance, in 1980 Lech Wałęsa bitterly reminded his co-workers of how little had been done with the workers' trust and support. If the Party was not prepared to listen to their grievances anymore, the workers would organise themselves outside the state. This essentially foreboded the inability to continue any profitable form of communism. As R.J. Crampton observed, Crampton, Eastern Europe, p. 367. Garton Ash, The Polish Revolution, p. 13. Konrád and Szelényi, The Intellectuals, p. 227. 'Solidarity had shown the bankruptcy of east European communism because the bulk of communism's chosen people, the workers, had looked outside the party for redress of their social grievances'.Crampton, Eastern Europe, p. 376. We have observed that the regime became increasingly inapt to rally the support of the workers for the socialist cause, or to reasonably incorporate them within a plan economy. It is, nonetheless, important to have a closer look at the reasons that caused the process to assume such a radical working class character. Surely, there existed a strong section of ""critical intelligentsia"", yet their position was precarious: whereas they assumed a role as the voice of the working class, they were also 'part of the intellectual class, from which neither its structural position nor its value-system [allowed] it to secede'. It is said, moreover, that moral decay could be observed across the intelligentsia - so much so, that one sociologist spoke of 'the 'semi-feudalisation' of Polish society'. The success propaganda of communist society that western historians are quite familiar with, also seemed to have blinded the regime, rendering it numb to criticism or alternative ideas. This hostile view, however, is only useful to a certain extent, and it is more likely that many members of the intelligentsia simply still '[had] a stake in the status quo and, therefore, support[ed] it'. Konrád and Szelényi, The Intellectuals, p. 221. Barker, Festival of the Oppressed. , p. 13. Ibid, p. 12. Connor, 'Social Change and Stability in Eastern Europe', p. 27. There were, however, also initiatives from the intelligentsia within broad daylight of the Party's political sphere, which provided powerful criticism of the way in which the state treated dissident workers. And while many intellectuals desiring reform continued to trap themselves in 'an iron cage' of 'the limits of possible reform', organisations such as the Committee in Defence of the Workers (KOR) set out in a number of semi-legal ways to create a bond of solidarity between workers and intellectuals, an number of them eventually becoming involved in Solidarity. Examples of selfless and risk-taking initiatives are not hard to find; intellectuals such as Miroslaw Chojecki lost their jobs, continuously spent time in jail and got beaten up, in the meanwhile enabling the publication of 150 illegal books. The problem of intellectualism and political involvement, however, would eventually cause problems by 1989, when, during the round table talks intellectualism gave the negotiations an air of secrecy and 'institutional horse trading'. Crampton, Eastern Europe, p. 364. Barker, Festival of the Oppressed, p. 16. Garton Ash, The Polish Revolution, p. 18. Barker, Festival of the Oppressed, p. 15. Jadwiga Staniszkis, The Dynamics of the Breakthrough in Eastern Europe. The Polish Experience (Berkeley: 1991), p. 143. By 1970, n increasing awareness of the existence of social class was observed in Polish society. This classification was not so much based upon political affiliation as upon material wealth. Rapid industrialisation since World War Two had brought new realities urban workers, a large number of them of young age, in a setting of narrowly confined economic possibilities. When rapid economic and industrial development began to slow down and the initial success story of social mobility began to lose its stretch, problems became observable. The popular official state narrative of egalitarianism became increasingly exposed, whereas the state had little means to provide alternative justifications. In a situation that was becoming more and more complicated, the workers soon discovered the power of strike and protest, something which the regime was rather fearful of. With no alternative means of political participation, the working class was rapidly becoming self-aware and radical, while the regime, with a large number of critical intellectuals torn between the rigor of party policy and the difficulty of reform, proved unable to control economic decline and the social outcry against party elitism and decadence. A state that claimed to hold the interests of workers as its highest principle was rapidly losing support from just that group. The strikers of Solidarity were no longer easily drawn back into the folds of socialist society which had, according to them, structurally failed to improve itself. A number of critical intellectuals, however, come to the aid of the workers. The KOR created effective means of communication between different workers' groups, and bridged the gap of solidarity between intellectuals and workers. Nonetheless, the development of Solidarity must be viewed as a broad expression of workers' grievances with the sympathy and support of a number of critical intellectuals.",False
28,"The British project of Imperialism was driven by the 'scientifically' proven belief that white males were the natural, biological, superiors to ethnic minorities and women. Consequently, Englishmen considered it to be their right and duty to subjugate the populations of non-Western countries, disseminating their norms of propriety and their notions of what it meant to be 'civilised'. This essay is concerned with showing that this quest was glorified in the popular fiction of late nineteenth century England, and that the gender identities of English men and women were shaped and reinforced by the portrayal of 'uncivilised' people and uncharted, fantastical, territories. This essay will argue that the construction of gender identity is related to Imperialism and racism as the indigenous people of conquered nations provided white Englishmen with an image of 'uncivilised' people with which they could compare themselves to. Initially, it will be shown that the adventure story of late nineteenth century England provided young Englishmen with the belief that it was their right to propagate Imperialism because of their natural superiority to other ethnicities. It will then be shown that the subconscious dissatisfaction many Englishmen had with the restrictive nature of Victorian society was expressed in their enthusiasm for adventure stories set in societies ungoverned by such constraining norms. The contrast between English women and the women depicted in the adventure stories will be shown as reinforcing women's submissive role in Victorian society. Finally, the latent appeal to Victorian men in the adventure stories of a regression to a 'primitive' state reveals racist presuppositions. The essay begins with a discussion of the adventure story's role in reproducing Imperialist ideals. The adventure stories strongly conveyed the notion that it was legitimate for white males to be the rulers of other nations and cultures. Racism was connected to the construction of the Victorian white male's identity as it was believed that ethnic minorities were superstitious and depraved, whereas the Englishmen perceived themselves as austere, courageous and self-controlled; this led to the conclusion that only white males possessed the qualities necessary for governing other nations (Showalter, 1991; 93). Consequently, the adventure story relayed this notion to its young, male readership, and can therefore be described as a device through which the ideals of Imperialism were transmitted from generation to generation, thus ensuring the political goal of Empire building was continually achieved (80). Kipling presented this notion in The Man Who Would Be King when he discusses how the men of Kafiristan are suitable for building an Empire because they are ""English"" in their customs and practices, whereby they ""sit on chairs"", and are not simply ""niggers"" like the rest of the indigenous population (Kipling, 1888; 269). He reinforces this notion with his repeated reference to Indian men as a ""Brown mass"" which dehumanises them and reduces them to a rabble in which individuals are indistinguishable from one another and are characterised merely by their colour (262). Therefore, adventure stories shaped the identity of Victorian men as it encouraged them to believe that they were the highest distinction of people. Whilst they did illuminate the Victorian perception of other cultures, the adventure stories also revealed a latent male perspective of English society. The concept of Imperialism appealed to Victorian men because it represented the possibility of escaping from the confining structure of English society to unexplored regions of the world. Victorian society was rigidly structured in terms of gender roles and class and race distinctions, the parameters of which were determined by the restrictive Victorian morality and ideology of propriety (Showalter, 1991; 81). The adventure stories provided the Englishmen with an arena in which to explore facets of their character that would have otherwise been suppressed by society (81). The manifestation of this is the repeated blurring in the adventure stories of the boundaries that differentiated between acceptable behaviour and practices, and anti-social and uncivilised acts. These predominantly included a departure from the accepted norms governing sexuality in Victorian society, whereby the adventure story was characterised by close, or even homoerotic, relationships between the male protagonists that would have been suppressed in society because of the stigmatisation of homosexuality (81). Therefore, Imperialism contributed to the construction of gender identities as it fostered the exploration (by the male readership of adventure stories) of aspects of their character which would have been seen as taboo in Victorian society. Although adventure stories encouraged Englishmen to explore their identity, it can be argued that they served to reinforce the identities of women. The stark contrast between the portrayal of English women and 'native' women in the adventure stories reaffirmed women's subordinate position in Victorian society. The predominant depiction of English women was as naïve, and in need of the protection by men from the savage nature of the Imperial world and the indigenous people that inhabited it (98). The image of English women as docile and domesticated is opposed by the perception of 'native' women as wild and savage (98). In Joseph Conrad's story Heart Of Darkness, one of the most vivid depictions is of the powerful and magnificent mistress of Kurtz (a man who has abandoned the project of bringing civilisation to Africa and established himself as a dictator) whom he describes as ""savage and superb; wild-eyed and magnificent"" (Conrad, 1890; 220). Despite this, the inclusion of any women in the adventure stories was rare. When women of any nationality were included, it was generally in the capacity of an adversary or as a vice for the otherwise virtuous lead male protagonist (Showalter, 1991; 78). In Kipling's The Man Who Would Be King it is the insistence of Dravot (one of the two leading male, English, protagonists) on claiming a wife from the tribes that he has attained power over that leads to his downfall (Kipling, 1888; 273). Therefore, although the readership of adventure stories was predominantly male, they reinforced the role of women in Victorian society by associating them with the downfall of imperialist projects and ideals, leading the Englishmen to continue to suppress them. The idea that it was white Englishmen who were the arbiters of Imperialism can be understood further by examining the racist attitudes that were generative of their fascination with the 'primitive'. Adventure stories illuminate the fascination of the white Englishmen with the 'primitive' cultures they subjugated. Although the purpose of the Imperialist project was to impose English notions of propriety upon the indigenous people of the nations they conquered, it has been argued that these cultures had a seductive appeal to the Englishmen (Showalter, 1991; 95). The basis for this was the curiosity of the Englishmen about the lives of the 'primitive' people, in which greed, savagery and falsehood were endemic (95). This shows the fundamentally racist assumptions of the Englishmen about the people they oppressed. In spite of this elitist and judgemental perspective, it was these very 'qualities' that evoked such an intense response from Englishmen, which found expression in the adventure stories. In Heart Of Darkness (the most successful of all the adventure stories) one Englishman (Kurtz) has been seduced by the lure of power attainable by adopting the characteristics of the indigenous people he was supposed to be civilising, whilst another (Marlow) is engaged in a struggle not to emulate Kurtz and abandon his notions of civilisation as well. Therefore, the male identity was shaped by Imperialism and racism as an inherently racist perception of other cultures generated an immense interest in the Imperialist project, which consequently produced the notion of the seductiveness of a regression to a 'primitive' state of existence in which greed and a thirst for power would predominate if left unchecked. In this essay it has been shown that the male identity in the late Victorian era was influenced by adventure stories through their dissemination of the racist idea that Imperialism was justified by the Englishman's right to rule over people who were too uncivilised to govern themselves. Imperialism also contributed to the construction of gender identity by providing the imaginations of Englishmen with an abstracted territory within which they could be unaffected by the finite nature of a Victorian society founded upon repressive norms and values. The role of women as a destructive force of the project of Imperialism in most adventure stories reaffirmed their subordinate position within society as it was implied that they needed to be sheltered from the harsh realities and the savagery of the oppressed cultures. This 'savagery', however, influenced the Englishman's identity by exposing him to desires which stood in opposition to the conventional ideals of Imperialism. It therefore follows from the evidence presented here that the construction of gender identities was related to Imperialism and racism through the emphasis that the Imperialist project (with its racist presuppositions) placed upon white British masculinity. The primacy of white men is conveyed strongly in the adventure stories of the late nineteenth century; 'white manhood' is portrayed as an example of civility and the epitome of the Victorian notions of propriety, whereas women and the oppressed cultures involved are depicted as inferior or dangerous. Imperialism, and the racism it is imbued with, therefore affected men and women's sense of identity by confirming that there was a natural order in British society and the Imperial world, in which white men formed the rightly dominant group.","Sociologists have traditionally been divided into two groups depending upon their ontological position; Individualists emphasise the importance of an individual's action, whilst Collectivists give primacy to the role of social structure in the construction of social reality. Recently, however, these rigidly demarcated ontological positions have been challenged by the emergence of Realism and Structuration Theory. This essay is concerned with assessing the strengths and weaknesses of Individualism and Collectivism; the main issue under consideration is how far these opposing ontologies disprove each other. This essay will argue that neither of these ontologies sufficiently describes or explains the complexity of social reality, and it is because of their deficiencies that I have been persuaded to adopt a Realist perspective. This argument will be pursued by outlining the debate between Nicos Poulantzas and Ralph Miliband concerning who or what constitutes the 'Ruling Class', in order to show how I was first alerted to the stratified nature of social realty. It will then be discussed how Collectivism successfully discredits the methodological premises of Individualism, yet does not constitute a comprehensive ontological alternative. Following this will be a description of how Realism provides the ontological explanations that Collectivism cannot (due to Collectivism's grounding in Empiricism). Finally, the decision to adopt a Realist approach instead of supporting Structuration Theory will then be justified. The essay begins by proving that having an ontology is not an option. Having a view of social reality is a prerequisite for both constructing a methodology and conducting research. The role of the researcher's ontology is to regulate which explanations for social phenomena are acceptable; this therefore determines the researcher's area of focus and influences how they devise their practical social theories (Archer, 1995; 20-21). This means that the Individualist's premise that social reality is ultimately constituted by individuals determines that their explanations must consist of statements about a person's 'dispositions, beliefs, resources and interaction' (Watkins, 1971; 106). Similarly, the Collectivist's belief that society has structural features which cannot be reduced to the dispositions of individuals determines that their explanations of human action must refer to the influence of non-human features (such as the banking system) (Mendelbaum, 1973; 223-224). Therefore, practical social theories are only considered acceptable if they are predicated upon the explanations (or methodology) generated by the researcher's description of social reality (their ontology) (Archer, 1995; 21). Having a theoretical starting point when conducting research is vital because researchers who perform data collection and then generate their theories (through inductive reasoning or the grounded theory approach) inevitably reproduce the stereotypes and assumptions of everyday life (May, 2001; 31). Therefore theory is required to provide a level of abstraction which detaches the researcher from the subject (30). It must also be noted, however, that the results of research actually regulate the researcher's own ontological and methodological perspective, as data that challenges the original theory forces the researcher to re-consider their position. The necessity of a having a view of social reality because social reality itself is not united shall now be explored. The debate between Ralph Miliband and Nicos Poulantzas reveals that a clear distinction exists between social structure and individual action. By debating the importance of either structure or action, these two Marxist writers reveal that having an ontology is necessary as there can be no one unified theory of society, as social reality itself is not unified (Craib, 1997; 269). Ralph Miliband adopts an Individualist approach to answering the question 'who is the Ruling Class?' in his book The State in Capitalist Society (1969), (which generated the debate with Poulantzas). Due to his ontology Miliband investigates the specific members of the 'Ruling Class' and how they are connected to each other; he concludes that the 'Ruling Class' is constituted of individuals who share a common cultural background (such as attending the same schools) and the common interest of reproducing the capitalist system (as they benefit from it) (Miliband, 1973; 254-259). Alternatively, Poulantzas answers the question form a Collectivist perspective. He emphasises the importance of state institutions that persist regardless of the individual occupying them, and that individuals merely fulfil the 'role requirements' of their position within the capitalist system (Poulantzas, 1973; 242-245). Although these perspectives were presented as oppositional, I considered the merits of each to be complimentary. Neither satisfactorily dismissed the evidence of the other, and the possibility that both interpretations of the 'Ruling Class' could be correct (if reconciled) emerged. Therefore, I was encouraged to believe that there were two distinct strata of the 'Ruling Class', which could not be reduced to each other (and this is one of the central tenets of Realism). In light of this, the premises of Individualism and Collectivism needed to be evaluated. I was dissuaded from adopting the Individualist's perspective because of its inherent contradictions regarding reductionism and the role of the social context. Individualism's principle that social structures and phenomena must be reduced to statements about the interrelations of individuals (because the individual is the smallest constituent part of society) leaves it susceptible to criticism from psychologists who state that people themselves can be reduced further still into their underlying psychological features (Archer, 1995; 39). Due to their belief in reductionism it becomes difficult for Individualists to deny the psychologists' claim that society is the reflection of the combined psyches of the population (40). However, to defend their position from psychologists Individualists invoke the Collectivist notion of emergence. They claim that their concept of the individual is unique because it is defined by the individual's participation in relationships that pre-date them, for example 'English speakers do require other English speakers to become such themselves' (40). This recognition of an emergent structure (such as the English language) contradicts Individualism's premise that everything in society is reducible to an individual's dispositions. The Individualist's response to this criticism is to incorporate all non-individual and non-dispositional factors into their concept of the individual, such as a person's 'physical resources and environment' (Watkins, 1971; 110). This again reveals a contradiction, as these aspects of social reality are not about individuals or dispositions (the exclusive focus of the Individualist) yet are central to the Individualist's concept of the individual. Individualism's inability to discredit the Collectivist's concept of social structure provided further impetus not to adopt their view. Individualism cannot successfully prove that social structure is not autonomous from, pre-existent to, and does not exert a causal influence over individuals (Archer, 1995; 42). Individualists claim that social structure is not independent from individuals because the social structure is constituted of interpersonal relations (43). Collectivists oppose this idea because it does not take into consideration the importance of social roles in determining behaviour; they state that people behave appropriately to the role they are fulfilling (such as the interaction between a solicitor and a client) regardless of the individuals involved (43). Individual's rebut this criticism by denying the pre-existence of social structure; they claim that features such as 'roles' only exist because the individuals concerned lack the desire or the knowledge to change them (44). Collectivists disprove this claim by adducing the resistance shown by structures (such as the rate of recruitment into the police force) to the concerted efforts of individuals trying to change them; this proves that structures pre-exist and outlive specific individuals and their attempts to alter them by showing that they possess properties independent of the individual's influence (44). Additionally, it can be argued that the dispositions which are so important to Individualists are shaped and restricted by their historical context (Gellner, 1971; 100). Consequently, Individualist's cannot prove that structures do not have a causal influence (such as a low level of recruitment into the police force being causal of a high crime rate in urban areas). Although Collectivism highlights the inadequacy of Individualism's methodological approach, it does not offer a significant counter-ontology, and is therefore inferior to the Realist approach. Realists show that the flaws of Collectivism stem from its inability to transcend Empiricist concepts. Collectivists were never able to define the exact ontological status of 'societal properties' (as defined by Mendelbaum) (49). This is because a societal property cannot be empirically proven as it does not have a tangible existence which can be directly detected by an individual's senses (49). In addition to this, the Empiricist concept of causation (whereby a phenomenon only has causal power if it produces constant conjunctions at an empirical level) denied social structures causal power, because they are neither observable nor causal in their effect (based upon this definition) (52). Realism solves this problem as it recognises that society is an open system in which events are influenced by other contingencies, so the causal influence of emergent properties is not to consistently produce observable effects (53). Therefore, Collectivism could only challenge Individualism methodologically (by showing that some features of society cannot be reduced to statements about individuals) but could not advance a counter-ontology (because of the dominance of Empiricism) stating the existence of non-observable properties (54). Realism, however, does not recognise the validity of the Empiricist notion that all things which are held to exist must be detected through human sensory experience. The basic tenet of Realism is that society is stratified into individuals and the emergent, non-observable, social structure, and that these two non-reducible strata interact and influence each other (61).The differences in perspective between Structuration Theory and Realism clarifies the justification for my decision to adopt a Realist view of social reality. Both Structuration Theory and Realism re-conceptualized the relationship between structure and action. Structuration Theory endeavours to transcend the dualism of structure versus action by depicting them as inseparable, whereby structures are produced by social action, yet social action is only possible because of the existence of social structures (such as the use of language, which allows individuals to express themselves, but only exists because individuals use it correctly) (Taylor, 1995; 688). Regardless of their claim to have transcended the structure versus action dualism, it can be argued that Structuration theorists actually give primacy to the role of social action, and merely reduce structure to being a property of action (Craib, 1997; 268). Therefore, Structuration Theory does not offer an adequate alternative to Individualism and Collectivism as all three are ontologically preoccupied with either reducing structure to action or action to structure. Realism manages to overcome the structure versus action dualism by recognising that neither structure nor action is a completely dependent feature (Archer, 1995; 61). Realists maintain that social reality consists of both structures and actors; although the existence of structures is dependent upon the actions of individuals, once they have emerged they possess their own properties and powers which can then have an effect upon the actors, therefore the proper object of sociology should be investigating the interplay between these two strata (63). In this essay it has been described why a sociologist must have a view about the nature of social reality. It was also discussed how the different manifestations of the 'Ruling Class' are representative of the stratified nature of social reality. The different methodologies and ontologies of Individualism and Collectivism were compared to show how Collectivism exposes Individualism's methodological flaws. However, it was then shown that that the ontological premise of Collectivism is not far reaching enough to persuade me to adopt its view of social reality. Finally, Realism was depicted as providing the ontological depth that the Collectivist approach lacks, in addition to its superiority over any other theory that also rejects (or purports to reject) the structure versus action dualism. It therefore follows from the evidence presented here that I adopted a Realist view of social reality because of the insufficient level of importance and autonomy assigned to either action or structure by other perspectives. Individualism cannot successfully deny the importance of an autonomous, activity causal social structure, whilst Collectivism cannot disprove Individualism's ontological foundations because of its own Empiricist nature. Similarly, Structuration Theory does not transcend the structure versus action dualism, but merely reinvents the terms of the debate in another terminology. Consequently, Realism offers the most comprehensive perspective of social reality because it takes the existence of both individual action and an autonomous social structure as its central tenet, and avoids the ontological and methodological contradictions of the approaches that define these features as inseparable.",True
29,"Sociologists have traditionally been divided into two groups depending upon their ontological position; Individualists emphasise the importance of an individual's action, whilst Collectivists give primacy to the role of social structure in the construction of social reality. Recently, however, these rigidly demarcated ontological positions have been challenged by the emergence of Realism and Structuration Theory. This essay is concerned with assessing the strengths and weaknesses of Individualism and Collectivism; the main issue under consideration is how far these opposing ontologies disprove each other. This essay will argue that neither of these ontologies sufficiently describes or explains the complexity of social reality, and it is because of their deficiencies that I have been persuaded to adopt a Realist perspective. This argument will be pursued by outlining the debate between Nicos Poulantzas and Ralph Miliband concerning who or what constitutes the 'Ruling Class', in order to show how I was first alerted to the stratified nature of social realty. It will then be discussed how Collectivism successfully discredits the methodological premises of Individualism, yet does not constitute a comprehensive ontological alternative. Following this will be a description of how Realism provides the ontological explanations that Collectivism cannot (due to Collectivism's grounding in Empiricism). Finally, the decision to adopt a Realist approach instead of supporting Structuration Theory will then be justified. The essay begins by proving that having an ontology is not an option. Having a view of social reality is a prerequisite for both constructing a methodology and conducting research. The role of the researcher's ontology is to regulate which explanations for social phenomena are acceptable; this therefore determines the researcher's area of focus and influences how they devise their practical social theories (Archer, 1995; 20-21). This means that the Individualist's premise that social reality is ultimately constituted by individuals determines that their explanations must consist of statements about a person's 'dispositions, beliefs, resources and interaction' (Watkins, 1971; 106). Similarly, the Collectivist's belief that society has structural features which cannot be reduced to the dispositions of individuals determines that their explanations of human action must refer to the influence of non-human features (such as the banking system) (Mendelbaum, 1973; 223-224). Therefore, practical social theories are only considered acceptable if they are predicated upon the explanations (or methodology) generated by the researcher's description of social reality (their ontology) (Archer, 1995; 21). Having a theoretical starting point when conducting research is vital because researchers who perform data collection and then generate their theories (through inductive reasoning or the grounded theory approach) inevitably reproduce the stereotypes and assumptions of everyday life (May, 2001; 31). Therefore theory is required to provide a level of abstraction which detaches the researcher from the subject (30). It must also be noted, however, that the results of research actually regulate the researcher's own ontological and methodological perspective, as data that challenges the original theory forces the researcher to re-consider their position. The necessity of a having a view of social reality because social reality itself is not united shall now be explored. The debate between Ralph Miliband and Nicos Poulantzas reveals that a clear distinction exists between social structure and individual action. By debating the importance of either structure or action, these two Marxist writers reveal that having an ontology is necessary as there can be no one unified theory of society, as social reality itself is not unified (Craib, 1997; 269). Ralph Miliband adopts an Individualist approach to answering the question 'who is the Ruling Class?' in his book The State in Capitalist Society (1969), (which generated the debate with Poulantzas). Due to his ontology Miliband investigates the specific members of the 'Ruling Class' and how they are connected to each other; he concludes that the 'Ruling Class' is constituted of individuals who share a common cultural background (such as attending the same schools) and the common interest of reproducing the capitalist system (as they benefit from it) (Miliband, 1973; 254-259). Alternatively, Poulantzas answers the question form a Collectivist perspective. He emphasises the importance of state institutions that persist regardless of the individual occupying them, and that individuals merely fulfil the 'role requirements' of their position within the capitalist system (Poulantzas, 1973; 242-245). Although these perspectives were presented as oppositional, I considered the merits of each to be complimentary. Neither satisfactorily dismissed the evidence of the other, and the possibility that both interpretations of the 'Ruling Class' could be correct (if reconciled) emerged. Therefore, I was encouraged to believe that there were two distinct strata of the 'Ruling Class', which could not be reduced to each other (and this is one of the central tenets of Realism). In light of this, the premises of Individualism and Collectivism needed to be evaluated. I was dissuaded from adopting the Individualist's perspective because of its inherent contradictions regarding reductionism and the role of the social context. Individualism's principle that social structures and phenomena must be reduced to statements about the interrelations of individuals (because the individual is the smallest constituent part of society) leaves it susceptible to criticism from psychologists who state that people themselves can be reduced further still into their underlying psychological features (Archer, 1995; 39). Due to their belief in reductionism it becomes difficult for Individualists to deny the psychologists' claim that society is the reflection of the combined psyches of the population (40). However, to defend their position from psychologists Individualists invoke the Collectivist notion of emergence. They claim that their concept of the individual is unique because it is defined by the individual's participation in relationships that pre-date them, for example 'English speakers do require other English speakers to become such themselves' (40). This recognition of an emergent structure (such as the English language) contradicts Individualism's premise that everything in society is reducible to an individual's dispositions. The Individualist's response to this criticism is to incorporate all non-individual and non-dispositional factors into their concept of the individual, such as a person's 'physical resources and environment' (Watkins, 1971; 110). This again reveals a contradiction, as these aspects of social reality are not about individuals or dispositions (the exclusive focus of the Individualist) yet are central to the Individualist's concept of the individual. Individualism's inability to discredit the Collectivist's concept of social structure provided further impetus not to adopt their view. Individualism cannot successfully prove that social structure is not autonomous from, pre-existent to, and does not exert a causal influence over individuals (Archer, 1995; 42). Individualists claim that social structure is not independent from individuals because the social structure is constituted of interpersonal relations (43). Collectivists oppose this idea because it does not take into consideration the importance of social roles in determining behaviour; they state that people behave appropriately to the role they are fulfilling (such as the interaction between a solicitor and a client) regardless of the individuals involved (43). Individual's rebut this criticism by denying the pre-existence of social structure; they claim that features such as 'roles' only exist because the individuals concerned lack the desire or the knowledge to change them (44). Collectivists disprove this claim by adducing the resistance shown by structures (such as the rate of recruitment into the police force) to the concerted efforts of individuals trying to change them; this proves that structures pre-exist and outlive specific individuals and their attempts to alter them by showing that they possess properties independent of the individual's influence (44). Additionally, it can be argued that the dispositions which are so important to Individualists are shaped and restricted by their historical context (Gellner, 1971; 100). Consequently, Individualist's cannot prove that structures do not have a causal influence (such as a low level of recruitment into the police force being causal of a high crime rate in urban areas). Although Collectivism highlights the inadequacy of Individualism's methodological approach, it does not offer a significant counter-ontology, and is therefore inferior to the Realist approach. Realists show that the flaws of Collectivism stem from its inability to transcend Empiricist concepts. Collectivists were never able to define the exact ontological status of 'societal properties' (as defined by Mendelbaum) (49). This is because a societal property cannot be empirically proven as it does not have a tangible existence which can be directly detected by an individual's senses (49). In addition to this, the Empiricist concept of causation (whereby a phenomenon only has causal power if it produces constant conjunctions at an empirical level) denied social structures causal power, because they are neither observable nor causal in their effect (based upon this definition) (52). Realism solves this problem as it recognises that society is an open system in which events are influenced by other contingencies, so the causal influence of emergent properties is not to consistently produce observable effects (53). Therefore, Collectivism could only challenge Individualism methodologically (by showing that some features of society cannot be reduced to statements about individuals) but could not advance a counter-ontology (because of the dominance of Empiricism) stating the existence of non-observable properties (54). Realism, however, does not recognise the validity of the Empiricist notion that all things which are held to exist must be detected through human sensory experience. The basic tenet of Realism is that society is stratified into individuals and the emergent, non-observable, social structure, and that these two non-reducible strata interact and influence each other (61).The differences in perspective between Structuration Theory and Realism clarifies the justification for my decision to adopt a Realist view of social reality. Both Structuration Theory and Realism re-conceptualized the relationship between structure and action. Structuration Theory endeavours to transcend the dualism of structure versus action by depicting them as inseparable, whereby structures are produced by social action, yet social action is only possible because of the existence of social structures (such as the use of language, which allows individuals to express themselves, but only exists because individuals use it correctly) (Taylor, 1995; 688). Regardless of their claim to have transcended the structure versus action dualism, it can be argued that Structuration theorists actually give primacy to the role of social action, and merely reduce structure to being a property of action (Craib, 1997; 268). Therefore, Structuration Theory does not offer an adequate alternative to Individualism and Collectivism as all three are ontologically preoccupied with either reducing structure to action or action to structure. Realism manages to overcome the structure versus action dualism by recognising that neither structure nor action is a completely dependent feature (Archer, 1995; 61). Realists maintain that social reality consists of both structures and actors; although the existence of structures is dependent upon the actions of individuals, once they have emerged they possess their own properties and powers which can then have an effect upon the actors, therefore the proper object of sociology should be investigating the interplay between these two strata (63). In this essay it has been described why a sociologist must have a view about the nature of social reality. It was also discussed how the different manifestations of the 'Ruling Class' are representative of the stratified nature of social reality. The different methodologies and ontologies of Individualism and Collectivism were compared to show how Collectivism exposes Individualism's methodological flaws. However, it was then shown that that the ontological premise of Collectivism is not far reaching enough to persuade me to adopt its view of social reality. Finally, Realism was depicted as providing the ontological depth that the Collectivist approach lacks, in addition to its superiority over any other theory that also rejects (or purports to reject) the structure versus action dualism. It therefore follows from the evidence presented here that I adopted a Realist view of social reality because of the insufficient level of importance and autonomy assigned to either action or structure by other perspectives. Individualism cannot successfully deny the importance of an autonomous, activity causal social structure, whilst Collectivism cannot disprove Individualism's ontological foundations because of its own Empiricist nature. Similarly, Structuration Theory does not transcend the structure versus action dualism, but merely reinvents the terms of the debate in another terminology. Consequently, Realism offers the most comprehensive perspective of social reality because it takes the existence of both individual action and an autonomous social structure as its central tenet, and avoids the ontological and methodological contradictions of the approaches that define these features as inseparable.","The British project of Imperialism was driven by the 'scientifically' proven belief that white males were the natural, biological, superiors to ethnic minorities and women. Consequently, Englishmen considered it to be their right and duty to subjugate the populations of non-Western countries, disseminating their norms of propriety and their notions of what it meant to be 'civilised'. This essay is concerned with showing that this quest was glorified in the popular fiction of late nineteenth century England, and that the gender identities of English men and women were shaped and reinforced by the portrayal of 'uncivilised' people and uncharted, fantastical, territories. This essay will argue that the construction of gender identity is related to Imperialism and racism as the indigenous people of conquered nations provided white Englishmen with an image of 'uncivilised' people with which they could compare themselves to. Initially, it will be shown that the adventure story of late nineteenth century England provided young Englishmen with the belief that it was their right to propagate Imperialism because of their natural superiority to other ethnicities. It will then be shown that the subconscious dissatisfaction many Englishmen had with the restrictive nature of Victorian society was expressed in their enthusiasm for adventure stories set in societies ungoverned by such constraining norms. The contrast between English women and the women depicted in the adventure stories will be shown as reinforcing women's submissive role in Victorian society. Finally, the latent appeal to Victorian men in the adventure stories of a regression to a 'primitive' state reveals racist presuppositions. The essay begins with a discussion of the adventure story's role in reproducing Imperialist ideals. The adventure stories strongly conveyed the notion that it was legitimate for white males to be the rulers of other nations and cultures. Racism was connected to the construction of the Victorian white male's identity as it was believed that ethnic minorities were superstitious and depraved, whereas the Englishmen perceived themselves as austere, courageous and self-controlled; this led to the conclusion that only white males possessed the qualities necessary for governing other nations (Showalter, 1991; 93). Consequently, the adventure story relayed this notion to its young, male readership, and can therefore be described as a device through which the ideals of Imperialism were transmitted from generation to generation, thus ensuring the political goal of Empire building was continually achieved (80). Kipling presented this notion in The Man Who Would Be King when he discusses how the men of Kafiristan are suitable for building an Empire because they are ""English"" in their customs and practices, whereby they ""sit on chairs"", and are not simply ""niggers"" like the rest of the indigenous population (Kipling, 1888; 269). He reinforces this notion with his repeated reference to Indian men as a ""Brown mass"" which dehumanises them and reduces them to a rabble in which individuals are indistinguishable from one another and are characterised merely by their colour (262). Therefore, adventure stories shaped the identity of Victorian men as it encouraged them to believe that they were the highest distinction of people. Whilst they did illuminate the Victorian perception of other cultures, the adventure stories also revealed a latent male perspective of English society. The concept of Imperialism appealed to Victorian men because it represented the possibility of escaping from the confining structure of English society to unexplored regions of the world. Victorian society was rigidly structured in terms of gender roles and class and race distinctions, the parameters of which were determined by the restrictive Victorian morality and ideology of propriety (Showalter, 1991; 81). The adventure stories provided the Englishmen with an arena in which to explore facets of their character that would have otherwise been suppressed by society (81). The manifestation of this is the repeated blurring in the adventure stories of the boundaries that differentiated between acceptable behaviour and practices, and anti-social and uncivilised acts. These predominantly included a departure from the accepted norms governing sexuality in Victorian society, whereby the adventure story was characterised by close, or even homoerotic, relationships between the male protagonists that would have been suppressed in society because of the stigmatisation of homosexuality (81). Therefore, Imperialism contributed to the construction of gender identities as it fostered the exploration (by the male readership of adventure stories) of aspects of their character which would have been seen as taboo in Victorian society. Although adventure stories encouraged Englishmen to explore their identity, it can be argued that they served to reinforce the identities of women. The stark contrast between the portrayal of English women and 'native' women in the adventure stories reaffirmed women's subordinate position in Victorian society. The predominant depiction of English women was as naïve, and in need of the protection by men from the savage nature of the Imperial world and the indigenous people that inhabited it (98). The image of English women as docile and domesticated is opposed by the perception of 'native' women as wild and savage (98). In Joseph Conrad's story Heart Of Darkness, one of the most vivid depictions is of the powerful and magnificent mistress of Kurtz (a man who has abandoned the project of bringing civilisation to Africa and established himself as a dictator) whom he describes as ""savage and superb; wild-eyed and magnificent"" (Conrad, 1890; 220). Despite this, the inclusion of any women in the adventure stories was rare. When women of any nationality were included, it was generally in the capacity of an adversary or as a vice for the otherwise virtuous lead male protagonist (Showalter, 1991; 78). In Kipling's The Man Who Would Be King it is the insistence of Dravot (one of the two leading male, English, protagonists) on claiming a wife from the tribes that he has attained power over that leads to his downfall (Kipling, 1888; 273). Therefore, although the readership of adventure stories was predominantly male, they reinforced the role of women in Victorian society by associating them with the downfall of imperialist projects and ideals, leading the Englishmen to continue to suppress them. The idea that it was white Englishmen who were the arbiters of Imperialism can be understood further by examining the racist attitudes that were generative of their fascination with the 'primitive'. Adventure stories illuminate the fascination of the white Englishmen with the 'primitive' cultures they subjugated. Although the purpose of the Imperialist project was to impose English notions of propriety upon the indigenous people of the nations they conquered, it has been argued that these cultures had a seductive appeal to the Englishmen (Showalter, 1991; 95). The basis for this was the curiosity of the Englishmen about the lives of the 'primitive' people, in which greed, savagery and falsehood were endemic (95). This shows the fundamentally racist assumptions of the Englishmen about the people they oppressed. In spite of this elitist and judgemental perspective, it was these very 'qualities' that evoked such an intense response from Englishmen, which found expression in the adventure stories. In Heart Of Darkness (the most successful of all the adventure stories) one Englishman (Kurtz) has been seduced by the lure of power attainable by adopting the characteristics of the indigenous people he was supposed to be civilising, whilst another (Marlow) is engaged in a struggle not to emulate Kurtz and abandon his notions of civilisation as well. Therefore, the male identity was shaped by Imperialism and racism as an inherently racist perception of other cultures generated an immense interest in the Imperialist project, which consequently produced the notion of the seductiveness of a regression to a 'primitive' state of existence in which greed and a thirst for power would predominate if left unchecked. In this essay it has been shown that the male identity in the late Victorian era was influenced by adventure stories through their dissemination of the racist idea that Imperialism was justified by the Englishman's right to rule over people who were too uncivilised to govern themselves. Imperialism also contributed to the construction of gender identity by providing the imaginations of Englishmen with an abstracted territory within which they could be unaffected by the finite nature of a Victorian society founded upon repressive norms and values. The role of women as a destructive force of the project of Imperialism in most adventure stories reaffirmed their subordinate position within society as it was implied that they needed to be sheltered from the harsh realities and the savagery of the oppressed cultures. This 'savagery', however, influenced the Englishman's identity by exposing him to desires which stood in opposition to the conventional ideals of Imperialism. It therefore follows from the evidence presented here that the construction of gender identities was related to Imperialism and racism through the emphasis that the Imperialist project (with its racist presuppositions) placed upon white British masculinity. The primacy of white men is conveyed strongly in the adventure stories of the late nineteenth century; 'white manhood' is portrayed as an example of civility and the epitome of the Victorian notions of propriety, whereas women and the oppressed cultures involved are depicted as inferior or dangerous. Imperialism, and the racism it is imbued with, therefore affected men and women's sense of identity by confirming that there was a natural order in British society and the Imperial world, in which white men formed the rightly dominant group.",False
30,"Sociologists have traditionally been divided into two groups depending upon their ontological position; Individualists emphasise the importance of an individual's action, whilst Collectivists give primacy to the role of social structure in the construction of social reality. Recently, however, these rigidly demarcated ontological positions have been challenged by the emergence of Realism and Structuration Theory. This essay is concerned with assessing the strengths and weaknesses of Individualism and Collectivism; the main issue under consideration is how far these opposing ontologies disprove each other. This essay will argue that neither of these ontologies sufficiently describes or explains the complexity of social reality, and it is because of their deficiencies that I have been persuaded to adopt a Realist perspective. This argument will be pursued by outlining the debate between Nicos Poulantzas and Ralph Miliband concerning who or what constitutes the 'Ruling Class', in order to show how I was first alerted to the stratified nature of social realty. It will then be discussed how Collectivism successfully discredits the methodological premises of Individualism, yet does not constitute a comprehensive ontological alternative. Following this will be a description of how Realism provides the ontological explanations that Collectivism cannot (due to Collectivism's grounding in Empiricism). Finally, the decision to adopt a Realist approach instead of supporting Structuration Theory will then be justified. The essay begins by proving that having an ontology is not an option. Having a view of social reality is a prerequisite for both constructing a methodology and conducting research. The role of the researcher's ontology is to regulate which explanations for social phenomena are acceptable; this therefore determines the researcher's area of focus and influences how they devise their practical social theories (Archer, 1995; 20-21). This means that the Individualist's premise that social reality is ultimately constituted by individuals determines that their explanations must consist of statements about a person's 'dispositions, beliefs, resources and interaction' (Watkins, 1971; 106). Similarly, the Collectivist's belief that society has structural features which cannot be reduced to the dispositions of individuals determines that their explanations of human action must refer to the influence of non-human features (such as the banking system) (Mendelbaum, 1973; 223-224). Therefore, practical social theories are only considered acceptable if they are predicated upon the explanations (or methodology) generated by the researcher's description of social reality (their ontology) (Archer, 1995; 21). Having a theoretical starting point when conducting research is vital because researchers who perform data collection and then generate their theories (through inductive reasoning or the grounded theory approach) inevitably reproduce the stereotypes and assumptions of everyday life (May, 2001; 31). Therefore theory is required to provide a level of abstraction which detaches the researcher from the subject (30). It must also be noted, however, that the results of research actually regulate the researcher's own ontological and methodological perspective, as data that challenges the original theory forces the researcher to re-consider their position. The necessity of a having a view of social reality because social reality itself is not united shall now be explored. The debate between Ralph Miliband and Nicos Poulantzas reveals that a clear distinction exists between social structure and individual action. By debating the importance of either structure or action, these two Marxist writers reveal that having an ontology is necessary as there can be no one unified theory of society, as social reality itself is not unified (Craib, 1997; 269). Ralph Miliband adopts an Individualist approach to answering the question 'who is the Ruling Class?' in his book The State in Capitalist Society (1969), (which generated the debate with Poulantzas). Due to his ontology Miliband investigates the specific members of the 'Ruling Class' and how they are connected to each other; he concludes that the 'Ruling Class' is constituted of individuals who share a common cultural background (such as attending the same schools) and the common interest of reproducing the capitalist system (as they benefit from it) (Miliband, 1973; 254-259). Alternatively, Poulantzas answers the question form a Collectivist perspective. He emphasises the importance of state institutions that persist regardless of the individual occupying them, and that individuals merely fulfil the 'role requirements' of their position within the capitalist system (Poulantzas, 1973; 242-245). Although these perspectives were presented as oppositional, I considered the merits of each to be complimentary. Neither satisfactorily dismissed the evidence of the other, and the possibility that both interpretations of the 'Ruling Class' could be correct (if reconciled) emerged. Therefore, I was encouraged to believe that there were two distinct strata of the 'Ruling Class', which could not be reduced to each other (and this is one of the central tenets of Realism). In light of this, the premises of Individualism and Collectivism needed to be evaluated. I was dissuaded from adopting the Individualist's perspective because of its inherent contradictions regarding reductionism and the role of the social context. Individualism's principle that social structures and phenomena must be reduced to statements about the interrelations of individuals (because the individual is the smallest constituent part of society) leaves it susceptible to criticism from psychologists who state that people themselves can be reduced further still into their underlying psychological features (Archer, 1995; 39). Due to their belief in reductionism it becomes difficult for Individualists to deny the psychologists' claim that society is the reflection of the combined psyches of the population (40). However, to defend their position from psychologists Individualists invoke the Collectivist notion of emergence. They claim that their concept of the individual is unique because it is defined by the individual's participation in relationships that pre-date them, for example 'English speakers do require other English speakers to become such themselves' (40). This recognition of an emergent structure (such as the English language) contradicts Individualism's premise that everything in society is reducible to an individual's dispositions. The Individualist's response to this criticism is to incorporate all non-individual and non-dispositional factors into their concept of the individual, such as a person's 'physical resources and environment' (Watkins, 1971; 110). This again reveals a contradiction, as these aspects of social reality are not about individuals or dispositions (the exclusive focus of the Individualist) yet are central to the Individualist's concept of the individual. Individualism's inability to discredit the Collectivist's concept of social structure provided further impetus not to adopt their view. Individualism cannot successfully prove that social structure is not autonomous from, pre-existent to, and does not exert a causal influence over individuals (Archer, 1995; 42). Individualists claim that social structure is not independent from individuals because the social structure is constituted of interpersonal relations (43). Collectivists oppose this idea because it does not take into consideration the importance of social roles in determining behaviour; they state that people behave appropriately to the role they are fulfilling (such as the interaction between a solicitor and a client) regardless of the individuals involved (43). Individual's rebut this criticism by denying the pre-existence of social structure; they claim that features such as 'roles' only exist because the individuals concerned lack the desire or the knowledge to change them (44). Collectivists disprove this claim by adducing the resistance shown by structures (such as the rate of recruitment into the police force) to the concerted efforts of individuals trying to change them; this proves that structures pre-exist and outlive specific individuals and their attempts to alter them by showing that they possess properties independent of the individual's influence (44). Additionally, it can be argued that the dispositions which are so important to Individualists are shaped and restricted by their historical context (Gellner, 1971; 100). Consequently, Individualist's cannot prove that structures do not have a causal influence (such as a low level of recruitment into the police force being causal of a high crime rate in urban areas). Although Collectivism highlights the inadequacy of Individualism's methodological approach, it does not offer a significant counter-ontology, and is therefore inferior to the Realist approach. Realists show that the flaws of Collectivism stem from its inability to transcend Empiricist concepts. Collectivists were never able to define the exact ontological status of 'societal properties' (as defined by Mendelbaum) (49). This is because a societal property cannot be empirically proven as it does not have a tangible existence which can be directly detected by an individual's senses (49). In addition to this, the Empiricist concept of causation (whereby a phenomenon only has causal power if it produces constant conjunctions at an empirical level) denied social structures causal power, because they are neither observable nor causal in their effect (based upon this definition) (52). Realism solves this problem as it recognises that society is an open system in which events are influenced by other contingencies, so the causal influence of emergent properties is not to consistently produce observable effects (53). Therefore, Collectivism could only challenge Individualism methodologically (by showing that some features of society cannot be reduced to statements about individuals) but could not advance a counter-ontology (because of the dominance of Empiricism) stating the existence of non-observable properties (54). Realism, however, does not recognise the validity of the Empiricist notion that all things which are held to exist must be detected through human sensory experience. The basic tenet of Realism is that society is stratified into individuals and the emergent, non-observable, social structure, and that these two non-reducible strata interact and influence each other (61).The differences in perspective between Structuration Theory and Realism clarifies the justification for my decision to adopt a Realist view of social reality. Both Structuration Theory and Realism re-conceptualized the relationship between structure and action. Structuration Theory endeavours to transcend the dualism of structure versus action by depicting them as inseparable, whereby structures are produced by social action, yet social action is only possible because of the existence of social structures (such as the use of language, which allows individuals to express themselves, but only exists because individuals use it correctly) (Taylor, 1995; 688). Regardless of their claim to have transcended the structure versus action dualism, it can be argued that Structuration theorists actually give primacy to the role of social action, and merely reduce structure to being a property of action (Craib, 1997; 268). Therefore, Structuration Theory does not offer an adequate alternative to Individualism and Collectivism as all three are ontologically preoccupied with either reducing structure to action or action to structure. Realism manages to overcome the structure versus action dualism by recognising that neither structure nor action is a completely dependent feature (Archer, 1995; 61). Realists maintain that social reality consists of both structures and actors; although the existence of structures is dependent upon the actions of individuals, once they have emerged they possess their own properties and powers which can then have an effect upon the actors, therefore the proper object of sociology should be investigating the interplay between these two strata (63). In this essay it has been described why a sociologist must have a view about the nature of social reality. It was also discussed how the different manifestations of the 'Ruling Class' are representative of the stratified nature of social reality. The different methodologies and ontologies of Individualism and Collectivism were compared to show how Collectivism exposes Individualism's methodological flaws. However, it was then shown that that the ontological premise of Collectivism is not far reaching enough to persuade me to adopt its view of social reality. Finally, Realism was depicted as providing the ontological depth that the Collectivist approach lacks, in addition to its superiority over any other theory that also rejects (or purports to reject) the structure versus action dualism. It therefore follows from the evidence presented here that I adopted a Realist view of social reality because of the insufficient level of importance and autonomy assigned to either action or structure by other perspectives. Individualism cannot successfully deny the importance of an autonomous, activity causal social structure, whilst Collectivism cannot disprove Individualism's ontological foundations because of its own Empiricist nature. Similarly, Structuration Theory does not transcend the structure versus action dualism, but merely reinvents the terms of the debate in another terminology. Consequently, Realism offers the most comprehensive perspective of social reality because it takes the existence of both individual action and an autonomous social structure as its central tenet, and avoids the ontological and methodological contradictions of the approaches that define these features as inseparable.","Sporting practices and their associated subcultures have to be conceptualised as possessing a unique nature that allows them to function independently of the societal context within which they exist, if it is to be argued that they bear no relation whatsoever to the social construction of masculine ideals. This is because masculine ideals are created and shaped by the society in which they are located, yet they also affect the natures of the institutions and cultural forces that influence them. This essay is concerned with tracing the historical construction of sport, and with assessing the relationship (if any) in contemporary society between sporting practices, masculine ideals, and the dominant cultural and institutional forces of modern society in order to evaluate whether or not sporting practices are related to the social construction of masculine ideals. This essay will argue that sporting practices are intimately bound up with the social construction of masculine ideals, and that the relationship between sport and society provides crucial insights into the other societal forces that contribute to the forms these ideals take. A central concept that will be employed throughout will be the concept of hegemonic masculinity. Masculine ideals as they exist in contemporary society will be conceptualised throughout the essay as forming the analytical tool that is hegemonic masculinity. Hegemonic masculinity can be described as occupying the dominant position in the gender order of society; this is very different to a conventional patriarchal view of society that argues all men are dominant over all women. Rather, the concept of hegemonic masculinity and the gender order proposes that the dominant force in gender relations is a particular kind of masculinity that has developed cultural primacy; consequently, all alternative masculinities and all femininities are subordinated and become involved in a dynamic relationship in which the power of the hegemonic masculinity has to adapt and evolve in order to retain its dominance. Therefore, this essay will attempt to deconstruct the concept of hegemonic masculinity into some of its constituent masculine ideals through discussing the role of sporting practices in ensuring the dominance of males over females; reinforcing the normality of heterosexuality; marginalising black males; and in emphasising the importance of physical prowess in order to show how sporting practices contribute to the stability of hegemonic masculinity itself. The essay begins, however, with an evaluation of perspectives that argue the development of institutionalised sporting practices was unaffected by issues of masculinity. The development of sporting practices in the 19 th Century has been conceptualised as reflecting the wider social transformations that have come to characterise modern society. It has been argued that modern sport shares the characteristics of modern society; in Weberian terms, sport is bureaucratised, secular, structured by rules and regulations, and can therefore be described as rational; in Marxists terms, sport is capitalist, specialised, quantified, defined by a distinction between work and leisure, and is consequently seen as bourgeois (Eitzen, cited in Crossett in Messner and Sabo, 1990; 47). Alternatively, the opposite has been argued. Although sport appears rationalised, it is essentially the institutionalisation of play, and is consequently irrational and purposeless, which means it has developed as an alternative to the forces of modernity (even if the institutional form that it takes is modern). Guttman has described sport as having its ""roots in the dark soil of our instinctive lives"" but that the form it takes is ""dictated by modern society"" (Guttman, cited in Crossett in Messner and Sabo, 1990; 47). Throughout these descriptions a connection between masculinity and the development of institutionalised sport has been ignored. The justification for the exclusion of issues of masculine ideals is derived from the belief that the 19 th Century concept of 'manliness' (which can be interpreted as embodying the masculine ideals of the period, and which was of central importance to the participants of sporting practices) was a 'confused moral concept' that embraced such contradictions as aggression and ruthlessness alongside compassion for the defeated; consequently manliness has been viewed as a front for nationalism and class cohesion (Mangan cited in Crossett in Messner and Dabo; 1990; 45). However, this perspective cannot explain how international competitions expanded upon already existing sporting institutions, and that class segregation was less prominent when sport was initially institutionalised (Crossett in Messner and Sabo, 1990; 47). Similarly, the perspective that sport is essentially irrational cannot explain how it has developed into a major social institution in an era that has witnessed increasing rationality (47). Therefore, although the Weberian and Marxists characterisations of sporting institutions aren't altogether inaccurate they need to be expanded to accommodate the influence of a gender ideology (of which 'manliness' formed a crucial part) upon the development of institutions in modern society (47). Todd Crossett endeavoured to show how the English public schools were a pivotal institution in disseminating the gender ideology, and his evidence for this shall now be discussed. Crossett convincingly described the mutually reinforcing relationship between sport in public schools and the emerging gender ideology of the 19 th Century. Crossett argues that the economic developments of the early 19 th Century resulted in a shift in power away from the aristocracy in favour of the bourgeoisie, and that the spread of liberal political theories which legitimised the ascendance of the bourgeoisie also increased the power of (bourgeois) women (Crossett in Messner and Sabo, 1990; 49). However, by the mid to late 19 th Century men began to displace women from the positions of power they had acquired (for example the male medical profession appropriated healthcare and midwifery and institutionally excluded women from positions of power), and they were able to do this because of the development of an ideology about the 'nature' and 'roles' of men and women (50). Pseudo-scientific justifications for the dominance of men over women became accepted as the truth, as ""almost without fail, experts related a woman's fragility and inferiority to her genital organs"" (50). Crossett argues that the development of sport in public schools socialised men into accepting this ideology and defining themselves as biologically superior to women (51). He describes how athleticism was placed in the foreground and deemed necessary for the development of manly, muscular students (52). The importance of athleticism was intimately connected with the popular view of the effect of sex upon the male body; sex and masturbation were perceived to make men suffer from exhaustion (as manly energy was believed to reside in sperm, and men were seen to possess a finite amount) and athletics helped to regenerate the body (52). Those men who were ""not in control of their passions"" (typically those who weren't involved in athletics) were labelled ""hysterical, sensitive and nervous"" which is significant as these were appellations commonly reserved exclusively for women (53). Therefore, without sport men were in danger of becoming womanlike, or ""delicate and degenerate"" (53). This shows that the development of institutionalised sport itself was predicated upon a societal ideology (that it served to reinforce) that sought to justify the masculine ideal of the dominance of men over women. Although sport as an institution has evolved beyond the 19 th Century public schools and women are not institutionally excluded from (most) sporting spheres, women are still marginal figures in contemporary sport, and this shall now be explored. The marginalisation of women in contemporary sporting spheres is made possible because of structural inequalities within sport itself, and because of the predominant societal perception that grants 'male sport' higher social esteem. The organisational bodies that govern most sports (even those where the majority of the athletes are female) are typically dominated by men (Bryson, 1994; 51). Similarly, institutions that are linked to sport such as sports journalism and corporations (involved in sponsorship) are conventionally male dominated spheres (52). Even when women do become professional athletes their achievements (and 'women's sport' in general) are largely ignored by the media and consequently receive very little attention or esteem (54). When women are given media coverage it is often in terms of their identity outside of their profession, typically with regards to their relational, marital, or familial situations (58). The justification for this marginalisation of women is the same stereotypes that originated in the public schools; namely that women are weaker and inferior to men and consequently their sport is less entertaining. Due to this contention there has been much discussion as to why women shouldn't compete with men in sports which rely on speed and strength (55). However, some sports rely on stamina and in other sports it is an advantage to be lighter, yet these sports in which women excel are excluded from the debate regarding sporting prowess, which shows this justification for marginalisation has an ideological nature (55). Therefore, contemporary sport influences the social construction of the masculine ideal of dominating women by ensuring that women are prevented from controlling how sporting institutions and organisational bodies are governed, which successfully negates any possibility that female athletes may use these institutions to re-shape the societal perception of 'female sport' as inferior. Whilst it has been established that women are subordinate to men in sport, the nature of men as the oppressing group needs to be discussed. The portrayal of sporting practices in the media reveals how specific types of masculinity that form the 'hegemonic masculinity' are glamorised and prioritised whilst other masculinities (and the men who enact them) are dismissed as deviant. The predominant image that emerges from the media representation of the male body is that a man should be muscular, athletic, powerful and healthy (Parker, 1996; 130). This perception emerges not only because of the extensive use of visual images of male athletes (who embody these physical ideals) but also because the male body exists at the intersection of sport, the media, and the consumer culture of society (130). Within contemporary society a culture has emerged that prioritises youth and beauty, and the portrayal of sportsmen in the media is part of the multiplicity of images and practices that individuals consume (such as yoga classes and healthy eating) in order to achieve these ideals (130). Therefore, the media representation of the male body shows how sport and the consumer culture are pivotal influences on the social construction of the hegemonic masculine ideal of physical prowess (as males who do not possess physical prowess are dismissed as inferior through the application of such labels as 'wimp') (132). Similarly, the masculine ideal of heterosexuality as the norm is reinforced by the media representation of sport. The depiction of homosexual male athletes is conspicuous largely because of its absence, and it can be argued that this is because there are comparatively few gay sportsmen (132). However, the relationship between the media and the dominant sporting culture of heterosexuality is more complex than this, as the media can be understood as actually creating this culture of heterosexuality through its obsession with ""the macho exploits of Britain's male athletes"" (132). Therefore, the media representation of the athletic male body and the normality of heterosexuality shows how sport is central to the social construction of these hegemonic masculine ideals as any alternative masculinities that do not feature in sport are labelled as inferior or ignored. The media representation of sport also contributes to the construction of the hegemonic masculine ideals of dominating women and the marginalisation of black men, and this relationship shall now be examined. The mediation of sporting practices (particularly violent sports) involves a complex and often contradictory relationship of identification and dissociation between male spectators from higher status backgrounds and the athletes themselves. The portrayal of sporting practices can be understood as naturalising the dominance of men over women, as media images of sport suppress the similarities between men and women and emphasise the differences, and then set about ""weaving a structure of symbols and interpretations"" around these differences to make them appear natural (Messner in Messner and Sabo, 1990; 101). Consequently, a sense of identification develops between male spectators and the images of sportsmen they consume which means that men identify themselves as physically superior to women; this sense of identification develops despite the fact that the majority of the male spectators are largely as physically dissimilar to professional male athletes as non-athlete females are (103). Conversely, the media representation of sporting practices also provides the male spectators with an image of a 'primitive other' embodied in the male athlete against whom he can define himself as modern and civilised (104). This sense of dissociation develops because the media representations portray images of sportsmen as predominantly black (and black men are historically associated with a notion of 'otherness'). It can be argued that the overrepresentation of ethnic minorities in sporting spheres is the result of a rational decision made by nascent black athletes to pursue a sports career, as it offered the best possibility of developing a career that would provide economic security because young black men are institutionally excluded from educational and occupational opportunities (104). Therefore, media images of sporting practices serve to reinforce the hegemonic masculine ideals of dominating women and marginalising black men through portraying 'natural' differences between men and women, and through depicting the black male body as an animalistic force. However, black men are not merely passive recipients of a racist social order, and their agency in relation to masculinity and sport shall now be elaborated. Through sporting practices black men have attempted to counteract the social invisibility imposed on them by an institutionally racist society, yet it is their acceptance of the ideals that compose hegemonic masculinity which ultimately ensures they do not affect the social order. Due to their exclusion from the educational and occupational institutions that confer social esteem on the individuals who have access to them, many black men channel their creative energies into constructing ""unique, expressive and conspicuous styles of speech, demeanour and walk etc"" (a phenomenon that Majors calls the 'cool pose') which offsets the invisibility they suffer due to their exclusion from the aforementioned institutions (Majors in Messner and Sabo, 1990; 111). Consequently, many black men pursue sporting careers because they can express themselves through the ""virtuosity of a performance"" and this offers the most realistic route to achieving the goals of hegemonic masculinity (such as being the breadwinner, having strength and dominating women) (110). Therefore, the response of black males through their concentration on sport represents an adaptation to, not a submission to, the hegemonic masculinity. However, in spite of their adaptation, their actions invariably do not allow them to transcend their subordination to the hegemonic masculinity. Through accepting the ideals of hegemonic masculinity black men pursue sporting careers in a context that is constructed by the racist wider society. In focusing predominantly on sport, black men accept the educational under-representation that is fostered by educational institutions; this is to their detriment as educational attainment is the principle route through which the ideals of hegemonic masculinity are realised and social power is attained (111). Even when black men do manage to become professional athletes, they are still confronted by institutionalised racism as there are remarkably few black managers or coaches in any sport, and the majority of sporting organisations are controlled by white males (113). Therefore, the participation of black males in sporting practices can be conceptualised as a response to the racism of the wider society, yet this response merely serves to reproduce the hegemonic masculine ideal of the marginalisation of black men, as it is predicated upon their acceptance of ideals that inevitably subordinate them. Although black athletes fulfil one element of the hegemonic masculinity (physical prowess), they are prevented from fulfilling other aspects (such as becoming professionals); this notion of contradiction is fundamental to the way hegemonic masculinity operates, and the role of sport in this process shall now be developed. The lives of professional sportsmen show how it is impossible to achieve all of the ideals that construct hegemonic masculinity, which therefore reveals how hegemonic masculinity operates in order to maintain its all encompassing dominance over the gender order. In his interview with Steve Donoghue (a champion in the iron man sporting practice) Connell asserts that Steve is an exemplar of hegemonic masculinity, as the life he leads makes him the embodiment of the masculine ideals of ""competitiveness, toughness and physical capability"" (Connell in Messner and Sabo, 1990; 94). However, Steve's testimony reveals that in order to maintain this status he has to sacrifice other masculine ideals that would jeopardise his physical performance, for example he can't ""go wild"" when he is socialising (92). Therefore, the representation of Steve and other sporting heroes like him can be understood as the symbolic embodiment of the epitome of an ideal. This means that sporting heroes are the tangible symbols for some aspects of hegemonic masculinity, namely the primacy of physical prowess, the domination of women, and the marginalisation of ethnic minorities and homosexuals. However, the sportsmen are unable to fulfil other aspects of hegemonic masculinity because they are antagonistic with the ideals they symbolise. Consequently, the other masculine ideals that contribute to hegemonic masculinity have different, often directly oppositional, symbols: for every sporting hero there is a 'bad boy', hedonistic Hollywood actor. Therefore, the existence of symbolic sporting heroes reveals that the hegemonic masculinity of which they form an integral part reproduces itself through the construction of a multiplicity of symbols that are often contradictory, yet always culturally exalted; and this shows how sporting practices contribute to the societal perceptions of masculinity and the roles of men and women that permeate all of society. In this essay it has been shown that the views stating the development of sport was unrelated to the social construction of masculine ideals are inadequate because of the primacy awarded to 'manliness' and athleticism in one of the major socialising institutions of the 19 th Century. It was also argued that the institutional subordination of women in sport contributes to the hegemonic masculine ideal of the subordination of women in society. The media representation of sporting practices was depicted as disseminating a consumerist, sexist, racist and homophobic ideology in order to maintain the dominance of hegemonic masculinity. It was then shown how black men pursue sporting careers to counteract the racism of wider society, but that ultimately the pursuance of a sporting career ensures the societal marginalisation of black men. Finally it was shown that hegemonic masculinity is a multi-dimensional phenomenon and sporting heroes constitute exemplars for specific ideals that construct hegemonic masculinity. It therefore follows from the evidence presented here that sporting practices are not a unique, socially isolated sphere and consequently do bear a relation to the social construction of masculine ideals. Sporting practices interact with other major societal forces in modern society, such as the perception that scientific knowledge equates to 'the truth' (as displayed in the relationship between the gender ideology and the public schools in the 19 th Century), or the prevalence of racism and consumerism, in order to socially construct hegemonic masculine ideals. Although sporting practices contribute to the social construction of the ideals that constitute hegemonic masculinity, it can be argued that the most important conclusion that can be drawn from an analysis of sporting practices and masculinity is that sporting heroes offer an insight into the way hegemonic masculinity structures the gender order of society in its entirety. Hegemonic masculinity is culturally exalted and stabilises a structure of dominance and subordination in the gender order as a whole, and in order to this it must have exemplars who are exalted as heroes. Sporting heroes as symbolic entities contribute to the vast network of images that cumulatively construct the societal perception of what a man should look like, or act like, or what the roles of men and women should be, which therefore means that sporting practices contribute to the social construction of characteristics that many individuals take for granted are part of our 'nature'.",True
31,"Sporting practices and their associated subcultures have to be conceptualised as possessing a unique nature that allows them to function independently of the societal context within which they exist, if it is to be argued that they bear no relation whatsoever to the social construction of masculine ideals. This is because masculine ideals are created and shaped by the society in which they are located, yet they also affect the natures of the institutions and cultural forces that influence them. This essay is concerned with tracing the historical construction of sport, and with assessing the relationship (if any) in contemporary society between sporting practices, masculine ideals, and the dominant cultural and institutional forces of modern society in order to evaluate whether or not sporting practices are related to the social construction of masculine ideals. This essay will argue that sporting practices are intimately bound up with the social construction of masculine ideals, and that the relationship between sport and society provides crucial insights into the other societal forces that contribute to the forms these ideals take. A central concept that will be employed throughout will be the concept of hegemonic masculinity. Masculine ideals as they exist in contemporary society will be conceptualised throughout the essay as forming the analytical tool that is hegemonic masculinity. Hegemonic masculinity can be described as occupying the dominant position in the gender order of society; this is very different to a conventional patriarchal view of society that argues all men are dominant over all women. Rather, the concept of hegemonic masculinity and the gender order proposes that the dominant force in gender relations is a particular kind of masculinity that has developed cultural primacy; consequently, all alternative masculinities and all femininities are subordinated and become involved in a dynamic relationship in which the power of the hegemonic masculinity has to adapt and evolve in order to retain its dominance. Therefore, this essay will attempt to deconstruct the concept of hegemonic masculinity into some of its constituent masculine ideals through discussing the role of sporting practices in ensuring the dominance of males over females; reinforcing the normality of heterosexuality; marginalising black males; and in emphasising the importance of physical prowess in order to show how sporting practices contribute to the stability of hegemonic masculinity itself. The essay begins, however, with an evaluation of perspectives that argue the development of institutionalised sporting practices was unaffected by issues of masculinity. The development of sporting practices in the 19 th Century has been conceptualised as reflecting the wider social transformations that have come to characterise modern society. It has been argued that modern sport shares the characteristics of modern society; in Weberian terms, sport is bureaucratised, secular, structured by rules and regulations, and can therefore be described as rational; in Marxists terms, sport is capitalist, specialised, quantified, defined by a distinction between work and leisure, and is consequently seen as bourgeois (Eitzen, cited in Crossett in Messner and Sabo, 1990; 47). Alternatively, the opposite has been argued. Although sport appears rationalised, it is essentially the institutionalisation of play, and is consequently irrational and purposeless, which means it has developed as an alternative to the forces of modernity (even if the institutional form that it takes is modern). Guttman has described sport as having its ""roots in the dark soil of our instinctive lives"" but that the form it takes is ""dictated by modern society"" (Guttman, cited in Crossett in Messner and Sabo, 1990; 47). Throughout these descriptions a connection between masculinity and the development of institutionalised sport has been ignored. The justification for the exclusion of issues of masculine ideals is derived from the belief that the 19 th Century concept of 'manliness' (which can be interpreted as embodying the masculine ideals of the period, and which was of central importance to the participants of sporting practices) was a 'confused moral concept' that embraced such contradictions as aggression and ruthlessness alongside compassion for the defeated; consequently manliness has been viewed as a front for nationalism and class cohesion (Mangan cited in Crossett in Messner and Dabo; 1990; 45). However, this perspective cannot explain how international competitions expanded upon already existing sporting institutions, and that class segregation was less prominent when sport was initially institutionalised (Crossett in Messner and Sabo, 1990; 47). Similarly, the perspective that sport is essentially irrational cannot explain how it has developed into a major social institution in an era that has witnessed increasing rationality (47). Therefore, although the Weberian and Marxists characterisations of sporting institutions aren't altogether inaccurate they need to be expanded to accommodate the influence of a gender ideology (of which 'manliness' formed a crucial part) upon the development of institutions in modern society (47). Todd Crossett endeavoured to show how the English public schools were a pivotal institution in disseminating the gender ideology, and his evidence for this shall now be discussed. Crossett convincingly described the mutually reinforcing relationship between sport in public schools and the emerging gender ideology of the 19 th Century. Crossett argues that the economic developments of the early 19 th Century resulted in a shift in power away from the aristocracy in favour of the bourgeoisie, and that the spread of liberal political theories which legitimised the ascendance of the bourgeoisie also increased the power of (bourgeois) women (Crossett in Messner and Sabo, 1990; 49). However, by the mid to late 19 th Century men began to displace women from the positions of power they had acquired (for example the male medical profession appropriated healthcare and midwifery and institutionally excluded women from positions of power), and they were able to do this because of the development of an ideology about the 'nature' and 'roles' of men and women (50). Pseudo-scientific justifications for the dominance of men over women became accepted as the truth, as ""almost without fail, experts related a woman's fragility and inferiority to her genital organs"" (50). Crossett argues that the development of sport in public schools socialised men into accepting this ideology and defining themselves as biologically superior to women (51). He describes how athleticism was placed in the foreground and deemed necessary for the development of manly, muscular students (52). The importance of athleticism was intimately connected with the popular view of the effect of sex upon the male body; sex and masturbation were perceived to make men suffer from exhaustion (as manly energy was believed to reside in sperm, and men were seen to possess a finite amount) and athletics helped to regenerate the body (52). Those men who were ""not in control of their passions"" (typically those who weren't involved in athletics) were labelled ""hysterical, sensitive and nervous"" which is significant as these were appellations commonly reserved exclusively for women (53). Therefore, without sport men were in danger of becoming womanlike, or ""delicate and degenerate"" (53). This shows that the development of institutionalised sport itself was predicated upon a societal ideology (that it served to reinforce) that sought to justify the masculine ideal of the dominance of men over women. Although sport as an institution has evolved beyond the 19 th Century public schools and women are not institutionally excluded from (most) sporting spheres, women are still marginal figures in contemporary sport, and this shall now be explored. The marginalisation of women in contemporary sporting spheres is made possible because of structural inequalities within sport itself, and because of the predominant societal perception that grants 'male sport' higher social esteem. The organisational bodies that govern most sports (even those where the majority of the athletes are female) are typically dominated by men (Bryson, 1994; 51). Similarly, institutions that are linked to sport such as sports journalism and corporations (involved in sponsorship) are conventionally male dominated spheres (52). Even when women do become professional athletes their achievements (and 'women's sport' in general) are largely ignored by the media and consequently receive very little attention or esteem (54). When women are given media coverage it is often in terms of their identity outside of their profession, typically with regards to their relational, marital, or familial situations (58). The justification for this marginalisation of women is the same stereotypes that originated in the public schools; namely that women are weaker and inferior to men and consequently their sport is less entertaining. Due to this contention there has been much discussion as to why women shouldn't compete with men in sports which rely on speed and strength (55). However, some sports rely on stamina and in other sports it is an advantage to be lighter, yet these sports in which women excel are excluded from the debate regarding sporting prowess, which shows this justification for marginalisation has an ideological nature (55). Therefore, contemporary sport influences the social construction of the masculine ideal of dominating women by ensuring that women are prevented from controlling how sporting institutions and organisational bodies are governed, which successfully negates any possibility that female athletes may use these institutions to re-shape the societal perception of 'female sport' as inferior. Whilst it has been established that women are subordinate to men in sport, the nature of men as the oppressing group needs to be discussed. The portrayal of sporting practices in the media reveals how specific types of masculinity that form the 'hegemonic masculinity' are glamorised and prioritised whilst other masculinities (and the men who enact them) are dismissed as deviant. The predominant image that emerges from the media representation of the male body is that a man should be muscular, athletic, powerful and healthy (Parker, 1996; 130). This perception emerges not only because of the extensive use of visual images of male athletes (who embody these physical ideals) but also because the male body exists at the intersection of sport, the media, and the consumer culture of society (130). Within contemporary society a culture has emerged that prioritises youth and beauty, and the portrayal of sportsmen in the media is part of the multiplicity of images and practices that individuals consume (such as yoga classes and healthy eating) in order to achieve these ideals (130). Therefore, the media representation of the male body shows how sport and the consumer culture are pivotal influences on the social construction of the hegemonic masculine ideal of physical prowess (as males who do not possess physical prowess are dismissed as inferior through the application of such labels as 'wimp') (132). Similarly, the masculine ideal of heterosexuality as the norm is reinforced by the media representation of sport. The depiction of homosexual male athletes is conspicuous largely because of its absence, and it can be argued that this is because there are comparatively few gay sportsmen (132). However, the relationship between the media and the dominant sporting culture of heterosexuality is more complex than this, as the media can be understood as actually creating this culture of heterosexuality through its obsession with ""the macho exploits of Britain's male athletes"" (132). Therefore, the media representation of the athletic male body and the normality of heterosexuality shows how sport is central to the social construction of these hegemonic masculine ideals as any alternative masculinities that do not feature in sport are labelled as inferior or ignored. The media representation of sport also contributes to the construction of the hegemonic masculine ideals of dominating women and the marginalisation of black men, and this relationship shall now be examined. The mediation of sporting practices (particularly violent sports) involves a complex and often contradictory relationship of identification and dissociation between male spectators from higher status backgrounds and the athletes themselves. The portrayal of sporting practices can be understood as naturalising the dominance of men over women, as media images of sport suppress the similarities between men and women and emphasise the differences, and then set about ""weaving a structure of symbols and interpretations"" around these differences to make them appear natural (Messner in Messner and Sabo, 1990; 101). Consequently, a sense of identification develops between male spectators and the images of sportsmen they consume which means that men identify themselves as physically superior to women; this sense of identification develops despite the fact that the majority of the male spectators are largely as physically dissimilar to professional male athletes as non-athlete females are (103). Conversely, the media representation of sporting practices also provides the male spectators with an image of a 'primitive other' embodied in the male athlete against whom he can define himself as modern and civilised (104). This sense of dissociation develops because the media representations portray images of sportsmen as predominantly black (and black men are historically associated with a notion of 'otherness'). It can be argued that the overrepresentation of ethnic minorities in sporting spheres is the result of a rational decision made by nascent black athletes to pursue a sports career, as it offered the best possibility of developing a career that would provide economic security because young black men are institutionally excluded from educational and occupational opportunities (104). Therefore, media images of sporting practices serve to reinforce the hegemonic masculine ideals of dominating women and marginalising black men through portraying 'natural' differences between men and women, and through depicting the black male body as an animalistic force. However, black men are not merely passive recipients of a racist social order, and their agency in relation to masculinity and sport shall now be elaborated. Through sporting practices black men have attempted to counteract the social invisibility imposed on them by an institutionally racist society, yet it is their acceptance of the ideals that compose hegemonic masculinity which ultimately ensures they do not affect the social order. Due to their exclusion from the educational and occupational institutions that confer social esteem on the individuals who have access to them, many black men channel their creative energies into constructing ""unique, expressive and conspicuous styles of speech, demeanour and walk etc"" (a phenomenon that Majors calls the 'cool pose') which offsets the invisibility they suffer due to their exclusion from the aforementioned institutions (Majors in Messner and Sabo, 1990; 111). Consequently, many black men pursue sporting careers because they can express themselves through the ""virtuosity of a performance"" and this offers the most realistic route to achieving the goals of hegemonic masculinity (such as being the breadwinner, having strength and dominating women) (110). Therefore, the response of black males through their concentration on sport represents an adaptation to, not a submission to, the hegemonic masculinity. However, in spite of their adaptation, their actions invariably do not allow them to transcend their subordination to the hegemonic masculinity. Through accepting the ideals of hegemonic masculinity black men pursue sporting careers in a context that is constructed by the racist wider society. In focusing predominantly on sport, black men accept the educational under-representation that is fostered by educational institutions; this is to their detriment as educational attainment is the principle route through which the ideals of hegemonic masculinity are realised and social power is attained (111). Even when black men do manage to become professional athletes, they are still confronted by institutionalised racism as there are remarkably few black managers or coaches in any sport, and the majority of sporting organisations are controlled by white males (113). Therefore, the participation of black males in sporting practices can be conceptualised as a response to the racism of the wider society, yet this response merely serves to reproduce the hegemonic masculine ideal of the marginalisation of black men, as it is predicated upon their acceptance of ideals that inevitably subordinate them. Although black athletes fulfil one element of the hegemonic masculinity (physical prowess), they are prevented from fulfilling other aspects (such as becoming professionals); this notion of contradiction is fundamental to the way hegemonic masculinity operates, and the role of sport in this process shall now be developed. The lives of professional sportsmen show how it is impossible to achieve all of the ideals that construct hegemonic masculinity, which therefore reveals how hegemonic masculinity operates in order to maintain its all encompassing dominance over the gender order. In his interview with Steve Donoghue (a champion in the iron man sporting practice) Connell asserts that Steve is an exemplar of hegemonic masculinity, as the life he leads makes him the embodiment of the masculine ideals of ""competitiveness, toughness and physical capability"" (Connell in Messner and Sabo, 1990; 94). However, Steve's testimony reveals that in order to maintain this status he has to sacrifice other masculine ideals that would jeopardise his physical performance, for example he can't ""go wild"" when he is socialising (92). Therefore, the representation of Steve and other sporting heroes like him can be understood as the symbolic embodiment of the epitome of an ideal. This means that sporting heroes are the tangible symbols for some aspects of hegemonic masculinity, namely the primacy of physical prowess, the domination of women, and the marginalisation of ethnic minorities and homosexuals. However, the sportsmen are unable to fulfil other aspects of hegemonic masculinity because they are antagonistic with the ideals they symbolise. Consequently, the other masculine ideals that contribute to hegemonic masculinity have different, often directly oppositional, symbols: for every sporting hero there is a 'bad boy', hedonistic Hollywood actor. Therefore, the existence of symbolic sporting heroes reveals that the hegemonic masculinity of which they form an integral part reproduces itself through the construction of a multiplicity of symbols that are often contradictory, yet always culturally exalted; and this shows how sporting practices contribute to the societal perceptions of masculinity and the roles of men and women that permeate all of society. In this essay it has been shown that the views stating the development of sport was unrelated to the social construction of masculine ideals are inadequate because of the primacy awarded to 'manliness' and athleticism in one of the major socialising institutions of the 19 th Century. It was also argued that the institutional subordination of women in sport contributes to the hegemonic masculine ideal of the subordination of women in society. The media representation of sporting practices was depicted as disseminating a consumerist, sexist, racist and homophobic ideology in order to maintain the dominance of hegemonic masculinity. It was then shown how black men pursue sporting careers to counteract the racism of wider society, but that ultimately the pursuance of a sporting career ensures the societal marginalisation of black men. Finally it was shown that hegemonic masculinity is a multi-dimensional phenomenon and sporting heroes constitute exemplars for specific ideals that construct hegemonic masculinity. It therefore follows from the evidence presented here that sporting practices are not a unique, socially isolated sphere and consequently do bear a relation to the social construction of masculine ideals. Sporting practices interact with other major societal forces in modern society, such as the perception that scientific knowledge equates to 'the truth' (as displayed in the relationship between the gender ideology and the public schools in the 19 th Century), or the prevalence of racism and consumerism, in order to socially construct hegemonic masculine ideals. Although sporting practices contribute to the social construction of the ideals that constitute hegemonic masculinity, it can be argued that the most important conclusion that can be drawn from an analysis of sporting practices and masculinity is that sporting heroes offer an insight into the way hegemonic masculinity structures the gender order of society in its entirety. Hegemonic masculinity is culturally exalted and stabilises a structure of dominance and subordination in the gender order as a whole, and in order to this it must have exemplars who are exalted as heroes. Sporting heroes as symbolic entities contribute to the vast network of images that cumulatively construct the societal perception of what a man should look like, or act like, or what the roles of men and women should be, which therefore means that sporting practices contribute to the social construction of characteristics that many individuals take for granted are part of our 'nature'.","Sociologists have traditionally been divided into two groups depending upon their ontological position; Individualists emphasise the importance of an individual's action, whilst Collectivists give primacy to the role of social structure in the construction of social reality. Recently, however, these rigidly demarcated ontological positions have been challenged by the emergence of Realism and Structuration Theory. This essay is concerned with assessing the strengths and weaknesses of Individualism and Collectivism; the main issue under consideration is how far these opposing ontologies disprove each other. This essay will argue that neither of these ontologies sufficiently describes or explains the complexity of social reality, and it is because of their deficiencies that I have been persuaded to adopt a Realist perspective. This argument will be pursued by outlining the debate between Nicos Poulantzas and Ralph Miliband concerning who or what constitutes the 'Ruling Class', in order to show how I was first alerted to the stratified nature of social realty. It will then be discussed how Collectivism successfully discredits the methodological premises of Individualism, yet does not constitute a comprehensive ontological alternative. Following this will be a description of how Realism provides the ontological explanations that Collectivism cannot (due to Collectivism's grounding in Empiricism). Finally, the decision to adopt a Realist approach instead of supporting Structuration Theory will then be justified. The essay begins by proving that having an ontology is not an option. Having a view of social reality is a prerequisite for both constructing a methodology and conducting research. The role of the researcher's ontology is to regulate which explanations for social phenomena are acceptable; this therefore determines the researcher's area of focus and influences how they devise their practical social theories (Archer, 1995; 20-21). This means that the Individualist's premise that social reality is ultimately constituted by individuals determines that their explanations must consist of statements about a person's 'dispositions, beliefs, resources and interaction' (Watkins, 1971; 106). Similarly, the Collectivist's belief that society has structural features which cannot be reduced to the dispositions of individuals determines that their explanations of human action must refer to the influence of non-human features (such as the banking system) (Mendelbaum, 1973; 223-224). Therefore, practical social theories are only considered acceptable if they are predicated upon the explanations (or methodology) generated by the researcher's description of social reality (their ontology) (Archer, 1995; 21). Having a theoretical starting point when conducting research is vital because researchers who perform data collection and then generate their theories (through inductive reasoning or the grounded theory approach) inevitably reproduce the stereotypes and assumptions of everyday life (May, 2001; 31). Therefore theory is required to provide a level of abstraction which detaches the researcher from the subject (30). It must also be noted, however, that the results of research actually regulate the researcher's own ontological and methodological perspective, as data that challenges the original theory forces the researcher to re-consider their position. The necessity of a having a view of social reality because social reality itself is not united shall now be explored. The debate between Ralph Miliband and Nicos Poulantzas reveals that a clear distinction exists between social structure and individual action. By debating the importance of either structure or action, these two Marxist writers reveal that having an ontology is necessary as there can be no one unified theory of society, as social reality itself is not unified (Craib, 1997; 269). Ralph Miliband adopts an Individualist approach to answering the question 'who is the Ruling Class?' in his book The State in Capitalist Society (1969), (which generated the debate with Poulantzas). Due to his ontology Miliband investigates the specific members of the 'Ruling Class' and how they are connected to each other; he concludes that the 'Ruling Class' is constituted of individuals who share a common cultural background (such as attending the same schools) and the common interest of reproducing the capitalist system (as they benefit from it) (Miliband, 1973; 254-259). Alternatively, Poulantzas answers the question form a Collectivist perspective. He emphasises the importance of state institutions that persist regardless of the individual occupying them, and that individuals merely fulfil the 'role requirements' of their position within the capitalist system (Poulantzas, 1973; 242-245). Although these perspectives were presented as oppositional, I considered the merits of each to be complimentary. Neither satisfactorily dismissed the evidence of the other, and the possibility that both interpretations of the 'Ruling Class' could be correct (if reconciled) emerged. Therefore, I was encouraged to believe that there were two distinct strata of the 'Ruling Class', which could not be reduced to each other (and this is one of the central tenets of Realism). In light of this, the premises of Individualism and Collectivism needed to be evaluated. I was dissuaded from adopting the Individualist's perspective because of its inherent contradictions regarding reductionism and the role of the social context. Individualism's principle that social structures and phenomena must be reduced to statements about the interrelations of individuals (because the individual is the smallest constituent part of society) leaves it susceptible to criticism from psychologists who state that people themselves can be reduced further still into their underlying psychological features (Archer, 1995; 39). Due to their belief in reductionism it becomes difficult for Individualists to deny the psychologists' claim that society is the reflection of the combined psyches of the population (40). However, to defend their position from psychologists Individualists invoke the Collectivist notion of emergence. They claim that their concept of the individual is unique because it is defined by the individual's participation in relationships that pre-date them, for example 'English speakers do require other English speakers to become such themselves' (40). This recognition of an emergent structure (such as the English language) contradicts Individualism's premise that everything in society is reducible to an individual's dispositions. The Individualist's response to this criticism is to incorporate all non-individual and non-dispositional factors into their concept of the individual, such as a person's 'physical resources and environment' (Watkins, 1971; 110). This again reveals a contradiction, as these aspects of social reality are not about individuals or dispositions (the exclusive focus of the Individualist) yet are central to the Individualist's concept of the individual. Individualism's inability to discredit the Collectivist's concept of social structure provided further impetus not to adopt their view. Individualism cannot successfully prove that social structure is not autonomous from, pre-existent to, and does not exert a causal influence over individuals (Archer, 1995; 42). Individualists claim that social structure is not independent from individuals because the social structure is constituted of interpersonal relations (43). Collectivists oppose this idea because it does not take into consideration the importance of social roles in determining behaviour; they state that people behave appropriately to the role they are fulfilling (such as the interaction between a solicitor and a client) regardless of the individuals involved (43). Individual's rebut this criticism by denying the pre-existence of social structure; they claim that features such as 'roles' only exist because the individuals concerned lack the desire or the knowledge to change them (44). Collectivists disprove this claim by adducing the resistance shown by structures (such as the rate of recruitment into the police force) to the concerted efforts of individuals trying to change them; this proves that structures pre-exist and outlive specific individuals and their attempts to alter them by showing that they possess properties independent of the individual's influence (44). Additionally, it can be argued that the dispositions which are so important to Individualists are shaped and restricted by their historical context (Gellner, 1971; 100). Consequently, Individualist's cannot prove that structures do not have a causal influence (such as a low level of recruitment into the police force being causal of a high crime rate in urban areas). Although Collectivism highlights the inadequacy of Individualism's methodological approach, it does not offer a significant counter-ontology, and is therefore inferior to the Realist approach. Realists show that the flaws of Collectivism stem from its inability to transcend Empiricist concepts. Collectivists were never able to define the exact ontological status of 'societal properties' (as defined by Mendelbaum) (49). This is because a societal property cannot be empirically proven as it does not have a tangible existence which can be directly detected by an individual's senses (49). In addition to this, the Empiricist concept of causation (whereby a phenomenon only has causal power if it produces constant conjunctions at an empirical level) denied social structures causal power, because they are neither observable nor causal in their effect (based upon this definition) (52). Realism solves this problem as it recognises that society is an open system in which events are influenced by other contingencies, so the causal influence of emergent properties is not to consistently produce observable effects (53). Therefore, Collectivism could only challenge Individualism methodologically (by showing that some features of society cannot be reduced to statements about individuals) but could not advance a counter-ontology (because of the dominance of Empiricism) stating the existence of non-observable properties (54). Realism, however, does not recognise the validity of the Empiricist notion that all things which are held to exist must be detected through human sensory experience. The basic tenet of Realism is that society is stratified into individuals and the emergent, non-observable, social structure, and that these two non-reducible strata interact and influence each other (61).The differences in perspective between Structuration Theory and Realism clarifies the justification for my decision to adopt a Realist view of social reality. Both Structuration Theory and Realism re-conceptualized the relationship between structure and action. Structuration Theory endeavours to transcend the dualism of structure versus action by depicting them as inseparable, whereby structures are produced by social action, yet social action is only possible because of the existence of social structures (such as the use of language, which allows individuals to express themselves, but only exists because individuals use it correctly) (Taylor, 1995; 688). Regardless of their claim to have transcended the structure versus action dualism, it can be argued that Structuration theorists actually give primacy to the role of social action, and merely reduce structure to being a property of action (Craib, 1997; 268). Therefore, Structuration Theory does not offer an adequate alternative to Individualism and Collectivism as all three are ontologically preoccupied with either reducing structure to action or action to structure. Realism manages to overcome the structure versus action dualism by recognising that neither structure nor action is a completely dependent feature (Archer, 1995; 61). Realists maintain that social reality consists of both structures and actors; although the existence of structures is dependent upon the actions of individuals, once they have emerged they possess their own properties and powers which can then have an effect upon the actors, therefore the proper object of sociology should be investigating the interplay between these two strata (63). In this essay it has been described why a sociologist must have a view about the nature of social reality. It was also discussed how the different manifestations of the 'Ruling Class' are representative of the stratified nature of social reality. The different methodologies and ontologies of Individualism and Collectivism were compared to show how Collectivism exposes Individualism's methodological flaws. However, it was then shown that that the ontological premise of Collectivism is not far reaching enough to persuade me to adopt its view of social reality. Finally, Realism was depicted as providing the ontological depth that the Collectivist approach lacks, in addition to its superiority over any other theory that also rejects (or purports to reject) the structure versus action dualism. It therefore follows from the evidence presented here that I adopted a Realist view of social reality because of the insufficient level of importance and autonomy assigned to either action or structure by other perspectives. Individualism cannot successfully deny the importance of an autonomous, activity causal social structure, whilst Collectivism cannot disprove Individualism's ontological foundations because of its own Empiricist nature. Similarly, Structuration Theory does not transcend the structure versus action dualism, but merely reinvents the terms of the debate in another terminology. Consequently, Realism offers the most comprehensive perspective of social reality because it takes the existence of both individual action and an autonomous social structure as its central tenet, and avoids the ontological and methodological contradictions of the approaches that define these features as inseparable.",False
32,"Sporting practices and their associated subcultures have to be conceptualised as possessing a unique nature that allows them to function independently of the societal context within which they exist, if it is to be argued that they bear no relation whatsoever to the social construction of masculine ideals. This is because masculine ideals are created and shaped by the society in which they are located, yet they also affect the natures of the institutions and cultural forces that influence them. This essay is concerned with tracing the historical construction of sport, and with assessing the relationship (if any) in contemporary society between sporting practices, masculine ideals, and the dominant cultural and institutional forces of modern society in order to evaluate whether or not sporting practices are related to the social construction of masculine ideals. This essay will argue that sporting practices are intimately bound up with the social construction of masculine ideals, and that the relationship between sport and society provides crucial insights into the other societal forces that contribute to the forms these ideals take. A central concept that will be employed throughout will be the concept of hegemonic masculinity. Masculine ideals as they exist in contemporary society will be conceptualised throughout the essay as forming the analytical tool that is hegemonic masculinity. Hegemonic masculinity can be described as occupying the dominant position in the gender order of society; this is very different to a conventional patriarchal view of society that argues all men are dominant over all women. Rather, the concept of hegemonic masculinity and the gender order proposes that the dominant force in gender relations is a particular kind of masculinity that has developed cultural primacy; consequently, all alternative masculinities and all femininities are subordinated and become involved in a dynamic relationship in which the power of the hegemonic masculinity has to adapt and evolve in order to retain its dominance. Therefore, this essay will attempt to deconstruct the concept of hegemonic masculinity into some of its constituent masculine ideals through discussing the role of sporting practices in ensuring the dominance of males over females; reinforcing the normality of heterosexuality; marginalising black males; and in emphasising the importance of physical prowess in order to show how sporting practices contribute to the stability of hegemonic masculinity itself. The essay begins, however, with an evaluation of perspectives that argue the development of institutionalised sporting practices was unaffected by issues of masculinity. The development of sporting practices in the 19 th Century has been conceptualised as reflecting the wider social transformations that have come to characterise modern society. It has been argued that modern sport shares the characteristics of modern society; in Weberian terms, sport is bureaucratised, secular, structured by rules and regulations, and can therefore be described as rational; in Marxists terms, sport is capitalist, specialised, quantified, defined by a distinction between work and leisure, and is consequently seen as bourgeois (Eitzen, cited in Crossett in Messner and Sabo, 1990; 47). Alternatively, the opposite has been argued. Although sport appears rationalised, it is essentially the institutionalisation of play, and is consequently irrational and purposeless, which means it has developed as an alternative to the forces of modernity (even if the institutional form that it takes is modern). Guttman has described sport as having its ""roots in the dark soil of our instinctive lives"" but that the form it takes is ""dictated by modern society"" (Guttman, cited in Crossett in Messner and Sabo, 1990; 47). Throughout these descriptions a connection between masculinity and the development of institutionalised sport has been ignored. The justification for the exclusion of issues of masculine ideals is derived from the belief that the 19 th Century concept of 'manliness' (which can be interpreted as embodying the masculine ideals of the period, and which was of central importance to the participants of sporting practices) was a 'confused moral concept' that embraced such contradictions as aggression and ruthlessness alongside compassion for the defeated; consequently manliness has been viewed as a front for nationalism and class cohesion (Mangan cited in Crossett in Messner and Dabo; 1990; 45). However, this perspective cannot explain how international competitions expanded upon already existing sporting institutions, and that class segregation was less prominent when sport was initially institutionalised (Crossett in Messner and Sabo, 1990; 47). Similarly, the perspective that sport is essentially irrational cannot explain how it has developed into a major social institution in an era that has witnessed increasing rationality (47). Therefore, although the Weberian and Marxists characterisations of sporting institutions aren't altogether inaccurate they need to be expanded to accommodate the influence of a gender ideology (of which 'manliness' formed a crucial part) upon the development of institutions in modern society (47). Todd Crossett endeavoured to show how the English public schools were a pivotal institution in disseminating the gender ideology, and his evidence for this shall now be discussed. Crossett convincingly described the mutually reinforcing relationship between sport in public schools and the emerging gender ideology of the 19 th Century. Crossett argues that the economic developments of the early 19 th Century resulted in a shift in power away from the aristocracy in favour of the bourgeoisie, and that the spread of liberal political theories which legitimised the ascendance of the bourgeoisie also increased the power of (bourgeois) women (Crossett in Messner and Sabo, 1990; 49). However, by the mid to late 19 th Century men began to displace women from the positions of power they had acquired (for example the male medical profession appropriated healthcare and midwifery and institutionally excluded women from positions of power), and they were able to do this because of the development of an ideology about the 'nature' and 'roles' of men and women (50). Pseudo-scientific justifications for the dominance of men over women became accepted as the truth, as ""almost without fail, experts related a woman's fragility and inferiority to her genital organs"" (50). Crossett argues that the development of sport in public schools socialised men into accepting this ideology and defining themselves as biologically superior to women (51). He describes how athleticism was placed in the foreground and deemed necessary for the development of manly, muscular students (52). The importance of athleticism was intimately connected with the popular view of the effect of sex upon the male body; sex and masturbation were perceived to make men suffer from exhaustion (as manly energy was believed to reside in sperm, and men were seen to possess a finite amount) and athletics helped to regenerate the body (52). Those men who were ""not in control of their passions"" (typically those who weren't involved in athletics) were labelled ""hysterical, sensitive and nervous"" which is significant as these were appellations commonly reserved exclusively for women (53). Therefore, without sport men were in danger of becoming womanlike, or ""delicate and degenerate"" (53). This shows that the development of institutionalised sport itself was predicated upon a societal ideology (that it served to reinforce) that sought to justify the masculine ideal of the dominance of men over women. Although sport as an institution has evolved beyond the 19 th Century public schools and women are not institutionally excluded from (most) sporting spheres, women are still marginal figures in contemporary sport, and this shall now be explored. The marginalisation of women in contemporary sporting spheres is made possible because of structural inequalities within sport itself, and because of the predominant societal perception that grants 'male sport' higher social esteem. The organisational bodies that govern most sports (even those where the majority of the athletes are female) are typically dominated by men (Bryson, 1994; 51). Similarly, institutions that are linked to sport such as sports journalism and corporations (involved in sponsorship) are conventionally male dominated spheres (52). Even when women do become professional athletes their achievements (and 'women's sport' in general) are largely ignored by the media and consequently receive very little attention or esteem (54). When women are given media coverage it is often in terms of their identity outside of their profession, typically with regards to their relational, marital, or familial situations (58). The justification for this marginalisation of women is the same stereotypes that originated in the public schools; namely that women are weaker and inferior to men and consequently their sport is less entertaining. Due to this contention there has been much discussion as to why women shouldn't compete with men in sports which rely on speed and strength (55). However, some sports rely on stamina and in other sports it is an advantage to be lighter, yet these sports in which women excel are excluded from the debate regarding sporting prowess, which shows this justification for marginalisation has an ideological nature (55). Therefore, contemporary sport influences the social construction of the masculine ideal of dominating women by ensuring that women are prevented from controlling how sporting institutions and organisational bodies are governed, which successfully negates any possibility that female athletes may use these institutions to re-shape the societal perception of 'female sport' as inferior. Whilst it has been established that women are subordinate to men in sport, the nature of men as the oppressing group needs to be discussed. The portrayal of sporting practices in the media reveals how specific types of masculinity that form the 'hegemonic masculinity' are glamorised and prioritised whilst other masculinities (and the men who enact them) are dismissed as deviant. The predominant image that emerges from the media representation of the male body is that a man should be muscular, athletic, powerful and healthy (Parker, 1996; 130). This perception emerges not only because of the extensive use of visual images of male athletes (who embody these physical ideals) but also because the male body exists at the intersection of sport, the media, and the consumer culture of society (130). Within contemporary society a culture has emerged that prioritises youth and beauty, and the portrayal of sportsmen in the media is part of the multiplicity of images and practices that individuals consume (such as yoga classes and healthy eating) in order to achieve these ideals (130). Therefore, the media representation of the male body shows how sport and the consumer culture are pivotal influences on the social construction of the hegemonic masculine ideal of physical prowess (as males who do not possess physical prowess are dismissed as inferior through the application of such labels as 'wimp') (132). Similarly, the masculine ideal of heterosexuality as the norm is reinforced by the media representation of sport. The depiction of homosexual male athletes is conspicuous largely because of its absence, and it can be argued that this is because there are comparatively few gay sportsmen (132). However, the relationship between the media and the dominant sporting culture of heterosexuality is more complex than this, as the media can be understood as actually creating this culture of heterosexuality through its obsession with ""the macho exploits of Britain's male athletes"" (132). Therefore, the media representation of the athletic male body and the normality of heterosexuality shows how sport is central to the social construction of these hegemonic masculine ideals as any alternative masculinities that do not feature in sport are labelled as inferior or ignored. The media representation of sport also contributes to the construction of the hegemonic masculine ideals of dominating women and the marginalisation of black men, and this relationship shall now be examined. The mediation of sporting practices (particularly violent sports) involves a complex and often contradictory relationship of identification and dissociation between male spectators from higher status backgrounds and the athletes themselves. The portrayal of sporting practices can be understood as naturalising the dominance of men over women, as media images of sport suppress the similarities between men and women and emphasise the differences, and then set about ""weaving a structure of symbols and interpretations"" around these differences to make them appear natural (Messner in Messner and Sabo, 1990; 101). Consequently, a sense of identification develops between male spectators and the images of sportsmen they consume which means that men identify themselves as physically superior to women; this sense of identification develops despite the fact that the majority of the male spectators are largely as physically dissimilar to professional male athletes as non-athlete females are (103). Conversely, the media representation of sporting practices also provides the male spectators with an image of a 'primitive other' embodied in the male athlete against whom he can define himself as modern and civilised (104). This sense of dissociation develops because the media representations portray images of sportsmen as predominantly black (and black men are historically associated with a notion of 'otherness'). It can be argued that the overrepresentation of ethnic minorities in sporting spheres is the result of a rational decision made by nascent black athletes to pursue a sports career, as it offered the best possibility of developing a career that would provide economic security because young black men are institutionally excluded from educational and occupational opportunities (104). Therefore, media images of sporting practices serve to reinforce the hegemonic masculine ideals of dominating women and marginalising black men through portraying 'natural' differences between men and women, and through depicting the black male body as an animalistic force. However, black men are not merely passive recipients of a racist social order, and their agency in relation to masculinity and sport shall now be elaborated. Through sporting practices black men have attempted to counteract the social invisibility imposed on them by an institutionally racist society, yet it is their acceptance of the ideals that compose hegemonic masculinity which ultimately ensures they do not affect the social order. Due to their exclusion from the educational and occupational institutions that confer social esteem on the individuals who have access to them, many black men channel their creative energies into constructing ""unique, expressive and conspicuous styles of speech, demeanour and walk etc"" (a phenomenon that Majors calls the 'cool pose') which offsets the invisibility they suffer due to their exclusion from the aforementioned institutions (Majors in Messner and Sabo, 1990; 111). Consequently, many black men pursue sporting careers because they can express themselves through the ""virtuosity of a performance"" and this offers the most realistic route to achieving the goals of hegemonic masculinity (such as being the breadwinner, having strength and dominating women) (110). Therefore, the response of black males through their concentration on sport represents an adaptation to, not a submission to, the hegemonic masculinity. However, in spite of their adaptation, their actions invariably do not allow them to transcend their subordination to the hegemonic masculinity. Through accepting the ideals of hegemonic masculinity black men pursue sporting careers in a context that is constructed by the racist wider society. In focusing predominantly on sport, black men accept the educational under-representation that is fostered by educational institutions; this is to their detriment as educational attainment is the principle route through which the ideals of hegemonic masculinity are realised and social power is attained (111). Even when black men do manage to become professional athletes, they are still confronted by institutionalised racism as there are remarkably few black managers or coaches in any sport, and the majority of sporting organisations are controlled by white males (113). Therefore, the participation of black males in sporting practices can be conceptualised as a response to the racism of the wider society, yet this response merely serves to reproduce the hegemonic masculine ideal of the marginalisation of black men, as it is predicated upon their acceptance of ideals that inevitably subordinate them. Although black athletes fulfil one element of the hegemonic masculinity (physical prowess), they are prevented from fulfilling other aspects (such as becoming professionals); this notion of contradiction is fundamental to the way hegemonic masculinity operates, and the role of sport in this process shall now be developed. The lives of professional sportsmen show how it is impossible to achieve all of the ideals that construct hegemonic masculinity, which therefore reveals how hegemonic masculinity operates in order to maintain its all encompassing dominance over the gender order. In his interview with Steve Donoghue (a champion in the iron man sporting practice) Connell asserts that Steve is an exemplar of hegemonic masculinity, as the life he leads makes him the embodiment of the masculine ideals of ""competitiveness, toughness and physical capability"" (Connell in Messner and Sabo, 1990; 94). However, Steve's testimony reveals that in order to maintain this status he has to sacrifice other masculine ideals that would jeopardise his physical performance, for example he can't ""go wild"" when he is socialising (92). Therefore, the representation of Steve and other sporting heroes like him can be understood as the symbolic embodiment of the epitome of an ideal. This means that sporting heroes are the tangible symbols for some aspects of hegemonic masculinity, namely the primacy of physical prowess, the domination of women, and the marginalisation of ethnic minorities and homosexuals. However, the sportsmen are unable to fulfil other aspects of hegemonic masculinity because they are antagonistic with the ideals they symbolise. Consequently, the other masculine ideals that contribute to hegemonic masculinity have different, often directly oppositional, symbols: for every sporting hero there is a 'bad boy', hedonistic Hollywood actor. Therefore, the existence of symbolic sporting heroes reveals that the hegemonic masculinity of which they form an integral part reproduces itself through the construction of a multiplicity of symbols that are often contradictory, yet always culturally exalted; and this shows how sporting practices contribute to the societal perceptions of masculinity and the roles of men and women that permeate all of society. In this essay it has been shown that the views stating the development of sport was unrelated to the social construction of masculine ideals are inadequate because of the primacy awarded to 'manliness' and athleticism in one of the major socialising institutions of the 19 th Century. It was also argued that the institutional subordination of women in sport contributes to the hegemonic masculine ideal of the subordination of women in society. The media representation of sporting practices was depicted as disseminating a consumerist, sexist, racist and homophobic ideology in order to maintain the dominance of hegemonic masculinity. It was then shown how black men pursue sporting careers to counteract the racism of wider society, but that ultimately the pursuance of a sporting career ensures the societal marginalisation of black men. Finally it was shown that hegemonic masculinity is a multi-dimensional phenomenon and sporting heroes constitute exemplars for specific ideals that construct hegemonic masculinity. It therefore follows from the evidence presented here that sporting practices are not a unique, socially isolated sphere and consequently do bear a relation to the social construction of masculine ideals. Sporting practices interact with other major societal forces in modern society, such as the perception that scientific knowledge equates to 'the truth' (as displayed in the relationship between the gender ideology and the public schools in the 19 th Century), or the prevalence of racism and consumerism, in order to socially construct hegemonic masculine ideals. Although sporting practices contribute to the social construction of the ideals that constitute hegemonic masculinity, it can be argued that the most important conclusion that can be drawn from an analysis of sporting practices and masculinity is that sporting heroes offer an insight into the way hegemonic masculinity structures the gender order of society in its entirety. Hegemonic masculinity is culturally exalted and stabilises a structure of dominance and subordination in the gender order as a whole, and in order to this it must have exemplars who are exalted as heroes. Sporting heroes as symbolic entities contribute to the vast network of images that cumulatively construct the societal perception of what a man should look like, or act like, or what the roles of men and women should be, which therefore means that sporting practices contribute to the social construction of characteristics that many individuals take for granted are part of our 'nature'.","AbstractIn this dissertation it is argued that the development of critical realism by Roy Bhaskar is a positive development for sociology. By successfully critiquing empiricism Bhaskar has developed an approach that can be used to highlight the deficiencies that have been associated with the traditional ontologies available in sociology. It will be shown that Margaret Archer effectively launched an immanent critique of the said sociological ontologies by using the realist ontology. It is also argued that she has constructed a worthwhile methodological complement to the realist ontology. The dissertation then attempts to prove these claims through a discussion of three of the most famous expositions of the concept of power. However, it is through these discussions that it is revealed that sociology will not have to sacrifice its critical character by adopting the realist ontology in toto. This is because there will inevitably be disagreement over the 'essentially contested' nature of our familiar concepts. In conclusion, the view will be advanced that the ascendancy of realism within sociology would provide us with a strong foundation from which to develop a body of theories and empirical evidence which would allow us to make a worthwhile contribution to the developments in society, and not just in the academe. IntroductionThe rich diversity of theoretical positions and their concomitant practical investigations within sociology are a reflection not only of the complexity of our subject matter, but also of the ongoing critical discussions between sociologists who have different ontological and conceptual commitments. With this in mind, it can be argued that sociology is inherently partial, and that this inherent partiality is one of the discipline's main strengths due to the breadth and depth of theoretical and methodological perspectives it has generated. However, it has been argued recently that the development of critical realism represents a threat to the continual development of sociological theory (through the process of critical discussion), because ""critical realists are engaged in an hegemonic project which seeks to conceptually re-tool the natural and social sciences"" (Cruickshank, 2004; 567). Cruickshank is arguing that critical realists refuse to contribute to the ongoing critical dialogue about reality, and consequently if the critical realist ontology is accepted as the 'infallible' definition of reality, then sociology must necessarily sacrifice its inherently partial character and the diversity of sociological theories contingent upon it. It will be the dual argument of this dissertation that critical realism has developed a valid ontological position through an immanent critique of the alternative ontologies which have traditionally framed sociological theorising, yet this development in no way threatens the proliferation of the multitude of theoretical ideas and contestations which give sociology its critical character. It takes as its core text Archer's Realist Social Theory: The Morphogenetic Approach (1995), henceforth TMGA, in which an immanent critique of individualism, collectivism, and structuration theory has been developed, and in which the 'morphogenetic methodology' (which seeks to, ""bridge the gap between the realist ontology and practical social theorising"") has been constructed (Archer, 1995; 192). The arguments of this dissertation will be pursued through applying the main contentions asserted in TMGA to three expositions of the concept of power, namely C. Wright Mills' conception of 'The Power Elite', Parsons' 'power as the symbolic medium of the goal attainment subsystem' and Lukes' 'Three-Dimensional view of power'. The decision to specifically assess conceptions of power from the ontologically realist and methodologically morphogenetic positions was not an arbitrary one. It will be the contention of this dissertation that power is part of a corpus of familiar concepts that permeate sociological theorising and are implicitly or explicitly present to a lesser or greater extent in sociological theories of any description; other familiar concepts include 'society', 'values', 'interests' etc. It will be shown how an ontological commitment is implied in the usage of these concepts, and consequently how most sociological theories can be reassessed and ontologically corrected in realist terms. However, it will also be shown that ontological commitment is not the only basis which furnishes critical dialogue. Therefore it will also be argued that the disagreement over the definition and practical application of concepts like power (and the interrelations and interdependencies between the familiar concepts themselves) is not solely determined (although undeniably influenced) by the specific analyst's ontology, and consequently the critical dialogue from which sociology benefits will always be preserved. Chapter one will outline the main ontological and epistemological tenets of the critical realist philosophy as presented by its principle exponent Roy Bhaskar. Archer's immanent critique of individualism, collectivism, and structuration theory (or 'elisionist' theorising) shall then be shown to be successful, and therefore justifying this dissertation's adoption and defence of the realist ontology. Chapter two will describe the morphogenetic methodological framework that informs the subsequent discussion of power. Chapter three will be devoted to an exposition and then criticism of the theories of power proffered by Mills and Parsons from an ontologically realist perspective, whilst an alternative morphogenetic approach to studying the substantive problems identified by Mills and Parsons will be provisionally sketched. Chapter four will assess Lukes' 'Three-Dimensional' (3-D) view of power from the realist perspective, yet will also seek to show how much of the critical dialogue regarding Lukes' view derives from the way he inappropriately subsumes many of the different applications of the concept of 'power' in to his 3-D view, and not from his ontological misconceptualisations. The final chapter will be devoted to a direct critique of Cruickshank's recent article A tale of two ontologies: an immanent critique of critical realism (The Sociological Review (54), 2004). The dissertation will conclude by opposing Cruickshank's claims that there is a logical contradiction at the core of realism because of its insistence on two distinct domains of reality (the intransitive and the transitive) in addition to his insistence that realism should engage in a 'dialogic' immanent critique of ontology. The discussion of Cruickshank's article will draw upon the main arguments presented throughout the dissertation in order to show the validity of the basic realist argument that there are two distinct realms of reality. Therefore, in order to arrive at the point where a discussion of the strengths and weaknesses of the criticisms of critical realism is possible, one must first understand the argument that critical realism is positing, and so it is to chapter one that we presently turn. 1. The Immanent Critique of Science and SociologyIn order to argue that critical realism has provided sociology with a superior ontological perspective of social reality to the individualist and collectivist ontologies, it will be necessary to show that the deficiencies of both of these traditional ontologies stem from their grounding in empiricism, whilst the impulse behind the establishment of the realist philosophy was the necessity for science to break free from the conceptual shackles imposed by empiricism. In 1975 Roy Bhaskar published A Realist Theory of Science (henceforth RTS), and this was the inaugural text in the establishment of transcendental realism (later to be reconceptualised as critical realism). Bhaskar has described his approach in RTS as ""an immanent critique of empiricism"" (Bhaskar, 1989 [1979]; 169). By this he meant that he was focussing on the, ""theoretical, explanatorily significant or 'fundamentally' experimental aspects of science"" as these were perceived to be the strengths of the empirical approach (and this is what is entailed in an immanent critique, criticising another philosophy or perspective based on its own standpoints). Therefore, Bhaskar developed a philosophy of science which attempted to argue against the assertion that all knowledge is derived from sense data, by developing a scientific approach that would reconceptualise the nature of causality, the role of experimental activity, and ultimately the constituent features of reality. Bhaskar argued that whilst laboratory experiments may produce observable regularities (or 'constant conjunctions'), these regularities cannot be taken to be causal laws (Bhaskar, 1975; 16). Bhaskar's justification for this assertion was his differentiation between 'open and closed systems'. 'Closed systems' are not naturally occurring in nature and require human intervention to exist (such as laboratory experiments); in a 'closed system' any intervening causes which may prevent a causal mechanism from having its normal effect are excluded (Bhaskar, 1986; 27). Bhaskar showed that the extrapolation of causal laws from observable regularities is predicated upon viewing reality as a 'closed system'. In contrast to this Bhaskar advanced the view that reality is an 'open system' in which causal mechanisms (like gravity) interact with other causal mechanisms and can interrupt their causal effects, therefore the manifestation of effects at the empirical level is contingent upon the interaction between unobservable entities. This view of reality as an 'open system' implies that reality is itself stratified; Bhaskar describes how ontology is layered into three levels: the real; the actual; and the empirical (Bhaskar, 1975; 19). The real consists of mechanisms that generate phenomena at the level of the actual, which may or may not be observed at the level of the empirical (Bhaskar, 1986; 27). More generally however, Bhaskar's use of the term 'stratification' refers to the simultaneous causal efficacy of the different emergent levels. Bhaskar uses the concept of 'emergence' to show how the different levels 'emerge' from the level below, but once they have emerged they have their own causal autonomy and are therefore irreducible to that from which they emerged (Bhaskar, 1986; 104). This implies a diachronic (as opposed to a synchronic) causal account of how the emergent entity develops from a pre-emergent level of the world (Bhaskar, 1986; 113). Bhaskar extends this seemingly exclusively ontological perspective to his view of epistemology, as he shows how a ""piece of knowledge is emergent from the cognitive structures which generate knowledge by transforming anterior knowledges"" (Bhaskar, 1986; 60). Ontology and epistemology in the realist philosophy of science correspond (roughly) to what Bhaskar terms the 'intransitive' domain and the 'transitive' domain respectively (Bhaskar, 1986; 24-25). Bhaskar claims that that stratum of 'the real' belongs to an 'intransitive' domain comprised of real phenomena that exist independently of our knowledge of them, whilst the hypothetical interpretations we have of these intransitive objects (which are inevitably historically contingent and socially constructed) belong to the 'transitive' domain (Bhaskar, 1986; 51-52). It should be kept in mind that the transitive/intransitive distinction applies both to reality and to our knowledge of reality. Within reality there is a distinction between intransitive features of reality and the transitive production of knowledge, which is a component of reality. Within that part of reality comprising the transitive production of knowledge, one can find the realist philosophy of science (with its distinction between the intransitive dimension and the transitive dimension). So the conceptualisation of 'intransitive' and 'transitive' dimensions is found within the transitive process of knowledge construction (Bhaskar, 1986; 24-25). All this states is that realism ""possesses the virtue of reflexive self-consistency"" in which it acknowledges that realism itself is transitively produced knowledge, and ""may be adequate to only one historically specific mode of science"" (Bhaskar, 1989 [1979]; 170). In summary, Bhaskar developed his realist approach to the philosophy of science because the dominant perspective of empiricism precluded the acknowledgment of the causal mechanisms that existed independently of the observable regularities produced in the artificially closed system environment of the laboratory. Realist philosophy views reality as robustly stratified into the three levels of the real, the actual and the empirical, all of which have their own emergent properties of causal autonomy that makes them irreducible to the stratum from which they emerged, whilst the philosophy also acknowledges two dimensions that are independent of each other, the intransitive and the transitive (within which the realist philosophy, like all human argument, is situated). In The Possibility of Naturalism (1979) Bhaskar argued that ""an essential unity of method between the natural and the social sciences"" was possible, based on the realist philosophy of science developed in RST (Bhaskar, 1989 [1979]; 2). It was here that Bhaskar developed the idea that social structures are emergent properties (as they 'emerge' from the actions of agents but then have causal autonomy and are therefore irreducible to the agents from whose activities they emerged) and that they exist in the 'open system' of society, which means that (when combined with the free will of agents who have the causal power to alter social structures) there are no 'observable regularities' in society (Bhaskar, 1989 [1979]; 45). However, Bhaskar does maintain that there are limits to his conception of the naturalism between the sciences, as he claims social structures (unlike natural structures) are not independent of the activities they govern or the conceptions that agents have of what they are doing, and that social structures are only relatively enduring (Bhaskar, 1989 [1979]; 38). Therefore, through his immanent critique of empiricism Bhaskar has developed an ontology that is applicable to the social sciences. However, it is not sufficient to explicate the realist ontology for social reality and declare it to be appropriate without substantiation. In TMGA Margaret Archer also undertakes an immanent critique in the same manner as Bhaskar did in RST; she critiques individualism and collectivism as she argues they were both deficient because of their adherence to empiricism, whilst structuration theory is criticised for sinking structure and agency together, instead of acknowledging their separability, which is the precondition for ""explaining why things are so and not otherwise"" in society (Archer, 1995; 64). Archer's critique is immanent as she shows exactly how both individualism and collectivism failed to respond to the mutually regulative function of ""the tri-partite link between ontology, methodology, and practical social theorising"" (Archer, 1995; 3). Archer critiques individualism because it starts from the empirically secure position that the ultimate constituents of society are people, yet has to incorporate non-human features into its conception of the individual when its ontology is challenged. Individualism's principle that social structures and phenomena must be reduced to statements about the interrelations of individuals (because the individual is the ultimate constituent part of society) leaves it susceptible to criticism from psychologists who state that people themselves can be reduced further still into their underlying psychological features (Archer, 1995; 39). Due to their belief in reductionism it becomes difficult for individualists to deny the psychologists' claim that society is the reflection of the combined psyches of the population (Archer, 1995; 40). However, to defend their position from psychologists Individualists invoke the realist (and collectivist) notion of emergence. They claim that their concept of the individual is unique because it is defined by the individual's participation in relationships that pre-date them, for example ""English speakers do require other English speakers to become such themselves"" (Archer, 1995; 40). This recognition of an emergent structure (such as the English language) contradicts individualism's premise that everything in society is reducible to an individual's dispositions. The response of individualism to this criticism is to incorporate all non-individual and non-dispositional factors into their concept of the individual, such as a person's 'physical resources and environment' (Watkins, 1971, in Archer, 1995; 39). This again reveals a contradiction, as these aspects of social reality are not about individuals or dispositions (the exclusive focus of individualism) yet are central to the its concept of the individual. Individualism's inability to discredit realism's (and collectivism's) conception of social structure provides further evidence of its inability to uphold 'the tri-partite link'. Individualism cannot successfully prove that social structures are not, ""autonomous from, pre-existent to, and do not exert a causal influence over individuals"" (as both realism and collectivism state) (Archer, 1995; 42). Individualism posits that social structure is not independent from individuals because the social structure is constituted of interpersonal relations (Archer, 1995; 43). However, this does not take into account that social structures condition behaviour, for example in the form of role fulfilment. Individualism refutes the validity of this criticism by denying the pre-existence of social structure; it claims that features such as 'roles' only exist because the individuals concerned lack the desire or the knowledge to change them (Archer, 1995; 44). Archer disproves this claim by discussing the resistance shown by structures (such as the rate of recruitment into the police force) to the concerted efforts of individuals trying to change them; this proves that structures pre-exist and outlive specific individuals and their attempts to alter them by showing that they possess properties independent of the individual's influence (Archer would also add that the process of 'role fulfilment' by actors is structurally conditioned, as opposed to structurally determined, nor is it contingent which actors are eligible for certain roles) (Archer, 1995; 44, 277). Additionally, it can be argued that the dispositions which are so important to individualism are shaped and restricted by their historical context (Gellner, 1971, in Archer, 1995; 42). Consequently, individualism cannot maintain that structures do not have a causal influence (such as a low level of recruitment into the police force being causal of a high crime rate in urban areas). Although collectivism is not susceptible to the criticisms of individualism's methodological approach, it does not offer a significant counter-ontology, and is therefore inferior to the realist approach. Archer shows that the flaws of collectivism stem from its inability to transcend empiricist concepts. Collectivism did not define the exact ontological status of 'societal properties' (as defined by Mendelbaum) (Archer, 1995; 49). This is because a societal property cannot be empirically proven as it does not have a tangible existence which can be directly detected by an individual's senses (Archer, 1995; 49). In addition to this, the empiricist concept of causation (whereby a phenomenon only has causal power if it produces constant conjunctions at an empirical level) denies social structures causal power, because they are neither observable nor causal in their effect (based upon this definition) (Archer, 1995; 52). Realism solves this problem as it recognises that society is an open system in which events are influenced by other contingencies, so the causal influence of emergent properties is not to consistently produce observable effects (Archer, 1995; 53). Therefore, collectivism could only challenge individualism methodologically (by showing that some features of society cannot be reduced to statements about individuals) but could not advance a counter-ontology (because of the dominance of empiricism) stating the existence of non-observable properties (Archer, 1995; 54). Realism, however, does not recognise the validity of the empiricist notion that all things which are held to exist must be detected through human sensory experience. The basic tenet of the realism advanced by Bhaskar and Archer is that society is stratified into individuals and the emergent, non-observable, social structure, and that these two non-reducible strata interact and influence each other (Archer, 1995; 61). After immanently critiquing individualism and collectivism, Archer then turned her attention towards refuting the ontology presented by 'elisionary theorising', represented by Giddens' 'structuration theory'. Although both 'elisionism' and 'emergentism' have rejected the 'terms of the old debate' between individualism and collectivism, they denote entirely different approaches because, ""they are based upon different ontological conceptions, related to disparate methodological injunctions and thus have quite distinct implications for practical social theorising"" (Archer, 1995; 60-61). The key difference between the two approaches turns on their ""ontological parting of the ways"" with regards to the issue as to whether structure and agency are separable or inseparable (Archer, 1995; 63). Elisionism states that social structures can not be examined unless the examination is in conjunction with the actors who 'instantiate' them, as structures only possess a virtual existence until they are 'instantiated' by actors (Archer, 1995; 63). This insistence upon inseparability of structure and agency precludes an examination of their interplay, and it is this examination that Archer stresses is essential to practical social theorising. In eliding structure and agency, Archer contends that elisionists are left with 'the remainders' of the material element of social structures, and the autonomous psychological properties of individuals, as these are distinct emergent strata that cannot be compressed into an 'ontology of praxis' (Archer, 1995; 133). Therefore in TMGA Archer provides an immanent critique of the alternative ontologies that are available in sociology that are all guilty of 'conflationary theorising'. By this Archer means individualism is guilty of 'upwards conflation', in which the social structure is seen to be inert and wholly dependent upon agents for its configuration (thus rendering structure as epiphenomenal), whilst collectivism is guilty of 'downwards conflation' whereby individuals do not possess causal autonomy and are entirely determined by a super-ordinate social structure (in this instance agency becomes the epiphenomenon). On the other hand, elisionism should also be dismissed as it commits 'central conflation' in which structure and agency are collapsed into an indistinguishable whole, which disallows the possibility of examining their interplay. It is hoped that this chapter has outlined how the development of Bhaskar's philosophical realism successfully provided an antidote to the ontological and epistemological misconceptualisations inherent in empiricism, whilst also showing that Bhaskar was correct to attempt to establish a naturalism between the natural and social sciences based upon realism. It has also been argued that Archer has provided an immanent critique of the alternative ontologies available in sociology, and that this critique has provided us with the appropriate focus for our study: namely, examining the interplay between structure and agency. It is therefore incumbent upon Archer to provide us with the methodological tools necessary to undertake such an endeavour, and the next chapter will illuminate how she has done this. 2. MorphogenesisIn TMGA Archer develops a methodology that seeks to examine the interplay between structure and agency over time in order to account for 'why things are so and not otherwise' in society. Her approach represents an elaboration of the realist principles outlined by Bhaskar, as some of his conclusions are challenged or reformulated. This chapter will attempt to outline the main features of the morphogenetic approach through an exposition of its three generic phases (structural conditioning, social interaction, structural elaboration) and its underlying ontology (analytical dualism). The morphogenetic methodology is ""based four-square on analytical dualism"" (Archer, 1995; 167). By this Archer means that 'the people' and 'the parts' of society have to be analytically discrete in order to examine their interplay. Like Bhaskar, Archer utilises a diachronic conception of emergence in order to construct 'analytical histories of emergence'. However, she contests the qualification that Bhaskar makes about the possibility of naturalism, in which he stated that social structures differ from natural structures because they are activity and concept dependent. Bhaskar's formulation of this qualification brings him too close to 'elisionary theorising' as it opens up the space in which society can be conceptualised as ""taking the form it does because of these people here present, and the conceptions they have"" of what they are doing (Archer, 1995; 149). In order to overcome this Archer incorporates a different conception of time when constructing the morphogenetic approach, as she argues that ""it must be identified on whose activities structures are dependent, and when"" (Archer, 1995; 60). Consequently, not only are structure and agency irreducible to one another, they are also temporally distinguishable, and it is the combination of these two conceptions that allows the interplay between them to be examined over time. Therefore, the two basic theorems of the morphogenetic approach are that ""a) structure necessarily pre-dates the action(s) which transform it, and b) that structural elaboration necessarily postdates those actions"" (Archer 1995; 168). Archer argues that there are three types of morphogenetic (or morphostatic) cycles occurring in society simultaneously, and that these cycles intersect in the middle phase (the interaction phase, as interaction is the efficient cause of structural and cultural elaboration); the three types of cycles are structural, cultural, and agential (Archer, 1995; 265). The cultural system is seen as the ideational equivalent of the material set of social structures, whilst social interaction (in which the negotiation of new structural elaboration takes place within the prior structural context) is seen as the parallel of socio-cultural interaction (in which the negotiation of new ideational elaboration takes place within the context of already emergent ideas/theories) (Archer, 1995; 257). It must also be noted, however, that agency itself undergoes new elaboration whilst elaborating new structural and cultural configurations, through the process of 'double morphogenesis'. In order to clarify the above description, each stage of the morphogenetic cycle will be discussed. Structural and cultural conditioning should be conceptualised as matters of mediation, as ""the causal power of social forms is mediated through social agency"" (Bhaskar 1989 [1979], in Archer, 1995; 195). Consequently, agents are the only 'efficient' causes in social life yet material and ideational emergent properties have causal powers. In order to resolve this apparent contradiction Archer shows that it is the creative powers of human beings that allows society to be conceptualised as an open system, which means they are able to ""resist, repudiate, suspend or circumvent structural and cultural tendencies"", therefore the effect of socio-cultural powers is never direct, but mediated (Archer, 1995; 195). However, it is equally the case that the exercise of agential powers can be ""suspended, modified and re-directed by the social forms in which they are developed or deployed"" (Archer, 1995; 196). Therefore, the morphogenetic perspective maintains that all structural influences ""are mediated to people by shaping the situations in which they find themselves"" (Archer, 1995; 196 original italics). The structural and cultural system 'shapes the situations we find ourselves in' in five ways. 'Involuntaristic placement' refers to how ""we are quite literally born into life chances which are defined by prior distributions of material resources"" (Archer, 1995; 202). This is an objective feature of social life which refers to the fact that the world exists before we arrive into it, and we arrive into it in a certain location due to our background; however it also refers to the fact that we are never free from an involuntaristic involvement in structures and their situational conditioning. It follows on from this that different sections of society are endowed with different 'vested interests' because ""vested interests are embedded in all socially constructed positions"" (Archer, 1995; 203 original italics). Consequently there are different 'opportunity costs' involved in not acting in a manner that promotes one's vested interests; however, it is not only action that is conditioned but also 'the degrees of interpretative freedom' whereby the interpretation of a situation by an agent is conditioned by the placement of prices and premiums upon certain interpretations of it (Archer, 1995; 208-209). Finally, 'directional guidance' for a strategic course of action in which the promotion of vested interests is conditioned by the configuration of 'second order' structural and cultural emergent properties (systemic properties that are the 'results of the results' of social interaction), whereby certain courses of action can be pursued (or prevented) if social structures (or ideas) are in 'compatible or incongruent' relationships to one another respectively (Archer, 1995; 215). The middle phase of the morphogenetic cycle refers to the process of social interaction in which the new structural formations are elaborated but also in which agency itself is elaborated. Archer's conception of the elaboration of agency is derived from her stratified model of people. People are stratified into ""persons, agents, and actors"" (Archer, 1995; 254). Each stratum is emergent from the one immediately below it. A concept of 'personal identity' is necessary if an individual is to develop a concept of their 'social identity'; this is because human beings have to have a sense of self in order to identify the interests of a group with their own interests, and this is possible through the human being's capacity to retain a sense of themselves as being the same person who is passing through time (i.e. the capacity to narrate their own history and future) (Archer, 1995; 282). This personal identity then ""fathers agency"", as agents are ""the agents of the socio-cultural system into which they are born""; however, this involves a distinction between 'corporate agents' and 'primary agents' (Archer, 1995; 255). 'Primary agents' do not express interests or organise for their strategic pursuit, yet everyone can be considered an agent as ""the similarities of response from those similarly placed can generate powerful, though unintended, aggregate effects"" (Archer, 1995; 259). 'Corporate agents' on the other hand are those who have articulated their interests and have organised in order to attain them, therefore they are the ones who are involved in a concerted effort to re-shape the structural or cultural feature that affects their interests (Archer, 1995; 258). In turn, the kind of actors we can become is largely influenced by the kind of agents we are involuntarily placed as, this is because ""certain opportunities and information are open to the privileged and closed to the non-privileged"" (Archer, 1995; 277). The final phase of an 'analytical history of emergence' concerns the form that new structural elaboration takes, and whether or not it is new elaboration (morphogenesis) or reproduction (morphostasis). The way that new social elaboration occurs is ""through exchange transactions and power relations"" (Archer, 1995; 296). Archer employs a quasi-weberian understanding of social stratification in her discussion of social change, as all processes of exchange and power necessarily ""involve the use of resources, namely political sanctions, liquid assets and expertise"" (Archer, 1995; 297). The 'raw bargaining power' of primary agents depends upon their access to resources as conditioned by their involuntaristic placement in the social structure; essentially those with a higher access to the resources outlined above will have more bargaining power (Archer, 1995; 300). However, Archer emphasises that the crucial relationship is between the distribution of interest groups and the availability of the resources; if the concentration of resources is high then there will be fewer groups in society who have access to them and therefore less groups who are in a strong bargaining position (meaning a ""steeper gradient between elites and masses"") (Archer, 1995; 298). Raw bargaining power alone is insufficient to describe why some corporate agents successfully pursue their vested interests whilst others are impeded. The concept of bargaining power has to be supplemented with the relational notion of 'negotiating strength' (Archer, 1995; 300). Negotiating strength refers to the relationships between and amongst corporate agents in situations of power and exchange; a corporate agent has more negotiating strength relative to the corporate agent it is dealing with if it supplies a resource that the other cannot either, ""a) reciprocate, b) acquire from an alternative source, c) coerce the other party into supplying, and d) cannot reconcile itself to dispensing with"" (Archer, 1995; 302). Therefore the effect of bargaining power is to define what resources a corporate agent can bring to bear in the struggle to promote vested interests when confronted by the (second order) constraints and enablements arising from cultural emergent properties and structural emergent properties (Archer, 1995; 302). Ultimately societal morphogenesis or morphostasis is the result of the interaction between third order emergent properties (the results of the results of the results of social interaction) (Archer, 1995; 327). However, this is not to imply that third order systemic properties are hypostatised entities; Archer insists that the cultural realm and structural domain are distinct from one another and the influence of one or the other on societal morphogenesis or morphostasis depends upon the relationship between the four different configurations possible if culture and structure can be perceived to interact in a state of either internal morphogenesis or morphostasis (Archer, 1995; 323). Archer maintains throughout that the linkages between the structural and cultural realms are situated in the process of interaction, therefore the elaboration proceeds in terms of exchange and power between agents (which protects against the criticism that structures are reified entities in the morphogenetic approach). It therefore follows from the evidence presented here that the morphogenetic approach successfully provides the methodological accompaniment to Bhaskar's critical realist ontology as it utilises the concepts of stratification and emergence in order to chart 'analytical histories of emergence' of particular social configurations. This means that the speculation as to the ""tendential powers of generative mechanisms"" is complemented by ""an historical analysis of the concrete contingencies which intervened to produce particular outcomes"" (Archer, 1995; 327). In the subsequent chapters the morphogenetic approach will be used to critique three famous sociological conceptions of power in order to illustrate the utility of both the realist ontology and the morphogenetic methodology. 3. The Mills Vs Parsons Debate Reconsidered The famous debate between C Wright Mills and Talcott Parsons regarding the 'correct' conceptualisation of power provides the quintessential example of the benefits to sociology of ongoing critical discussion. However, it shall presently be argued that the deficiencies of their approaches can be traced back to their ontological underpinnings. Both views will be explicated and then subjected to a realist critique, whilst the morphogenetic approach will be used to show that the substantive problems identified by Mills and Parsons are worthy of investigation. In The Power Elite (1956) Mills developed the notion that in the first fifty years of the 20 th Century American society underwent a dramatic structural change, which resulted in the concentration of power in the hands of a minority whom he called 'the power elite'. Mills draws the historical comparison between the richest and most politically influential members of US society (and the economy and polity within which they operated) from different eras of US history, in order to show how the structures of power had become ""centralised and enlarged and interlocking in the present era [1950s USA]"" (Mills, 1956; 4). Mills goes on to describe how these enlarged and centralised structures of the economy, the political directorate, and the military have come to penetrate all the other institutions of US society, and consequently influencing the lives of all members of 'the mass population' (those who are not members of the elite) (Mills, 1956; 275). His view of a power elite stems from his belief that these enlarged structures of 'the big three' have come to form an interlocking whole, and that the members of each of the elites are interchangeable; this has fostered the development of one 'power elite': a self-conscious social class whose interests are intertwined with those of the other members of the elite (Mills, 1956; 210). The elite reproduces itself through a process of recruitment that begins at 'the right' schools (Mills gives the examples of the appropriate private schools for elite membership), and carries on through Ivy league colleges and results in the eventual careers of men destined for elite membership, in which they characteristically hold positions in more than one of the structures that comprise the interlocking elite (Mills, 1956; 188). The members of the elite tend to have a common social background as this is crucial to the ""easy intermingling"" which characterises the interaction between members of the elite, and it is in these small scale social milieux that the common psychology of the elite is passed on to new members who learn the art of impersonal decision making and the intimate sensibilities of elite membership (Mills, 1956; 178). It was in response to these claims by Mills that Parsons began to reveal his own concept of power, which marked a break from his conceptualisation of it in his early works (such as The Structure of Social Action in which he accepted the 'traditional' view of power). In his essay 'The Distribution of Power in American Society' (published originally in 1960) Parsons develops a critique of Mills' The Power Elite, in which Mills is criticised for viewing power as a ""zero-sum concept"" whereby there is a fixed 'amount' of power to be 'held' by an individual or a group, which necessarily means that one group holds power to the extent that another group (""the outs"") does not (Parsons, 1968; 219). Parsons declares that conceptualising power in this way confines Mills to exclusively discussing instances where one individual or group ""has power over other"" (Parsons, 1968; 219 original italics). Parsons on the other hand, wanted to argue that the concept of power should refer to instances where actors had the ""power to make authoritative decisions"" (Parsons, 1968; 220 my italics). It is useful to quote Parsons at length here: Power, then, is generalised capacity to secure the performance of binding obligations by units in a system of collective organisation when the obligations are legitimised with reference to their bearing on collective goals and where in case of recalcitrance there is an assumption of enforcement by negative situational sanctions - whatever the agency of that enforcement. (Parsons, 1969; 361). In this oft quoted passage Parsons shows how he has situated power within his wider conceptual framework termed 'normative functionalism' and tied it to the conceptions of generalisation and legitimation. By 'generalisation' it is meant that he conceptualised power as a symbolic medium through which authoritative decisions made by those who occupy position of power are implemented in order to further collective goals (Parsons, 1968; 362). Decisions are deemed 'legitimate' because they are 'authoritative'; for Parsons, power resides in positions within the political system that have legitimate authority (Parsons, 1968; 372). By this he meant that power is the facility for the achievement of collective goals because the members of a society 'agree' to legitimise leadership positions; they do this by giving such positions a mandate to develop and implement policies to further the goals of the system (or society) as a whole (Parsons, 1968; 373). Therefore, power is directly derivative of authority (as this is the institutionalised legitimation which underlines power). Consequently, for Parsons what is important is not the amount of power 'possessed' by individuals, but the amount of power 'generated' by the system which would ultimately be beneficial to all, as the more power that leaders have at their disposal, the greater their mandate for implementing changes that would further society's collective goals; of course for Parsons society's collective goals depended upon the common value system, and in America he conceptualised this as 'instrumental activism', in which the main goal was the furtherance of society's economic productivity (Giddens, 1992 [1968]; 179). In spite of the manifest differences of these two conceptions of power, both contain features that are consistent with the realist perspective and the morphogenetic methodology. Mills' use of an historical approach to assessing how a group substantively changes over time reveals he has an understanding of the diachronic nature of emergence, and that he resists falling into the trap of nomenclature, in which groups who have the same name (such as 'the working class') are considered to be qualitatively the same (despite the clear differences between 'the working class' of the early 18 th Century and the 21 st Century) (Archer, 1995; 74). Parsons, on the other hand, did acknowledge the existential intransitivity of social structures (as the political system of Parsons' conception existed independently of the human knowledge of the system). However, these consistencies with realism are only partial, and it is the argument of this chapter that the deficiencies of these two conceptions of power are essentially ontological in nature. Neither Mills nor Parsons adopted the ontology of 'analytical dualism' (in which the 'people' and the 'parts' of society are conceptualised as distinct strata, which allows their interplay over time to be examined). For Mills, this means that he essentially denies causal efficacy to the social structures of the three domains of the power elite; as Parsons insightfully points out, ""the power elite is free from the 'checks and balances' associated with our political system.... and has been freed from the historic restraints of our society"" (Parsons, 1969; 205 my italics). Although Mills contends that the ability of the elite to wield power is dependent upon their ""occupation of the command posts of society"" once they are the incumbents of these posts the elite is free to act as they choose (Mills, 1956; 3); consequently, Mills offers no discussion of how the structural and cultural context conditions their actions, whilst their actions again contribute to the elaboration of new structures. This points to the key criticism of Mills, and why the similarity between his conception and that of realism outlined above remains only partial. Mills' conception of the diachronic emergence of the power elite ends with the emergence of the power elite. Again, Parsons offers an insightful contribution as to why this is; he describes how Mills has a prior commitment to a ""utopian conception of an ideal society in which power plays no part at all"" and a fundamental mistrust of capitalism which means for him ""the power elite is bringing the capitalist evil to a climax"" (Parsons, 1969; 224 my italics). Throughout The Power Elite there is a tone of finality to Mills' discussion of the development of American society, and the only consideration of what new structural elaboration and social configurations may arise are implicit in the continued consolidation of the power elite, meaning that there will be no new elaboration (and this is consistent with Parsons appraisal with Mills' value commitments). For the realist, the actual form society takes is an unintended consequence because it is the combined product of different outcomes pursued simultaneously by different groups (Archer, 1995; 91). The implication of this for Mills is that he over-emphasises the ability of one group in society to shape society in its entirety, as he argues ""elites have the unique ability to smash one structure and set up another in which they enact quite different roles"" (Mills, 1956; 24-25). Therefore, Mills' position is untenable from the realist perspective as he makes no attempt to link up cycles of development with posterior developments (although he does discuss the anterior social context) and he denies the causal efficacy of all 'primary' agents, and any 'corporate' agents who are not members of the elite, in addition to denying the relative autonomy and causal efficacy of social structures. Many of the criticisms that can be made of Parsons' conception of power are consistent with the criticisms of collectivism outlined earlier, in which agency is reduced to the status of the epiphenomenal. Parsons is guilty of ""making present-tense distinctions between roles and incumbents"" which reveals how he does not conceptualise the 'parts' and the 'people' of society as analytically separable; this is so because analysing their interplay over time is only possible if they are separate entities, and Parsons makes no allowance for the fact that many roles (such as those in the political system) pre-condition who is eligible to fulfil them (Archer, 1995; 71). The Parsonian conception of power precludes the notion that different groups of agents have different vested interests to pursue because of their involuntaristic placement within the social system (or of the vested interests they acquire with roles that they come to fulfil) because it refers only to the 'societal goal', and this is predicated upon viewing society as essentially stable. Therefore Parsons makes no allowance for the way that social reality is stratified and that agents and structure interact over time and are both reformulated through this interaction; what Parsons presents is an essentially static view of societal development. Although the purpose of power is presented as dynamic (as it allows the furtherance of collective goals) Parsons' conception ultimately precludes the possibility of new social and structural elaboration as agents are denied the causal power to elaborate new social forms (societal development proceeds harmoniously under the direction of cultural values, yet Parsons offers no explanation as to why these cultural values develop). Giddens captures this deficiency in Parsons' conception when he describes how Parsons offers no explanation of how ""legitimacy is achieved, and therefore consensus is assumed, despite the historical evidence that power can grow out of the barrel of a gun"" (Giddens, 1992 [1968]; 187 original italics). Therefore a similar criticism can be made of Parsons' conception of power as can be made of Mills', namely that neither successfully incorporates a conception of the causal interdependency of structure and agency as revealed by examining their interplay over time. However, this realist critique of the ontological foundations of these two conceptions of power can (and should) be supplemented by a discussion of the criticisms launched from within the ongoing debate about the concept of 'power' in the social sciences itself; unfortunately for the purposes of brevity these shall not be covered in the present work (although the reader is referred to Giddens (1968) for a discussion of Parsons, whilst Parsons' own discussion of Mills (1960) is still as insightful today as ever). All that remains for this chapter to do is sketch a possible approach to the substantive problems raised by Mills and Parsons from the morphogenetic approach (as this is the ultimate function of a methodology, to provide a framework with which to analyse substantive sociological issues). The substantive problems identified by Mills was that a monopoly was developing between the most prominent figures within the economic, political and military domains over the access to decisions which had consequences on a national scale. The morphogenetic approach describes this kind of situation as one in which both the cultural and structural domains are in a state of morphostasis; therefore a sociologist who is using analytical histories of emergence would seek to empirically reveal the contingencies that contributed to the lack of articulation of an alternative set of ideas, or of an articulated interest group within a society. They would therefore have to chart the progression from the initial social context confronting the agents who have become dominant, through the process of interaction that grouped the agents into the groups that became dominant and elaborated the structure that reinforced their dominance. The substantive area implied by Parsons' conception would be how a particular political structure came to embody the legitimate authority that is representative of the ideal workings of western democracy. However, an analytical history of emergence of these structures would discuss how individuals as active social agents come to develop their own vested interests in the positions of the political system, and would therefore have to discuss whose activities the particular form of structure were dependent upon, and when. Therefore the results would most likely yield conclusions that are incongruent with Parsons' implication that the political structure is a direct reflection of society's underlying collective goals, as the particular form a structure takes is the result of interaction between competing interest groups. The above critique was intended to show how two expositions of one of the most familiar concepts within the social sciences were susceptible to criticism based upon their misconceptualisations of ontology. If the matter was left here then one could appreciate Cruickshank's concern that sociology is in danger of losing its critical nature, as the only clarification of concepts would be to align them with the principles of realism. However, the matter does not end here, and it shall presently be argued that sociology is, and always shall be, a necessarily critical endeavour. 4. The 'Essentially Contested' Status of Power In order to argue that the critical discussions inherent in sociology are not threatened by the ascendancy of the critical realist ontology one must show that these discussions are not entirely derivative of ontological differences. It is the argument of this chapter that the familiar concepts already existing in social science can be ontologically corrected, yet critical discussions will also take the form of contesting the parameters of such far reaching concepts, and debates will continue over which phenomena should be conceptualised in these terms. This argument will take the form of an exposition of Lukes' 3-D view of power, a critique of his concept from the realist perspective, and then finally a consideration of the other kinds of critical discussion that his concept has generated. In his short book Power: A Radical View (1974) Lukes developed his 3-D concept of power. Essentially, the 3-D view of power is concerned with establishing that one agent, A, (whether they are an individual or a collectivity) affects another agent, B, in a manner contrary to B's interests (Lukes, 1974; 24). Politics is crucial for Lukes' argument as he believes A doesn't exercise power over B simply through having the greater ability to effect their own will in a conflict situation, but by being able to shape B's very wants, presumably through ideological distortion (Lukes, 1974; 23). Consequently, the political sphere is of central importance to this abstract conception as it is the key site in the decision making process of society, which means agents who can wield political power will have the greater ability to disseminate an ideology than an agent in any other sphere. In light of this, political power (or power in the political sphere) is the only form of power discussed throughout Lukes' work. It is fundamental to Lukes' conception that one agent possesses power and has 'power over' another agent who does not posses power (Lukes, 1974; 31). This shows that he gives primacy to the agential potential of agents, as instances where power is exercised are the result of choices made by A (the possessor of power) which are in their own interests and which facilitate B acting in a manner that is contrary to B's own interests. This notion of choice is crucial to Lukes' perception, as he believes instances where power is exercised are derived from an agents's decision and not because the instances are structurally determined; he elaborates this by arguing that the instances where power is exercised can only be identified (and therefore differentiated from instances of structural determinism) when ""it is in the exerciser's or exercisers' power to act differently"" (Lukes, 1974; 55, original italics). Therefore, this shows that Lukes argues power rests solely in agency because power itself is a commodity that can be exercised (or not exercised) in a manner that is dependent upon the exerciser's or exercisers' will. For Lukes power should be understood as the latent conflict of interests between the interests of those who exercise power and the ""real interests of those they exclude"" (Lukes, 1974; 25 original italics). Consequently, instances where power is exercised are constructed by the discrepancy between the interests of A and the 'real interests' of B, meaning that the relation is defined by A's concealing of B's 'real interests' (Lukes, 1974; 25). Therefore, B is invariably disadvantaged whenever power is exercised, even if B is unaware of it. The concept of 'intentionality' is also central to the realist position; in fact Bhaskar went as far as to describe it as the ""sine qua non"" of the potential problem of naturalism (Bhaskar, 1989; 173). However, in realist terms Lukes emphasises the agential character of instances where power is exercised at the expense of a detailed understanding of the influence of structure. It is insufficient merely to state that 'the political sphere' is the key decision making site for society, and is therefore central to the exercise of power. This in no way accounts for the processes in which different agents come to occupy the positions within the political sphere, and consequently what is lost in Lukes' conception is how A becomes A in the first place; or why the political sphere has developed in such a way that it has come to embody the 'real interests' of those agents who have become A. Additionally, realism takes issue with Lukes' conceptualisation of agents as agents of the cultural system. Lukes gives no account of the processes of interaction that generate a situation in which 'ideological distortion' is possible. Lukes' conception of B denotes only one configuration of the cultural system, and that is morphostasis where there is no articulated alternative to the ideology of the cultural elite, whose vested interests are represented by the cultural system. Therefore, Lukes' conception is open to ontological revision as it focuses exclusively on the influence of agency in instances where power is exercised, and cannot give any description of the way that structure and agency interact to condition who the interest groups represented by A and  B are, or why the political system is structured to reflect the interests of A, or even if agents who comprise A actually acquire their interests due to their embodiment of the roles within the political sphere. Although these realist criticisms are pertinent, it would be misleading to contend that ontological grounds are the only grounds upon which a concept can be contested. In the second edition of Power: A Radical View (2005) Lukes admits that many of the valid criticisms of his conception of power stem from the way that he subsumes all of the possible applications of the concept of 'power' into his totalising 3-D view (Lukes, 2005; 109). He acknowledged that power should be conceptualised as a dispositional concept (which means it may or may not be exercised); he takes on board the valid assumptions of Parsons' conception, in which 'power over' (or as Parsons' would describe it, 'power to') others may be productive for all and authoritative, and actually compatible with dignity (Lukes, 2005; 109). Consequently, Lukes is led to redefine his conception with a higher degree of analytical specificity; instead of using the generic term 'power' he argues that the 3-D view should be defined as ""securing the consent to domination of willing subjects"" (Lukes, 2005; 108). Therefore, in the time between the publications of Power: A Radical View 1 st edition and 2 nd edition there emerged an extensive body of critical discussions regarding the correct definition of Lukes' concept of power, which thus aided in the development of other conceptions of power as the different facets of social reality that it could be applied to were delineated. These contributions to the critical dialogue of power were not derived from the ontological misconceptualisations outlined above. The critical dialogue of Lukes' 3-D concept of power essentially turned on the issue that drives much of the critical dialogue in sociology: the explanatory purchase of a concept. It has been shown above that this is both an ontological matter, but also a conceptual one. Bhaskar has shown that there are ""rational criteria for judging some theories and concepts as better and more explanatory than others"" by which he means that if one concept can explain more on its terms than another can on its own terms then the first concept has greater explanatory power and is therefore superior (Bhaskar, 1986; 43). Consequently, if the concepts of sociology were subjected to a realist critique and were then re-stated in realist terms, there would still be critical dialogue that was beyond mere definitional squabbles; the very explanatory power of the concepts we employ is at stake, and given the nature of our subject matter these debates cannot help but be ongoing, as social reality is unpredictable and ever changing. The discussion of power concludes with a statement that reaffirms that the critical dialogue of power is not merely an ontological issue: power is (as Lukes describes) an 'essentially contested concept' (Lukes, 1974; 20). By this he means that a conception of power is necessarily tied to further background assumptions that are not only ontological and epistemological but also moral and political. A concept of power necessarily relates to contested and political features of social life as it invariably involves the disadvantage (in one form or another) of an individual or group. As Giddens points out, ""power has a necessarily hierarchical character"" (Giddens, 1992 [1968]; 184). The final word on power of this dissertation is to boldly state that the critical discussions of the concept of power cannot be entirely reduced to matters of ontology as a concept of power is always related to a concept of interests. The 'interests' of the agents involved are either explicitly declared or implied in any concept of power advanced in sociology to date. It is the contention of this dissertation that discussing 'interests' necessarily precludes the reduction of discussions of power to solely ontological issues as there is an element of moral judgment in determining how 'interests' are to be calculated or conceptualised. 5. The 'Essentially Contested' Status of Sociology This dissertation concludes with a critique of the main points of the recent article 'A Tale of Two Ontologies: an Immanent Critique of Critical Realism' by Justin Cruickshank. It will be argued that Cruickshank depicts sociology as in danger of losing its critical character if the 'hegemonic' project of critical realism to conceptually re-tool the sciences is successful, and that ontological debate should continue in terms of 'critical discussion' because there can never be a 'one true' definition of social reality. It will also be shown how Cruickshank has argued that Realism has developed two mutually-exclusive ontologies in order to dodge the 'epistemic fallacy'. The above assertions by Cruickshank will be criticised through a defence of the realist ontology and in terms of the certainty of the continuity of critical dialogue within sociology. Cruickshank's main reservation with realism is the perceived ""tension concerning the definition of ontology"" it employs (Cruickshank, 2004; 567). He states that the realist ontology refers to both the transitive and intransitive domains. However, in order to avoid the epistemic fallacy (in which statements about ontology (reality) are transposed into questions about epistemology (knowing reality)) one would ""have to step outside of transitivity....... and assume a God's eye view in order to know the essential features of reality beyond our knowledge"" (Cruickshank, 2004; 568). However, he argues that the possibility of this is tantamount to 'metaphysical dogmatism' in which it is claimed that it is possible to know a realm of ultimate reality which is beyond knowledge, hence ""the aporia that is generated by the realist project"" (Cruickshank, 2004; 581). Therefore, realism fails in its 'hegemonic project' of re-tooling the sciences as its contention to ""provide the definitively correct definition of an ultimate stratum of reality"" must fail, as this is intrinsically impossible (Cruickshank, 2004; 581). However, Cruickshank refuses to dismiss realism altogether, and claims that it should be conceptualised as contributing to the ongoing debate regarding the status of ontology, as ""ontological definitions are fallible and situated in the transitive domain"" (Cruickshank, 2004; 582). Therefore, critical realism should not be conceptualised as an 'infallible' ontology, but rather as one perspective among others that constitute the transitive domain. Cruickshank's conclusions will be criticised as they are predicated upon the way he collapses the entire realist philosophy into a conception of 'ontology', and because it is in the nature of theorising about ontology that there can be no critical discussions based upon 'dialogic immanent critiques'. It was shown in chapter one that Bhaskar does insist that realism is located within the transitive domain, because realism is a human argument and therefore fallible. Bhaskar does not deny that philosophical knowledge (as a human argument) is necessarily historically contingent, essentially transformable, and fallible. With this in mind, realism is free to speculate as to the nature of reality, and in realist terms there are two domains, the intransitive and the transitive. It is from within the transitive domain of the production of knowledge and philosophy that realism has declared that the existence of an intransitive domain is the necessary precondition for scientific enquiry. Bhaskar successfully claims (through his immanent critique of empiricism) that realism is ""uniquely consistent with the historical emergence, practical presuppositions and substantive content of the sciences"" (Bhaskar, 1989; 170). Therefore, the validity of the realist approach is contingent upon ""the belief-worthiness or credibility of science"" and, at this point in time, ""most of us are inclined to tip our hand with science"" yet Bhaskar acknowledges that science itself ""may come to be replaced by some other cognition"" (Bhaskar, 1989; 171). The implications of this for Cruickshank's misgivings about the realist 'ontologies' is that he confuses the realist ontology with its epistemology, and therefore realism is not guilty of committing its own epistemic fallacy. It therefore follows from this that realism is right to declare that its ontology is the correct definition of reality whilst science is the accepted form of cognition. This has implications for Cruickshank's argument that there can be no one true ontology, and therefore realism should only contribute to the critical discussion of ontology in the form of a dialogic immanent critique (as this would facilitate the ongoing critical and constructive dialogue about ontology). It would appear that Cruickshank does not accept the mutual constitution of description and explanation in science; Archer has shown us in her immanent critique of the alternative ontologies available in social science that ""what social reality is held to be also is that which we seek to explain"" (Archer, 1995; 17 original italics). Therefore, it must be asked, if realism has provided a successful immanent critique of the alternative ontologies, what is to be gained from entering into a dialogic immanent critique of those ontologies? Due to the direct link between explanation and description it is hard to see how immanent critiques regarding ontology can be anything but 'monologic', in which they pull apart the theory that they are critiquing because of the inherent internal inconsistencies. Ontologies conceptualise reality in a certain way that necessarily excludes and contradicts alternative ontologies, if this was not so then there could be no consistency between description and explanation, or theoretical underpinnings and social analysis, in short, the tri-partite link between ontology, methodology and practical social analysis would be unsustainable if the ontology lacked the conviction that its conceptualisation of reality was correct. Therefore, as Archer has shown that the alternative ontologies of individualism, collectivism and elisionary theorising are incorrect as they cannot maintain this link, it must be concluded that realism is correct in pursuing its 'hegemonic' project of re-tooling the social sciences with a correct ontology. However, as has been stressed throughout this dissertation, this will not result in the loss of sociology's critical character as sociology will continue to be 'essentially contested' and the correct conceptualisation of social phenomena will be fought over vigorously by sociologists with different political and moral convictions (even if they do accept the realist ontology as correct!) for many years to come. Conclusion: Towards a Realist Sociology? The aim of this dissertation was to describe how realism has offered sociology a logically coherent ontological perspective from which to begin its theoretical and empirical studies. The alternative ontologies traditionally available in sociology have been assessed and dismissed through a process of immanent critique, as each was revealed as unable to uphold the tri-partite like between ontology, methodology, and practical social analysis. Through a discussion of three famous expositions of the concept of power it was shown that sociological concepts would benefit from being reconceptualised in terms compatible with the realist ontology. However, it was also shown how 're-tooling' the social sciences does not represent a threat to the critical character that has generated the rich diversity of concepts, studies and theories. This is because there are relationships between the concepts themselves that cannot be reduced to issues of ontology, whilst sociologists themselves will never be 'programmed' by their ontological understanding and will always have the scope to explore their own moral and political convictions. In light of the convictions of this dissertation it can legitimately be asked, what would a realist sociology look like? If sociology accepted the realist ontology in toto, then it would certainly have to cede any claims to possess predictive powers. Sociology would be a discipline solely concerned with the emergence of the phenomena which generate the substantive problems we have become accustomed to theorising: the consistent prevalence of discrimination, the structurally conditioned disadvantages of minority groups etc. Although sociology is concerned with these issues already, the comprehensive understanding of the interplay between structure and agency provided by the realist ontology would allow sociologists to make firm recommendations for modifications of institutions, or of group formation, which would ultimately contribute to the project of 'emancipation' which is for Bhaskar at the heart of the sociological endeavour. It is also the contention of this dissertation that the production of knowledge about social phenomena (however corrigible and transitive it is) should not be without some purpose other than for its own sake. Sociology no longer occupies a strategic position in relation to governmental social policy, as it has been replaced with focus groups and quasi-academic 'think tanks'. This relegation of sociology to the margins of actual social life is a trend that should be fought vehemently from within the discipline; sociologists must have the conviction that they have a useful contribution to make to the actual constitution of the societies within which they are located. Therefore, the final word on the idea that realism can conceptually re-tool the social sciences, is that it would allow us 'to get our house in order', and consequently provide us with a solid platform from which to develop the contribution to society (and not simply the academe) that it is incumbent upon us to make.",True
33,"AbstractIn this dissertation it is argued that the development of critical realism by Roy Bhaskar is a positive development for sociology. By successfully critiquing empiricism Bhaskar has developed an approach that can be used to highlight the deficiencies that have been associated with the traditional ontologies available in sociology. It will be shown that Margaret Archer effectively launched an immanent critique of the said sociological ontologies by using the realist ontology. It is also argued that she has constructed a worthwhile methodological complement to the realist ontology. The dissertation then attempts to prove these claims through a discussion of three of the most famous expositions of the concept of power. However, it is through these discussions that it is revealed that sociology will not have to sacrifice its critical character by adopting the realist ontology in toto. This is because there will inevitably be disagreement over the 'essentially contested' nature of our familiar concepts. In conclusion, the view will be advanced that the ascendancy of realism within sociology would provide us with a strong foundation from which to develop a body of theories and empirical evidence which would allow us to make a worthwhile contribution to the developments in society, and not just in the academe. IntroductionThe rich diversity of theoretical positions and their concomitant practical investigations within sociology are a reflection not only of the complexity of our subject matter, but also of the ongoing critical discussions between sociologists who have different ontological and conceptual commitments. With this in mind, it can be argued that sociology is inherently partial, and that this inherent partiality is one of the discipline's main strengths due to the breadth and depth of theoretical and methodological perspectives it has generated. However, it has been argued recently that the development of critical realism represents a threat to the continual development of sociological theory (through the process of critical discussion), because ""critical realists are engaged in an hegemonic project which seeks to conceptually re-tool the natural and social sciences"" (Cruickshank, 2004; 567). Cruickshank is arguing that critical realists refuse to contribute to the ongoing critical dialogue about reality, and consequently if the critical realist ontology is accepted as the 'infallible' definition of reality, then sociology must necessarily sacrifice its inherently partial character and the diversity of sociological theories contingent upon it. It will be the dual argument of this dissertation that critical realism has developed a valid ontological position through an immanent critique of the alternative ontologies which have traditionally framed sociological theorising, yet this development in no way threatens the proliferation of the multitude of theoretical ideas and contestations which give sociology its critical character. It takes as its core text Archer's Realist Social Theory: The Morphogenetic Approach (1995), henceforth TMGA, in which an immanent critique of individualism, collectivism, and structuration theory has been developed, and in which the 'morphogenetic methodology' (which seeks to, ""bridge the gap between the realist ontology and practical social theorising"") has been constructed (Archer, 1995; 192). The arguments of this dissertation will be pursued through applying the main contentions asserted in TMGA to three expositions of the concept of power, namely C. Wright Mills' conception of 'The Power Elite', Parsons' 'power as the symbolic medium of the goal attainment subsystem' and Lukes' 'Three-Dimensional view of power'. The decision to specifically assess conceptions of power from the ontologically realist and methodologically morphogenetic positions was not an arbitrary one. It will be the contention of this dissertation that power is part of a corpus of familiar concepts that permeate sociological theorising and are implicitly or explicitly present to a lesser or greater extent in sociological theories of any description; other familiar concepts include 'society', 'values', 'interests' etc. It will be shown how an ontological commitment is implied in the usage of these concepts, and consequently how most sociological theories can be reassessed and ontologically corrected in realist terms. However, it will also be shown that ontological commitment is not the only basis which furnishes critical dialogue. Therefore it will also be argued that the disagreement over the definition and practical application of concepts like power (and the interrelations and interdependencies between the familiar concepts themselves) is not solely determined (although undeniably influenced) by the specific analyst's ontology, and consequently the critical dialogue from which sociology benefits will always be preserved. Chapter one will outline the main ontological and epistemological tenets of the critical realist philosophy as presented by its principle exponent Roy Bhaskar. Archer's immanent critique of individualism, collectivism, and structuration theory (or 'elisionist' theorising) shall then be shown to be successful, and therefore justifying this dissertation's adoption and defence of the realist ontology. Chapter two will describe the morphogenetic methodological framework that informs the subsequent discussion of power. Chapter three will be devoted to an exposition and then criticism of the theories of power proffered by Mills and Parsons from an ontologically realist perspective, whilst an alternative morphogenetic approach to studying the substantive problems identified by Mills and Parsons will be provisionally sketched. Chapter four will assess Lukes' 'Three-Dimensional' (3-D) view of power from the realist perspective, yet will also seek to show how much of the critical dialogue regarding Lukes' view derives from the way he inappropriately subsumes many of the different applications of the concept of 'power' in to his 3-D view, and not from his ontological misconceptualisations. The final chapter will be devoted to a direct critique of Cruickshank's recent article A tale of two ontologies: an immanent critique of critical realism (The Sociological Review (54), 2004). The dissertation will conclude by opposing Cruickshank's claims that there is a logical contradiction at the core of realism because of its insistence on two distinct domains of reality (the intransitive and the transitive) in addition to his insistence that realism should engage in a 'dialogic' immanent critique of ontology. The discussion of Cruickshank's article will draw upon the main arguments presented throughout the dissertation in order to show the validity of the basic realist argument that there are two distinct realms of reality. Therefore, in order to arrive at the point where a discussion of the strengths and weaknesses of the criticisms of critical realism is possible, one must first understand the argument that critical realism is positing, and so it is to chapter one that we presently turn. 1. The Immanent Critique of Science and SociologyIn order to argue that critical realism has provided sociology with a superior ontological perspective of social reality to the individualist and collectivist ontologies, it will be necessary to show that the deficiencies of both of these traditional ontologies stem from their grounding in empiricism, whilst the impulse behind the establishment of the realist philosophy was the necessity for science to break free from the conceptual shackles imposed by empiricism. In 1975 Roy Bhaskar published A Realist Theory of Science (henceforth RTS), and this was the inaugural text in the establishment of transcendental realism (later to be reconceptualised as critical realism). Bhaskar has described his approach in RTS as ""an immanent critique of empiricism"" (Bhaskar, 1989 [1979]; 169). By this he meant that he was focussing on the, ""theoretical, explanatorily significant or 'fundamentally' experimental aspects of science"" as these were perceived to be the strengths of the empirical approach (and this is what is entailed in an immanent critique, criticising another philosophy or perspective based on its own standpoints). Therefore, Bhaskar developed a philosophy of science which attempted to argue against the assertion that all knowledge is derived from sense data, by developing a scientific approach that would reconceptualise the nature of causality, the role of experimental activity, and ultimately the constituent features of reality. Bhaskar argued that whilst laboratory experiments may produce observable regularities (or 'constant conjunctions'), these regularities cannot be taken to be causal laws (Bhaskar, 1975; 16). Bhaskar's justification for this assertion was his differentiation between 'open and closed systems'. 'Closed systems' are not naturally occurring in nature and require human intervention to exist (such as laboratory experiments); in a 'closed system' any intervening causes which may prevent a causal mechanism from having its normal effect are excluded (Bhaskar, 1986; 27). Bhaskar showed that the extrapolation of causal laws from observable regularities is predicated upon viewing reality as a 'closed system'. In contrast to this Bhaskar advanced the view that reality is an 'open system' in which causal mechanisms (like gravity) interact with other causal mechanisms and can interrupt their causal effects, therefore the manifestation of effects at the empirical level is contingent upon the interaction between unobservable entities. This view of reality as an 'open system' implies that reality is itself stratified; Bhaskar describes how ontology is layered into three levels: the real; the actual; and the empirical (Bhaskar, 1975; 19). The real consists of mechanisms that generate phenomena at the level of the actual, which may or may not be observed at the level of the empirical (Bhaskar, 1986; 27). More generally however, Bhaskar's use of the term 'stratification' refers to the simultaneous causal efficacy of the different emergent levels. Bhaskar uses the concept of 'emergence' to show how the different levels 'emerge' from the level below, but once they have emerged they have their own causal autonomy and are therefore irreducible to that from which they emerged (Bhaskar, 1986; 104). This implies a diachronic (as opposed to a synchronic) causal account of how the emergent entity develops from a pre-emergent level of the world (Bhaskar, 1986; 113). Bhaskar extends this seemingly exclusively ontological perspective to his view of epistemology, as he shows how a ""piece of knowledge is emergent from the cognitive structures which generate knowledge by transforming anterior knowledges"" (Bhaskar, 1986; 60). Ontology and epistemology in the realist philosophy of science correspond (roughly) to what Bhaskar terms the 'intransitive' domain and the 'transitive' domain respectively (Bhaskar, 1986; 24-25). Bhaskar claims that that stratum of 'the real' belongs to an 'intransitive' domain comprised of real phenomena that exist independently of our knowledge of them, whilst the hypothetical interpretations we have of these intransitive objects (which are inevitably historically contingent and socially constructed) belong to the 'transitive' domain (Bhaskar, 1986; 51-52). It should be kept in mind that the transitive/intransitive distinction applies both to reality and to our knowledge of reality. Within reality there is a distinction between intransitive features of reality and the transitive production of knowledge, which is a component of reality. Within that part of reality comprising the transitive production of knowledge, one can find the realist philosophy of science (with its distinction between the intransitive dimension and the transitive dimension). So the conceptualisation of 'intransitive' and 'transitive' dimensions is found within the transitive process of knowledge construction (Bhaskar, 1986; 24-25). All this states is that realism ""possesses the virtue of reflexive self-consistency"" in which it acknowledges that realism itself is transitively produced knowledge, and ""may be adequate to only one historically specific mode of science"" (Bhaskar, 1989 [1979]; 170). In summary, Bhaskar developed his realist approach to the philosophy of science because the dominant perspective of empiricism precluded the acknowledgment of the causal mechanisms that existed independently of the observable regularities produced in the artificially closed system environment of the laboratory. Realist philosophy views reality as robustly stratified into the three levels of the real, the actual and the empirical, all of which have their own emergent properties of causal autonomy that makes them irreducible to the stratum from which they emerged, whilst the philosophy also acknowledges two dimensions that are independent of each other, the intransitive and the transitive (within which the realist philosophy, like all human argument, is situated). In The Possibility of Naturalism (1979) Bhaskar argued that ""an essential unity of method between the natural and the social sciences"" was possible, based on the realist philosophy of science developed in RST (Bhaskar, 1989 [1979]; 2). It was here that Bhaskar developed the idea that social structures are emergent properties (as they 'emerge' from the actions of agents but then have causal autonomy and are therefore irreducible to the agents from whose activities they emerged) and that they exist in the 'open system' of society, which means that (when combined with the free will of agents who have the causal power to alter social structures) there are no 'observable regularities' in society (Bhaskar, 1989 [1979]; 45). However, Bhaskar does maintain that there are limits to his conception of the naturalism between the sciences, as he claims social structures (unlike natural structures) are not independent of the activities they govern or the conceptions that agents have of what they are doing, and that social structures are only relatively enduring (Bhaskar, 1989 [1979]; 38). Therefore, through his immanent critique of empiricism Bhaskar has developed an ontology that is applicable to the social sciences. However, it is not sufficient to explicate the realist ontology for social reality and declare it to be appropriate without substantiation. In TMGA Margaret Archer also undertakes an immanent critique in the same manner as Bhaskar did in RST; she critiques individualism and collectivism as she argues they were both deficient because of their adherence to empiricism, whilst structuration theory is criticised for sinking structure and agency together, instead of acknowledging their separability, which is the precondition for ""explaining why things are so and not otherwise"" in society (Archer, 1995; 64). Archer's critique is immanent as she shows exactly how both individualism and collectivism failed to respond to the mutually regulative function of ""the tri-partite link between ontology, methodology, and practical social theorising"" (Archer, 1995; 3). Archer critiques individualism because it starts from the empirically secure position that the ultimate constituents of society are people, yet has to incorporate non-human features into its conception of the individual when its ontology is challenged. Individualism's principle that social structures and phenomena must be reduced to statements about the interrelations of individuals (because the individual is the ultimate constituent part of society) leaves it susceptible to criticism from psychologists who state that people themselves can be reduced further still into their underlying psychological features (Archer, 1995; 39). Due to their belief in reductionism it becomes difficult for individualists to deny the psychologists' claim that society is the reflection of the combined psyches of the population (Archer, 1995; 40). However, to defend their position from psychologists Individualists invoke the realist (and collectivist) notion of emergence. They claim that their concept of the individual is unique because it is defined by the individual's participation in relationships that pre-date them, for example ""English speakers do require other English speakers to become such themselves"" (Archer, 1995; 40). This recognition of an emergent structure (such as the English language) contradicts individualism's premise that everything in society is reducible to an individual's dispositions. The response of individualism to this criticism is to incorporate all non-individual and non-dispositional factors into their concept of the individual, such as a person's 'physical resources and environment' (Watkins, 1971, in Archer, 1995; 39). This again reveals a contradiction, as these aspects of social reality are not about individuals or dispositions (the exclusive focus of individualism) yet are central to the its concept of the individual. Individualism's inability to discredit realism's (and collectivism's) conception of social structure provides further evidence of its inability to uphold 'the tri-partite link'. Individualism cannot successfully prove that social structures are not, ""autonomous from, pre-existent to, and do not exert a causal influence over individuals"" (as both realism and collectivism state) (Archer, 1995; 42). Individualism posits that social structure is not independent from individuals because the social structure is constituted of interpersonal relations (Archer, 1995; 43). However, this does not take into account that social structures condition behaviour, for example in the form of role fulfilment. Individualism refutes the validity of this criticism by denying the pre-existence of social structure; it claims that features such as 'roles' only exist because the individuals concerned lack the desire or the knowledge to change them (Archer, 1995; 44). Archer disproves this claim by discussing the resistance shown by structures (such as the rate of recruitment into the police force) to the concerted efforts of individuals trying to change them; this proves that structures pre-exist and outlive specific individuals and their attempts to alter them by showing that they possess properties independent of the individual's influence (Archer would also add that the process of 'role fulfilment' by actors is structurally conditioned, as opposed to structurally determined, nor is it contingent which actors are eligible for certain roles) (Archer, 1995; 44, 277). Additionally, it can be argued that the dispositions which are so important to individualism are shaped and restricted by their historical context (Gellner, 1971, in Archer, 1995; 42). Consequently, individualism cannot maintain that structures do not have a causal influence (such as a low level of recruitment into the police force being causal of a high crime rate in urban areas). Although collectivism is not susceptible to the criticisms of individualism's methodological approach, it does not offer a significant counter-ontology, and is therefore inferior to the realist approach. Archer shows that the flaws of collectivism stem from its inability to transcend empiricist concepts. Collectivism did not define the exact ontological status of 'societal properties' (as defined by Mendelbaum) (Archer, 1995; 49). This is because a societal property cannot be empirically proven as it does not have a tangible existence which can be directly detected by an individual's senses (Archer, 1995; 49). In addition to this, the empiricist concept of causation (whereby a phenomenon only has causal power if it produces constant conjunctions at an empirical level) denies social structures causal power, because they are neither observable nor causal in their effect (based upon this definition) (Archer, 1995; 52). Realism solves this problem as it recognises that society is an open system in which events are influenced by other contingencies, so the causal influence of emergent properties is not to consistently produce observable effects (Archer, 1995; 53). Therefore, collectivism could only challenge individualism methodologically (by showing that some features of society cannot be reduced to statements about individuals) but could not advance a counter-ontology (because of the dominance of empiricism) stating the existence of non-observable properties (Archer, 1995; 54). Realism, however, does not recognise the validity of the empiricist notion that all things which are held to exist must be detected through human sensory experience. The basic tenet of the realism advanced by Bhaskar and Archer is that society is stratified into individuals and the emergent, non-observable, social structure, and that these two non-reducible strata interact and influence each other (Archer, 1995; 61). After immanently critiquing individualism and collectivism, Archer then turned her attention towards refuting the ontology presented by 'elisionary theorising', represented by Giddens' 'structuration theory'. Although both 'elisionism' and 'emergentism' have rejected the 'terms of the old debate' between individualism and collectivism, they denote entirely different approaches because, ""they are based upon different ontological conceptions, related to disparate methodological injunctions and thus have quite distinct implications for practical social theorising"" (Archer, 1995; 60-61). The key difference between the two approaches turns on their ""ontological parting of the ways"" with regards to the issue as to whether structure and agency are separable or inseparable (Archer, 1995; 63). Elisionism states that social structures can not be examined unless the examination is in conjunction with the actors who 'instantiate' them, as structures only possess a virtual existence until they are 'instantiated' by actors (Archer, 1995; 63). This insistence upon inseparability of structure and agency precludes an examination of their interplay, and it is this examination that Archer stresses is essential to practical social theorising. In eliding structure and agency, Archer contends that elisionists are left with 'the remainders' of the material element of social structures, and the autonomous psychological properties of individuals, as these are distinct emergent strata that cannot be compressed into an 'ontology of praxis' (Archer, 1995; 133). Therefore in TMGA Archer provides an immanent critique of the alternative ontologies that are available in sociology that are all guilty of 'conflationary theorising'. By this Archer means individualism is guilty of 'upwards conflation', in which the social structure is seen to be inert and wholly dependent upon agents for its configuration (thus rendering structure as epiphenomenal), whilst collectivism is guilty of 'downwards conflation' whereby individuals do not possess causal autonomy and are entirely determined by a super-ordinate social structure (in this instance agency becomes the epiphenomenon). On the other hand, elisionism should also be dismissed as it commits 'central conflation' in which structure and agency are collapsed into an indistinguishable whole, which disallows the possibility of examining their interplay. It is hoped that this chapter has outlined how the development of Bhaskar's philosophical realism successfully provided an antidote to the ontological and epistemological misconceptualisations inherent in empiricism, whilst also showing that Bhaskar was correct to attempt to establish a naturalism between the natural and social sciences based upon realism. It has also been argued that Archer has provided an immanent critique of the alternative ontologies available in sociology, and that this critique has provided us with the appropriate focus for our study: namely, examining the interplay between structure and agency. It is therefore incumbent upon Archer to provide us with the methodological tools necessary to undertake such an endeavour, and the next chapter will illuminate how she has done this. 2. MorphogenesisIn TMGA Archer develops a methodology that seeks to examine the interplay between structure and agency over time in order to account for 'why things are so and not otherwise' in society. Her approach represents an elaboration of the realist principles outlined by Bhaskar, as some of his conclusions are challenged or reformulated. This chapter will attempt to outline the main features of the morphogenetic approach through an exposition of its three generic phases (structural conditioning, social interaction, structural elaboration) and its underlying ontology (analytical dualism). The morphogenetic methodology is ""based four-square on analytical dualism"" (Archer, 1995; 167). By this Archer means that 'the people' and 'the parts' of society have to be analytically discrete in order to examine their interplay. Like Bhaskar, Archer utilises a diachronic conception of emergence in order to construct 'analytical histories of emergence'. However, she contests the qualification that Bhaskar makes about the possibility of naturalism, in which he stated that social structures differ from natural structures because they are activity and concept dependent. Bhaskar's formulation of this qualification brings him too close to 'elisionary theorising' as it opens up the space in which society can be conceptualised as ""taking the form it does because of these people here present, and the conceptions they have"" of what they are doing (Archer, 1995; 149). In order to overcome this Archer incorporates a different conception of time when constructing the morphogenetic approach, as she argues that ""it must be identified on whose activities structures are dependent, and when"" (Archer, 1995; 60). Consequently, not only are structure and agency irreducible to one another, they are also temporally distinguishable, and it is the combination of these two conceptions that allows the interplay between them to be examined over time. Therefore, the two basic theorems of the morphogenetic approach are that ""a) structure necessarily pre-dates the action(s) which transform it, and b) that structural elaboration necessarily postdates those actions"" (Archer 1995; 168). Archer argues that there are three types of morphogenetic (or morphostatic) cycles occurring in society simultaneously, and that these cycles intersect in the middle phase (the interaction phase, as interaction is the efficient cause of structural and cultural elaboration); the three types of cycles are structural, cultural, and agential (Archer, 1995; 265). The cultural system is seen as the ideational equivalent of the material set of social structures, whilst social interaction (in which the negotiation of new structural elaboration takes place within the prior structural context) is seen as the parallel of socio-cultural interaction (in which the negotiation of new ideational elaboration takes place within the context of already emergent ideas/theories) (Archer, 1995; 257). It must also be noted, however, that agency itself undergoes new elaboration whilst elaborating new structural and cultural configurations, through the process of 'double morphogenesis'. In order to clarify the above description, each stage of the morphogenetic cycle will be discussed. Structural and cultural conditioning should be conceptualised as matters of mediation, as ""the causal power of social forms is mediated through social agency"" (Bhaskar 1989 [1979], in Archer, 1995; 195). Consequently, agents are the only 'efficient' causes in social life yet material and ideational emergent properties have causal powers. In order to resolve this apparent contradiction Archer shows that it is the creative powers of human beings that allows society to be conceptualised as an open system, which means they are able to ""resist, repudiate, suspend or circumvent structural and cultural tendencies"", therefore the effect of socio-cultural powers is never direct, but mediated (Archer, 1995; 195). However, it is equally the case that the exercise of agential powers can be ""suspended, modified and re-directed by the social forms in which they are developed or deployed"" (Archer, 1995; 196). Therefore, the morphogenetic perspective maintains that all structural influences ""are mediated to people by shaping the situations in which they find themselves"" (Archer, 1995; 196 original italics). The structural and cultural system 'shapes the situations we find ourselves in' in five ways. 'Involuntaristic placement' refers to how ""we are quite literally born into life chances which are defined by prior distributions of material resources"" (Archer, 1995; 202). This is an objective feature of social life which refers to the fact that the world exists before we arrive into it, and we arrive into it in a certain location due to our background; however it also refers to the fact that we are never free from an involuntaristic involvement in structures and their situational conditioning. It follows on from this that different sections of society are endowed with different 'vested interests' because ""vested interests are embedded in all socially constructed positions"" (Archer, 1995; 203 original italics). Consequently there are different 'opportunity costs' involved in not acting in a manner that promotes one's vested interests; however, it is not only action that is conditioned but also 'the degrees of interpretative freedom' whereby the interpretation of a situation by an agent is conditioned by the placement of prices and premiums upon certain interpretations of it (Archer, 1995; 208-209). Finally, 'directional guidance' for a strategic course of action in which the promotion of vested interests is conditioned by the configuration of 'second order' structural and cultural emergent properties (systemic properties that are the 'results of the results' of social interaction), whereby certain courses of action can be pursued (or prevented) if social structures (or ideas) are in 'compatible or incongruent' relationships to one another respectively (Archer, 1995; 215). The middle phase of the morphogenetic cycle refers to the process of social interaction in which the new structural formations are elaborated but also in which agency itself is elaborated. Archer's conception of the elaboration of agency is derived from her stratified model of people. People are stratified into ""persons, agents, and actors"" (Archer, 1995; 254). Each stratum is emergent from the one immediately below it. A concept of 'personal identity' is necessary if an individual is to develop a concept of their 'social identity'; this is because human beings have to have a sense of self in order to identify the interests of a group with their own interests, and this is possible through the human being's capacity to retain a sense of themselves as being the same person who is passing through time (i.e. the capacity to narrate their own history and future) (Archer, 1995; 282). This personal identity then ""fathers agency"", as agents are ""the agents of the socio-cultural system into which they are born""; however, this involves a distinction between 'corporate agents' and 'primary agents' (Archer, 1995; 255). 'Primary agents' do not express interests or organise for their strategic pursuit, yet everyone can be considered an agent as ""the similarities of response from those similarly placed can generate powerful, though unintended, aggregate effects"" (Archer, 1995; 259). 'Corporate agents' on the other hand are those who have articulated their interests and have organised in order to attain them, therefore they are the ones who are involved in a concerted effort to re-shape the structural or cultural feature that affects their interests (Archer, 1995; 258). In turn, the kind of actors we can become is largely influenced by the kind of agents we are involuntarily placed as, this is because ""certain opportunities and information are open to the privileged and closed to the non-privileged"" (Archer, 1995; 277). The final phase of an 'analytical history of emergence' concerns the form that new structural elaboration takes, and whether or not it is new elaboration (morphogenesis) or reproduction (morphostasis). The way that new social elaboration occurs is ""through exchange transactions and power relations"" (Archer, 1995; 296). Archer employs a quasi-weberian understanding of social stratification in her discussion of social change, as all processes of exchange and power necessarily ""involve the use of resources, namely political sanctions, liquid assets and expertise"" (Archer, 1995; 297). The 'raw bargaining power' of primary agents depends upon their access to resources as conditioned by their involuntaristic placement in the social structure; essentially those with a higher access to the resources outlined above will have more bargaining power (Archer, 1995; 300). However, Archer emphasises that the crucial relationship is between the distribution of interest groups and the availability of the resources; if the concentration of resources is high then there will be fewer groups in society who have access to them and therefore less groups who are in a strong bargaining position (meaning a ""steeper gradient between elites and masses"") (Archer, 1995; 298). Raw bargaining power alone is insufficient to describe why some corporate agents successfully pursue their vested interests whilst others are impeded. The concept of bargaining power has to be supplemented with the relational notion of 'negotiating strength' (Archer, 1995; 300). Negotiating strength refers to the relationships between and amongst corporate agents in situations of power and exchange; a corporate agent has more negotiating strength relative to the corporate agent it is dealing with if it supplies a resource that the other cannot either, ""a) reciprocate, b) acquire from an alternative source, c) coerce the other party into supplying, and d) cannot reconcile itself to dispensing with"" (Archer, 1995; 302). Therefore the effect of bargaining power is to define what resources a corporate agent can bring to bear in the struggle to promote vested interests when confronted by the (second order) constraints and enablements arising from cultural emergent properties and structural emergent properties (Archer, 1995; 302). Ultimately societal morphogenesis or morphostasis is the result of the interaction between third order emergent properties (the results of the results of the results of social interaction) (Archer, 1995; 327). However, this is not to imply that third order systemic properties are hypostatised entities; Archer insists that the cultural realm and structural domain are distinct from one another and the influence of one or the other on societal morphogenesis or morphostasis depends upon the relationship between the four different configurations possible if culture and structure can be perceived to interact in a state of either internal morphogenesis or morphostasis (Archer, 1995; 323). Archer maintains throughout that the linkages between the structural and cultural realms are situated in the process of interaction, therefore the elaboration proceeds in terms of exchange and power between agents (which protects against the criticism that structures are reified entities in the morphogenetic approach). It therefore follows from the evidence presented here that the morphogenetic approach successfully provides the methodological accompaniment to Bhaskar's critical realist ontology as it utilises the concepts of stratification and emergence in order to chart 'analytical histories of emergence' of particular social configurations. This means that the speculation as to the ""tendential powers of generative mechanisms"" is complemented by ""an historical analysis of the concrete contingencies which intervened to produce particular outcomes"" (Archer, 1995; 327). In the subsequent chapters the morphogenetic approach will be used to critique three famous sociological conceptions of power in order to illustrate the utility of both the realist ontology and the morphogenetic methodology. 3. The Mills Vs Parsons Debate Reconsidered The famous debate between C Wright Mills and Talcott Parsons regarding the 'correct' conceptualisation of power provides the quintessential example of the benefits to sociology of ongoing critical discussion. However, it shall presently be argued that the deficiencies of their approaches can be traced back to their ontological underpinnings. Both views will be explicated and then subjected to a realist critique, whilst the morphogenetic approach will be used to show that the substantive problems identified by Mills and Parsons are worthy of investigation. In The Power Elite (1956) Mills developed the notion that in the first fifty years of the 20 th Century American society underwent a dramatic structural change, which resulted in the concentration of power in the hands of a minority whom he called 'the power elite'. Mills draws the historical comparison between the richest and most politically influential members of US society (and the economy and polity within which they operated) from different eras of US history, in order to show how the structures of power had become ""centralised and enlarged and interlocking in the present era [1950s USA]"" (Mills, 1956; 4). Mills goes on to describe how these enlarged and centralised structures of the economy, the political directorate, and the military have come to penetrate all the other institutions of US society, and consequently influencing the lives of all members of 'the mass population' (those who are not members of the elite) (Mills, 1956; 275). His view of a power elite stems from his belief that these enlarged structures of 'the big three' have come to form an interlocking whole, and that the members of each of the elites are interchangeable; this has fostered the development of one 'power elite': a self-conscious social class whose interests are intertwined with those of the other members of the elite (Mills, 1956; 210). The elite reproduces itself through a process of recruitment that begins at 'the right' schools (Mills gives the examples of the appropriate private schools for elite membership), and carries on through Ivy league colleges and results in the eventual careers of men destined for elite membership, in which they characteristically hold positions in more than one of the structures that comprise the interlocking elite (Mills, 1956; 188). The members of the elite tend to have a common social background as this is crucial to the ""easy intermingling"" which characterises the interaction between members of the elite, and it is in these small scale social milieux that the common psychology of the elite is passed on to new members who learn the art of impersonal decision making and the intimate sensibilities of elite membership (Mills, 1956; 178). It was in response to these claims by Mills that Parsons began to reveal his own concept of power, which marked a break from his conceptualisation of it in his early works (such as The Structure of Social Action in which he accepted the 'traditional' view of power). In his essay 'The Distribution of Power in American Society' (published originally in 1960) Parsons develops a critique of Mills' The Power Elite, in which Mills is criticised for viewing power as a ""zero-sum concept"" whereby there is a fixed 'amount' of power to be 'held' by an individual or a group, which necessarily means that one group holds power to the extent that another group (""the outs"") does not (Parsons, 1968; 219). Parsons declares that conceptualising power in this way confines Mills to exclusively discussing instances where one individual or group ""has power over other"" (Parsons, 1968; 219 original italics). Parsons on the other hand, wanted to argue that the concept of power should refer to instances where actors had the ""power to make authoritative decisions"" (Parsons, 1968; 220 my italics). It is useful to quote Parsons at length here: Power, then, is generalised capacity to secure the performance of binding obligations by units in a system of collective organisation when the obligations are legitimised with reference to their bearing on collective goals and where in case of recalcitrance there is an assumption of enforcement by negative situational sanctions - whatever the agency of that enforcement. (Parsons, 1969; 361). In this oft quoted passage Parsons shows how he has situated power within his wider conceptual framework termed 'normative functionalism' and tied it to the conceptions of generalisation and legitimation. By 'generalisation' it is meant that he conceptualised power as a symbolic medium through which authoritative decisions made by those who occupy position of power are implemented in order to further collective goals (Parsons, 1968; 362). Decisions are deemed 'legitimate' because they are 'authoritative'; for Parsons, power resides in positions within the political system that have legitimate authority (Parsons, 1968; 372). By this he meant that power is the facility for the achievement of collective goals because the members of a society 'agree' to legitimise leadership positions; they do this by giving such positions a mandate to develop and implement policies to further the goals of the system (or society) as a whole (Parsons, 1968; 373). Therefore, power is directly derivative of authority (as this is the institutionalised legitimation which underlines power). Consequently, for Parsons what is important is not the amount of power 'possessed' by individuals, but the amount of power 'generated' by the system which would ultimately be beneficial to all, as the more power that leaders have at their disposal, the greater their mandate for implementing changes that would further society's collective goals; of course for Parsons society's collective goals depended upon the common value system, and in America he conceptualised this as 'instrumental activism', in which the main goal was the furtherance of society's economic productivity (Giddens, 1992 [1968]; 179). In spite of the manifest differences of these two conceptions of power, both contain features that are consistent with the realist perspective and the morphogenetic methodology. Mills' use of an historical approach to assessing how a group substantively changes over time reveals he has an understanding of the diachronic nature of emergence, and that he resists falling into the trap of nomenclature, in which groups who have the same name (such as 'the working class') are considered to be qualitatively the same (despite the clear differences between 'the working class' of the early 18 th Century and the 21 st Century) (Archer, 1995; 74). Parsons, on the other hand, did acknowledge the existential intransitivity of social structures (as the political system of Parsons' conception existed independently of the human knowledge of the system). However, these consistencies with realism are only partial, and it is the argument of this chapter that the deficiencies of these two conceptions of power are essentially ontological in nature. Neither Mills nor Parsons adopted the ontology of 'analytical dualism' (in which the 'people' and the 'parts' of society are conceptualised as distinct strata, which allows their interplay over time to be examined). For Mills, this means that he essentially denies causal efficacy to the social structures of the three domains of the power elite; as Parsons insightfully points out, ""the power elite is free from the 'checks and balances' associated with our political system.... and has been freed from the historic restraints of our society"" (Parsons, 1969; 205 my italics). Although Mills contends that the ability of the elite to wield power is dependent upon their ""occupation of the command posts of society"" once they are the incumbents of these posts the elite is free to act as they choose (Mills, 1956; 3); consequently, Mills offers no discussion of how the structural and cultural context conditions their actions, whilst their actions again contribute to the elaboration of new structures. This points to the key criticism of Mills, and why the similarity between his conception and that of realism outlined above remains only partial. Mills' conception of the diachronic emergence of the power elite ends with the emergence of the power elite. Again, Parsons offers an insightful contribution as to why this is; he describes how Mills has a prior commitment to a ""utopian conception of an ideal society in which power plays no part at all"" and a fundamental mistrust of capitalism which means for him ""the power elite is bringing the capitalist evil to a climax"" (Parsons, 1969; 224 my italics). Throughout The Power Elite there is a tone of finality to Mills' discussion of the development of American society, and the only consideration of what new structural elaboration and social configurations may arise are implicit in the continued consolidation of the power elite, meaning that there will be no new elaboration (and this is consistent with Parsons appraisal with Mills' value commitments). For the realist, the actual form society takes is an unintended consequence because it is the combined product of different outcomes pursued simultaneously by different groups (Archer, 1995; 91). The implication of this for Mills is that he over-emphasises the ability of one group in society to shape society in its entirety, as he argues ""elites have the unique ability to smash one structure and set up another in which they enact quite different roles"" (Mills, 1956; 24-25). Therefore, Mills' position is untenable from the realist perspective as he makes no attempt to link up cycles of development with posterior developments (although he does discuss the anterior social context) and he denies the causal efficacy of all 'primary' agents, and any 'corporate' agents who are not members of the elite, in addition to denying the relative autonomy and causal efficacy of social structures. Many of the criticisms that can be made of Parsons' conception of power are consistent with the criticisms of collectivism outlined earlier, in which agency is reduced to the status of the epiphenomenal. Parsons is guilty of ""making present-tense distinctions between roles and incumbents"" which reveals how he does not conceptualise the 'parts' and the 'people' of society as analytically separable; this is so because analysing their interplay over time is only possible if they are separate entities, and Parsons makes no allowance for the fact that many roles (such as those in the political system) pre-condition who is eligible to fulfil them (Archer, 1995; 71). The Parsonian conception of power precludes the notion that different groups of agents have different vested interests to pursue because of their involuntaristic placement within the social system (or of the vested interests they acquire with roles that they come to fulfil) because it refers only to the 'societal goal', and this is predicated upon viewing society as essentially stable. Therefore Parsons makes no allowance for the way that social reality is stratified and that agents and structure interact over time and are both reformulated through this interaction; what Parsons presents is an essentially static view of societal development. Although the purpose of power is presented as dynamic (as it allows the furtherance of collective goals) Parsons' conception ultimately precludes the possibility of new social and structural elaboration as agents are denied the causal power to elaborate new social forms (societal development proceeds harmoniously under the direction of cultural values, yet Parsons offers no explanation as to why these cultural values develop). Giddens captures this deficiency in Parsons' conception when he describes how Parsons offers no explanation of how ""legitimacy is achieved, and therefore consensus is assumed, despite the historical evidence that power can grow out of the barrel of a gun"" (Giddens, 1992 [1968]; 187 original italics). Therefore a similar criticism can be made of Parsons' conception of power as can be made of Mills', namely that neither successfully incorporates a conception of the causal interdependency of structure and agency as revealed by examining their interplay over time. However, this realist critique of the ontological foundations of these two conceptions of power can (and should) be supplemented by a discussion of the criticisms launched from within the ongoing debate about the concept of 'power' in the social sciences itself; unfortunately for the purposes of brevity these shall not be covered in the present work (although the reader is referred to Giddens (1968) for a discussion of Parsons, whilst Parsons' own discussion of Mills (1960) is still as insightful today as ever). All that remains for this chapter to do is sketch a possible approach to the substantive problems raised by Mills and Parsons from the morphogenetic approach (as this is the ultimate function of a methodology, to provide a framework with which to analyse substantive sociological issues). The substantive problems identified by Mills was that a monopoly was developing between the most prominent figures within the economic, political and military domains over the access to decisions which had consequences on a national scale. The morphogenetic approach describes this kind of situation as one in which both the cultural and structural domains are in a state of morphostasis; therefore a sociologist who is using analytical histories of emergence would seek to empirically reveal the contingencies that contributed to the lack of articulation of an alternative set of ideas, or of an articulated interest group within a society. They would therefore have to chart the progression from the initial social context confronting the agents who have become dominant, through the process of interaction that grouped the agents into the groups that became dominant and elaborated the structure that reinforced their dominance. The substantive area implied by Parsons' conception would be how a particular political structure came to embody the legitimate authority that is representative of the ideal workings of western democracy. However, an analytical history of emergence of these structures would discuss how individuals as active social agents come to develop their own vested interests in the positions of the political system, and would therefore have to discuss whose activities the particular form of structure were dependent upon, and when. Therefore the results would most likely yield conclusions that are incongruent with Parsons' implication that the political structure is a direct reflection of society's underlying collective goals, as the particular form a structure takes is the result of interaction between competing interest groups. The above critique was intended to show how two expositions of one of the most familiar concepts within the social sciences were susceptible to criticism based upon their misconceptualisations of ontology. If the matter was left here then one could appreciate Cruickshank's concern that sociology is in danger of losing its critical nature, as the only clarification of concepts would be to align them with the principles of realism. However, the matter does not end here, and it shall presently be argued that sociology is, and always shall be, a necessarily critical endeavour. 4. The 'Essentially Contested' Status of Power In order to argue that the critical discussions inherent in sociology are not threatened by the ascendancy of the critical realist ontology one must show that these discussions are not entirely derivative of ontological differences. It is the argument of this chapter that the familiar concepts already existing in social science can be ontologically corrected, yet critical discussions will also take the form of contesting the parameters of such far reaching concepts, and debates will continue over which phenomena should be conceptualised in these terms. This argument will take the form of an exposition of Lukes' 3-D view of power, a critique of his concept from the realist perspective, and then finally a consideration of the other kinds of critical discussion that his concept has generated. In his short book Power: A Radical View (1974) Lukes developed his 3-D concept of power. Essentially, the 3-D view of power is concerned with establishing that one agent, A, (whether they are an individual or a collectivity) affects another agent, B, in a manner contrary to B's interests (Lukes, 1974; 24). Politics is crucial for Lukes' argument as he believes A doesn't exercise power over B simply through having the greater ability to effect their own will in a conflict situation, but by being able to shape B's very wants, presumably through ideological distortion (Lukes, 1974; 23). Consequently, the political sphere is of central importance to this abstract conception as it is the key site in the decision making process of society, which means agents who can wield political power will have the greater ability to disseminate an ideology than an agent in any other sphere. In light of this, political power (or power in the political sphere) is the only form of power discussed throughout Lukes' work. It is fundamental to Lukes' conception that one agent possesses power and has 'power over' another agent who does not posses power (Lukes, 1974; 31). This shows that he gives primacy to the agential potential of agents, as instances where power is exercised are the result of choices made by A (the possessor of power) which are in their own interests and which facilitate B acting in a manner that is contrary to B's own interests. This notion of choice is crucial to Lukes' perception, as he believes instances where power is exercised are derived from an agents's decision and not because the instances are structurally determined; he elaborates this by arguing that the instances where power is exercised can only be identified (and therefore differentiated from instances of structural determinism) when ""it is in the exerciser's or exercisers' power to act differently"" (Lukes, 1974; 55, original italics). Therefore, this shows that Lukes argues power rests solely in agency because power itself is a commodity that can be exercised (or not exercised) in a manner that is dependent upon the exerciser's or exercisers' will. For Lukes power should be understood as the latent conflict of interests between the interests of those who exercise power and the ""real interests of those they exclude"" (Lukes, 1974; 25 original italics). Consequently, instances where power is exercised are constructed by the discrepancy between the interests of A and the 'real interests' of B, meaning that the relation is defined by A's concealing of B's 'real interests' (Lukes, 1974; 25). Therefore, B is invariably disadvantaged whenever power is exercised, even if B is unaware of it. The concept of 'intentionality' is also central to the realist position; in fact Bhaskar went as far as to describe it as the ""sine qua non"" of the potential problem of naturalism (Bhaskar, 1989; 173). However, in realist terms Lukes emphasises the agential character of instances where power is exercised at the expense of a detailed understanding of the influence of structure. It is insufficient merely to state that 'the political sphere' is the key decision making site for society, and is therefore central to the exercise of power. This in no way accounts for the processes in which different agents come to occupy the positions within the political sphere, and consequently what is lost in Lukes' conception is how A becomes A in the first place; or why the political sphere has developed in such a way that it has come to embody the 'real interests' of those agents who have become A. Additionally, realism takes issue with Lukes' conceptualisation of agents as agents of the cultural system. Lukes gives no account of the processes of interaction that generate a situation in which 'ideological distortion' is possible. Lukes' conception of B denotes only one configuration of the cultural system, and that is morphostasis where there is no articulated alternative to the ideology of the cultural elite, whose vested interests are represented by the cultural system. Therefore, Lukes' conception is open to ontological revision as it focuses exclusively on the influence of agency in instances where power is exercised, and cannot give any description of the way that structure and agency interact to condition who the interest groups represented by A and  B are, or why the political system is structured to reflect the interests of A, or even if agents who comprise A actually acquire their interests due to their embodiment of the roles within the political sphere. Although these realist criticisms are pertinent, it would be misleading to contend that ontological grounds are the only grounds upon which a concept can be contested. In the second edition of Power: A Radical View (2005) Lukes admits that many of the valid criticisms of his conception of power stem from the way that he subsumes all of the possible applications of the concept of 'power' into his totalising 3-D view (Lukes, 2005; 109). He acknowledged that power should be conceptualised as a dispositional concept (which means it may or may not be exercised); he takes on board the valid assumptions of Parsons' conception, in which 'power over' (or as Parsons' would describe it, 'power to') others may be productive for all and authoritative, and actually compatible with dignity (Lukes, 2005; 109). Consequently, Lukes is led to redefine his conception with a higher degree of analytical specificity; instead of using the generic term 'power' he argues that the 3-D view should be defined as ""securing the consent to domination of willing subjects"" (Lukes, 2005; 108). Therefore, in the time between the publications of Power: A Radical View 1 st edition and 2 nd edition there emerged an extensive body of critical discussions regarding the correct definition of Lukes' concept of power, which thus aided in the development of other conceptions of power as the different facets of social reality that it could be applied to were delineated. These contributions to the critical dialogue of power were not derived from the ontological misconceptualisations outlined above. The critical dialogue of Lukes' 3-D concept of power essentially turned on the issue that drives much of the critical dialogue in sociology: the explanatory purchase of a concept. It has been shown above that this is both an ontological matter, but also a conceptual one. Bhaskar has shown that there are ""rational criteria for judging some theories and concepts as better and more explanatory than others"" by which he means that if one concept can explain more on its terms than another can on its own terms then the first concept has greater explanatory power and is therefore superior (Bhaskar, 1986; 43). Consequently, if the concepts of sociology were subjected to a realist critique and were then re-stated in realist terms, there would still be critical dialogue that was beyond mere definitional squabbles; the very explanatory power of the concepts we employ is at stake, and given the nature of our subject matter these debates cannot help but be ongoing, as social reality is unpredictable and ever changing. The discussion of power concludes with a statement that reaffirms that the critical dialogue of power is not merely an ontological issue: power is (as Lukes describes) an 'essentially contested concept' (Lukes, 1974; 20). By this he means that a conception of power is necessarily tied to further background assumptions that are not only ontological and epistemological but also moral and political. A concept of power necessarily relates to contested and political features of social life as it invariably involves the disadvantage (in one form or another) of an individual or group. As Giddens points out, ""power has a necessarily hierarchical character"" (Giddens, 1992 [1968]; 184). The final word on power of this dissertation is to boldly state that the critical discussions of the concept of power cannot be entirely reduced to matters of ontology as a concept of power is always related to a concept of interests. The 'interests' of the agents involved are either explicitly declared or implied in any concept of power advanced in sociology to date. It is the contention of this dissertation that discussing 'interests' necessarily precludes the reduction of discussions of power to solely ontological issues as there is an element of moral judgment in determining how 'interests' are to be calculated or conceptualised. 5. The 'Essentially Contested' Status of Sociology This dissertation concludes with a critique of the main points of the recent article 'A Tale of Two Ontologies: an Immanent Critique of Critical Realism' by Justin Cruickshank. It will be argued that Cruickshank depicts sociology as in danger of losing its critical character if the 'hegemonic' project of critical realism to conceptually re-tool the sciences is successful, and that ontological debate should continue in terms of 'critical discussion' because there can never be a 'one true' definition of social reality. It will also be shown how Cruickshank has argued that Realism has developed two mutually-exclusive ontologies in order to dodge the 'epistemic fallacy'. The above assertions by Cruickshank will be criticised through a defence of the realist ontology and in terms of the certainty of the continuity of critical dialogue within sociology. Cruickshank's main reservation with realism is the perceived ""tension concerning the definition of ontology"" it employs (Cruickshank, 2004; 567). He states that the realist ontology refers to both the transitive and intransitive domains. However, in order to avoid the epistemic fallacy (in which statements about ontology (reality) are transposed into questions about epistemology (knowing reality)) one would ""have to step outside of transitivity....... and assume a God's eye view in order to know the essential features of reality beyond our knowledge"" (Cruickshank, 2004; 568). However, he argues that the possibility of this is tantamount to 'metaphysical dogmatism' in which it is claimed that it is possible to know a realm of ultimate reality which is beyond knowledge, hence ""the aporia that is generated by the realist project"" (Cruickshank, 2004; 581). Therefore, realism fails in its 'hegemonic project' of re-tooling the sciences as its contention to ""provide the definitively correct definition of an ultimate stratum of reality"" must fail, as this is intrinsically impossible (Cruickshank, 2004; 581). However, Cruickshank refuses to dismiss realism altogether, and claims that it should be conceptualised as contributing to the ongoing debate regarding the status of ontology, as ""ontological definitions are fallible and situated in the transitive domain"" (Cruickshank, 2004; 582). Therefore, critical realism should not be conceptualised as an 'infallible' ontology, but rather as one perspective among others that constitute the transitive domain. Cruickshank's conclusions will be criticised as they are predicated upon the way he collapses the entire realist philosophy into a conception of 'ontology', and because it is in the nature of theorising about ontology that there can be no critical discussions based upon 'dialogic immanent critiques'. It was shown in chapter one that Bhaskar does insist that realism is located within the transitive domain, because realism is a human argument and therefore fallible. Bhaskar does not deny that philosophical knowledge (as a human argument) is necessarily historically contingent, essentially transformable, and fallible. With this in mind, realism is free to speculate as to the nature of reality, and in realist terms there are two domains, the intransitive and the transitive. It is from within the transitive domain of the production of knowledge and philosophy that realism has declared that the existence of an intransitive domain is the necessary precondition for scientific enquiry. Bhaskar successfully claims (through his immanent critique of empiricism) that realism is ""uniquely consistent with the historical emergence, practical presuppositions and substantive content of the sciences"" (Bhaskar, 1989; 170). Therefore, the validity of the realist approach is contingent upon ""the belief-worthiness or credibility of science"" and, at this point in time, ""most of us are inclined to tip our hand with science"" yet Bhaskar acknowledges that science itself ""may come to be replaced by some other cognition"" (Bhaskar, 1989; 171). The implications of this for Cruickshank's misgivings about the realist 'ontologies' is that he confuses the realist ontology with its epistemology, and therefore realism is not guilty of committing its own epistemic fallacy. It therefore follows from this that realism is right to declare that its ontology is the correct definition of reality whilst science is the accepted form of cognition. This has implications for Cruickshank's argument that there can be no one true ontology, and therefore realism should only contribute to the critical discussion of ontology in the form of a dialogic immanent critique (as this would facilitate the ongoing critical and constructive dialogue about ontology). It would appear that Cruickshank does not accept the mutual constitution of description and explanation in science; Archer has shown us in her immanent critique of the alternative ontologies available in social science that ""what social reality is held to be also is that which we seek to explain"" (Archer, 1995; 17 original italics). Therefore, it must be asked, if realism has provided a successful immanent critique of the alternative ontologies, what is to be gained from entering into a dialogic immanent critique of those ontologies? Due to the direct link between explanation and description it is hard to see how immanent critiques regarding ontology can be anything but 'monologic', in which they pull apart the theory that they are critiquing because of the inherent internal inconsistencies. Ontologies conceptualise reality in a certain way that necessarily excludes and contradicts alternative ontologies, if this was not so then there could be no consistency between description and explanation, or theoretical underpinnings and social analysis, in short, the tri-partite link between ontology, methodology and practical social analysis would be unsustainable if the ontology lacked the conviction that its conceptualisation of reality was correct. Therefore, as Archer has shown that the alternative ontologies of individualism, collectivism and elisionary theorising are incorrect as they cannot maintain this link, it must be concluded that realism is correct in pursuing its 'hegemonic' project of re-tooling the social sciences with a correct ontology. However, as has been stressed throughout this dissertation, this will not result in the loss of sociology's critical character as sociology will continue to be 'essentially contested' and the correct conceptualisation of social phenomena will be fought over vigorously by sociologists with different political and moral convictions (even if they do accept the realist ontology as correct!) for many years to come. Conclusion: Towards a Realist Sociology? The aim of this dissertation was to describe how realism has offered sociology a logically coherent ontological perspective from which to begin its theoretical and empirical studies. The alternative ontologies traditionally available in sociology have been assessed and dismissed through a process of immanent critique, as each was revealed as unable to uphold the tri-partite like between ontology, methodology, and practical social analysis. Through a discussion of three famous expositions of the concept of power it was shown that sociological concepts would benefit from being reconceptualised in terms compatible with the realist ontology. However, it was also shown how 're-tooling' the social sciences does not represent a threat to the critical character that has generated the rich diversity of concepts, studies and theories. This is because there are relationships between the concepts themselves that cannot be reduced to issues of ontology, whilst sociologists themselves will never be 'programmed' by their ontological understanding and will always have the scope to explore their own moral and political convictions. In light of the convictions of this dissertation it can legitimately be asked, what would a realist sociology look like? If sociology accepted the realist ontology in toto, then it would certainly have to cede any claims to possess predictive powers. Sociology would be a discipline solely concerned with the emergence of the phenomena which generate the substantive problems we have become accustomed to theorising: the consistent prevalence of discrimination, the structurally conditioned disadvantages of minority groups etc. Although sociology is concerned with these issues already, the comprehensive understanding of the interplay between structure and agency provided by the realist ontology would allow sociologists to make firm recommendations for modifications of institutions, or of group formation, which would ultimately contribute to the project of 'emancipation' which is for Bhaskar at the heart of the sociological endeavour. It is also the contention of this dissertation that the production of knowledge about social phenomena (however corrigible and transitive it is) should not be without some purpose other than for its own sake. Sociology no longer occupies a strategic position in relation to governmental social policy, as it has been replaced with focus groups and quasi-academic 'think tanks'. This relegation of sociology to the margins of actual social life is a trend that should be fought vehemently from within the discipline; sociologists must have the conviction that they have a useful contribution to make to the actual constitution of the societies within which they are located. Therefore, the final word on the idea that realism can conceptually re-tool the social sciences, is that it would allow us 'to get our house in order', and consequently provide us with a solid platform from which to develop the contribution to society (and not simply the academe) that it is incumbent upon us to make.","Sporting practices and their associated subcultures have to be conceptualised as possessing a unique nature that allows them to function independently of the societal context within which they exist, if it is to be argued that they bear no relation whatsoever to the social construction of masculine ideals. This is because masculine ideals are created and shaped by the society in which they are located, yet they also affect the natures of the institutions and cultural forces that influence them. This essay is concerned with tracing the historical construction of sport, and with assessing the relationship (if any) in contemporary society between sporting practices, masculine ideals, and the dominant cultural and institutional forces of modern society in order to evaluate whether or not sporting practices are related to the social construction of masculine ideals. This essay will argue that sporting practices are intimately bound up with the social construction of masculine ideals, and that the relationship between sport and society provides crucial insights into the other societal forces that contribute to the forms these ideals take. A central concept that will be employed throughout will be the concept of hegemonic masculinity. Masculine ideals as they exist in contemporary society will be conceptualised throughout the essay as forming the analytical tool that is hegemonic masculinity. Hegemonic masculinity can be described as occupying the dominant position in the gender order of society; this is very different to a conventional patriarchal view of society that argues all men are dominant over all women. Rather, the concept of hegemonic masculinity and the gender order proposes that the dominant force in gender relations is a particular kind of masculinity that has developed cultural primacy; consequently, all alternative masculinities and all femininities are subordinated and become involved in a dynamic relationship in which the power of the hegemonic masculinity has to adapt and evolve in order to retain its dominance. Therefore, this essay will attempt to deconstruct the concept of hegemonic masculinity into some of its constituent masculine ideals through discussing the role of sporting practices in ensuring the dominance of males over females; reinforcing the normality of heterosexuality; marginalising black males; and in emphasising the importance of physical prowess in order to show how sporting practices contribute to the stability of hegemonic masculinity itself. The essay begins, however, with an evaluation of perspectives that argue the development of institutionalised sporting practices was unaffected by issues of masculinity. The development of sporting practices in the 19 th Century has been conceptualised as reflecting the wider social transformations that have come to characterise modern society. It has been argued that modern sport shares the characteristics of modern society; in Weberian terms, sport is bureaucratised, secular, structured by rules and regulations, and can therefore be described as rational; in Marxists terms, sport is capitalist, specialised, quantified, defined by a distinction between work and leisure, and is consequently seen as bourgeois (Eitzen, cited in Crossett in Messner and Sabo, 1990; 47). Alternatively, the opposite has been argued. Although sport appears rationalised, it is essentially the institutionalisation of play, and is consequently irrational and purposeless, which means it has developed as an alternative to the forces of modernity (even if the institutional form that it takes is modern). Guttman has described sport as having its ""roots in the dark soil of our instinctive lives"" but that the form it takes is ""dictated by modern society"" (Guttman, cited in Crossett in Messner and Sabo, 1990; 47). Throughout these descriptions a connection between masculinity and the development of institutionalised sport has been ignored. The justification for the exclusion of issues of masculine ideals is derived from the belief that the 19 th Century concept of 'manliness' (which can be interpreted as embodying the masculine ideals of the period, and which was of central importance to the participants of sporting practices) was a 'confused moral concept' that embraced such contradictions as aggression and ruthlessness alongside compassion for the defeated; consequently manliness has been viewed as a front for nationalism and class cohesion (Mangan cited in Crossett in Messner and Dabo; 1990; 45). However, this perspective cannot explain how international competitions expanded upon already existing sporting institutions, and that class segregation was less prominent when sport was initially institutionalised (Crossett in Messner and Sabo, 1990; 47). Similarly, the perspective that sport is essentially irrational cannot explain how it has developed into a major social institution in an era that has witnessed increasing rationality (47). Therefore, although the Weberian and Marxists characterisations of sporting institutions aren't altogether inaccurate they need to be expanded to accommodate the influence of a gender ideology (of which 'manliness' formed a crucial part) upon the development of institutions in modern society (47). Todd Crossett endeavoured to show how the English public schools were a pivotal institution in disseminating the gender ideology, and his evidence for this shall now be discussed. Crossett convincingly described the mutually reinforcing relationship between sport in public schools and the emerging gender ideology of the 19 th Century. Crossett argues that the economic developments of the early 19 th Century resulted in a shift in power away from the aristocracy in favour of the bourgeoisie, and that the spread of liberal political theories which legitimised the ascendance of the bourgeoisie also increased the power of (bourgeois) women (Crossett in Messner and Sabo, 1990; 49). However, by the mid to late 19 th Century men began to displace women from the positions of power they had acquired (for example the male medical profession appropriated healthcare and midwifery and institutionally excluded women from positions of power), and they were able to do this because of the development of an ideology about the 'nature' and 'roles' of men and women (50). Pseudo-scientific justifications for the dominance of men over women became accepted as the truth, as ""almost without fail, experts related a woman's fragility and inferiority to her genital organs"" (50). Crossett argues that the development of sport in public schools socialised men into accepting this ideology and defining themselves as biologically superior to women (51). He describes how athleticism was placed in the foreground and deemed necessary for the development of manly, muscular students (52). The importance of athleticism was intimately connected with the popular view of the effect of sex upon the male body; sex and masturbation were perceived to make men suffer from exhaustion (as manly energy was believed to reside in sperm, and men were seen to possess a finite amount) and athletics helped to regenerate the body (52). Those men who were ""not in control of their passions"" (typically those who weren't involved in athletics) were labelled ""hysterical, sensitive and nervous"" which is significant as these were appellations commonly reserved exclusively for women (53). Therefore, without sport men were in danger of becoming womanlike, or ""delicate and degenerate"" (53). This shows that the development of institutionalised sport itself was predicated upon a societal ideology (that it served to reinforce) that sought to justify the masculine ideal of the dominance of men over women. Although sport as an institution has evolved beyond the 19 th Century public schools and women are not institutionally excluded from (most) sporting spheres, women are still marginal figures in contemporary sport, and this shall now be explored. The marginalisation of women in contemporary sporting spheres is made possible because of structural inequalities within sport itself, and because of the predominant societal perception that grants 'male sport' higher social esteem. The organisational bodies that govern most sports (even those where the majority of the athletes are female) are typically dominated by men (Bryson, 1994; 51). Similarly, institutions that are linked to sport such as sports journalism and corporations (involved in sponsorship) are conventionally male dominated spheres (52). Even when women do become professional athletes their achievements (and 'women's sport' in general) are largely ignored by the media and consequently receive very little attention or esteem (54). When women are given media coverage it is often in terms of their identity outside of their profession, typically with regards to their relational, marital, or familial situations (58). The justification for this marginalisation of women is the same stereotypes that originated in the public schools; namely that women are weaker and inferior to men and consequently their sport is less entertaining. Due to this contention there has been much discussion as to why women shouldn't compete with men in sports which rely on speed and strength (55). However, some sports rely on stamina and in other sports it is an advantage to be lighter, yet these sports in which women excel are excluded from the debate regarding sporting prowess, which shows this justification for marginalisation has an ideological nature (55). Therefore, contemporary sport influences the social construction of the masculine ideal of dominating women by ensuring that women are prevented from controlling how sporting institutions and organisational bodies are governed, which successfully negates any possibility that female athletes may use these institutions to re-shape the societal perception of 'female sport' as inferior. Whilst it has been established that women are subordinate to men in sport, the nature of men as the oppressing group needs to be discussed. The portrayal of sporting practices in the media reveals how specific types of masculinity that form the 'hegemonic masculinity' are glamorised and prioritised whilst other masculinities (and the men who enact them) are dismissed as deviant. The predominant image that emerges from the media representation of the male body is that a man should be muscular, athletic, powerful and healthy (Parker, 1996; 130). This perception emerges not only because of the extensive use of visual images of male athletes (who embody these physical ideals) but also because the male body exists at the intersection of sport, the media, and the consumer culture of society (130). Within contemporary society a culture has emerged that prioritises youth and beauty, and the portrayal of sportsmen in the media is part of the multiplicity of images and practices that individuals consume (such as yoga classes and healthy eating) in order to achieve these ideals (130). Therefore, the media representation of the male body shows how sport and the consumer culture are pivotal influences on the social construction of the hegemonic masculine ideal of physical prowess (as males who do not possess physical prowess are dismissed as inferior through the application of such labels as 'wimp') (132). Similarly, the masculine ideal of heterosexuality as the norm is reinforced by the media representation of sport. The depiction of homosexual male athletes is conspicuous largely because of its absence, and it can be argued that this is because there are comparatively few gay sportsmen (132). However, the relationship between the media and the dominant sporting culture of heterosexuality is more complex than this, as the media can be understood as actually creating this culture of heterosexuality through its obsession with ""the macho exploits of Britain's male athletes"" (132). Therefore, the media representation of the athletic male body and the normality of heterosexuality shows how sport is central to the social construction of these hegemonic masculine ideals as any alternative masculinities that do not feature in sport are labelled as inferior or ignored. The media representation of sport also contributes to the construction of the hegemonic masculine ideals of dominating women and the marginalisation of black men, and this relationship shall now be examined. The mediation of sporting practices (particularly violent sports) involves a complex and often contradictory relationship of identification and dissociation between male spectators from higher status backgrounds and the athletes themselves. The portrayal of sporting practices can be understood as naturalising the dominance of men over women, as media images of sport suppress the similarities between men and women and emphasise the differences, and then set about ""weaving a structure of symbols and interpretations"" around these differences to make them appear natural (Messner in Messner and Sabo, 1990; 101). Consequently, a sense of identification develops between male spectators and the images of sportsmen they consume which means that men identify themselves as physically superior to women; this sense of identification develops despite the fact that the majority of the male spectators are largely as physically dissimilar to professional male athletes as non-athlete females are (103). Conversely, the media representation of sporting practices also provides the male spectators with an image of a 'primitive other' embodied in the male athlete against whom he can define himself as modern and civilised (104). This sense of dissociation develops because the media representations portray images of sportsmen as predominantly black (and black men are historically associated with a notion of 'otherness'). It can be argued that the overrepresentation of ethnic minorities in sporting spheres is the result of a rational decision made by nascent black athletes to pursue a sports career, as it offered the best possibility of developing a career that would provide economic security because young black men are institutionally excluded from educational and occupational opportunities (104). Therefore, media images of sporting practices serve to reinforce the hegemonic masculine ideals of dominating women and marginalising black men through portraying 'natural' differences between men and women, and through depicting the black male body as an animalistic force. However, black men are not merely passive recipients of a racist social order, and their agency in relation to masculinity and sport shall now be elaborated. Through sporting practices black men have attempted to counteract the social invisibility imposed on them by an institutionally racist society, yet it is their acceptance of the ideals that compose hegemonic masculinity which ultimately ensures they do not affect the social order. Due to their exclusion from the educational and occupational institutions that confer social esteem on the individuals who have access to them, many black men channel their creative energies into constructing ""unique, expressive and conspicuous styles of speech, demeanour and walk etc"" (a phenomenon that Majors calls the 'cool pose') which offsets the invisibility they suffer due to their exclusion from the aforementioned institutions (Majors in Messner and Sabo, 1990; 111). Consequently, many black men pursue sporting careers because they can express themselves through the ""virtuosity of a performance"" and this offers the most realistic route to achieving the goals of hegemonic masculinity (such as being the breadwinner, having strength and dominating women) (110). Therefore, the response of black males through their concentration on sport represents an adaptation to, not a submission to, the hegemonic masculinity. However, in spite of their adaptation, their actions invariably do not allow them to transcend their subordination to the hegemonic masculinity. Through accepting the ideals of hegemonic masculinity black men pursue sporting careers in a context that is constructed by the racist wider society. In focusing predominantly on sport, black men accept the educational under-representation that is fostered by educational institutions; this is to their detriment as educational attainment is the principle route through which the ideals of hegemonic masculinity are realised and social power is attained (111). Even when black men do manage to become professional athletes, they are still confronted by institutionalised racism as there are remarkably few black managers or coaches in any sport, and the majority of sporting organisations are controlled by white males (113). Therefore, the participation of black males in sporting practices can be conceptualised as a response to the racism of the wider society, yet this response merely serves to reproduce the hegemonic masculine ideal of the marginalisation of black men, as it is predicated upon their acceptance of ideals that inevitably subordinate them. Although black athletes fulfil one element of the hegemonic masculinity (physical prowess), they are prevented from fulfilling other aspects (such as becoming professionals); this notion of contradiction is fundamental to the way hegemonic masculinity operates, and the role of sport in this process shall now be developed. The lives of professional sportsmen show how it is impossible to achieve all of the ideals that construct hegemonic masculinity, which therefore reveals how hegemonic masculinity operates in order to maintain its all encompassing dominance over the gender order. In his interview with Steve Donoghue (a champion in the iron man sporting practice) Connell asserts that Steve is an exemplar of hegemonic masculinity, as the life he leads makes him the embodiment of the masculine ideals of ""competitiveness, toughness and physical capability"" (Connell in Messner and Sabo, 1990; 94). However, Steve's testimony reveals that in order to maintain this status he has to sacrifice other masculine ideals that would jeopardise his physical performance, for example he can't ""go wild"" when he is socialising (92). Therefore, the representation of Steve and other sporting heroes like him can be understood as the symbolic embodiment of the epitome of an ideal. This means that sporting heroes are the tangible symbols for some aspects of hegemonic masculinity, namely the primacy of physical prowess, the domination of women, and the marginalisation of ethnic minorities and homosexuals. However, the sportsmen are unable to fulfil other aspects of hegemonic masculinity because they are antagonistic with the ideals they symbolise. Consequently, the other masculine ideals that contribute to hegemonic masculinity have different, often directly oppositional, symbols: for every sporting hero there is a 'bad boy', hedonistic Hollywood actor. Therefore, the existence of symbolic sporting heroes reveals that the hegemonic masculinity of which they form an integral part reproduces itself through the construction of a multiplicity of symbols that are often contradictory, yet always culturally exalted; and this shows how sporting practices contribute to the societal perceptions of masculinity and the roles of men and women that permeate all of society. In this essay it has been shown that the views stating the development of sport was unrelated to the social construction of masculine ideals are inadequate because of the primacy awarded to 'manliness' and athleticism in one of the major socialising institutions of the 19 th Century. It was also argued that the institutional subordination of women in sport contributes to the hegemonic masculine ideal of the subordination of women in society. The media representation of sporting practices was depicted as disseminating a consumerist, sexist, racist and homophobic ideology in order to maintain the dominance of hegemonic masculinity. It was then shown how black men pursue sporting careers to counteract the racism of wider society, but that ultimately the pursuance of a sporting career ensures the societal marginalisation of black men. Finally it was shown that hegemonic masculinity is a multi-dimensional phenomenon and sporting heroes constitute exemplars for specific ideals that construct hegemonic masculinity. It therefore follows from the evidence presented here that sporting practices are not a unique, socially isolated sphere and consequently do bear a relation to the social construction of masculine ideals. Sporting practices interact with other major societal forces in modern society, such as the perception that scientific knowledge equates to 'the truth' (as displayed in the relationship between the gender ideology and the public schools in the 19 th Century), or the prevalence of racism and consumerism, in order to socially construct hegemonic masculine ideals. Although sporting practices contribute to the social construction of the ideals that constitute hegemonic masculinity, it can be argued that the most important conclusion that can be drawn from an analysis of sporting practices and masculinity is that sporting heroes offer an insight into the way hegemonic masculinity structures the gender order of society in its entirety. Hegemonic masculinity is culturally exalted and stabilises a structure of dominance and subordination in the gender order as a whole, and in order to this it must have exemplars who are exalted as heroes. Sporting heroes as symbolic entities contribute to the vast network of images that cumulatively construct the societal perception of what a man should look like, or act like, or what the roles of men and women should be, which therefore means that sporting practices contribute to the social construction of characteristics that many individuals take for granted are part of our 'nature'.",False
34,"For too long the history of the Cold War has been written from a 'top-down' perspective examining elites and decision-makers. Whilst political scientists have become mired in using rational International Relations models, historians have rarely cast their net beyond realpolitik, frequently falling foul of detailed narrative around contingencies which often never occurred. The fall of the Berlin Wall in 1989 and the end of the 30-year rule for many documents in the West, which both led to a spate of archival-based sources available to historians, cemented this diplomatic and empirical approach. Moreover, archival work alone is a dangerous place from which to survey the Cold War, since both sides of the Iron Curtain employed 'newspeak' rather than rational discourse, even behind closed doors. In short, this 'New Cold War history' can become overly narrative driven and blind to broader analysis. However, as a result of the growing interest in cultural history following the 1980s 'linguistic turn', recent study, particularly in the so called 'Constructivist' school, has challenged Cold War historians to spread their wings methodologically, and explore the culture of the conflict. In spite of noteworthy 'hot' episodes, such as Vietnam, the Cold War was in most part a conflict of words and images delivered to an audience soaked in its products. One unmined area of Cold War culture is Hollywood Bomb Cinema. Bomb Cinema denotes films where the nuclear bomb is an explicit part of the theme or narrative and includes a body of films, widely forgotten, from classics such as Dr Strangelove (1963) to off-beat cult favourites like Them (1954). However, Bomb Cinema remains predominantly the preserve of film departments and media journalists who examine issues such as plot synopsis and entertainment value. Working under Freud's avowal that a cigar is sometimes only a cigar, their inquiry is devoid of any attempt to unearth hidden realities. Apocryphal moments are not examined for anything more than meaningless and pleasurable fantasy. For example, in reviewing Strangelove, The Times film critic simply called it a 'farcical comedy about the end of the world' and paid most attention to the 'remarkable performances by Mr Peter Sellers and Mr George. C. Scott'. Dr Strangelove will be referred to as  Strangelove from this point onwards. Film Critic, 'Film Comedy about the End of the World', The Times, 30 January 1964, p. 16. So why should historians concern themselves about films with images of the bomb? Put simply, the sheer number of these films reveal how deeply concerned Americans were about the bomb. Paul Boyer has argued that the bomb itself is a virtual Kantian category - an internal filter - that shapes our very understanding of the world. Taking a psychoanalytic perspective, the bomb is indicative of a repressed dread in US society, concerned about survival, morality, and rebirth. Thinking about the function of these films, Hayden White argues that historical and fictional discourses have common aspects in narratives and transmit messages about shared reality. Accordingly, at any given historical point, there exists a common pool of narratives that every culture disposes for its members who might wish to draw upon them for the transmission of messages. As a statistically proven part of the filmgoer's diet - indeed The Times reported in February 1964 that 'demand to see Strangelove has been so great...it has been decided to put on special late night showings' - Bomb Cinema is one amongst many narratives available to society in order to understand the bomb. Certainly, from an early stage, the significance of the intersection between the cinema and the bomb was recognised by policymakers. For example, the Supreme Command Allied Powers (SCAP) censored references to Hiroshima and Nagasaki in films made during the occupation of Japan. Jacques Derrida, in his work on the 'nuclear subject', has argued that since the terrifying reality of nuclear war can only be the signified referent and never the real referent of a discourse, the view of fictitious projects, such as Bomb Cinema, occupy a space equal to the views of 'experts'. Furthermore, in its satirical fashioning or 'black comedy', as apparent in Strangelove, Bomb Cinema is increasingly about the workings of power. By giving a privileged opportunity to laugh at those in authority, Bomb Cinema can afford the individual the bitter-sweet illusion of panoptical power on the inside, by stripping away his real Cold War position of powerlessness on the outside. Seen in this way, the term subversion is synonymous with liberated power for the individual. Paul Boyer, By the Bomb's Early Light: American Thought and Culture at the Dawn of the Atomic Age (Raleigh, 1994). Hayden White, The Content of the Form: Narrative Discourse and Historical Representation (Baltimore, 1990). Own Correspondent, 'Late Night Showings at Columbia Cinema', The Times, 10 October 1963, p.6. Nicholas Royle, Jacques Derrida (Oxford, 2003). By deconstructing key bomb films, it will be argued that by 1963 and  Strangelove, Bomb Cinema had become an agent of subversion. The state sponsored Strategic Air Command 1955 (SAC), which projected a halcyon era of acquiescence and all-pervasive Cold War conformity, stands as the archetypal propaganda film for point of comparison. Strangelove exploded absolutely the state led propaganda line. However, far from heralding the first cataclysmic step outside of the propaganda paradigm, Strangelove is best seen as an umbrella for earlier subversive messages emerging in certain 1950s bomb films. Working under the constant eye of the House Un-American Activities Committee (HUAC), the distinctly science fiction wing of Bomb Cinema used covert images to depict the anxieties that accompanied life with the bomb. Indeed, Stanley Kubrick was an avid watcher of 1950s SF, and Strangelove can be seen as consciously linking disparate aspects of a genre. Part of the so-called 'Cold War camp', SF films like The Thing (1951), The Day The Earth Stood Still (1951), Them, Forbidden Planet (1956) and The Incredible Shrinking Man (1957) were intentionally self-parodic, utilising 'grand guignol' horror techniques of invasions of predatory insects and plants, in order to appeal to a generation of rebels without a cause, plagued by fears of isolation. Yet, as Peter Biskind recognises, these films were very rarely in universal agreement and did have varying degrees of departure from the propaganda paradigm. My methodology is interdisciplinary and borrows from the recent Cold War Cultures School in history, including my supervisor, Dr. Patrick Major, requiring the socio-political contextualisation of films, as well as the production and reception history. Thus, this paper uses not only the films themselves but also draft scripts, biographies and newspaper reviews and articles of the period. The issues examined here - nuclear proliferation, Organisation Man, scientists and technology - are the most prevalent in Bomb films. For brevity, I will refer to Science Fiction as SF from now onwards. Peter Biskind, Seeing is Believing: How Hollywood Taught us to Stop Worrying and Love the Fifties (New York, 1983). Nuclear proliferation is the most obvious theme in bomb films. In our period, the nuclear powers exponentially increased their arsenals of destructive potential, within the prevailing and public strategy of Mutually Assured Destruction (MAD). Ex-US Defense Secretary (1961-68), and architect of MAD, Robert McNamara later called proliferation 'the foundation of stable deterrence in a nuclear world...resting on the understanding that if either side initiates the use of nuclear weapons, the other side will respond with sufficient power to inflict unacceptable damage'. World War II laid a precedent for the state taking a manipulative role in fashioning Hollywood films. SAC - aided in its making by the US Air Force, who allowed the B-36 bomber to be filmed - defends the arms race completely. It depicts the bomber force as a safeguard to protect Americans from the bomb. Real SAC imagery is always at work, such as the billboard sign at the base entrance, 'the nation and your security are at stake', and the Command's motto, 'Peace is our Profession'. The dialogue supports this argument too with Rusty asserting 'we're the only thing that keeps the peace'. In the hands of trustworthy fellows, such as Dutch, nuclear weapons are seen as banal and manageable. However, the main function of the film was to ingrain in citizens the moral resolve needed to live in the Cold War. Security officials knew it would have been impossible to protect Americans from nuclear attack. For example, Millard Caldwell - head of the Federal Civil Defense Administration 1950-52 - believed that 'when a situation is utterly hopeless, it may be better not to say so. The doctors often conclude against telling the hopeless cancer patient what the situation is'. By cultivating an image of safety, SAC hoped not only to curb the problem of panic, the invisible fifth column at home, but to ensure continued support for the policy of deterrence. It is worth remembering that, at the time of the film's release, the Alert America campaigns were in full swing, and the gospel of self-help was the new mantra of civil defence planners. By showing reluctant cold warriors, and their even more reluctant wives, subordinating their civilian lives to the demands of the state, SAC urges civilians both to accept the secrecy of the Military Industrial Complex (MIC) and even buy into it. Robert McNamara, 'CNN Interactive: Episode 12 MAD. Interview with Robert McNamara', 1995. URL  (1 December 2003). Strategic Air Command (Anthony Mann, 1955). Cited in Guy Oakes, The Imaginary War: Civil Defence and American Cold War Culture (Oxford, 1994), p. 147. Subversive bomb cinema exploded the propaganda line of the security of American defence. The Day the Earth Stood Still challenged the safety of placing the world under the nuclear sword of Damocles. Sending a clear signal about the illusory security of America, the alien spaceship lands in Washington, despite being monitored by security forces, and proves immune to human weaponry. Indeed, Cold War power is shown in the most microcosmic terms when the alien Klaatu makes the earth stand still, and declares that 'at the first sign of violence, your earth will be reduced to a burned out cinder'. The Day the Earth Stood Still (Robert Wise, 1951). Released in 1951, the film presents a direct challenge to the pursuit of the fusion bomb, which, when developed, would make nuclear holocaust possible. This message on arms control is poignantly captured when Klaatu charges 'live in peace or face obliteration'. Moreover, it is worth noting that Klaatu threatens to destroy the entire Earth, in spite of proliferation clearly emerging from the nationalism of the US State Department. The arms race is thus presented as an international problem which the United Nations must solve. The Thing, despite an over-arching propagandist rubric in which the Russians are suspected of wrongdoing, contains a subversive message on proliferation. The film ends on the thought that American power could not prevent annihilation, with Scotty lamenting 'I bring you warning... Watch the skies. Everywhere... Keep watching the skies'. Ibid. The Thing (Christopher Nyby and Howard Hanks, 1951). Them also looks despairingly at proliferation and examines the harmful effects of radioactive fallout; an issue particularly prevalent at the time of the film's release given the alarming implications of the Bikini test shots and the 1954 Lucky Dragon incident. According to Allan Winkler, fallout focused public fears because it threatened a less violent form of extinction and presented the danger of both psychological, as well as physical deformity. Learning that the radioactive man-eating ants crawled out of a New Mexico test site, Them rejects the safety of atmospheric testing, and the ability of statesmen to keep a lid on the nuclear Pandora's box. Indeed, one character questions 'if these monsters got started as a result of the first atomic bomb in 1945, what about all the others that have been exploded since then?' Them reflects the anxiety with the all or nothing policy of MAD. By showing the military unable to use its major weapons against the ants, since it was fighting on American soil, Them champions the search for a flexible and limited response policy. Yet Them maintains certain propaganda messages. Like The Thing, the monsters act as a metaphor for Communists. When Dr Medford laments, in clearly Biblical terms, 'and there shall be destruction... and the Beasts shall reign over the earth', the likeness is just discernible. Despite casting a shadow over the defence establishment's blithe acceptance of the arms race, in contrast with The Thing, Them maintains the legitimacy of state authority, illustrating the measure of divergence within SF. Loudspeakers advise that 'your personal safety...depend(s) on your full co-operation with the military authorities'. Reconciling audiences to the knowledge that the Military Scientific Complex (MSC) was there to protect them, even if the threat was produced by it in the first instance, the ants are defeated by the MSC. Allan Winkler, Life Under a Cloud: American Anxiety About the Atom (New York, 1993), p. 108. 15 Them (Gordon Douglas, 1954). Ibid. Ibid. The black comedy ending of Strangelove, with its mushroom clouds and musical serenade of 'We'll meet again', tapped the belief that the superpowers were so caught up in the arms race that nuclear holocaust had become inevitable. For Kubrick, the logical result of Soviets 'lofting satellites as easily as Americans dunked baskets' and bomb shelters proliferating like greenhouses, was nuclear Armageddon. This ending would have appeared more real given the recent Cuban Missile Crisis and brinksmanship in practice. Cynically rejecting the safeguards projected in SAC, we witness a pitched battle between US contingents at Burpleson Air Force Base, under the 'Peace is our Profession' billboard. When General Turgidson argues for 'catching them with their pants down', in a pre-emptive first strike, Strangelove questions the willingness of policymakers to stay within the confines of MAD. Pre-emptive first strike formulations, and its idea that a massive nuclear attack could swiftly paralyse the enemy by destroying its retaliatory forces, had been growing in the light of technological advances made with Anti-Ballistic Missiles and the stockpiling of US nuclear weapons. Turgidson's insane proposal is no different from the calls by Air Force chief General Curtis LeMay to give them 'Sunday punch'. Recent findings support Kubrick's observation on the instability of the arms race. We now know that the military had developed the 'Single Integrated Operations Plan', which worked on the assumption that America would have been willing to be the first to use nuclear weapons. This replaced Operation DROPSHOT, written in 1947, which assumed a long period of conventional war between NATO and the Soviet Union before any nuclear weapons would be employed by both sides. Biskind, Seeing is Believing, p.337. Dr Strangelove (Stanley Kubrick, 1963). Cited in Sven Lindquist, A History of Bombing (London, 2001), Section. 312. Military critics derided Strangelove for its inaccuracy and, unlike SAC, the Air Force did not co-operate in the making of the film, refusing Kubrick permission to film with the B-52 bomber. Indeed, Kubrick famously created the interior of his B-52 based on pictures in an old flight manual. There was also a forced Air Force disclaimer that 'safeguards would prevent the occurrence of events depicted in this film', as the military had always maintained that no attack could be launched without 'civilian authority'. However, exposing one of the Cold War's deepest secrets, Bill Barr of Washington's National Security Archive states that 'top military commanders had presidential authorised instructions providing advance authority to use nuclear weapons under specified conditions'. Thus, General Ripper's exploitation of a putative retaliatory safeguard, which allows him to launch a nuclear strike, suggests that Kubrick's anxiety in the wisdom of the arms race was not unfounded. Indeed, LeMay was known for goading the Soviet Union with unauthorised reconnaissance flights. By 1964, missions had resulted in 26 planes being shot down over the Soviet Union. Strangelove thus shows that the delicate nuclear balance could all too easily be disturbed. Dr Strangelove (Stanley Kubrick, 1963). 22 Cited in Paul Lashmar, 'Dr Strangelove's Secrets: Terrifying Truths about the Cold War Period are Leaking out from Archives Around the World', Independent, 8 September 1998. 23 Cited in Bruce Franklin, War Stars: The Superweapon and the American Imagination (Oxford, 1988), p. 185. Beyond the proliferation issue, there were other recurring themes in Bomb Cinema. The next of these is the impact of the organization on man. In The Organization Man (1956), William Whyte observed that 'man exists as a unit of society. Of himself, he isolated, meaningless...by sublimating himself in the group, he helps produce a whole that is greater than the sum of its parts'. Whyte showed that under late capitalism, every aspect of life had become regimented and scheduled. SAC supports this triumph of conformity. Even the role of husband and father has to take a back seat to the organization, as Dutch declares 'we can't schedule the flights according to the birth schedule'. The Air Force is the natural habitat for the organization man, with General Hawkes educating Dutch about the importance of the group. When Dutch falls prey on mission to an injury he has failed to disclose, the crew takes over and saves the day. He is subsequently denounced for jeopardising the lives of his team-mates and the health of the organization. Echoing Whyte's comment that 'the man of the future is not the individualist but the man who works through others for others', SAC shows that the day of the daring pilot striving for personal glory - an image built up with maverick stereotypes of World War II pilots - had been replaced by the corporate '9 to 5' aviator. William Whyte, The Organization Man (New York, 1956), p. 12. 25 Strategic Air Command (Anthony Mann, 1955). 26 Whyte, The Organization Man, p. 22. However, most subversive bomb cinema showed that the growing regimentation of corporate culture made man bereft of any genuine individual identity and sense of accomplishment. The most pertinent statement in  The Incredible Shrinking Man is the lead character's existential condition. Living in suburbia, 'the dormitory of the new generation of organization men', and working as a reluctant organization man in his brother's firm, Robert Carey finds little achievement in his life. His shrinking is thus symbolic of a broader personal turmoil. In his ritual killing of the spider, Carey loses his fear of alienation from the group, experiencing a psychological transformation. In revealing, 'and then I meant something too... To God there is no zero', Carey asks the audience to embrace his liberation. The Incredible Shrinking Man is thus a covert attack on the psychological ramifications of organization fetishism. In addition, the film shows that the famous sentence in the Communist Manifesto stating that the proletariat has nothing to lose but its chains has little relevance for the organization man. Robert Carey has something to lose conjecturally - his standard of living. Carey represents a wider metonym for contemporary American middle-class families who, having run up large debts, became trapped in the net of the bourgeois milieu. Whyte, The Organization Man, p. 14. 29 The Incredible Shrinking Man (Jack Arnold, 1957). 30 The Thing (Christopher Nyby and Howard Hanks, 1951). The Thing, in its depiction of Air Force Captain Pat Hendry, directs its subversive message more towards the everyday practicalities of organization man. Asserting 'I'm not working for the world. I'm working for the Air Force', Hendry begins as the archetypal wheel-in-machine conformist, who cannot blow his nose without clearing it first with Headquarters. The Thing serves as a critique of the absurdity of 'going by the book'. For example, using standard procedure to free an alien ship, Hendry accidentally blows it up. Hendry is actually immobilised by bureaucracy, stating 'until I receive my instructions from my superior officers, we'll have to mark time'. Hendry's idleness testifies to Whyte's argument, that 'in further institutionalising the great power of the majority, we are making the individual distrust himself'. When orders arrive to avoid injuring the monster, he has to break free of the parameters of the organization and kill it, in order to stay alive. However, as we have shown, SF films were anything but unanimous in agreement. Them, in contrast to The Thing, was not subversive in this regard. It argues that society should embrace the organization man culture and posits that to leave the group is to be beyond the safe parameters of society. All the victims succumb when they leave the confines of the group. Furthermore, it takes a consortium of scientists, police and soldiers to defeat the ants. Them is indicative that the line of demarcation between films of propaganda and subversion can often be blurred. The Thing (Christopher Nyby and Howard Hanks, 1951). Whyte, The Organization Man, p. 59. The role of scientists manifests itself in Bomb Cinema. Social critic Bertrand Russell argued that 'we are perhaps living in the last age of man, and, if so, it is to science that we will owe this extinction'. In Russell's eyes, scientists had become Devil Gods, violating the delicate boundary between conquering nature for the benefit of mankind and conquering nature at the expense of man's survival. In its portrayal of the nefarious Dr Carrington, The Thing shows the scientist twisting the dragon's tail. Admiring the creature, Carrington helps it reproduce itself, and interferes with attempts on its life. The scientist inherent in Carrington is the extremist head-over-heart zealot, assiduously pursuing his parochial interests and immune to the fact that he might, in the process, be fomenting Armageddon. For example, Carrington's morally blind quest for advancement is made explicitly clear when he states 'knowledge is more important than life'. Indeed, Carrington represents a wider public concern, following the defeat of scientific internationalism and its idea that knowledge should be universally shared, that science was working increasingly behind the closed doors of the MIC. These fears were particularly acute at the time of the film's release, following the public disclosure of the secret Hydrogen bomb project. Cited in Robert Jungk, Brighter than a Thousand Suns: A Personal History of the Atomic Scientists (1956), p. 70. The Thing (Christopher Nyby and Howard Hanks, 1951). Scientists, however, were both reviled and revered. In Flash Effect, David Tietge argues that in a campaign to promote science in American consciousness, many scientists were symbolised as both father figures and as saviours, working in an environment that celebrated the technological objects of their labour. Certain SF bomb films portray the benevolent and wise scientist. In contrast to Carrington, Dr Medford, in Them, is presented as an avuncular intellectual. David Tietge, Flash Effect: Science and the Rhetorical Origins of Cold War America (Ohio, 2002). In his closing statement, 'We've opened the door into another world. What we'll eventually find nobody can predict', Medford echoes the concerns of Robert Oppenheimer, and recognises the dangers of using the test tube to tamper with nature. Echoing Eisenhower's 'Atoms for Peace' programme and his imminent formation of The 'Technological Capabilities Panel', Them embraces the idea that in a nuclear age, science and politics should no longer inhabit separate spheres. We see scientists working at the kernel of government policy. Medford lectures public officials, commands the resources of the state and devises the plan to destroy the menace. Indeed, Medford sees his elevated role as a 'scientist's dream'. The cachet of science becomes so strong it distorts the traditional hierarchy of gender roles. When his daughter insists on joining the men in a dangerous trip, Agent Graham remarks 'it's no place for you or any other woman'. However, she demands that he treat her as a scientist first and a woman second, asserting 'somebody with scientific knowledge, a trained observer, has to go'. Released in the wake of the arrest of the British physicist, Klaus Fuchs, for communicating atomic research to the Russians, and the subsequent executions of Julius and Ethel Rosenberg in June 1953 for their involvement in the spy ring, Them attempts to rehabilitate the image of the good and trustworthy scientist. Them (Gordon Douglas, 1954) Ibid. Ibid. Ibid. During his terms in office, Eisenhower did more than any previous President to integrate science into the state. Yet, in his farewell address he exhibited more than a powerful note of irony, warning that 'we must be alert to the...danger that policy could become the captive of a scientific-technological elite'. Strangelove shares Eisenhower's concern. As the premier authority on the Doomsday Machine, Dr Strangelove - struggling with his 'Heil-Hitler' hand and speaking in a terse German accent, which denotes his Nazi affiliations - is devoid of love for anything except the bomb. Despite his mental deterioration, the leadership cede all power to him by the end, as they are completely distracted by his plans to continue the human race. Dr Strangelove can be seen as a parody of the Hydrogen Bomb scientist Dr Edward Teller. Repudiated by many of his scientific colleagues for testifying against Robert Oppenheimer, who called for nuclear restraint and internationalism, Teller ran with a more military crowd, becoming the darling of conservative thinkers for his advocacy of American scientific supremacy. John McAdams, 'Dwight Eisenhower :Farewell Address to the Nation 17 January 1961', 1995. (1 December 2003). 41 Strategic Air Command (Anthony Mann, 1955) The final area of this paper addresses a theme unique to the films of subversion. All subversive films involve some measure of destruction and they question how this is allowed to happen. On Judgement Day who, or what, is at fault; the nature of man and his failings to control events or unrestrained technology growth? Witnessing no destruction, SAC sits outside of this question, reflective of the fact that the Air Force aided making of the film. The characters exhibit no militant anticommunist hysteria, preferring not even to name the enemy, and simply refer to him as 'the other fella'. Although recognising the contemporary problem of peacetime mobilisation for war - indeed Dutch states 'there is a new kind of war', one where 'we've got to stay ready to fight without fighting' - the men are portrayed as hard-working, reluctant cold warriors, charged with a mission others neither admired nor would accept. Unlike the characters in Strangelove, who flaunt the bomb aggressively, the dedicated public servants of SAC present man as no apocalyptic threat and consider the bomb only as the gun behind the door. Ibid. The Day the Earth Stood Still presents man as the threat. This supports Stanley Kauffman's argument that 'ban the bomb and they'll find another way. The real Doomsday machine is men'. The film suggests that Cold War suspicion in America had overwhelmed all reason. Unable to attain an audience with the world's leaders, given the bipolar tensions of the time, Klaatu complains to the President, 'I'm not concerned...with the internal affairs of your planet...my people have learned to live without [such stupidity]'. The President responds 'I'm afraid my people haven't'. When the peaceful Klaatu escapes from prison, the entire nation is unrelenting in its bellicosity towards him. One man calls him 'a wild animal'. The film illustrates man's dangerous lack of control as exemplified when Klaatu is shot by a trigger-happy soldier while delivering the greeting 'we have come to visit you in peace'. Moreover, the film suggests that, in the right hands, technology is far from an apocalyptic threat but instead a possible path to utopia. Learning that the alien spacecraft is nuclear, Bobby exposes man's ignorance by stating 'I thought that was only for bombs'. The film thus echoes Einstein's statement that 'the atomic bomb has changed everything except the nature of man'. Stanley Kauffman, 'Dean Swift in the Twentieth Century', The New Republic, 150:5 (1964), p. 24. The Day the Earth Stood Still (Robert Wise, 1951) 45 Ibid. 46 Ibid. 47 Ibid. 48 Ibid. Cited in Margot Henriksen, Dr Strangelove's America: Society and Culture in the Atomic Age (Berkeley, 1997), p. xvi. 50 The Forbidden Planet (Fred Wilcox, 1956). 51 Cited in Franklin, War Stars, p. 4. The custodians of the bomb are dealt a particularly vicious blow in subversive Bomb Cinema. Driven by virulent anti-communism, the American leadership is shown as nothing more than a group of modern Prometheans, morally blind to the implications of military conflagration. Strangelove harnesses the idea by admitting that nuclear war is absurd, but understands that its absurdity only makes it more likely because of the world's propensity for producing psychotic individuals and elevating them into positions of power. Supposedly working for the 'good' of the people, these men are presented as captivated by deadly technology and oblivious to the destructive nature of their policies. It is Ripper who orchestrates the attack and Turgidson's remark, 'the human element seems to have failed us here', clearly illustrates the chasm between man's scientific skill and his political ineptitude. Major Kong serves as the perfect metaphor for the failure of man to recognise how nuclear weapons have fundamentally changed the nature of war. In his pep talk - 'the folks back home is a-countin' on you and, by golly, we ain't about to let-em down' - Kong is blind to the severity of the task he is being asked to do. Wearing a cowboy hat, which connects him to the frontier tradition and with the patriotic war song 'Johnny Comes Marching Home' providing music, Kong is presented as the dangerous short-sighted hero riding the bomb. Kubrick was not a lone voice in his negative treatment of the American leadership. For example, in his expressive Cold War jeremiad 'Gentleman: You are Mad', social commentator Lewis Mumford asserted that 'madmen govern our affairs in the name of security'. 52 Dr Strangelove (Stanley Kubrick, 1963). 53 Ibid. Another aspect of the 'madmen' character explored in Strangelove is the triumph of the radical right-wing cold warrior in America, exposed in the insanity of General Ripper. Echoing Senator Joseph McCarthy, who spoke of the 'communist with a razor blade poised over the jugular vein of this nation', Ripper manifests similar obsessive anti-communism by asserting, 'I can no longer sit back and allow...the international communist conspiracy to impurify all of our bodily fluids'. Ripper's willingness to annihilate the world because of fluoridation illustrates that the nature of man is at fault again. Cited in Henriksen, Dr Strangelove's America, p. 47.; Dr Strangelove (Stanley Kubrick, 1963). Civilians are equally culpable. President Muffley is presented as the ineffectual Liberal who is unable to halt the madness around him. By arguing with Premier Kissof over who is sorrier for imminent global holocaust, Muffley not only exhibits the preposterous nature of Cold War tensions but also appears to see the event as no more than a social faux pas. Although, 'relentlessly perceptive of human beings to the point of inhumanity', Strangelove is also tinged with technophobia. The Doomsday Machine, 'that rules out human meddling', signals the ultimate triumph of destructive technology. It is ironic that the machines of destruction, such as the Doomsday Machine, work with great efficiency, whilst more humane machines, such as telephones, are ineffectual. It is worth remembering that since 1945 delivery system advances had grown considerably, resulting in the land based missile, which is impervious to any recall signal. In part then, Strangelove shows that developing more sophisticated defence systems will lead to technology manipulating man and ultimately annihilation. Stanley Kauffman, 'Dean Swift in the Twentieth Century', The New Republic, 150:5 (1964), p. 24. 56 Dr Strangelove (Stanley Kubrick, 1963). In conclusion, Bomb Cinema between 1950 and 1964 revealed two cultural personalities in conflict. Under the surface of SAC, which sat at the high-table of state propaganda, there existed films subverting the portrayal of a benign nuclear age. These reached their climax with Strangelove. Lewis Mumford stated in 1964 that Strangelove represented 'the first break in the catatonic Cold War trance that had so long held our country in its rigid grip'. However, by deconstructing 1950s SF films, Strangelove is best seen as the culmination of creeping subversion, rather than first explosion.1950s SF films had the freedom for subversive statements, because they appeared so thoroughly removed from reality. Lost on Film Studies scholars and reviewers, these films, even in their most apocryphal moments, were brimming with surreptitious statements on the atomic age. In our efforts to excavate the hidden and the not so hidden, we have also proved that within SF there existed a network of competing ideologues and measures of departure from the overriding propaganda paradigm. Them, despite its abhorrence to proliferation, maintains the legitimacy of state power and support for the organization. Cited in Charles Maland, 'Dr Strangelove: A Nightmare Comedy and the Ideology of the Liberal Consensus', American Quarterly 31:5 (1979), p. 716. Yet, the importance of Strangelove cannot be underplayed. Arguably Strangelove tells us something about the constantly renegotiated nature of hegemonic apparatuses. Gramsci argued that culture is a non-coercive means whereby the ruling-classes maintain their dominance by securing the consent of subordinate groups. Moreover, hegemony operates by allowing mild concessions to groups who do not pose a threat to the framework of domination. Thus, it is not surprising that we find in certain 1950s SF bomb films moments of divergence from the dominant paradigm. Here is concession at work. What Strangelove does is reverse the dialectic to make the previous mild concessions the now prevailing hegemony and signifies the moment when previously unthreatening groups rose to prominence. It also signifies the decisive moment when audiences are no longer able to watch the nuclear subject straight. Kubrick originally had intended a straight adaptation of Peter George's novel Red Alert, but it dawned on him that the most truthful elements of scenes were absurd, and so he transformed the story into a nightmare comedy. Released shortly after Strangelove in 1964, the straight-laced Fail-Safe, positing a similar end of the world scenario, was a box office disaster. Strangelove's 'horrible grin' of black humour had become the only means by which society could learn to stop worrying and love the bomb.","The intention of this paper is move outward from new and old statistical empirical evidence, found on the one hand in the National Archives (PRO) and the RAF Museum Archive, and on the other, in existing secondary literature, and ask what this reveals about British air strategy in the 1920s and 1930s. The methodology used is to take researched evidence, portray it in the form of charts, data bases or spreadsheets, and then use this IT generated information as the foundation for discussion. The archive material was not examined with any pre-conceived conclusions in mind. It will be our job to use all the empirical meat to create an interpretable abstract of air strategy. We will question whether these sources either perpetuate or shed new light on existing discourse. Along the road the sources will be challenged for their usefulness as historical pieces of evidence. As a point of entry into the mise-en-scène of air strategy in the 1920s, let us analyse a RAF proposed peace strength chart (Appendix 1), as laid down by the Chief of the Air Staff in a November 1918 paper. Figure 1 below, which is drawn from this Appendix, illustrates the proposed location of the squadrons. Most significantly, the Figure expounds the increasing importance placed by strategists on using air power in the empire, with 34 squadrons placed in colonial possessions. The Chief enthused that 'British possessions...are spread over a wide area and aircraft should prove of the greatest service'. Aircraft were seen as a form of control without occupation, providing a cheap and ubiquitous means of imperial policing. He also opined that 'on the conclusion of peace all available aircraft material should be utilised...in the Dominions and India'. Although this kind of commitment is not realised in the Figure, it shows that air strategists were starting to see aircraft within the typology of imperialism, as a possible means of lightening the white man's burden. However, the Figure is fundamentally flawed in understanding air strategy. It is only an aspiration of air strength in the coming 1920s and does not take into account the financial or procurement feasibility of its estimations. Moreover, in its rich dedication to the supposed war-winning bombers (shown at Figure 2 below), the source bears the fingerprints of the Air Staff's political motive to protect the service from the crossfire of its seniors and attempts to undermine its autonomy. The National Archives, AIR 8/6. Attached memorandum to Proposed Peace Strength by Chief of the Air Staff, November 1918 2Ibid Overall, the Figure is an idealist snapshot that ultimately is best at revealing Air Staff pressure on air strategy, captured in a accompanying remark by the Chief - 'we wish to turn our swords into ploughshare... but not so fast' - than it is in understanding what actually happened in the 1920s. Ibid It is now important to focus our attention on the true position of the RAF in the 1920s. We can do this through the lens of air service expenditure estimates at Appendix 2 and the resultant Figure 3 below. It is clear that the earlier optimism of the Air Staff proved ephemeral, as air service expenditure initially sharply fell, and then remained relatively austere. The Figure supports existing discourse about air strategy, which contends that the RAF just about 'held on' during its cradle days, in the face of ferocious economic retrenchment, the Ten-Year Rule and a zetheist that condemned recourse to conflict. The impact of the imposed permanent war planning blight, is particularly evident if we note the acute fall in Technical and Warlike Stores estimates in the Appendix - elements that constitute the kernel of aggressive air strength - from £6,172,850 in 1920 to £1,295,000 in 1922. It is also significant to note from that source that research was particularly barren throughout the period, falling from £2,575,540 in 1920 to £498,000 in 1929. This goes a long way in explaining the poor quality of aircraft the RAF possessed as late as 1934, still consisting of inchoate wooden biplanes. The data also reveal that air strategy was far from a blueprinted plan, and was in fact often pragmatic and respondent to changes in geopolitik. For example, the slight rise in expenditure in 1924 can be seen as a direct response to the perceived threat of the Armée de l'Air emerging from 1923. However, the data has limitations that prevent a challenge to the existing historiography. They give no indication of how many planes were actually produced or what important fields of operation fared better than others. How far was the empire really given wings as the Chief of the Air Staff had proposed in 1918? They also do not offer any information on the Army and Navy, against which to draw priority comparisons; and industrial capacity is unknown. Moving into the 1930s, let us start by examining the early years, before any significant expenditure rise occurred. Again, we will move our discussions from the empirical base of Appendix 2, as reproduced in Figure 4 below. The Figure shows that air service expenditure in the years 1930-34 also remained relatively low and stable; this can be seen as an indication of the government's first strategy to avert fears of the knock out blow, through the panacea of air disarmament. The knock out blow principle is important, as it weighs so heavily on air strategy throughout the 1930s. Founded in the futurological literature of HG Wells -' after the smashing of City Hall...the white flag had been hoisted' - and politically confirmed in Baldwin's famous dictum 'the bomber will always get through', the knock out blow endangered the heart of Empire for the first time. This anxiety was reinforced by the Great Depression, where it was felt that a major dislocation, such as war, would catastrophically weaken the durability of industrialised society. In the context of increasing demands to put Britain's financial house in order, and in preparation for the international Disarmament Conference in 1932, an air disarmament strategy was the diplomats preferred negotiating hand. Our sources support this policy. Funding of the important Technical and Warlike Stores element remained about the same between 1930 and 1934. A consequence of this was that production became devoted simply to re-equipping old planes. An illustration of this was an apparent 'moratorium' in number of new airframes being produced, as shown in Figure 5 below. H G Wells, The War in the Air (London, 1908), p. 130. 5 Cited in Uri Bailer, The Shadow of the Bomber: The Fear of Air Attack and British Politics (London, 1980), p. 14. However, a glaring limitation to the data is again the absence of plane types and their contribution towards the overall force mix and objectives. Focusing now on the period 1934 to 1938, before the financial picture would be dramatically changed by war, Figure 6 below illustrates that these years were nonetheless marked by a massive increase in air service expenditure. Appendix 2 indicates that the Technical and Warlike Stores indicator rose by a staggering 337 per cent, whilst two major supplementary estimates were added in 1935 and 1936, amounting to £17,000,000. The source shows Britain dedicating greater spending to research (99 per cent), which may go some way in explaining the relative superiority of British planes by 1939. The prima facie backdrop to the data is the failure of air disarmament and the révanchiste position of Germany. Indeed, the British Foreign Secretary opined in 1935, 'the verb to disarm should be classified by grammarians as a defective verb'. However, these two reasons represent the mere surface disturbance and crests of foam behind the policy of increased air expenditure, and do not engage with policymaker's more directed mentalités. To understand this, we need to examine the type of planes being produced. Cited in Uri Bailer, The Shadow of the Bomber: The Fear of Air Attack and British Politics (London, 1980), p. 9. The various air schemes (Appendix 3) developed and amended from 1933 onwards, outlining the types and numbers of planes required for the front line by a certain date, provide the key to unlocking the deeper roots associated with air policy in this period. Figure 7 below, produced from this Appendix, shows that the Britain was increasingly dedicating her resources to bombers. With the collapse of air disarmament talks, following the ignominious end to the Geneva Conference, policymakers sought a new means to prevent the knock out blow. It was axiomatic to air strategy that a commitment to building a bomber force would act as a deterrent against German air attack, presaging the Cold War concept of Mutually Assured Destruction. Increasing the number of bombers from 476 in 1934 to 1589 by 1937, as shown in the Appendix, worked under the belief that 'qui desiderat pacem, praeparet bellum'. On a diplomatic level, it was felt that a bomber force might bring Germany back to the Conference table. On a political level, it allayed public fears about a lack of preparation for air attack. The data also reveal a proportionally sharp increase of bomber requirements from February 1936 (see Figure 7) onwards. This can be seen as an equalising knee-jerk response to Hitler's alarming 1935 parity-achievement claim, which 'set the Nazi cat squarely among the democratic pigeons'. One limitation to our source, however, is that it does not show the number of bombers required in a reserve capacity to sustain operations. This would distinguish absolutely how far Britain worked under the strategy of shop window deterrence. Nevertheless, this source illustrates that air strategy was doing far more than aimlessly stirring Britain from her slumbers as Figure 6 suggests, and was in fact, in its dedication to bombers, a piece of political conjuring to hypnotise its audience. Cited in John Terraine, A Time for Courage: The RAF in the European War 1939-1945 (New York, 1985), p. 3 8 Denis Richards, The Royal Air Force Volume 1: The Fight at Odds (London, 1974), p. 12. The next area of our paper will examine the immediate pre-war years. Firstly, let us look at the data given in an Air Ministry report, as indicated in Figure 8 below, on the production of airframes between August 1938 and November 1939. It is clear that, from January 1939, airframe manufacturers started to exceed their production promises. These promises would have been established with policymakers in the deterrence by parity years. Although not explicit in the chart, we can draw three possible conclusions from this. The surpassing of airframe guarantees can almost incontestably be seen as the final nail in the coffin for the parity strategy, as the government encouraged air manufacturers to move towards a war footing. Not only had hopes of an air convention been lost but also the promises of Munich seemed long forgotten. Britannia's trident was now a bayonet and her shield a gas mask. Less probably, it could reflect that manufacturers had simply become more efficient in their production, no longer cruising at barely economical speed. More sinisterly, the data could suggest production profiteering, as captured in the illustration below (albeit in 1935). Ultimately, however, the source is best at confirming the end of air parity, than it is at hinting at more polemical speculations. However, these delivery numbers fail to paint the whole picture. We need to examine the types of planes produced. Figure 9, working on a microcosmic level of production, shows the number of Spitfires delivered from the Eastleigh Production Facility. It is quite apparent that a significant rise in production occurred from October 1938 onwards. However, this source is fundamentally limited in understanding air strategy, as Spitfires were neither the archetypal fighter in 1938-9 nor accounted for the bulk of the fighter force. Hurricane production is also required, in order to obtain a true picture of any significant change in production, and ergo air strategy. The difference in orders was 8647 more Hurricanes (Appendix 4). Importantly, Figure 10 below shows that from June 1938 the number of all fighters being delivered rose feverishly. This supports existing discourse, which argues that policymakers from early 1938 onwards increased fighter production for air defence purposes. A 1938 Command Paper reflects this, lamenting the failure of the bomber deterrence strategy, and stating 'taking risks for peace has not removed the dangers of war'. Major technological improvements with fighter capability, coupled with the coming of radar, presented the Command Paper, Statement Relating to Defence: Presented by the Primeminister to Parliament 3 March 1938 (London, 1938), p. 4. possibility of withstanding the knock out blow. This in turn threatened Germany with the prospect of a long war, which was arguably a more credible deterrent than simply a bomber build up, given the greater long term resources of the Empire. However, this Figure has a limitation of its own, because it does not tell us the relative fighter position vis-a-vis the wider picture of total aircraft production and in particular that of bombers. To combat these problems, we can refer to the data provided in the Cabinet approved air schemes L and M (Appendix 3). The two pie charts below comprising Figure 11 compare the required aircraft by proportion of all total planes in April 1938 and November 1938. It is immediately apparent that only a small shift away from bombers towards fighters occurred. This is by no means supportive of existing discourse. Literature on 1930's air power recognises the shift but argues that it was much more pronounced, with production geared three to one in favour of fighters. However, Figure 11 indicates that, in receiving well over 40 per cent of total aircraft production, the bomber remained the main player in air strategy, notwithstanding a greater emphasis to fighters. Thus, the pie charts provide an empirically tested challenge to the current epistemological understanding of the period. The final part of this paper will question the common usage by contemporary historians of simple comparative air power strength charts, as exemplified by Figure 12 below. Use of such charts to critique Britain's air strategy vis-a-vis other nations to illustrate Britain's 'dreadful note of preparation' is potentially too shallow. Although not wishing to be overly judgmental - this would require an exhaustive analysis far beyond the scope of this paper - it is essential that we lay bare certain qualifications the air historian needs to comprehend if he or she wishes to use such charts as an empirical authority. It is difficult to compare aircraft production in basic numerical terms, as the quality of aircraft varied greatly from country to country and year to year and was driven by different operational demands. Furthermore the numbers include Denis Richards, The Royal Air Force Volume 1: The Fight at Odds (London, 1974), p. 12. more than front line types (for example, Germany devoted far larger amounts of production than Britain to trainers), and they give us no insight into how many of the aircraft delivered were immediately deployable, through pilot availability and other factors. Also ignored in assessing proportional difference in air strength is the contribution of anti-aircraft defences. The contemporary historian, therefore, needs to take these sensitivities in his metaphorical knapsack when he enters the archives. To conclude, it has been the intention of this paper to work with primary and secondary statistical source material obtained from the National Archives, the RAF Museum and existing literature, in an effort to see what estimate and production data reveal about air strategy in the 1920s and 1930s. Much of the evidence has converged with existing historiographical discourse, outlining a gradual maturity of air strategy, extending from labouring within the parameters of a restricted budget through to the mantra of home defence. But some of the recently researched material sheds new light on the subject. For example, our data on fighter requirements refutes existing understanding that air strategy moved absolutely from bombers to fighters in the years 1938 to 1939. We have also suggested that certain sources, particularly comparative charts, remain too simplistic in composition to hold any more than mere subjective value. Furthermore, the usefulness of solely Air Ministry derived sources, such as the proposed peace strength chart, are somewhat sullied because their technical estimates were shaped by non-financial predilections and bias. Although ultimately recognising, as Clifford Geertz laments 'what we call our data are really our own constructions of other people's constructions', the charts here, despite certain limitations, not only move our understanding of air strategy slowly towards a more definitive picture but also hint at possible areas for further in-depth study. Indeed, there remains a vast sum of non-researched information relating to air strategy in the 1920s and 1930s waiting for the historian in the archives. Arguably, what our fighter discovery shows above all is that literature on air strategy in the 1920s and 1930s has itself become a too entrenched 'matter of faith'. It is thus the job of the air historian to revise further the subject, particularly the years 1938 to 1939, in greater detail. 11 Clifford Geertz, The Interpretation of Cultures (New York, 1973), p. 9.",True
35,"The intention of this paper is move outward from new and old statistical empirical evidence, found on the one hand in the National Archives (PRO) and the RAF Museum Archive, and on the other, in existing secondary literature, and ask what this reveals about British air strategy in the 1920s and 1930s. The methodology used is to take researched evidence, portray it in the form of charts, data bases or spreadsheets, and then use this IT generated information as the foundation for discussion. The archive material was not examined with any pre-conceived conclusions in mind. It will be our job to use all the empirical meat to create an interpretable abstract of air strategy. We will question whether these sources either perpetuate or shed new light on existing discourse. Along the road the sources will be challenged for their usefulness as historical pieces of evidence. As a point of entry into the mise-en-scène of air strategy in the 1920s, let us analyse a RAF proposed peace strength chart (Appendix 1), as laid down by the Chief of the Air Staff in a November 1918 paper. Figure 1 below, which is drawn from this Appendix, illustrates the proposed location of the squadrons. Most significantly, the Figure expounds the increasing importance placed by strategists on using air power in the empire, with 34 squadrons placed in colonial possessions. The Chief enthused that 'British possessions...are spread over a wide area and aircraft should prove of the greatest service'. Aircraft were seen as a form of control without occupation, providing a cheap and ubiquitous means of imperial policing. He also opined that 'on the conclusion of peace all available aircraft material should be utilised...in the Dominions and India'. Although this kind of commitment is not realised in the Figure, it shows that air strategists were starting to see aircraft within the typology of imperialism, as a possible means of lightening the white man's burden. However, the Figure is fundamentally flawed in understanding air strategy. It is only an aspiration of air strength in the coming 1920s and does not take into account the financial or procurement feasibility of its estimations. Moreover, in its rich dedication to the supposed war-winning bombers (shown at Figure 2 below), the source bears the fingerprints of the Air Staff's political motive to protect the service from the crossfire of its seniors and attempts to undermine its autonomy. The National Archives, AIR 8/6. Attached memorandum to Proposed Peace Strength by Chief of the Air Staff, November 1918 2Ibid Overall, the Figure is an idealist snapshot that ultimately is best at revealing Air Staff pressure on air strategy, captured in a accompanying remark by the Chief - 'we wish to turn our swords into ploughshare... but not so fast' - than it is in understanding what actually happened in the 1920s. Ibid It is now important to focus our attention on the true position of the RAF in the 1920s. We can do this through the lens of air service expenditure estimates at Appendix 2 and the resultant Figure 3 below. It is clear that the earlier optimism of the Air Staff proved ephemeral, as air service expenditure initially sharply fell, and then remained relatively austere. The Figure supports existing discourse about air strategy, which contends that the RAF just about 'held on' during its cradle days, in the face of ferocious economic retrenchment, the Ten-Year Rule and a zetheist that condemned recourse to conflict. The impact of the imposed permanent war planning blight, is particularly evident if we note the acute fall in Technical and Warlike Stores estimates in the Appendix - elements that constitute the kernel of aggressive air strength - from £6,172,850 in 1920 to £1,295,000 in 1922. It is also significant to note from that source that research was particularly barren throughout the period, falling from £2,575,540 in 1920 to £498,000 in 1929. This goes a long way in explaining the poor quality of aircraft the RAF possessed as late as 1934, still consisting of inchoate wooden biplanes. The data also reveal that air strategy was far from a blueprinted plan, and was in fact often pragmatic and respondent to changes in geopolitik. For example, the slight rise in expenditure in 1924 can be seen as a direct response to the perceived threat of the Armée de l'Air emerging from 1923. However, the data has limitations that prevent a challenge to the existing historiography. They give no indication of how many planes were actually produced or what important fields of operation fared better than others. How far was the empire really given wings as the Chief of the Air Staff had proposed in 1918? They also do not offer any information on the Army and Navy, against which to draw priority comparisons; and industrial capacity is unknown. Moving into the 1930s, let us start by examining the early years, before any significant expenditure rise occurred. Again, we will move our discussions from the empirical base of Appendix 2, as reproduced in Figure 4 below. The Figure shows that air service expenditure in the years 1930-34 also remained relatively low and stable; this can be seen as an indication of the government's first strategy to avert fears of the knock out blow, through the panacea of air disarmament. The knock out blow principle is important, as it weighs so heavily on air strategy throughout the 1930s. Founded in the futurological literature of HG Wells -' after the smashing of City Hall...the white flag had been hoisted' - and politically confirmed in Baldwin's famous dictum 'the bomber will always get through', the knock out blow endangered the heart of Empire for the first time. This anxiety was reinforced by the Great Depression, where it was felt that a major dislocation, such as war, would catastrophically weaken the durability of industrialised society. In the context of increasing demands to put Britain's financial house in order, and in preparation for the international Disarmament Conference in 1932, an air disarmament strategy was the diplomats preferred negotiating hand. Our sources support this policy. Funding of the important Technical and Warlike Stores element remained about the same between 1930 and 1934. A consequence of this was that production became devoted simply to re-equipping old planes. An illustration of this was an apparent 'moratorium' in number of new airframes being produced, as shown in Figure 5 below. H G Wells, The War in the Air (London, 1908), p. 130. 5 Cited in Uri Bailer, The Shadow of the Bomber: The Fear of Air Attack and British Politics (London, 1980), p. 14. However, a glaring limitation to the data is again the absence of plane types and their contribution towards the overall force mix and objectives. Focusing now on the period 1934 to 1938, before the financial picture would be dramatically changed by war, Figure 6 below illustrates that these years were nonetheless marked by a massive increase in air service expenditure. Appendix 2 indicates that the Technical and Warlike Stores indicator rose by a staggering 337 per cent, whilst two major supplementary estimates were added in 1935 and 1936, amounting to £17,000,000. The source shows Britain dedicating greater spending to research (99 per cent), which may go some way in explaining the relative superiority of British planes by 1939. The prima facie backdrop to the data is the failure of air disarmament and the révanchiste position of Germany. Indeed, the British Foreign Secretary opined in 1935, 'the verb to disarm should be classified by grammarians as a defective verb'. However, these two reasons represent the mere surface disturbance and crests of foam behind the policy of increased air expenditure, and do not engage with policymaker's more directed mentalités. To understand this, we need to examine the type of planes being produced. Cited in Uri Bailer, The Shadow of the Bomber: The Fear of Air Attack and British Politics (London, 1980), p. 9. The various air schemes (Appendix 3) developed and amended from 1933 onwards, outlining the types and numbers of planes required for the front line by a certain date, provide the key to unlocking the deeper roots associated with air policy in this period. Figure 7 below, produced from this Appendix, shows that the Britain was increasingly dedicating her resources to bombers. With the collapse of air disarmament talks, following the ignominious end to the Geneva Conference, policymakers sought a new means to prevent the knock out blow. It was axiomatic to air strategy that a commitment to building a bomber force would act as a deterrent against German air attack, presaging the Cold War concept of Mutually Assured Destruction. Increasing the number of bombers from 476 in 1934 to 1589 by 1937, as shown in the Appendix, worked under the belief that 'qui desiderat pacem, praeparet bellum'. On a diplomatic level, it was felt that a bomber force might bring Germany back to the Conference table. On a political level, it allayed public fears about a lack of preparation for air attack. The data also reveal a proportionally sharp increase of bomber requirements from February 1936 (see Figure 7) onwards. This can be seen as an equalising knee-jerk response to Hitler's alarming 1935 parity-achievement claim, which 'set the Nazi cat squarely among the democratic pigeons'. One limitation to our source, however, is that it does not show the number of bombers required in a reserve capacity to sustain operations. This would distinguish absolutely how far Britain worked under the strategy of shop window deterrence. Nevertheless, this source illustrates that air strategy was doing far more than aimlessly stirring Britain from her slumbers as Figure 6 suggests, and was in fact, in its dedication to bombers, a piece of political conjuring to hypnotise its audience. Cited in John Terraine, A Time for Courage: The RAF in the European War 1939-1945 (New York, 1985), p. 3 8 Denis Richards, The Royal Air Force Volume 1: The Fight at Odds (London, 1974), p. 12. The next area of our paper will examine the immediate pre-war years. Firstly, let us look at the data given in an Air Ministry report, as indicated in Figure 8 below, on the production of airframes between August 1938 and November 1939. It is clear that, from January 1939, airframe manufacturers started to exceed their production promises. These promises would have been established with policymakers in the deterrence by parity years. Although not explicit in the chart, we can draw three possible conclusions from this. The surpassing of airframe guarantees can almost incontestably be seen as the final nail in the coffin for the parity strategy, as the government encouraged air manufacturers to move towards a war footing. Not only had hopes of an air convention been lost but also the promises of Munich seemed long forgotten. Britannia's trident was now a bayonet and her shield a gas mask. Less probably, it could reflect that manufacturers had simply become more efficient in their production, no longer cruising at barely economical speed. More sinisterly, the data could suggest production profiteering, as captured in the illustration below (albeit in 1935). Ultimately, however, the source is best at confirming the end of air parity, than it is at hinting at more polemical speculations. However, these delivery numbers fail to paint the whole picture. We need to examine the types of planes produced. Figure 9, working on a microcosmic level of production, shows the number of Spitfires delivered from the Eastleigh Production Facility. It is quite apparent that a significant rise in production occurred from October 1938 onwards. However, this source is fundamentally limited in understanding air strategy, as Spitfires were neither the archetypal fighter in 1938-9 nor accounted for the bulk of the fighter force. Hurricane production is also required, in order to obtain a true picture of any significant change in production, and ergo air strategy. The difference in orders was 8647 more Hurricanes (Appendix 4). Importantly, Figure 10 below shows that from June 1938 the number of all fighters being delivered rose feverishly. This supports existing discourse, which argues that policymakers from early 1938 onwards increased fighter production for air defence purposes. A 1938 Command Paper reflects this, lamenting the failure of the bomber deterrence strategy, and stating 'taking risks for peace has not removed the dangers of war'. Major technological improvements with fighter capability, coupled with the coming of radar, presented the Command Paper, Statement Relating to Defence: Presented by the Primeminister to Parliament 3 March 1938 (London, 1938), p. 4. possibility of withstanding the knock out blow. This in turn threatened Germany with the prospect of a long war, which was arguably a more credible deterrent than simply a bomber build up, given the greater long term resources of the Empire. However, this Figure has a limitation of its own, because it does not tell us the relative fighter position vis-a-vis the wider picture of total aircraft production and in particular that of bombers. To combat these problems, we can refer to the data provided in the Cabinet approved air schemes L and M (Appendix 3). The two pie charts below comprising Figure 11 compare the required aircraft by proportion of all total planes in April 1938 and November 1938. It is immediately apparent that only a small shift away from bombers towards fighters occurred. This is by no means supportive of existing discourse. Literature on 1930's air power recognises the shift but argues that it was much more pronounced, with production geared three to one in favour of fighters. However, Figure 11 indicates that, in receiving well over 40 per cent of total aircraft production, the bomber remained the main player in air strategy, notwithstanding a greater emphasis to fighters. Thus, the pie charts provide an empirically tested challenge to the current epistemological understanding of the period. The final part of this paper will question the common usage by contemporary historians of simple comparative air power strength charts, as exemplified by Figure 12 below. Use of such charts to critique Britain's air strategy vis-a-vis other nations to illustrate Britain's 'dreadful note of preparation' is potentially too shallow. Although not wishing to be overly judgmental - this would require an exhaustive analysis far beyond the scope of this paper - it is essential that we lay bare certain qualifications the air historian needs to comprehend if he or she wishes to use such charts as an empirical authority. It is difficult to compare aircraft production in basic numerical terms, as the quality of aircraft varied greatly from country to country and year to year and was driven by different operational demands. Furthermore the numbers include Denis Richards, The Royal Air Force Volume 1: The Fight at Odds (London, 1974), p. 12. more than front line types (for example, Germany devoted far larger amounts of production than Britain to trainers), and they give us no insight into how many of the aircraft delivered were immediately deployable, through pilot availability and other factors. Also ignored in assessing proportional difference in air strength is the contribution of anti-aircraft defences. The contemporary historian, therefore, needs to take these sensitivities in his metaphorical knapsack when he enters the archives. To conclude, it has been the intention of this paper to work with primary and secondary statistical source material obtained from the National Archives, the RAF Museum and existing literature, in an effort to see what estimate and production data reveal about air strategy in the 1920s and 1930s. Much of the evidence has converged with existing historiographical discourse, outlining a gradual maturity of air strategy, extending from labouring within the parameters of a restricted budget through to the mantra of home defence. But some of the recently researched material sheds new light on the subject. For example, our data on fighter requirements refutes existing understanding that air strategy moved absolutely from bombers to fighters in the years 1938 to 1939. We have also suggested that certain sources, particularly comparative charts, remain too simplistic in composition to hold any more than mere subjective value. Furthermore, the usefulness of solely Air Ministry derived sources, such as the proposed peace strength chart, are somewhat sullied because their technical estimates were shaped by non-financial predilections and bias. Although ultimately recognising, as Clifford Geertz laments 'what we call our data are really our own constructions of other people's constructions', the charts here, despite certain limitations, not only move our understanding of air strategy slowly towards a more definitive picture but also hint at possible areas for further in-depth study. Indeed, there remains a vast sum of non-researched information relating to air strategy in the 1920s and 1930s waiting for the historian in the archives. Arguably, what our fighter discovery shows above all is that literature on air strategy in the 1920s and 1930s has itself become a too entrenched 'matter of faith'. It is thus the job of the air historian to revise further the subject, particularly the years 1938 to 1939, in greater detail. 11 Clifford Geertz, The Interpretation of Cultures (New York, 1973), p. 9.","For too long the history of the Cold War has been written from a 'top-down' perspective examining elites and decision-makers. Whilst political scientists have become mired in using rational International Relations models, historians have rarely cast their net beyond realpolitik, frequently falling foul of detailed narrative around contingencies which often never occurred. The fall of the Berlin Wall in 1989 and the end of the 30-year rule for many documents in the West, which both led to a spate of archival-based sources available to historians, cemented this diplomatic and empirical approach. Moreover, archival work alone is a dangerous place from which to survey the Cold War, since both sides of the Iron Curtain employed 'newspeak' rather than rational discourse, even behind closed doors. In short, this 'New Cold War history' can become overly narrative driven and blind to broader analysis. However, as a result of the growing interest in cultural history following the 1980s 'linguistic turn', recent study, particularly in the so called 'Constructivist' school, has challenged Cold War historians to spread their wings methodologically, and explore the culture of the conflict. In spite of noteworthy 'hot' episodes, such as Vietnam, the Cold War was in most part a conflict of words and images delivered to an audience soaked in its products. One unmined area of Cold War culture is Hollywood Bomb Cinema. Bomb Cinema denotes films where the nuclear bomb is an explicit part of the theme or narrative and includes a body of films, widely forgotten, from classics such as Dr Strangelove (1963) to off-beat cult favourites like Them (1954). However, Bomb Cinema remains predominantly the preserve of film departments and media journalists who examine issues such as plot synopsis and entertainment value. Working under Freud's avowal that a cigar is sometimes only a cigar, their inquiry is devoid of any attempt to unearth hidden realities. Apocryphal moments are not examined for anything more than meaningless and pleasurable fantasy. For example, in reviewing Strangelove, The Times film critic simply called it a 'farcical comedy about the end of the world' and paid most attention to the 'remarkable performances by Mr Peter Sellers and Mr George. C. Scott'. Dr Strangelove will be referred to as  Strangelove from this point onwards. Film Critic, 'Film Comedy about the End of the World', The Times, 30 January 1964, p. 16. So why should historians concern themselves about films with images of the bomb? Put simply, the sheer number of these films reveal how deeply concerned Americans were about the bomb. Paul Boyer has argued that the bomb itself is a virtual Kantian category - an internal filter - that shapes our very understanding of the world. Taking a psychoanalytic perspective, the bomb is indicative of a repressed dread in US society, concerned about survival, morality, and rebirth. Thinking about the function of these films, Hayden White argues that historical and fictional discourses have common aspects in narratives and transmit messages about shared reality. Accordingly, at any given historical point, there exists a common pool of narratives that every culture disposes for its members who might wish to draw upon them for the transmission of messages. As a statistically proven part of the filmgoer's diet - indeed The Times reported in February 1964 that 'demand to see Strangelove has been so great...it has been decided to put on special late night showings' - Bomb Cinema is one amongst many narratives available to society in order to understand the bomb. Certainly, from an early stage, the significance of the intersection between the cinema and the bomb was recognised by policymakers. For example, the Supreme Command Allied Powers (SCAP) censored references to Hiroshima and Nagasaki in films made during the occupation of Japan. Jacques Derrida, in his work on the 'nuclear subject', has argued that since the terrifying reality of nuclear war can only be the signified referent and never the real referent of a discourse, the view of fictitious projects, such as Bomb Cinema, occupy a space equal to the views of 'experts'. Furthermore, in its satirical fashioning or 'black comedy', as apparent in Strangelove, Bomb Cinema is increasingly about the workings of power. By giving a privileged opportunity to laugh at those in authority, Bomb Cinema can afford the individual the bitter-sweet illusion of panoptical power on the inside, by stripping away his real Cold War position of powerlessness on the outside. Seen in this way, the term subversion is synonymous with liberated power for the individual. Paul Boyer, By the Bomb's Early Light: American Thought and Culture at the Dawn of the Atomic Age (Raleigh, 1994). Hayden White, The Content of the Form: Narrative Discourse and Historical Representation (Baltimore, 1990). Own Correspondent, 'Late Night Showings at Columbia Cinema', The Times, 10 October 1963, p.6. Nicholas Royle, Jacques Derrida (Oxford, 2003). By deconstructing key bomb films, it will be argued that by 1963 and  Strangelove, Bomb Cinema had become an agent of subversion. The state sponsored Strategic Air Command 1955 (SAC), which projected a halcyon era of acquiescence and all-pervasive Cold War conformity, stands as the archetypal propaganda film for point of comparison. Strangelove exploded absolutely the state led propaganda line. However, far from heralding the first cataclysmic step outside of the propaganda paradigm, Strangelove is best seen as an umbrella for earlier subversive messages emerging in certain 1950s bomb films. Working under the constant eye of the House Un-American Activities Committee (HUAC), the distinctly science fiction wing of Bomb Cinema used covert images to depict the anxieties that accompanied life with the bomb. Indeed, Stanley Kubrick was an avid watcher of 1950s SF, and Strangelove can be seen as consciously linking disparate aspects of a genre. Part of the so-called 'Cold War camp', SF films like The Thing (1951), The Day The Earth Stood Still (1951), Them, Forbidden Planet (1956) and The Incredible Shrinking Man (1957) were intentionally self-parodic, utilising 'grand guignol' horror techniques of invasions of predatory insects and plants, in order to appeal to a generation of rebels without a cause, plagued by fears of isolation. Yet, as Peter Biskind recognises, these films were very rarely in universal agreement and did have varying degrees of departure from the propaganda paradigm. My methodology is interdisciplinary and borrows from the recent Cold War Cultures School in history, including my supervisor, Dr. Patrick Major, requiring the socio-political contextualisation of films, as well as the production and reception history. Thus, this paper uses not only the films themselves but also draft scripts, biographies and newspaper reviews and articles of the period. The issues examined here - nuclear proliferation, Organisation Man, scientists and technology - are the most prevalent in Bomb films. For brevity, I will refer to Science Fiction as SF from now onwards. Peter Biskind, Seeing is Believing: How Hollywood Taught us to Stop Worrying and Love the Fifties (New York, 1983). Nuclear proliferation is the most obvious theme in bomb films. In our period, the nuclear powers exponentially increased their arsenals of destructive potential, within the prevailing and public strategy of Mutually Assured Destruction (MAD). Ex-US Defense Secretary (1961-68), and architect of MAD, Robert McNamara later called proliferation 'the foundation of stable deterrence in a nuclear world...resting on the understanding that if either side initiates the use of nuclear weapons, the other side will respond with sufficient power to inflict unacceptable damage'. World War II laid a precedent for the state taking a manipulative role in fashioning Hollywood films. SAC - aided in its making by the US Air Force, who allowed the B-36 bomber to be filmed - defends the arms race completely. It depicts the bomber force as a safeguard to protect Americans from the bomb. Real SAC imagery is always at work, such as the billboard sign at the base entrance, 'the nation and your security are at stake', and the Command's motto, 'Peace is our Profession'. The dialogue supports this argument too with Rusty asserting 'we're the only thing that keeps the peace'. In the hands of trustworthy fellows, such as Dutch, nuclear weapons are seen as banal and manageable. However, the main function of the film was to ingrain in citizens the moral resolve needed to live in the Cold War. Security officials knew it would have been impossible to protect Americans from nuclear attack. For example, Millard Caldwell - head of the Federal Civil Defense Administration 1950-52 - believed that 'when a situation is utterly hopeless, it may be better not to say so. The doctors often conclude against telling the hopeless cancer patient what the situation is'. By cultivating an image of safety, SAC hoped not only to curb the problem of panic, the invisible fifth column at home, but to ensure continued support for the policy of deterrence. It is worth remembering that, at the time of the film's release, the Alert America campaigns were in full swing, and the gospel of self-help was the new mantra of civil defence planners. By showing reluctant cold warriors, and their even more reluctant wives, subordinating their civilian lives to the demands of the state, SAC urges civilians both to accept the secrecy of the Military Industrial Complex (MIC) and even buy into it. Robert McNamara, 'CNN Interactive: Episode 12 MAD. Interview with Robert McNamara', 1995. URL  (1 December 2003). Strategic Air Command (Anthony Mann, 1955). Cited in Guy Oakes, The Imaginary War: Civil Defence and American Cold War Culture (Oxford, 1994), p. 147. Subversive bomb cinema exploded the propaganda line of the security of American defence. The Day the Earth Stood Still challenged the safety of placing the world under the nuclear sword of Damocles. Sending a clear signal about the illusory security of America, the alien spaceship lands in Washington, despite being monitored by security forces, and proves immune to human weaponry. Indeed, Cold War power is shown in the most microcosmic terms when the alien Klaatu makes the earth stand still, and declares that 'at the first sign of violence, your earth will be reduced to a burned out cinder'. The Day the Earth Stood Still (Robert Wise, 1951). Released in 1951, the film presents a direct challenge to the pursuit of the fusion bomb, which, when developed, would make nuclear holocaust possible. This message on arms control is poignantly captured when Klaatu charges 'live in peace or face obliteration'. Moreover, it is worth noting that Klaatu threatens to destroy the entire Earth, in spite of proliferation clearly emerging from the nationalism of the US State Department. The arms race is thus presented as an international problem which the United Nations must solve. The Thing, despite an over-arching propagandist rubric in which the Russians are suspected of wrongdoing, contains a subversive message on proliferation. The film ends on the thought that American power could not prevent annihilation, with Scotty lamenting 'I bring you warning... Watch the skies. Everywhere... Keep watching the skies'. Ibid. The Thing (Christopher Nyby and Howard Hanks, 1951). Them also looks despairingly at proliferation and examines the harmful effects of radioactive fallout; an issue particularly prevalent at the time of the film's release given the alarming implications of the Bikini test shots and the 1954 Lucky Dragon incident. According to Allan Winkler, fallout focused public fears because it threatened a less violent form of extinction and presented the danger of both psychological, as well as physical deformity. Learning that the radioactive man-eating ants crawled out of a New Mexico test site, Them rejects the safety of atmospheric testing, and the ability of statesmen to keep a lid on the nuclear Pandora's box. Indeed, one character questions 'if these monsters got started as a result of the first atomic bomb in 1945, what about all the others that have been exploded since then?' Them reflects the anxiety with the all or nothing policy of MAD. By showing the military unable to use its major weapons against the ants, since it was fighting on American soil, Them champions the search for a flexible and limited response policy. Yet Them maintains certain propaganda messages. Like The Thing, the monsters act as a metaphor for Communists. When Dr Medford laments, in clearly Biblical terms, 'and there shall be destruction... and the Beasts shall reign over the earth', the likeness is just discernible. Despite casting a shadow over the defence establishment's blithe acceptance of the arms race, in contrast with The Thing, Them maintains the legitimacy of state authority, illustrating the measure of divergence within SF. Loudspeakers advise that 'your personal safety...depend(s) on your full co-operation with the military authorities'. Reconciling audiences to the knowledge that the Military Scientific Complex (MSC) was there to protect them, even if the threat was produced by it in the first instance, the ants are defeated by the MSC. Allan Winkler, Life Under a Cloud: American Anxiety About the Atom (New York, 1993), p. 108. 15 Them (Gordon Douglas, 1954). Ibid. Ibid. The black comedy ending of Strangelove, with its mushroom clouds and musical serenade of 'We'll meet again', tapped the belief that the superpowers were so caught up in the arms race that nuclear holocaust had become inevitable. For Kubrick, the logical result of Soviets 'lofting satellites as easily as Americans dunked baskets' and bomb shelters proliferating like greenhouses, was nuclear Armageddon. This ending would have appeared more real given the recent Cuban Missile Crisis and brinksmanship in practice. Cynically rejecting the safeguards projected in SAC, we witness a pitched battle between US contingents at Burpleson Air Force Base, under the 'Peace is our Profession' billboard. When General Turgidson argues for 'catching them with their pants down', in a pre-emptive first strike, Strangelove questions the willingness of policymakers to stay within the confines of MAD. Pre-emptive first strike formulations, and its idea that a massive nuclear attack could swiftly paralyse the enemy by destroying its retaliatory forces, had been growing in the light of technological advances made with Anti-Ballistic Missiles and the stockpiling of US nuclear weapons. Turgidson's insane proposal is no different from the calls by Air Force chief General Curtis LeMay to give them 'Sunday punch'. Recent findings support Kubrick's observation on the instability of the arms race. We now know that the military had developed the 'Single Integrated Operations Plan', which worked on the assumption that America would have been willing to be the first to use nuclear weapons. This replaced Operation DROPSHOT, written in 1947, which assumed a long period of conventional war between NATO and the Soviet Union before any nuclear weapons would be employed by both sides. Biskind, Seeing is Believing, p.337. Dr Strangelove (Stanley Kubrick, 1963). Cited in Sven Lindquist, A History of Bombing (London, 2001), Section. 312. Military critics derided Strangelove for its inaccuracy and, unlike SAC, the Air Force did not co-operate in the making of the film, refusing Kubrick permission to film with the B-52 bomber. Indeed, Kubrick famously created the interior of his B-52 based on pictures in an old flight manual. There was also a forced Air Force disclaimer that 'safeguards would prevent the occurrence of events depicted in this film', as the military had always maintained that no attack could be launched without 'civilian authority'. However, exposing one of the Cold War's deepest secrets, Bill Barr of Washington's National Security Archive states that 'top military commanders had presidential authorised instructions providing advance authority to use nuclear weapons under specified conditions'. Thus, General Ripper's exploitation of a putative retaliatory safeguard, which allows him to launch a nuclear strike, suggests that Kubrick's anxiety in the wisdom of the arms race was not unfounded. Indeed, LeMay was known for goading the Soviet Union with unauthorised reconnaissance flights. By 1964, missions had resulted in 26 planes being shot down over the Soviet Union. Strangelove thus shows that the delicate nuclear balance could all too easily be disturbed. Dr Strangelove (Stanley Kubrick, 1963). 22 Cited in Paul Lashmar, 'Dr Strangelove's Secrets: Terrifying Truths about the Cold War Period are Leaking out from Archives Around the World', Independent, 8 September 1998. 23 Cited in Bruce Franklin, War Stars: The Superweapon and the American Imagination (Oxford, 1988), p. 185. Beyond the proliferation issue, there were other recurring themes in Bomb Cinema. The next of these is the impact of the organization on man. In The Organization Man (1956), William Whyte observed that 'man exists as a unit of society. Of himself, he isolated, meaningless...by sublimating himself in the group, he helps produce a whole that is greater than the sum of its parts'. Whyte showed that under late capitalism, every aspect of life had become regimented and scheduled. SAC supports this triumph of conformity. Even the role of husband and father has to take a back seat to the organization, as Dutch declares 'we can't schedule the flights according to the birth schedule'. The Air Force is the natural habitat for the organization man, with General Hawkes educating Dutch about the importance of the group. When Dutch falls prey on mission to an injury he has failed to disclose, the crew takes over and saves the day. He is subsequently denounced for jeopardising the lives of his team-mates and the health of the organization. Echoing Whyte's comment that 'the man of the future is not the individualist but the man who works through others for others', SAC shows that the day of the daring pilot striving for personal glory - an image built up with maverick stereotypes of World War II pilots - had been replaced by the corporate '9 to 5' aviator. William Whyte, The Organization Man (New York, 1956), p. 12. 25 Strategic Air Command (Anthony Mann, 1955). 26 Whyte, The Organization Man, p. 22. However, most subversive bomb cinema showed that the growing regimentation of corporate culture made man bereft of any genuine individual identity and sense of accomplishment. The most pertinent statement in  The Incredible Shrinking Man is the lead character's existential condition. Living in suburbia, 'the dormitory of the new generation of organization men', and working as a reluctant organization man in his brother's firm, Robert Carey finds little achievement in his life. His shrinking is thus symbolic of a broader personal turmoil. In his ritual killing of the spider, Carey loses his fear of alienation from the group, experiencing a psychological transformation. In revealing, 'and then I meant something too... To God there is no zero', Carey asks the audience to embrace his liberation. The Incredible Shrinking Man is thus a covert attack on the psychological ramifications of organization fetishism. In addition, the film shows that the famous sentence in the Communist Manifesto stating that the proletariat has nothing to lose but its chains has little relevance for the organization man. Robert Carey has something to lose conjecturally - his standard of living. Carey represents a wider metonym for contemporary American middle-class families who, having run up large debts, became trapped in the net of the bourgeois milieu. Whyte, The Organization Man, p. 14. 29 The Incredible Shrinking Man (Jack Arnold, 1957). 30 The Thing (Christopher Nyby and Howard Hanks, 1951). The Thing, in its depiction of Air Force Captain Pat Hendry, directs its subversive message more towards the everyday practicalities of organization man. Asserting 'I'm not working for the world. I'm working for the Air Force', Hendry begins as the archetypal wheel-in-machine conformist, who cannot blow his nose without clearing it first with Headquarters. The Thing serves as a critique of the absurdity of 'going by the book'. For example, using standard procedure to free an alien ship, Hendry accidentally blows it up. Hendry is actually immobilised by bureaucracy, stating 'until I receive my instructions from my superior officers, we'll have to mark time'. Hendry's idleness testifies to Whyte's argument, that 'in further institutionalising the great power of the majority, we are making the individual distrust himself'. When orders arrive to avoid injuring the monster, he has to break free of the parameters of the organization and kill it, in order to stay alive. However, as we have shown, SF films were anything but unanimous in agreement. Them, in contrast to The Thing, was not subversive in this regard. It argues that society should embrace the organization man culture and posits that to leave the group is to be beyond the safe parameters of society. All the victims succumb when they leave the confines of the group. Furthermore, it takes a consortium of scientists, police and soldiers to defeat the ants. Them is indicative that the line of demarcation between films of propaganda and subversion can often be blurred. The Thing (Christopher Nyby and Howard Hanks, 1951). Whyte, The Organization Man, p. 59. The role of scientists manifests itself in Bomb Cinema. Social critic Bertrand Russell argued that 'we are perhaps living in the last age of man, and, if so, it is to science that we will owe this extinction'. In Russell's eyes, scientists had become Devil Gods, violating the delicate boundary between conquering nature for the benefit of mankind and conquering nature at the expense of man's survival. In its portrayal of the nefarious Dr Carrington, The Thing shows the scientist twisting the dragon's tail. Admiring the creature, Carrington helps it reproduce itself, and interferes with attempts on its life. The scientist inherent in Carrington is the extremist head-over-heart zealot, assiduously pursuing his parochial interests and immune to the fact that he might, in the process, be fomenting Armageddon. For example, Carrington's morally blind quest for advancement is made explicitly clear when he states 'knowledge is more important than life'. Indeed, Carrington represents a wider public concern, following the defeat of scientific internationalism and its idea that knowledge should be universally shared, that science was working increasingly behind the closed doors of the MIC. These fears were particularly acute at the time of the film's release, following the public disclosure of the secret Hydrogen bomb project. Cited in Robert Jungk, Brighter than a Thousand Suns: A Personal History of the Atomic Scientists (1956), p. 70. The Thing (Christopher Nyby and Howard Hanks, 1951). Scientists, however, were both reviled and revered. In Flash Effect, David Tietge argues that in a campaign to promote science in American consciousness, many scientists were symbolised as both father figures and as saviours, working in an environment that celebrated the technological objects of their labour. Certain SF bomb films portray the benevolent and wise scientist. In contrast to Carrington, Dr Medford, in Them, is presented as an avuncular intellectual. David Tietge, Flash Effect: Science and the Rhetorical Origins of Cold War America (Ohio, 2002). In his closing statement, 'We've opened the door into another world. What we'll eventually find nobody can predict', Medford echoes the concerns of Robert Oppenheimer, and recognises the dangers of using the test tube to tamper with nature. Echoing Eisenhower's 'Atoms for Peace' programme and his imminent formation of The 'Technological Capabilities Panel', Them embraces the idea that in a nuclear age, science and politics should no longer inhabit separate spheres. We see scientists working at the kernel of government policy. Medford lectures public officials, commands the resources of the state and devises the plan to destroy the menace. Indeed, Medford sees his elevated role as a 'scientist's dream'. The cachet of science becomes so strong it distorts the traditional hierarchy of gender roles. When his daughter insists on joining the men in a dangerous trip, Agent Graham remarks 'it's no place for you or any other woman'. However, she demands that he treat her as a scientist first and a woman second, asserting 'somebody with scientific knowledge, a trained observer, has to go'. Released in the wake of the arrest of the British physicist, Klaus Fuchs, for communicating atomic research to the Russians, and the subsequent executions of Julius and Ethel Rosenberg in June 1953 for their involvement in the spy ring, Them attempts to rehabilitate the image of the good and trustworthy scientist. Them (Gordon Douglas, 1954) Ibid. Ibid. Ibid. During his terms in office, Eisenhower did more than any previous President to integrate science into the state. Yet, in his farewell address he exhibited more than a powerful note of irony, warning that 'we must be alert to the...danger that policy could become the captive of a scientific-technological elite'. Strangelove shares Eisenhower's concern. As the premier authority on the Doomsday Machine, Dr Strangelove - struggling with his 'Heil-Hitler' hand and speaking in a terse German accent, which denotes his Nazi affiliations - is devoid of love for anything except the bomb. Despite his mental deterioration, the leadership cede all power to him by the end, as they are completely distracted by his plans to continue the human race. Dr Strangelove can be seen as a parody of the Hydrogen Bomb scientist Dr Edward Teller. Repudiated by many of his scientific colleagues for testifying against Robert Oppenheimer, who called for nuclear restraint and internationalism, Teller ran with a more military crowd, becoming the darling of conservative thinkers for his advocacy of American scientific supremacy. John McAdams, 'Dwight Eisenhower :Farewell Address to the Nation 17 January 1961', 1995. (1 December 2003). 41 Strategic Air Command (Anthony Mann, 1955) The final area of this paper addresses a theme unique to the films of subversion. All subversive films involve some measure of destruction and they question how this is allowed to happen. On Judgement Day who, or what, is at fault; the nature of man and his failings to control events or unrestrained technology growth? Witnessing no destruction, SAC sits outside of this question, reflective of the fact that the Air Force aided making of the film. The characters exhibit no militant anticommunist hysteria, preferring not even to name the enemy, and simply refer to him as 'the other fella'. Although recognising the contemporary problem of peacetime mobilisation for war - indeed Dutch states 'there is a new kind of war', one where 'we've got to stay ready to fight without fighting' - the men are portrayed as hard-working, reluctant cold warriors, charged with a mission others neither admired nor would accept. Unlike the characters in Strangelove, who flaunt the bomb aggressively, the dedicated public servants of SAC present man as no apocalyptic threat and consider the bomb only as the gun behind the door. Ibid. The Day the Earth Stood Still presents man as the threat. This supports Stanley Kauffman's argument that 'ban the bomb and they'll find another way. The real Doomsday machine is men'. The film suggests that Cold War suspicion in America had overwhelmed all reason. Unable to attain an audience with the world's leaders, given the bipolar tensions of the time, Klaatu complains to the President, 'I'm not concerned...with the internal affairs of your planet...my people have learned to live without [such stupidity]'. The President responds 'I'm afraid my people haven't'. When the peaceful Klaatu escapes from prison, the entire nation is unrelenting in its bellicosity towards him. One man calls him 'a wild animal'. The film illustrates man's dangerous lack of control as exemplified when Klaatu is shot by a trigger-happy soldier while delivering the greeting 'we have come to visit you in peace'. Moreover, the film suggests that, in the right hands, technology is far from an apocalyptic threat but instead a possible path to utopia. Learning that the alien spacecraft is nuclear, Bobby exposes man's ignorance by stating 'I thought that was only for bombs'. The film thus echoes Einstein's statement that 'the atomic bomb has changed everything except the nature of man'. Stanley Kauffman, 'Dean Swift in the Twentieth Century', The New Republic, 150:5 (1964), p. 24. The Day the Earth Stood Still (Robert Wise, 1951) 45 Ibid. 46 Ibid. 47 Ibid. 48 Ibid. Cited in Margot Henriksen, Dr Strangelove's America: Society and Culture in the Atomic Age (Berkeley, 1997), p. xvi. 50 The Forbidden Planet (Fred Wilcox, 1956). 51 Cited in Franklin, War Stars, p. 4. The custodians of the bomb are dealt a particularly vicious blow in subversive Bomb Cinema. Driven by virulent anti-communism, the American leadership is shown as nothing more than a group of modern Prometheans, morally blind to the implications of military conflagration. Strangelove harnesses the idea by admitting that nuclear war is absurd, but understands that its absurdity only makes it more likely because of the world's propensity for producing psychotic individuals and elevating them into positions of power. Supposedly working for the 'good' of the people, these men are presented as captivated by deadly technology and oblivious to the destructive nature of their policies. It is Ripper who orchestrates the attack and Turgidson's remark, 'the human element seems to have failed us here', clearly illustrates the chasm between man's scientific skill and his political ineptitude. Major Kong serves as the perfect metaphor for the failure of man to recognise how nuclear weapons have fundamentally changed the nature of war. In his pep talk - 'the folks back home is a-countin' on you and, by golly, we ain't about to let-em down' - Kong is blind to the severity of the task he is being asked to do. Wearing a cowboy hat, which connects him to the frontier tradition and with the patriotic war song 'Johnny Comes Marching Home' providing music, Kong is presented as the dangerous short-sighted hero riding the bomb. Kubrick was not a lone voice in his negative treatment of the American leadership. For example, in his expressive Cold War jeremiad 'Gentleman: You are Mad', social commentator Lewis Mumford asserted that 'madmen govern our affairs in the name of security'. 52 Dr Strangelove (Stanley Kubrick, 1963). 53 Ibid. Another aspect of the 'madmen' character explored in Strangelove is the triumph of the radical right-wing cold warrior in America, exposed in the insanity of General Ripper. Echoing Senator Joseph McCarthy, who spoke of the 'communist with a razor blade poised over the jugular vein of this nation', Ripper manifests similar obsessive anti-communism by asserting, 'I can no longer sit back and allow...the international communist conspiracy to impurify all of our bodily fluids'. Ripper's willingness to annihilate the world because of fluoridation illustrates that the nature of man is at fault again. Cited in Henriksen, Dr Strangelove's America, p. 47.; Dr Strangelove (Stanley Kubrick, 1963). Civilians are equally culpable. President Muffley is presented as the ineffectual Liberal who is unable to halt the madness around him. By arguing with Premier Kissof over who is sorrier for imminent global holocaust, Muffley not only exhibits the preposterous nature of Cold War tensions but also appears to see the event as no more than a social faux pas. Although, 'relentlessly perceptive of human beings to the point of inhumanity', Strangelove is also tinged with technophobia. The Doomsday Machine, 'that rules out human meddling', signals the ultimate triumph of destructive technology. It is ironic that the machines of destruction, such as the Doomsday Machine, work with great efficiency, whilst more humane machines, such as telephones, are ineffectual. It is worth remembering that since 1945 delivery system advances had grown considerably, resulting in the land based missile, which is impervious to any recall signal. In part then, Strangelove shows that developing more sophisticated defence systems will lead to technology manipulating man and ultimately annihilation. Stanley Kauffman, 'Dean Swift in the Twentieth Century', The New Republic, 150:5 (1964), p. 24. 56 Dr Strangelove (Stanley Kubrick, 1963). In conclusion, Bomb Cinema between 1950 and 1964 revealed two cultural personalities in conflict. Under the surface of SAC, which sat at the high-table of state propaganda, there existed films subverting the portrayal of a benign nuclear age. These reached their climax with Strangelove. Lewis Mumford stated in 1964 that Strangelove represented 'the first break in the catatonic Cold War trance that had so long held our country in its rigid grip'. However, by deconstructing 1950s SF films, Strangelove is best seen as the culmination of creeping subversion, rather than first explosion.1950s SF films had the freedom for subversive statements, because they appeared so thoroughly removed from reality. Lost on Film Studies scholars and reviewers, these films, even in their most apocryphal moments, were brimming with surreptitious statements on the atomic age. In our efforts to excavate the hidden and the not so hidden, we have also proved that within SF there existed a network of competing ideologues and measures of departure from the overriding propaganda paradigm. Them, despite its abhorrence to proliferation, maintains the legitimacy of state power and support for the organization. Cited in Charles Maland, 'Dr Strangelove: A Nightmare Comedy and the Ideology of the Liberal Consensus', American Quarterly 31:5 (1979), p. 716. Yet, the importance of Strangelove cannot be underplayed. Arguably Strangelove tells us something about the constantly renegotiated nature of hegemonic apparatuses. Gramsci argued that culture is a non-coercive means whereby the ruling-classes maintain their dominance by securing the consent of subordinate groups. Moreover, hegemony operates by allowing mild concessions to groups who do not pose a threat to the framework of domination. Thus, it is not surprising that we find in certain 1950s SF bomb films moments of divergence from the dominant paradigm. Here is concession at work. What Strangelove does is reverse the dialectic to make the previous mild concessions the now prevailing hegemony and signifies the moment when previously unthreatening groups rose to prominence. It also signifies the decisive moment when audiences are no longer able to watch the nuclear subject straight. Kubrick originally had intended a straight adaptation of Peter George's novel Red Alert, but it dawned on him that the most truthful elements of scenes were absurd, and so he transformed the story into a nightmare comedy. Released shortly after Strangelove in 1964, the straight-laced Fail-Safe, positing a similar end of the world scenario, was a box office disaster. Strangelove's 'horrible grin' of black humour had become the only means by which society could learn to stop worrying and love the bomb.",False
36,"The intention of this paper is move outward from new and old statistical empirical evidence, found on the one hand in the National Archives (PRO) and the RAF Museum Archive, and on the other, in existing secondary literature, and ask what this reveals about British air strategy in the 1920s and 1930s. The methodology used is to take researched evidence, portray it in the form of charts, data bases or spreadsheets, and then use this IT generated information as the foundation for discussion. The archive material was not examined with any pre-conceived conclusions in mind. It will be our job to use all the empirical meat to create an interpretable abstract of air strategy. We will question whether these sources either perpetuate or shed new light on existing discourse. Along the road the sources will be challenged for their usefulness as historical pieces of evidence. As a point of entry into the mise-en-scène of air strategy in the 1920s, let us analyse a RAF proposed peace strength chart (Appendix 1), as laid down by the Chief of the Air Staff in a November 1918 paper. Figure 1 below, which is drawn from this Appendix, illustrates the proposed location of the squadrons. Most significantly, the Figure expounds the increasing importance placed by strategists on using air power in the empire, with 34 squadrons placed in colonial possessions. The Chief enthused that 'British possessions...are spread over a wide area and aircraft should prove of the greatest service'. Aircraft were seen as a form of control without occupation, providing a cheap and ubiquitous means of imperial policing. He also opined that 'on the conclusion of peace all available aircraft material should be utilised...in the Dominions and India'. Although this kind of commitment is not realised in the Figure, it shows that air strategists were starting to see aircraft within the typology of imperialism, as a possible means of lightening the white man's burden. However, the Figure is fundamentally flawed in understanding air strategy. It is only an aspiration of air strength in the coming 1920s and does not take into account the financial or procurement feasibility of its estimations. Moreover, in its rich dedication to the supposed war-winning bombers (shown at Figure 2 below), the source bears the fingerprints of the Air Staff's political motive to protect the service from the crossfire of its seniors and attempts to undermine its autonomy. The National Archives, AIR 8/6. Attached memorandum to Proposed Peace Strength by Chief of the Air Staff, November 1918 2Ibid Overall, the Figure is an idealist snapshot that ultimately is best at revealing Air Staff pressure on air strategy, captured in a accompanying remark by the Chief - 'we wish to turn our swords into ploughshare... but not so fast' - than it is in understanding what actually happened in the 1920s. Ibid It is now important to focus our attention on the true position of the RAF in the 1920s. We can do this through the lens of air service expenditure estimates at Appendix 2 and the resultant Figure 3 below. It is clear that the earlier optimism of the Air Staff proved ephemeral, as air service expenditure initially sharply fell, and then remained relatively austere. The Figure supports existing discourse about air strategy, which contends that the RAF just about 'held on' during its cradle days, in the face of ferocious economic retrenchment, the Ten-Year Rule and a zetheist that condemned recourse to conflict. The impact of the imposed permanent war planning blight, is particularly evident if we note the acute fall in Technical and Warlike Stores estimates in the Appendix - elements that constitute the kernel of aggressive air strength - from £6,172,850 in 1920 to £1,295,000 in 1922. It is also significant to note from that source that research was particularly barren throughout the period, falling from £2,575,540 in 1920 to £498,000 in 1929. This goes a long way in explaining the poor quality of aircraft the RAF possessed as late as 1934, still consisting of inchoate wooden biplanes. The data also reveal that air strategy was far from a blueprinted plan, and was in fact often pragmatic and respondent to changes in geopolitik. For example, the slight rise in expenditure in 1924 can be seen as a direct response to the perceived threat of the Armée de l'Air emerging from 1923. However, the data has limitations that prevent a challenge to the existing historiography. They give no indication of how many planes were actually produced or what important fields of operation fared better than others. How far was the empire really given wings as the Chief of the Air Staff had proposed in 1918? They also do not offer any information on the Army and Navy, against which to draw priority comparisons; and industrial capacity is unknown. Moving into the 1930s, let us start by examining the early years, before any significant expenditure rise occurred. Again, we will move our discussions from the empirical base of Appendix 2, as reproduced in Figure 4 below. The Figure shows that air service expenditure in the years 1930-34 also remained relatively low and stable; this can be seen as an indication of the government's first strategy to avert fears of the knock out blow, through the panacea of air disarmament. The knock out blow principle is important, as it weighs so heavily on air strategy throughout the 1930s. Founded in the futurological literature of HG Wells -' after the smashing of City Hall...the white flag had been hoisted' - and politically confirmed in Baldwin's famous dictum 'the bomber will always get through', the knock out blow endangered the heart of Empire for the first time. This anxiety was reinforced by the Great Depression, where it was felt that a major dislocation, such as war, would catastrophically weaken the durability of industrialised society. In the context of increasing demands to put Britain's financial house in order, and in preparation for the international Disarmament Conference in 1932, an air disarmament strategy was the diplomats preferred negotiating hand. Our sources support this policy. Funding of the important Technical and Warlike Stores element remained about the same between 1930 and 1934. A consequence of this was that production became devoted simply to re-equipping old planes. An illustration of this was an apparent 'moratorium' in number of new airframes being produced, as shown in Figure 5 below. H G Wells, The War in the Air (London, 1908), p. 130. 5 Cited in Uri Bailer, The Shadow of the Bomber: The Fear of Air Attack and British Politics (London, 1980), p. 14. However, a glaring limitation to the data is again the absence of plane types and their contribution towards the overall force mix and objectives. Focusing now on the period 1934 to 1938, before the financial picture would be dramatically changed by war, Figure 6 below illustrates that these years were nonetheless marked by a massive increase in air service expenditure. Appendix 2 indicates that the Technical and Warlike Stores indicator rose by a staggering 337 per cent, whilst two major supplementary estimates were added in 1935 and 1936, amounting to £17,000,000. The source shows Britain dedicating greater spending to research (99 per cent), which may go some way in explaining the relative superiority of British planes by 1939. The prima facie backdrop to the data is the failure of air disarmament and the révanchiste position of Germany. Indeed, the British Foreign Secretary opined in 1935, 'the verb to disarm should be classified by grammarians as a defective verb'. However, these two reasons represent the mere surface disturbance and crests of foam behind the policy of increased air expenditure, and do not engage with policymaker's more directed mentalités. To understand this, we need to examine the type of planes being produced. Cited in Uri Bailer, The Shadow of the Bomber: The Fear of Air Attack and British Politics (London, 1980), p. 9. The various air schemes (Appendix 3) developed and amended from 1933 onwards, outlining the types and numbers of planes required for the front line by a certain date, provide the key to unlocking the deeper roots associated with air policy in this period. Figure 7 below, produced from this Appendix, shows that the Britain was increasingly dedicating her resources to bombers. With the collapse of air disarmament talks, following the ignominious end to the Geneva Conference, policymakers sought a new means to prevent the knock out blow. It was axiomatic to air strategy that a commitment to building a bomber force would act as a deterrent against German air attack, presaging the Cold War concept of Mutually Assured Destruction. Increasing the number of bombers from 476 in 1934 to 1589 by 1937, as shown in the Appendix, worked under the belief that 'qui desiderat pacem, praeparet bellum'. On a diplomatic level, it was felt that a bomber force might bring Germany back to the Conference table. On a political level, it allayed public fears about a lack of preparation for air attack. The data also reveal a proportionally sharp increase of bomber requirements from February 1936 (see Figure 7) onwards. This can be seen as an equalising knee-jerk response to Hitler's alarming 1935 parity-achievement claim, which 'set the Nazi cat squarely among the democratic pigeons'. One limitation to our source, however, is that it does not show the number of bombers required in a reserve capacity to sustain operations. This would distinguish absolutely how far Britain worked under the strategy of shop window deterrence. Nevertheless, this source illustrates that air strategy was doing far more than aimlessly stirring Britain from her slumbers as Figure 6 suggests, and was in fact, in its dedication to bombers, a piece of political conjuring to hypnotise its audience. Cited in John Terraine, A Time for Courage: The RAF in the European War 1939-1945 (New York, 1985), p. 3 8 Denis Richards, The Royal Air Force Volume 1: The Fight at Odds (London, 1974), p. 12. The next area of our paper will examine the immediate pre-war years. Firstly, let us look at the data given in an Air Ministry report, as indicated in Figure 8 below, on the production of airframes between August 1938 and November 1939. It is clear that, from January 1939, airframe manufacturers started to exceed their production promises. These promises would have been established with policymakers in the deterrence by parity years. Although not explicit in the chart, we can draw three possible conclusions from this. The surpassing of airframe guarantees can almost incontestably be seen as the final nail in the coffin for the parity strategy, as the government encouraged air manufacturers to move towards a war footing. Not only had hopes of an air convention been lost but also the promises of Munich seemed long forgotten. Britannia's trident was now a bayonet and her shield a gas mask. Less probably, it could reflect that manufacturers had simply become more efficient in their production, no longer cruising at barely economical speed. More sinisterly, the data could suggest production profiteering, as captured in the illustration below (albeit in 1935). Ultimately, however, the source is best at confirming the end of air parity, than it is at hinting at more polemical speculations. However, these delivery numbers fail to paint the whole picture. We need to examine the types of planes produced. Figure 9, working on a microcosmic level of production, shows the number of Spitfires delivered from the Eastleigh Production Facility. It is quite apparent that a significant rise in production occurred from October 1938 onwards. However, this source is fundamentally limited in understanding air strategy, as Spitfires were neither the archetypal fighter in 1938-9 nor accounted for the bulk of the fighter force. Hurricane production is also required, in order to obtain a true picture of any significant change in production, and ergo air strategy. The difference in orders was 8647 more Hurricanes (Appendix 4). Importantly, Figure 10 below shows that from June 1938 the number of all fighters being delivered rose feverishly. This supports existing discourse, which argues that policymakers from early 1938 onwards increased fighter production for air defence purposes. A 1938 Command Paper reflects this, lamenting the failure of the bomber deterrence strategy, and stating 'taking risks for peace has not removed the dangers of war'. Major technological improvements with fighter capability, coupled with the coming of radar, presented the Command Paper, Statement Relating to Defence: Presented by the Primeminister to Parliament 3 March 1938 (London, 1938), p. 4. possibility of withstanding the knock out blow. This in turn threatened Germany with the prospect of a long war, which was arguably a more credible deterrent than simply a bomber build up, given the greater long term resources of the Empire. However, this Figure has a limitation of its own, because it does not tell us the relative fighter position vis-a-vis the wider picture of total aircraft production and in particular that of bombers. To combat these problems, we can refer to the data provided in the Cabinet approved air schemes L and M (Appendix 3). The two pie charts below comprising Figure 11 compare the required aircraft by proportion of all total planes in April 1938 and November 1938. It is immediately apparent that only a small shift away from bombers towards fighters occurred. This is by no means supportive of existing discourse. Literature on 1930's air power recognises the shift but argues that it was much more pronounced, with production geared three to one in favour of fighters. However, Figure 11 indicates that, in receiving well over 40 per cent of total aircraft production, the bomber remained the main player in air strategy, notwithstanding a greater emphasis to fighters. Thus, the pie charts provide an empirically tested challenge to the current epistemological understanding of the period. The final part of this paper will question the common usage by contemporary historians of simple comparative air power strength charts, as exemplified by Figure 12 below. Use of such charts to critique Britain's air strategy vis-a-vis other nations to illustrate Britain's 'dreadful note of preparation' is potentially too shallow. Although not wishing to be overly judgmental - this would require an exhaustive analysis far beyond the scope of this paper - it is essential that we lay bare certain qualifications the air historian needs to comprehend if he or she wishes to use such charts as an empirical authority. It is difficult to compare aircraft production in basic numerical terms, as the quality of aircraft varied greatly from country to country and year to year and was driven by different operational demands. Furthermore the numbers include Denis Richards, The Royal Air Force Volume 1: The Fight at Odds (London, 1974), p. 12. more than front line types (for example, Germany devoted far larger amounts of production than Britain to trainers), and they give us no insight into how many of the aircraft delivered were immediately deployable, through pilot availability and other factors. Also ignored in assessing proportional difference in air strength is the contribution of anti-aircraft defences. The contemporary historian, therefore, needs to take these sensitivities in his metaphorical knapsack when he enters the archives. To conclude, it has been the intention of this paper to work with primary and secondary statistical source material obtained from the National Archives, the RAF Museum and existing literature, in an effort to see what estimate and production data reveal about air strategy in the 1920s and 1930s. Much of the evidence has converged with existing historiographical discourse, outlining a gradual maturity of air strategy, extending from labouring within the parameters of a restricted budget through to the mantra of home defence. But some of the recently researched material sheds new light on the subject. For example, our data on fighter requirements refutes existing understanding that air strategy moved absolutely from bombers to fighters in the years 1938 to 1939. We have also suggested that certain sources, particularly comparative charts, remain too simplistic in composition to hold any more than mere subjective value. Furthermore, the usefulness of solely Air Ministry derived sources, such as the proposed peace strength chart, are somewhat sullied because their technical estimates were shaped by non-financial predilections and bias. Although ultimately recognising, as Clifford Geertz laments 'what we call our data are really our own constructions of other people's constructions', the charts here, despite certain limitations, not only move our understanding of air strategy slowly towards a more definitive picture but also hint at possible areas for further in-depth study. Indeed, there remains a vast sum of non-researched information relating to air strategy in the 1920s and 1930s waiting for the historian in the archives. Arguably, what our fighter discovery shows above all is that literature on air strategy in the 1920s and 1930s has itself become a too entrenched 'matter of faith'. It is thus the job of the air historian to revise further the subject, particularly the years 1938 to 1939, in greater detail. 11 Clifford Geertz, The Interpretation of Cultures (New York, 1973), p. 9.","The once fashionable belief that class analysis provides the key to history has increasingly come under attack. For many, both inside and outside of the Academy, class is no longer a viable way of looking at the world and examining history; it has become part of history itself. Jan Pakulski and Malcolm Waters assert that 'like beads and Che Guevera berets, class is passé', and they dedicate their work to 'confirm the good news that class has collapsed'. This paper examines the reasons behind the 'death of class' contention and offers an opinion on whether they are valid. Jan Pakulski and Malcolm Waters, The Death of Class (London, 1996), p. 1. Ibid, p. 7. Ibid, p. 1. Karl Marx's name is synonymous with the concept of class. He states that men were always constituted by the antagonistic class relations into whom they are cast 'and the history of all hitherto existing society is the history of class struggles'. This assertion was seen to not only have a capacity to unmask the structure of capitalist power, particularly in its inequitable and exploitative guises, but also to change the political terrain irrevocably. However, it is significant to note that he gives class no precise definition. Using his work, later Marxists developed what they saw as a duality to class. In an 'objective' sense, they spoke of a class 'in itself', which was no more than a form of collective identity. This grouped people together by their shared economic characteristics, principally in their relationship to the means of production. In a 'subjective' sense, there was a class 'for itself', which describes the process by which these aggregates become politically and consciously transformed, aware of themselves as a class. Others later took up the torch. Led by E P Thompson, the British Marxist historians present a history of class 'from the bottom up'. Thompson attacked the tendency of historians to see class as merely a structure or category and in doing so 'obscure the agency of the working people'. He began to recognise the prima facie relevance to other emerging social identities, such as feminism, thereby relinquishing Marx's economic 'determination in the last instance'. Despite this, to the Marxist, class remains primarily the anvil on which history was formed. Karl Marx and Friedrich Engels, The Communist Manifesto (London, 1967), p. 79. Cited in Harvey Kaye, The Education of Desire (New York, 1992), p.27. 6 J F Lyotard, The Postmodern Condition (Minnesota, 1984), p. 13. We now turn to the factors that, in some eyes, have made class irrelevant to historical analysis, starting with Postmodernism. With the collapse of the Soviet Union, defeat of organised labour, the fall of great Victorian staple industries, redundancy of Trade Unions and recrudescence of the New Right, scholars began to rethink the meaning of past verities. These changing dimensions had struck a fatal blow to the class politics of the Left, confirmed in the Thatcher electoral victories in 1979, 1983 and 1987. To Postmodernists, even the Miners' strike of the 1980s has more to do with the effect on community life, rather than traditional direct class struggle. Whereas for Marx, class was the essence of history, for the Thatcherite New Right, class was its perversion and they sought a 'classless society'. Postmodernists claim that the world today is no more than a global triumph of free market economies and pluralistic democracies, thereby bringing the old metanarratives, such as class, to their knees. Indeed, J F Lyotard opined that class had been 'blurred to the point of losing all radicality'. However, the license for the blanket repudiation of class on these grounds is weak. Firstly, Postmodernist theory misses the continued critical relevance of class to the events it describes. Class can be used to explain the collapse of the USSR. The New Right traditionally sees this as a triumphant consequence of transnational capitalism and an overburdening system of military expenditure. The corruption, bureaucratisation and nepotism of the Soviet ruling class, particularly in the excesses of the Brezhnev era, ensured the continued oppression and alienation of workers. Against Postmodernist critiques, this alienation of labour and the consequent role of the masses in overthrowing the system suggest the continued viability of class. Secondly, although recognising the New Right's desire to drive class off the agenda - Thatcher called it a 'communist concept' - they remained sensitive to its continued power. The New Right was simply adept at disguising the language of class within the ideological cover of national security, jingoism and honour. Furthermore, the Postmodernist concept of a 'classless society' inherent in the language of the New Right is misleading. The New Right used the term not to describe a society without class but as the goal of a meritocratic society providing opportunity for people to advance regardless of class. Thus, the New Right is in fact reinforcing the existence of classes that it has only linguistically abolished. Finally, the Postmodernist charge rests on a haphazard assumption that the world is totally equitable under conditions of market capitalism. However, in its latest manifestation, capitalism continues to distribute inequitably its product. There is a clustering of power in the hands of a small homogenous elite, whilst the people of the Third World increasingly live in conditions of chronic penury. Therefore, we need a class element, in order to explain the negative outcomes of capitalism's distributive rationalism. Cited in David Cannadine, Class in Britain (London, 1998), p. 2. The USSR collapse is seen by many as the fundamental nail in the coffin of Marxist class theories. Joyce believes that any mention of class automatically locks us into a set of discourses belonging only to discredited Stalinists and a failed socialist project. Class for Joyce is made solely of Soviet clay. This is indicative of a post-communist age, where all forms of socialism and class have been brought into question, and tarred with the same brush. However, class has long been pluralistic in form, and attacking only the most discredited type of class is only 'scoring easy polemical points'. Indeed, advocates of this approach are mounting simply an ideological, rather than empirically justifiable, charge. Although recognising that orthodox Marxist class theory, and socialism as the telos of history, does look a little dated, class is more fluid than this attack would have us believe. Geoffrey Eley and Keith Neild, 'Starting Over: the present, the Postmodern, and the moment of Social History', in Keith Jenkins (ed. ), The Postmodern History Reader (London, 1997), p. 372. Linked to the Postmodernist agenda, class has also come under scrutiny from political historians, such as Frank Parkin. They claim that Marx's triadic society of classes, particularly the two 'great hostile camps' of proletariat and bourgeoisie, outlined in the Communist Manifesto, were historically over-simplified abstractions and it is impossible to locate precisely the politically relevant line of cleavage. This argument has gained momentum in the light of recent political claims that 'we are all middle class'. The working class, severed from the symbolism of old labour movements and increasingly a white-collar composite, has been absorbed into the middle class. This is emblematic in John Prescott's opinion that he is 'anything else than middle class now' despite a working class background. However, in spite of this, class remains a valuable dividing line. Firstly, concentration on headline reductive aspects represents a perverted reading of Marxist class theory. In the Eighteenth Brumaire, Marx recognised the existence of other classes but simply felt that 'of all the classes that stand face to face with the bourgeoisie today, the proletariat alone is the really revolutionary class'. Secondly, it is possible to locate lines of cleavage in modern social structures, even if it means abandoning previous Marxist reductive forms. For example, the emergence of a new Super-Class, born in the financial world of London, and a new underclass, living in a permanent state of dependence, are indicative of a reconfigured class structure. Although it is arguably harder to identify, class remains a useful measurement of social hierarchies. Cited in George Jones, 'We're all middle class, says Blair', The Independent, 15 January 1999 Cited in Andrew Adonis and Stephen Pollard, A Class Act: the Myth of Britain's Classless Society (London, 1997), p. 5. Karl Marx, The Eighteenth Brumaire of Louis Napoleon (Moscow, 1960), p. 12. Proponents of the 'linguistic turn' have also taken up the cacophony of voices declaring the redundancy of class. Born in the theoretical implosions of post-structuralism, language is deemed the foundation of all human activity, exemplified in Levi Strauss's claim 'if you solve the problem...of language, we can explain the rest'. Disciples of this school argue that historians cannot study the vexed relations between capital and labour because it was the language that people employed that provided the essential source of their identity. Class becomes no more than a simple rhetorical construction, still less a genuine motor of change. Gareth Steedman Jones provides the seminal work. He argued that social movements, such as Chartism, could be constituted on ideological and political planes, autonomous from Marxist economic conditions, and irrespective of class connections between them. Chartism spoke a language of class, which owed less to material underpinnings of class than it did to a vision of political evils of capitalist power, which would have predated class-consciousness. By stressing the political content of Chartism, emblematic in the language of parasitism and oppression, he forces us to consider the non-class character of popular radicalism. However, he abandons class at a considerable cost and, indeed, he concludes that his model leads to 'an unconvincing idealism'. He underplays the class nature of Chartism, ignoring the class antagonism at the level of production, and views the position of the state with rose tinted glasses in which the glare of reform obscures undercurrents of coercion and economic controls. If class is to be sacrificed on the altar, it cannot be done with such an idealised reading of discourse. Cited in Bryan Palmer, 'Critical Theory: Historical Materialism and the ostensible end of Marxism: the poverty of theory revisited', in Keith Jenkins (ed. ), The Postmodern History Reader (London, 1997), p. 108. Cited in Bryan Palmer, The Descent into Discourse (Philadelphia, 1990), p. 133. We must now discuss the impact of race and gender on class. The new social movements have argued that the class preoccupation with the realm of production, held up as its mark of theoretical rigour, downplays the salience of race and gender as categories of historical analysis. Where Marx would argue that ethnic groups are no more than 'national left overs', and it is the material changes in production that explain ethnicity, champions of race and gender analysis have begun to opine that class is a stultifying straight jacket, obscuring other lines of cleavage. Voices emanating from women's history, for example, claim that few women appear in the canonical texts of class and therefore this argues that class analysis should be abandoned, because it takes into account only half of possible historical actors. Although sympathetic to this debate, it is wrong to reject class unconditionally. Instead of bifurcating class, race and gender, historians need to recognise that they are integrated at a conceptual level. Certainly, industrial reconstructing of class relations in Britain resonated in the social and racial engineering of empire. The British exported the vocabulary of class to describe Africans as the same as the lower classes in Britain - 'the residuum'. Race is thus also about class. Furthermore, recent gender studies have shown that working class women never see themselves as just women. Class for many working class women has become a ' hidden injury of class', because they see class inferiority as a sign of personal failure. Ergo, they do not speak of class in the traditional sense - 'I am working class'. Class is thus not just about representation, 'a subject position which can be taken off a discursive shelf and worn at will', but is lived at an intimate level of feeling, inherently bound with gendered identity. Gender is thus also about class. So where does this leave us? Arguably, we should take the Weberian line. Class is neither the sole nor historically universal principle of social structure. Class is best seen as part of a complex interplay with other aspects of social structure such as race and gender Cited in Beverley Skeggs, Formations of Class and Gender: Becoming Respectable (London, 1997), p. 3. For arguments see Beverley Skeggs, Formations of Class and Gender: Becoming Respectable (London, 1997) T Lears, 'The Concept of Cultural Hegemony: Problems and Possibilities', American Historical Review, 90 (1985), p. 577. Beverley Skeggs, Formations of Class and Gender: Becoming Respectable (London, 1997), p. 94. It is now necessary to examine the impact of consumption patterns on class. It can be opined that humans today are defined not by their relationship to the means of production but by the goods that they purchase in a world of increasing material fetishism. Consumption, therefore, becomes an independent dimension of social stratification and distinction. With humans self-consciously measuring themselves under the slogan, 'I am what I buy' rather than ' I am what I work', class is apparently consigned to the rubbish bin of history. This can potentially upset not only sociological thinking on today's society but also ways in which historians have understood social groups in the past. In his portrait of common Parisian life before 1789, Daniel Roche shows a wider world of rich materialism, thereby suggesting a possible review of the pre-Revolutionary social categorisation dynamics that led to the regime change. However, consumption remains inherently joined to the hip of class. For example, the nouveaux riches cannot necessarily look for a concomitant entry into the traditional halls of upper class society. This may be for 'snobbery' reasons (the 'my mother would not invite yours home for tea' adage) or through failure to adopt the values of their new class, as expressed in Marx's idea of 'false consciousness'. The final part of this paper examines the social fabric of modern Britain, to see whether, despite the emergent views to the contrary, class in reality is still an historical tool that cannot be ignored. In contrast to Marx, who argues that classes serve to distribute power in society, Foucault opines that realities are constructed by discourse and there are no 'real' classes with 'real' interests. However, to proclaim the obsolescence of class on Foucaultian lines is fundamentally flawed, in the face of the evidence of continuing relevance of class in the country. The retired Archbishop of Canterbury, Dr Carey, lamented 'we are still a class-ridden society'. This is also the image shared by foreigners. A survey by the Independent on 10 November 2000 offered that 75 per cent of foreigners said Britain showed marked class distinctions. We must give a few examples of why this is seen to be the case. Britain's education system bears the hallmarks of class inequality. Meritocracy in education is the creed of the elite, and the public schools still serve a predominantly professional clientele and the higher levels of the armed forces. The Education Minister, Alan Johnson, recently bemoaned an 'obscene social class gap' in British Universities, arguing that 'we need to get rid of the rope ladder and build a substantial staircase', in order to allow working class children to attend higher-achieving Universities. The data supports this, as exemplified by recent figures from Cambridge University showing that around half of its students are still from private fee paying schools. The existence of the monarchy and hereditary titles still provides a noticeable statement on Britain's 'ancien regime' structure, suggesting a continuation of inherent influence and privilege. Despite equality advances in recent years, it is difficult to perceive the honour system as essentially other than remaining harnessed to class rather than achievement, with the lowest level, the MBE, being the common award for the masses. Culture too betrays George Orwell's 'most class ridden country under the sun'. BBC Radio 4, and its 'thought for the day', remains the house station for the professional classes and 'good country stock' exclusivity. The television programmes we watch, for example the 'Bucket' family - no that's 'Bouquet' - in 'Keeping Up Appearances', contain clear class patterns. Thus all aspects of daily life provide a constant testament to the endurance of Britain's class structure. Class is not only about discourse and language; there is reality as well as representation. Cited in Jonathan Petre, 'Society is still riven by class, says Carey', The Independent, 30 October 2002 Nicole Martin, 'Arrogant Britain is split by class, claim foreigners', The Independent, 10 November 2000 Cited in Rachel Sylvester, 'Minister attacks 'obscene' Oxbridge class gap', The Independent 6 December 2003 Ibid Laura Clark, 'Cambridge says no to 5,000 pupils with three A grades', The Daily Mail, 13 March 2004 Cited in Andrew Adonis and Stephen Pollard, A Class Act (London, 1997), p. ix. To conclude, despite a climate in which the old metanarratives are seen as intellectually bankrupt and it is claimed that social identities are created by discourse, class remains of fundamental importance. Underpinning the debates is a constant and perhaps non-solvable epistemological problem - what is class? Much of the critique proclaiming the 'death of class' has only focused on its more Marxist reductive definitions and inability to look beyond the economic world. Although, no longer holding the master key to unlocking the entire historical process as seasoned Marxists would assert, class retains an analytical purchase in examining social structures, hierarchies and collective groupings, particularly when used in conjunction with race and gender, in an inter-linked 'Holy Trinity'. Furthermore, class is not just an esoteric occupation, consumed in the world of theory, but has a reality on the streets of modern Britain. To believe that class is not 'real' is only a prerogative of those unaffected by its inherent exclusions and prejudices. Ultimately, this is not the eschatological end for class as an operative word, and there is certainly no need to find a new theoretical terra firma to replace it. It is simply a matter of recognising a suitable resting-place for it between the economic reductionism of Marxist historians and the nihilistic and subjective world of Postmodernism and linguistics.",True
37,"The once fashionable belief that class analysis provides the key to history has increasingly come under attack. For many, both inside and outside of the Academy, class is no longer a viable way of looking at the world and examining history; it has become part of history itself. Jan Pakulski and Malcolm Waters assert that 'like beads and Che Guevera berets, class is passé', and they dedicate their work to 'confirm the good news that class has collapsed'. This paper examines the reasons behind the 'death of class' contention and offers an opinion on whether they are valid. Jan Pakulski and Malcolm Waters, The Death of Class (London, 1996), p. 1. Ibid, p. 7. Ibid, p. 1. Karl Marx's name is synonymous with the concept of class. He states that men were always constituted by the antagonistic class relations into whom they are cast 'and the history of all hitherto existing society is the history of class struggles'. This assertion was seen to not only have a capacity to unmask the structure of capitalist power, particularly in its inequitable and exploitative guises, but also to change the political terrain irrevocably. However, it is significant to note that he gives class no precise definition. Using his work, later Marxists developed what they saw as a duality to class. In an 'objective' sense, they spoke of a class 'in itself', which was no more than a form of collective identity. This grouped people together by their shared economic characteristics, principally in their relationship to the means of production. In a 'subjective' sense, there was a class 'for itself', which describes the process by which these aggregates become politically and consciously transformed, aware of themselves as a class. Others later took up the torch. Led by E P Thompson, the British Marxist historians present a history of class 'from the bottom up'. Thompson attacked the tendency of historians to see class as merely a structure or category and in doing so 'obscure the agency of the working people'. He began to recognise the prima facie relevance to other emerging social identities, such as feminism, thereby relinquishing Marx's economic 'determination in the last instance'. Despite this, to the Marxist, class remains primarily the anvil on which history was formed. Karl Marx and Friedrich Engels, The Communist Manifesto (London, 1967), p. 79. Cited in Harvey Kaye, The Education of Desire (New York, 1992), p.27. 6 J F Lyotard, The Postmodern Condition (Minnesota, 1984), p. 13. We now turn to the factors that, in some eyes, have made class irrelevant to historical analysis, starting with Postmodernism. With the collapse of the Soviet Union, defeat of organised labour, the fall of great Victorian staple industries, redundancy of Trade Unions and recrudescence of the New Right, scholars began to rethink the meaning of past verities. These changing dimensions had struck a fatal blow to the class politics of the Left, confirmed in the Thatcher electoral victories in 1979, 1983 and 1987. To Postmodernists, even the Miners' strike of the 1980s has more to do with the effect on community life, rather than traditional direct class struggle. Whereas for Marx, class was the essence of history, for the Thatcherite New Right, class was its perversion and they sought a 'classless society'. Postmodernists claim that the world today is no more than a global triumph of free market economies and pluralistic democracies, thereby bringing the old metanarratives, such as class, to their knees. Indeed, J F Lyotard opined that class had been 'blurred to the point of losing all radicality'. However, the license for the blanket repudiation of class on these grounds is weak. Firstly, Postmodernist theory misses the continued critical relevance of class to the events it describes. Class can be used to explain the collapse of the USSR. The New Right traditionally sees this as a triumphant consequence of transnational capitalism and an overburdening system of military expenditure. The corruption, bureaucratisation and nepotism of the Soviet ruling class, particularly in the excesses of the Brezhnev era, ensured the continued oppression and alienation of workers. Against Postmodernist critiques, this alienation of labour and the consequent role of the masses in overthrowing the system suggest the continued viability of class. Secondly, although recognising the New Right's desire to drive class off the agenda - Thatcher called it a 'communist concept' - they remained sensitive to its continued power. The New Right was simply adept at disguising the language of class within the ideological cover of national security, jingoism and honour. Furthermore, the Postmodernist concept of a 'classless society' inherent in the language of the New Right is misleading. The New Right used the term not to describe a society without class but as the goal of a meritocratic society providing opportunity for people to advance regardless of class. Thus, the New Right is in fact reinforcing the existence of classes that it has only linguistically abolished. Finally, the Postmodernist charge rests on a haphazard assumption that the world is totally equitable under conditions of market capitalism. However, in its latest manifestation, capitalism continues to distribute inequitably its product. There is a clustering of power in the hands of a small homogenous elite, whilst the people of the Third World increasingly live in conditions of chronic penury. Therefore, we need a class element, in order to explain the negative outcomes of capitalism's distributive rationalism. Cited in David Cannadine, Class in Britain (London, 1998), p. 2. The USSR collapse is seen by many as the fundamental nail in the coffin of Marxist class theories. Joyce believes that any mention of class automatically locks us into a set of discourses belonging only to discredited Stalinists and a failed socialist project. Class for Joyce is made solely of Soviet clay. This is indicative of a post-communist age, where all forms of socialism and class have been brought into question, and tarred with the same brush. However, class has long been pluralistic in form, and attacking only the most discredited type of class is only 'scoring easy polemical points'. Indeed, advocates of this approach are mounting simply an ideological, rather than empirically justifiable, charge. Although recognising that orthodox Marxist class theory, and socialism as the telos of history, does look a little dated, class is more fluid than this attack would have us believe. Geoffrey Eley and Keith Neild, 'Starting Over: the present, the Postmodern, and the moment of Social History', in Keith Jenkins (ed. ), The Postmodern History Reader (London, 1997), p. 372. Linked to the Postmodernist agenda, class has also come under scrutiny from political historians, such as Frank Parkin. They claim that Marx's triadic society of classes, particularly the two 'great hostile camps' of proletariat and bourgeoisie, outlined in the Communist Manifesto, were historically over-simplified abstractions and it is impossible to locate precisely the politically relevant line of cleavage. This argument has gained momentum in the light of recent political claims that 'we are all middle class'. The working class, severed from the symbolism of old labour movements and increasingly a white-collar composite, has been absorbed into the middle class. This is emblematic in John Prescott's opinion that he is 'anything else than middle class now' despite a working class background. However, in spite of this, class remains a valuable dividing line. Firstly, concentration on headline reductive aspects represents a perverted reading of Marxist class theory. In the Eighteenth Brumaire, Marx recognised the existence of other classes but simply felt that 'of all the classes that stand face to face with the bourgeoisie today, the proletariat alone is the really revolutionary class'. Secondly, it is possible to locate lines of cleavage in modern social structures, even if it means abandoning previous Marxist reductive forms. For example, the emergence of a new Super-Class, born in the financial world of London, and a new underclass, living in a permanent state of dependence, are indicative of a reconfigured class structure. Although it is arguably harder to identify, class remains a useful measurement of social hierarchies. Cited in George Jones, 'We're all middle class, says Blair', The Independent, 15 January 1999 Cited in Andrew Adonis and Stephen Pollard, A Class Act: the Myth of Britain's Classless Society (London, 1997), p. 5. Karl Marx, The Eighteenth Brumaire of Louis Napoleon (Moscow, 1960), p. 12. Proponents of the 'linguistic turn' have also taken up the cacophony of voices declaring the redundancy of class. Born in the theoretical implosions of post-structuralism, language is deemed the foundation of all human activity, exemplified in Levi Strauss's claim 'if you solve the problem...of language, we can explain the rest'. Disciples of this school argue that historians cannot study the vexed relations between capital and labour because it was the language that people employed that provided the essential source of their identity. Class becomes no more than a simple rhetorical construction, still less a genuine motor of change. Gareth Steedman Jones provides the seminal work. He argued that social movements, such as Chartism, could be constituted on ideological and political planes, autonomous from Marxist economic conditions, and irrespective of class connections between them. Chartism spoke a language of class, which owed less to material underpinnings of class than it did to a vision of political evils of capitalist power, which would have predated class-consciousness. By stressing the political content of Chartism, emblematic in the language of parasitism and oppression, he forces us to consider the non-class character of popular radicalism. However, he abandons class at a considerable cost and, indeed, he concludes that his model leads to 'an unconvincing idealism'. He underplays the class nature of Chartism, ignoring the class antagonism at the level of production, and views the position of the state with rose tinted glasses in which the glare of reform obscures undercurrents of coercion and economic controls. If class is to be sacrificed on the altar, it cannot be done with such an idealised reading of discourse. Cited in Bryan Palmer, 'Critical Theory: Historical Materialism and the ostensible end of Marxism: the poverty of theory revisited', in Keith Jenkins (ed. ), The Postmodern History Reader (London, 1997), p. 108. Cited in Bryan Palmer, The Descent into Discourse (Philadelphia, 1990), p. 133. We must now discuss the impact of race and gender on class. The new social movements have argued that the class preoccupation with the realm of production, held up as its mark of theoretical rigour, downplays the salience of race and gender as categories of historical analysis. Where Marx would argue that ethnic groups are no more than 'national left overs', and it is the material changes in production that explain ethnicity, champions of race and gender analysis have begun to opine that class is a stultifying straight jacket, obscuring other lines of cleavage. Voices emanating from women's history, for example, claim that few women appear in the canonical texts of class and therefore this argues that class analysis should be abandoned, because it takes into account only half of possible historical actors. Although sympathetic to this debate, it is wrong to reject class unconditionally. Instead of bifurcating class, race and gender, historians need to recognise that they are integrated at a conceptual level. Certainly, industrial reconstructing of class relations in Britain resonated in the social and racial engineering of empire. The British exported the vocabulary of class to describe Africans as the same as the lower classes in Britain - 'the residuum'. Race is thus also about class. Furthermore, recent gender studies have shown that working class women never see themselves as just women. Class for many working class women has become a ' hidden injury of class', because they see class inferiority as a sign of personal failure. Ergo, they do not speak of class in the traditional sense - 'I am working class'. Class is thus not just about representation, 'a subject position which can be taken off a discursive shelf and worn at will', but is lived at an intimate level of feeling, inherently bound with gendered identity. Gender is thus also about class. So where does this leave us? Arguably, we should take the Weberian line. Class is neither the sole nor historically universal principle of social structure. Class is best seen as part of a complex interplay with other aspects of social structure such as race and gender Cited in Beverley Skeggs, Formations of Class and Gender: Becoming Respectable (London, 1997), p. 3. For arguments see Beverley Skeggs, Formations of Class and Gender: Becoming Respectable (London, 1997) T Lears, 'The Concept of Cultural Hegemony: Problems and Possibilities', American Historical Review, 90 (1985), p. 577. Beverley Skeggs, Formations of Class and Gender: Becoming Respectable (London, 1997), p. 94. It is now necessary to examine the impact of consumption patterns on class. It can be opined that humans today are defined not by their relationship to the means of production but by the goods that they purchase in a world of increasing material fetishism. Consumption, therefore, becomes an independent dimension of social stratification and distinction. With humans self-consciously measuring themselves under the slogan, 'I am what I buy' rather than ' I am what I work', class is apparently consigned to the rubbish bin of history. This can potentially upset not only sociological thinking on today's society but also ways in which historians have understood social groups in the past. In his portrait of common Parisian life before 1789, Daniel Roche shows a wider world of rich materialism, thereby suggesting a possible review of the pre-Revolutionary social categorisation dynamics that led to the regime change. However, consumption remains inherently joined to the hip of class. For example, the nouveaux riches cannot necessarily look for a concomitant entry into the traditional halls of upper class society. This may be for 'snobbery' reasons (the 'my mother would not invite yours home for tea' adage) or through failure to adopt the values of their new class, as expressed in Marx's idea of 'false consciousness'. The final part of this paper examines the social fabric of modern Britain, to see whether, despite the emergent views to the contrary, class in reality is still an historical tool that cannot be ignored. In contrast to Marx, who argues that classes serve to distribute power in society, Foucault opines that realities are constructed by discourse and there are no 'real' classes with 'real' interests. However, to proclaim the obsolescence of class on Foucaultian lines is fundamentally flawed, in the face of the evidence of continuing relevance of class in the country. The retired Archbishop of Canterbury, Dr Carey, lamented 'we are still a class-ridden society'. This is also the image shared by foreigners. A survey by the Independent on 10 November 2000 offered that 75 per cent of foreigners said Britain showed marked class distinctions. We must give a few examples of why this is seen to be the case. Britain's education system bears the hallmarks of class inequality. Meritocracy in education is the creed of the elite, and the public schools still serve a predominantly professional clientele and the higher levels of the armed forces. The Education Minister, Alan Johnson, recently bemoaned an 'obscene social class gap' in British Universities, arguing that 'we need to get rid of the rope ladder and build a substantial staircase', in order to allow working class children to attend higher-achieving Universities. The data supports this, as exemplified by recent figures from Cambridge University showing that around half of its students are still from private fee paying schools. The existence of the monarchy and hereditary titles still provides a noticeable statement on Britain's 'ancien regime' structure, suggesting a continuation of inherent influence and privilege. Despite equality advances in recent years, it is difficult to perceive the honour system as essentially other than remaining harnessed to class rather than achievement, with the lowest level, the MBE, being the common award for the masses. Culture too betrays George Orwell's 'most class ridden country under the sun'. BBC Radio 4, and its 'thought for the day', remains the house station for the professional classes and 'good country stock' exclusivity. The television programmes we watch, for example the 'Bucket' family - no that's 'Bouquet' - in 'Keeping Up Appearances', contain clear class patterns. Thus all aspects of daily life provide a constant testament to the endurance of Britain's class structure. Class is not only about discourse and language; there is reality as well as representation. Cited in Jonathan Petre, 'Society is still riven by class, says Carey', The Independent, 30 October 2002 Nicole Martin, 'Arrogant Britain is split by class, claim foreigners', The Independent, 10 November 2000 Cited in Rachel Sylvester, 'Minister attacks 'obscene' Oxbridge class gap', The Independent 6 December 2003 Ibid Laura Clark, 'Cambridge says no to 5,000 pupils with three A grades', The Daily Mail, 13 March 2004 Cited in Andrew Adonis and Stephen Pollard, A Class Act (London, 1997), p. ix. To conclude, despite a climate in which the old metanarratives are seen as intellectually bankrupt and it is claimed that social identities are created by discourse, class remains of fundamental importance. Underpinning the debates is a constant and perhaps non-solvable epistemological problem - what is class? Much of the critique proclaiming the 'death of class' has only focused on its more Marxist reductive definitions and inability to look beyond the economic world. Although, no longer holding the master key to unlocking the entire historical process as seasoned Marxists would assert, class retains an analytical purchase in examining social structures, hierarchies and collective groupings, particularly when used in conjunction with race and gender, in an inter-linked 'Holy Trinity'. Furthermore, class is not just an esoteric occupation, consumed in the world of theory, but has a reality on the streets of modern Britain. To believe that class is not 'real' is only a prerogative of those unaffected by its inherent exclusions and prejudices. Ultimately, this is not the eschatological end for class as an operative word, and there is certainly no need to find a new theoretical terra firma to replace it. It is simply a matter of recognising a suitable resting-place for it between the economic reductionism of Marxist historians and the nihilistic and subjective world of Postmodernism and linguistics.","The intention of this paper is move outward from new and old statistical empirical evidence, found on the one hand in the National Archives (PRO) and the RAF Museum Archive, and on the other, in existing secondary literature, and ask what this reveals about British air strategy in the 1920s and 1930s. The methodology used is to take researched evidence, portray it in the form of charts, data bases or spreadsheets, and then use this IT generated information as the foundation for discussion. The archive material was not examined with any pre-conceived conclusions in mind. It will be our job to use all the empirical meat to create an interpretable abstract of air strategy. We will question whether these sources either perpetuate or shed new light on existing discourse. Along the road the sources will be challenged for their usefulness as historical pieces of evidence. As a point of entry into the mise-en-scène of air strategy in the 1920s, let us analyse a RAF proposed peace strength chart (Appendix 1), as laid down by the Chief of the Air Staff in a November 1918 paper. Figure 1 below, which is drawn from this Appendix, illustrates the proposed location of the squadrons. Most significantly, the Figure expounds the increasing importance placed by strategists on using air power in the empire, with 34 squadrons placed in colonial possessions. The Chief enthused that 'British possessions...are spread over a wide area and aircraft should prove of the greatest service'. Aircraft were seen as a form of control without occupation, providing a cheap and ubiquitous means of imperial policing. He also opined that 'on the conclusion of peace all available aircraft material should be utilised...in the Dominions and India'. Although this kind of commitment is not realised in the Figure, it shows that air strategists were starting to see aircraft within the typology of imperialism, as a possible means of lightening the white man's burden. However, the Figure is fundamentally flawed in understanding air strategy. It is only an aspiration of air strength in the coming 1920s and does not take into account the financial or procurement feasibility of its estimations. Moreover, in its rich dedication to the supposed war-winning bombers (shown at Figure 2 below), the source bears the fingerprints of the Air Staff's political motive to protect the service from the crossfire of its seniors and attempts to undermine its autonomy. The National Archives, AIR 8/6. Attached memorandum to Proposed Peace Strength by Chief of the Air Staff, November 1918 2Ibid Overall, the Figure is an idealist snapshot that ultimately is best at revealing Air Staff pressure on air strategy, captured in a accompanying remark by the Chief - 'we wish to turn our swords into ploughshare... but not so fast' - than it is in understanding what actually happened in the 1920s. Ibid It is now important to focus our attention on the true position of the RAF in the 1920s. We can do this through the lens of air service expenditure estimates at Appendix 2 and the resultant Figure 3 below. It is clear that the earlier optimism of the Air Staff proved ephemeral, as air service expenditure initially sharply fell, and then remained relatively austere. The Figure supports existing discourse about air strategy, which contends that the RAF just about 'held on' during its cradle days, in the face of ferocious economic retrenchment, the Ten-Year Rule and a zetheist that condemned recourse to conflict. The impact of the imposed permanent war planning blight, is particularly evident if we note the acute fall in Technical and Warlike Stores estimates in the Appendix - elements that constitute the kernel of aggressive air strength - from £6,172,850 in 1920 to £1,295,000 in 1922. It is also significant to note from that source that research was particularly barren throughout the period, falling from £2,575,540 in 1920 to £498,000 in 1929. This goes a long way in explaining the poor quality of aircraft the RAF possessed as late as 1934, still consisting of inchoate wooden biplanes. The data also reveal that air strategy was far from a blueprinted plan, and was in fact often pragmatic and respondent to changes in geopolitik. For example, the slight rise in expenditure in 1924 can be seen as a direct response to the perceived threat of the Armée de l'Air emerging from 1923. However, the data has limitations that prevent a challenge to the existing historiography. They give no indication of how many planes were actually produced or what important fields of operation fared better than others. How far was the empire really given wings as the Chief of the Air Staff had proposed in 1918? They also do not offer any information on the Army and Navy, against which to draw priority comparisons; and industrial capacity is unknown. Moving into the 1930s, let us start by examining the early years, before any significant expenditure rise occurred. Again, we will move our discussions from the empirical base of Appendix 2, as reproduced in Figure 4 below. The Figure shows that air service expenditure in the years 1930-34 also remained relatively low and stable; this can be seen as an indication of the government's first strategy to avert fears of the knock out blow, through the panacea of air disarmament. The knock out blow principle is important, as it weighs so heavily on air strategy throughout the 1930s. Founded in the futurological literature of HG Wells -' after the smashing of City Hall...the white flag had been hoisted' - and politically confirmed in Baldwin's famous dictum 'the bomber will always get through', the knock out blow endangered the heart of Empire for the first time. This anxiety was reinforced by the Great Depression, where it was felt that a major dislocation, such as war, would catastrophically weaken the durability of industrialised society. In the context of increasing demands to put Britain's financial house in order, and in preparation for the international Disarmament Conference in 1932, an air disarmament strategy was the diplomats preferred negotiating hand. Our sources support this policy. Funding of the important Technical and Warlike Stores element remained about the same between 1930 and 1934. A consequence of this was that production became devoted simply to re-equipping old planes. An illustration of this was an apparent 'moratorium' in number of new airframes being produced, as shown in Figure 5 below. H G Wells, The War in the Air (London, 1908), p. 130. 5 Cited in Uri Bailer, The Shadow of the Bomber: The Fear of Air Attack and British Politics (London, 1980), p. 14. However, a glaring limitation to the data is again the absence of plane types and their contribution towards the overall force mix and objectives. Focusing now on the period 1934 to 1938, before the financial picture would be dramatically changed by war, Figure 6 below illustrates that these years were nonetheless marked by a massive increase in air service expenditure. Appendix 2 indicates that the Technical and Warlike Stores indicator rose by a staggering 337 per cent, whilst two major supplementary estimates were added in 1935 and 1936, amounting to £17,000,000. The source shows Britain dedicating greater spending to research (99 per cent), which may go some way in explaining the relative superiority of British planes by 1939. The prima facie backdrop to the data is the failure of air disarmament and the révanchiste position of Germany. Indeed, the British Foreign Secretary opined in 1935, 'the verb to disarm should be classified by grammarians as a defective verb'. However, these two reasons represent the mere surface disturbance and crests of foam behind the policy of increased air expenditure, and do not engage with policymaker's more directed mentalités. To understand this, we need to examine the type of planes being produced. Cited in Uri Bailer, The Shadow of the Bomber: The Fear of Air Attack and British Politics (London, 1980), p. 9. The various air schemes (Appendix 3) developed and amended from 1933 onwards, outlining the types and numbers of planes required for the front line by a certain date, provide the key to unlocking the deeper roots associated with air policy in this period. Figure 7 below, produced from this Appendix, shows that the Britain was increasingly dedicating her resources to bombers. With the collapse of air disarmament talks, following the ignominious end to the Geneva Conference, policymakers sought a new means to prevent the knock out blow. It was axiomatic to air strategy that a commitment to building a bomber force would act as a deterrent against German air attack, presaging the Cold War concept of Mutually Assured Destruction. Increasing the number of bombers from 476 in 1934 to 1589 by 1937, as shown in the Appendix, worked under the belief that 'qui desiderat pacem, praeparet bellum'. On a diplomatic level, it was felt that a bomber force might bring Germany back to the Conference table. On a political level, it allayed public fears about a lack of preparation for air attack. The data also reveal a proportionally sharp increase of bomber requirements from February 1936 (see Figure 7) onwards. This can be seen as an equalising knee-jerk response to Hitler's alarming 1935 parity-achievement claim, which 'set the Nazi cat squarely among the democratic pigeons'. One limitation to our source, however, is that it does not show the number of bombers required in a reserve capacity to sustain operations. This would distinguish absolutely how far Britain worked under the strategy of shop window deterrence. Nevertheless, this source illustrates that air strategy was doing far more than aimlessly stirring Britain from her slumbers as Figure 6 suggests, and was in fact, in its dedication to bombers, a piece of political conjuring to hypnotise its audience. Cited in John Terraine, A Time for Courage: The RAF in the European War 1939-1945 (New York, 1985), p. 3 8 Denis Richards, The Royal Air Force Volume 1: The Fight at Odds (London, 1974), p. 12. The next area of our paper will examine the immediate pre-war years. Firstly, let us look at the data given in an Air Ministry report, as indicated in Figure 8 below, on the production of airframes between August 1938 and November 1939. It is clear that, from January 1939, airframe manufacturers started to exceed their production promises. These promises would have been established with policymakers in the deterrence by parity years. Although not explicit in the chart, we can draw three possible conclusions from this. The surpassing of airframe guarantees can almost incontestably be seen as the final nail in the coffin for the parity strategy, as the government encouraged air manufacturers to move towards a war footing. Not only had hopes of an air convention been lost but also the promises of Munich seemed long forgotten. Britannia's trident was now a bayonet and her shield a gas mask. Less probably, it could reflect that manufacturers had simply become more efficient in their production, no longer cruising at barely economical speed. More sinisterly, the data could suggest production profiteering, as captured in the illustration below (albeit in 1935). Ultimately, however, the source is best at confirming the end of air parity, than it is at hinting at more polemical speculations. However, these delivery numbers fail to paint the whole picture. We need to examine the types of planes produced. Figure 9, working on a microcosmic level of production, shows the number of Spitfires delivered from the Eastleigh Production Facility. It is quite apparent that a significant rise in production occurred from October 1938 onwards. However, this source is fundamentally limited in understanding air strategy, as Spitfires were neither the archetypal fighter in 1938-9 nor accounted for the bulk of the fighter force. Hurricane production is also required, in order to obtain a true picture of any significant change in production, and ergo air strategy. The difference in orders was 8647 more Hurricanes (Appendix 4). Importantly, Figure 10 below shows that from June 1938 the number of all fighters being delivered rose feverishly. This supports existing discourse, which argues that policymakers from early 1938 onwards increased fighter production for air defence purposes. A 1938 Command Paper reflects this, lamenting the failure of the bomber deterrence strategy, and stating 'taking risks for peace has not removed the dangers of war'. Major technological improvements with fighter capability, coupled with the coming of radar, presented the Command Paper, Statement Relating to Defence: Presented by the Primeminister to Parliament 3 March 1938 (London, 1938), p. 4. possibility of withstanding the knock out blow. This in turn threatened Germany with the prospect of a long war, which was arguably a more credible deterrent than simply a bomber build up, given the greater long term resources of the Empire. However, this Figure has a limitation of its own, because it does not tell us the relative fighter position vis-a-vis the wider picture of total aircraft production and in particular that of bombers. To combat these problems, we can refer to the data provided in the Cabinet approved air schemes L and M (Appendix 3). The two pie charts below comprising Figure 11 compare the required aircraft by proportion of all total planes in April 1938 and November 1938. It is immediately apparent that only a small shift away from bombers towards fighters occurred. This is by no means supportive of existing discourse. Literature on 1930's air power recognises the shift but argues that it was much more pronounced, with production geared three to one in favour of fighters. However, Figure 11 indicates that, in receiving well over 40 per cent of total aircraft production, the bomber remained the main player in air strategy, notwithstanding a greater emphasis to fighters. Thus, the pie charts provide an empirically tested challenge to the current epistemological understanding of the period. The final part of this paper will question the common usage by contemporary historians of simple comparative air power strength charts, as exemplified by Figure 12 below. Use of such charts to critique Britain's air strategy vis-a-vis other nations to illustrate Britain's 'dreadful note of preparation' is potentially too shallow. Although not wishing to be overly judgmental - this would require an exhaustive analysis far beyond the scope of this paper - it is essential that we lay bare certain qualifications the air historian needs to comprehend if he or she wishes to use such charts as an empirical authority. It is difficult to compare aircraft production in basic numerical terms, as the quality of aircraft varied greatly from country to country and year to year and was driven by different operational demands. Furthermore the numbers include Denis Richards, The Royal Air Force Volume 1: The Fight at Odds (London, 1974), p. 12. more than front line types (for example, Germany devoted far larger amounts of production than Britain to trainers), and they give us no insight into how many of the aircraft delivered were immediately deployable, through pilot availability and other factors. Also ignored in assessing proportional difference in air strength is the contribution of anti-aircraft defences. The contemporary historian, therefore, needs to take these sensitivities in his metaphorical knapsack when he enters the archives. To conclude, it has been the intention of this paper to work with primary and secondary statistical source material obtained from the National Archives, the RAF Museum and existing literature, in an effort to see what estimate and production data reveal about air strategy in the 1920s and 1930s. Much of the evidence has converged with existing historiographical discourse, outlining a gradual maturity of air strategy, extending from labouring within the parameters of a restricted budget through to the mantra of home defence. But some of the recently researched material sheds new light on the subject. For example, our data on fighter requirements refutes existing understanding that air strategy moved absolutely from bombers to fighters in the years 1938 to 1939. We have also suggested that certain sources, particularly comparative charts, remain too simplistic in composition to hold any more than mere subjective value. Furthermore, the usefulness of solely Air Ministry derived sources, such as the proposed peace strength chart, are somewhat sullied because their technical estimates were shaped by non-financial predilections and bias. Although ultimately recognising, as Clifford Geertz laments 'what we call our data are really our own constructions of other people's constructions', the charts here, despite certain limitations, not only move our understanding of air strategy slowly towards a more definitive picture but also hint at possible areas for further in-depth study. Indeed, there remains a vast sum of non-researched information relating to air strategy in the 1920s and 1930s waiting for the historian in the archives. Arguably, what our fighter discovery shows above all is that literature on air strategy in the 1920s and 1930s has itself become a too entrenched 'matter of faith'. It is thus the job of the air historian to revise further the subject, particularly the years 1938 to 1939, in greater detail. 11 Clifford Geertz, The Interpretation of Cultures (New York, 1973), p. 9.",False
38,"SummaryThe aim of this experiment was to carry out an enzyme assay to study the effects of pH, temperature and product inhibition on an enzyme and calculate the Michaelis constant and order or reaction. This was carried out by assaying alkaline phosphotase under different conditions measuring the extent of reaction with a spectrophotometer. It was discovered that the optimum pH was 9.5 whilst the optimum temperature is 42 oC and activity decreases either side of these optimums due to disruption at the active site or denaturing of the enzyme. Phosphate was found to carry out product inhibition and the reaction was determined as first order. The Michaelis constant was calculated to be 0.0871. IntroductionEnzymes are biological catalysts (they accelerate the rate of a reaction without themselves undergoing any net change) and catalyse most of the chemical reactions taking place in the cell. They carry out this catalysis by providing an alternative reaction pathway with a lower energy transition state. Enzymes are highly specific binding a specific substrate in the active site. The amount of enzyme activity and elements of the kinetics of an enzyme-catalysed reaction can be measured by measuring the rate of appearance of one of the products of the reaction. The effect of a variety of factors on rate of enzyme activity such as pH, temperature, concentration of substrate and enzyme and presence of inhibitors can also be determined using this method. Inhibition of an enzyme involved in an initial step in a metabolic pathway is often carried out by the end product. This is known as feedback inhibition and is an important regulatory strategy. A similar effect, product inhibition occurs when the product of an enzyme catalysed reaction inhibits the enzyme carrying out that reaction. An enzyme catalysed reaction has a reaction rate order. If the rate of the reaction at any instant is proportional to the concentration of the substrate then this is known as first order kinetics. Enzyme kinetics can also be described using the Michaelis-Menten equation. This involves the use of two parameters to describe the kinetic properties of enzymes - V max, the maximum velocity, and K M, the Michaelis Menten constant. K M is related to the affinity of the enzyme for its substrate and is defined as being the concentration of substrate at which the velocity is at half its maximum value. These are linked in the following equation where V O is the rate of formation of product:  FORMULA  Here alkaline phosphotase was used to study principles of enzyme kinetics. Alkaline Phosphotase is a phosphotase (hydrolyses organic esters to inorganic phosphate and alcohol) which is optically active at alkaline pH. It is an excellent enzyme to use in the determination of enzyme kinetics as it is robust and easily assayed. The synthetic substrate p-nitrophenyl phosphate (pNPP) is a convenient choice of substrate to demonstrate the activity of the enzyme as is it colourless until hydrolysed by the enzyme to the products inorganic phosphate and p-nitrophenol (pNP). p-nitrophenol is yellow at alkaline pHs (although colourless below pH7) and so the hydrolysis of pNPP can be followed using a spectrophotometer if the solution is made more alkaline after completion of reaction (eg. By adding NaOH). This also stops the reaction as at a very high pH the enzyme is denatured. pNP is also very stable once the reaction is stopped. The aim of the experiment was to quantitatively assay the activity of alkaline phosphotase under different conditions in order to study the effects of temperature, pH and product inhibition on the enzyme. The kinetic properties of the enzyme were also determined with the aim of calculating the Michaelis constant and the order of reaction. MethodThe experiment was carried out as described in the lab manual with the following details: Effect of pH on enzyme activityThe blank used for spectrophotometer was the second tube at each pH made up with no enzyme present. Effect of temperature on enzyme activityThe blank used was a second tube at each temperature with no enzyme present. Product Inhibition of enzyme actionThe blank used was made up of 2ml glycine buffer, 1ml phosphate, 1ml enzyme and no substrate. A control was made up with 2ml pNPP, 1ml glycine butter, 1ml enzyme and no substrate. The concentrations of phosphate used were (in mM) 1, 10, 20, 40, 60, 80, 100. Please see Appendix (Table 3) for volumes used in making up the concentrations. Determination of Michaelis-Menten constantThe blank used contained no substrate ie. 3ml buffer, 1ml enzyme. The concentrations of substrate (PNP) used were (in mM) 0.5, 1.25, 2.5, 3.75, 5.0 (see Appendix table 4 for volumes used) Order of reaction determinationTubes were made up with 1ml enzyme, 1ml glycine buffer and 2ml pNPP and the time course of the reaction determined by stopping the reaction and measuring the absorbance at a different time for each tube. The times at which each tube was stopped are as follows (in minutes): 0, 0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 5, 6, 8, 10, 14, 18, 20. ResultsThe graph below shows the varying concentrations of product, pNP, as the pH changes. This shows a clear increase in concentration as the pH increases to start with up until pH 9.5 where the graph peaks and then falls away again as the pH continues to increase. The graph is fairly symmetrical around the peak. The graph showing the effect of temperature on the product concentration shows a gradual increase getting sharper towards the peak at about 42 oC. After this point the concentration drops sharply as the temperature is further increased. Boiled enzyme at 37 oC showed only trace absorbance. Graph 3 shows the effect of increasing concentration of phosphate present in relation to the amount of pNP present. It is clear that as the concentration of phosphate is increased the concentration of pNP present falls rapidly. Graph 4 is an Eadie-Hoftsee Plot to calculate the Michaelis constant. This was calculated from the slope of the graph to be 0.0871. (see Appendix Table 5 for calculation). This graph (graph 5) to determine the order of reaction shows a straight line and gives a value of k of 0.013 (see appendix for calculation). DiscussionThe concentration of product can be used as a measure of enzyme activity as all the tubes started off with the same amount of substrate in each part of the experiment. Therefore the tubes with more product at the end must have had more enzyme activity take place in order to produce more product. The effect of pH on enzyme activityThe graph clearly shows that the activity of the enzyme increases up until pH 9.5 before falling away again as the pH continues to increase. The optimum pH for alkaline phosphotase activity is therefore pH 9.5. The reason behind the change in activity with changing pH is to do with the structure of the enzyme and the mechanism with which is works. Firstly enzymes are proteins. Therefore at extremes of pH the enzyme will become denatured - its active site will bend out of shape and no longer function. Therefore enzymes only work over specific pH ranges. The presence of free hydrogen ions or hydroxyl ions can affect the charges on amino acid residues making up the polypeptide chain or chains in the enzyme distorting the three-dimensional shape and causing an irreversible change in the protein's tertiary structure. Secondly enzymes are sensitive to changes in pH due to the great sensitivity of their active site. Even a slight change in pH may upset the chemical arrangement at the active site upsetting substrate binding by disrupting the multiple weak interactions by which the substrate-enzyme complex is formed. The nature of the side chains present in the enzymes structure will therefore play a large part in determining the optimum pH of the enzyme. The effect of temperature on enzyme activityFrom the graph it is clear that the temperature at which alkaline phosphotase carries out the most activity (the optimum temperature) is 42 oC. Below this point the activity of the enzyme is gradually increasing and after the optimum the activity of the enzyme drops sharply. The gradual increase before the peak is as a result of the general effect of increasing temperature on the rate of any chemical reaction - more kinetic energy is present so the molecules can move around faster increasing the chance of a favourable collision. The descending portion of the graph is due to a decrease in catalytic ability as the enzyme molecules start to be denatured. The denaturation occurs because of decreased stability in the structure of the enzyme molecule at high temperatures resulting in changes to the shape of the active site reducing its affinity for substrate and thereby its activity. At very high temperatures all the enzyme molecules present will be denatured and no activity will take place. This is shown by the trace absorbance shown by the boiled enzyme. Virtually no activity has taken place. The small amount of absorbance present may be due to tiny amounts of product formed from the uncatalysed reaction or a few enzyme molecules still active or from contamination from another source. To have a high optimum pH an enzyme must therefore have a more stable structure than those functioning at lower temperatures, as it must be able to withstand these higher temperatures without losing stability. Product Inhibition of Enzyme ActionIt is clear from graph 3 above that increasing the concentration of phosphate (one of the products of the hydrolysis of p-nitrophenyl phosphate) causes a major decrease in the activity of the enzyme - the presence of phosphate is inhibiting the enzyme. As the rate of the reaction started to drop at 10mM phosphate it is clear that the lowest molarity of phosphate which will inhibit alkaline phosphotase lies somewhere between 1mM and 10mM. At 1mM the absorbance is almost the same as the control with no phosphate present and so only a very small amount of inhibition, if any, is occurring. Inhibition of the enzyme by the product of the reaction it catalyses in this way is known as product inhibition and has a regulatory role in many metabolic pathways. As the moles of phosphate being produced in the previous experiments were all a lot smaller than this (all were micromolar quantities) it seems that the phosphate released in these experiments would have been to small to have had an inhibitory effect on the enzyme. Determination of Michaelis constantThe figure arrived at for this constant was 0.0871. However a lot of error could have been introduced into the calculation throughout the experiment. Firstly in making up the test solutions at the start imprecision could have introduced errors. Translating the absorbances into concentrations involved the use of a calibration curve which means the concentrations are really estimates and the use of a trend line also introduces new error all of which could have resulted in an inaccurate result. The introduction of these errors could have occurred in any of the experiments carried out here. Order of reactionThe graph clearly shows a straight line indicating a first order reaction whilst the value of k is less than 1.4 also indicating a first order reaction. This means that the rate of the reaction at any moment is proportional to the concentration of the substrate. Although there are two reactants in the reaction (p-nitrophenyl phosphate and water) the fact that it is still a first order reaction must be due to the mechanism of the reaction. This may be because water is present in such a large excess as pure water is highly concentrated that it is not having an effect on the rate, that is the rate is still only dependent on one concentration - that of p-nitrophenylphosphate and so still only shows first-order kinetics. ConclusionsIn conclusion therefore it was determined that the optimum pH of alkaline phosphatase is 9.5 whilst the optimum temperature is about 42 oC. Activity of the enzyme decreases either side of these optimums. The presence of phosphate, one of the products, inhibits the reaction when in concentration of higher than 1mM with the inhibition being greater as the concentration of phosphate increases. This is known as product inhibition. The Michaelis constant for this reaction has a value of 0.0871 and the reaction is first order.","SummaryThe experiment aimed to investigate conjugation and recombination in Esherichia coli and determine the chromosomal order of a number of genes for amino acid synthesis and sugar metabolism. This was carried out by mixing donor and recipient strains and at certain time intervals interrupting the mating and plating on selective media. Qualitative assessment of the plates was then used to map the gene order. The chromosomal gene order was found to be thr, arg and xyl, ilv, leu, pro, his (see discussion for explanation of gene names). IntroductionBacterial DNA can be exchanged between different strains allowing new allele combinations to be produced. This means that a 'wild type' gene can be passed to a strain with a mutation in this gene and through recombination the gene activity can be restored. The most common mechanisms of genetic exchange are transformation, transduction and conjugation. In each case genetic material is transferred from the donor strain to the recipient strain and this is assessed by monitoring a change in phenotype of the recipient. This is usually carried out by arranging growth conditions to select for recipients that have received the genetic material as in bacteria the change is normally in a requirement for an amino acid, nucleotide or vitamin, and ability to use a compound as an energy source or a resistance to an antibacterial agent. Here conjugation will be looked at. This involves cell-cell contact between donor and recipient achieved through an F pilus (a protein appendage) synthesized by the donor and used to anchor the recipient to form a mating pair (or mating aggregate). Here conjugation will be investigated in Escherichia coli. In E. coli conjugation involves the F factor, which is present in the donor but not recipient strain (F-). This experiment involves conjugation between an Hfr (high frequency recombination - the F factor is integrated into the chromosome) strain, the donor, and a F- strain, the recipient. The E. coli chromosome is circular and transfer always starts from the same point, the F factor integration site, which allows time of entry mapping to be carried out: As soon as the two strains are mixed mating aggregates will form and transfer of the chromosome will start from a fixed site and in a fixed orientation. If the mating pairs are physically interrupted at intervals, mapping the time of entry of genes from the donor into the recipient can create a genetic map of the chromosome. The aim of the experiment was to understand these concepts and methods of bacterial genetics by exchanging pieces of E. coli chromosome between different strains by the process of conjugation and using a non-quantitative method to establish the order of some genes relating to amino acid metabolism and sugar catabolism. MethodThe experiment was carried out as laid out in the lab manual with the following detail. The donor strain used was E. coli KL14 (thi-1 Hfr KL14) The bacteria donor and recipient strains were allowed to grow in the shaking water bath at 37 oC for 120 minutes before being mixed together. ResultsFrom the results it is clear that the bacteria could grow earlier on some plates than others. In this case the sample selective media lacking threonine (Plate 6) grew first at time 0minutes (but also had growth on the recipient strain area), then on both plates lacking arginine (plate 1) and with xylose as the sugar (plate 7) at 15 minutes. The sample on plate 3, lacking isoleucine and valine was next to grow then plate 4 lacking leucine at 30 minutes. Plate 5 lacking proline showed growth at 60 minutes and finally plate 2 lacking histidine at 120minutes. The E. coli chromosomal gene order was determined to be: thr, arg and xyl, ilv, leu, pro, hisDiscussionFrom the table of results it is clear that the genes allowing growth to occur by transferring the wild type allele were transferred at different time points for each gene selected. This is due to the process of Hfr conjugation discussed previously in which the chromosome is passed into the recipient in a certain order. As the mating pairs were disrupted at the time intervals given above no further gene transfer could take place in that particular sample and the sample would only grow on a selective plate if the gene allowing synthesis of that particular amino acid or metabolism of the sugar had already been passed into the recipient strain. It was therefore clear that by looking at the time points at which the samples started growing on different plates it was possible to map the order in which the genes had been transferred from the Hfr strain and hence the order in which they are present on the chromosome. Both plate 1 and 7 (lacking arginine and with xylose) showed growth at 15mins and had roughly the same amount of growth at this time and following times. It can therefore be concluded that these genes transferred at times fairly close together between 0 and 15 minutes and are therefore likely to be fairly close together on the chromosome. Plates 3 and 4 (lacking isoleucine/valine and leucine respectively) both showed growth for the first time on the sample plated at 30minutes. However Plate 3 showed more growth at the next sample at 60 minutes (+++) compared to plate 4 (++) and therefore it was concluded that the isoleucine/valine gene would have been the one transferred first probably nearer to the start of the time period (ie. 15minutes) Plate 6, lacking threonine appears to have growth at time 0 and much growth (+ +) at 15 minutes. However this plate also shows growth of the recipient strain alone. This could be due to the mutation in the recipient reverting to the wild type. This is only possible in a point-mutation - a deletion cannot revert as the DNA is missing completely. The mutation in the threonine coding gene in the recipient strain is known to not be a deletion (see materials list in lab manual) and therefore this reversion in possible. This would then invalidate the results for this plate, as this growth does not represent gene transfer. However the growth on plate 6 does still appear to show a gradation increasing at 15 minutes and 30 minutes and therefore there is still a high chance that this is an early gene. Taking all this into account an approximate order of gene transfer from the Hfr strain to the recipient can therefore be determined as follows: thr, arg and xyl, ilv, leu, pro, hisWhere thr = gene coding for threonine, arg = gene coding for arginine, xyl = gene involved in xylose metabolism, ilv = gene coding for isoleucine and valine, leu = gene coding for leucine, pro = gene coding for proline, his = gene coding for histidine. Possible sources of errors in this experiment include the risk of dislodging the mating pairs by causing shaking or jarring whilst removing samples. This would have led to no further gene transfer in the bacteria concerned and hence errors to the results as growth may not have occurred at a time point in which it would have otherwise. The time at which the genes appeared to be transferred was determined by plating onto selective media. It was necessary to not only select for the recombinant (the recipient after it has received genetic material) but also against the donor. This is known as counter selection and here was carried out by including nalidixic acid in the medium as this prevents donor growth but not the recipient. Nalidixic acid will also prevent further gene transfer. If it was not included the donor strain would also grow on the selective media and all samples would show growth at all times due to the presence of the donor. The procedure and accuracy of results could be improved by using a quantitative approach in which the number of recombinant colonies are counted at each time point using a viable count. In conclusion the rough mapping of gene order along the E. coli chromosome can be determined qualitatively by physically interrupting mating pairs of a donor and recipient E. coli strain at certain intervals and noting growth levels after plating on selective media. This highlights the basic concepts and methods of conjugation and recombination in bacteria.",True
39,"SummaryThe experiment aimed to investigate conjugation and recombination in Esherichia coli and determine the chromosomal order of a number of genes for amino acid synthesis and sugar metabolism. This was carried out by mixing donor and recipient strains and at certain time intervals interrupting the mating and plating on selective media. Qualitative assessment of the plates was then used to map the gene order. The chromosomal gene order was found to be thr, arg and xyl, ilv, leu, pro, his (see discussion for explanation of gene names). IntroductionBacterial DNA can be exchanged between different strains allowing new allele combinations to be produced. This means that a 'wild type' gene can be passed to a strain with a mutation in this gene and through recombination the gene activity can be restored. The most common mechanisms of genetic exchange are transformation, transduction and conjugation. In each case genetic material is transferred from the donor strain to the recipient strain and this is assessed by monitoring a change in phenotype of the recipient. This is usually carried out by arranging growth conditions to select for recipients that have received the genetic material as in bacteria the change is normally in a requirement for an amino acid, nucleotide or vitamin, and ability to use a compound as an energy source or a resistance to an antibacterial agent. Here conjugation will be looked at. This involves cell-cell contact between donor and recipient achieved through an F pilus (a protein appendage) synthesized by the donor and used to anchor the recipient to form a mating pair (or mating aggregate). Here conjugation will be investigated in Escherichia coli. In E. coli conjugation involves the F factor, which is present in the donor but not recipient strain (F-). This experiment involves conjugation between an Hfr (high frequency recombination - the F factor is integrated into the chromosome) strain, the donor, and a F- strain, the recipient. The E. coli chromosome is circular and transfer always starts from the same point, the F factor integration site, which allows time of entry mapping to be carried out: As soon as the two strains are mixed mating aggregates will form and transfer of the chromosome will start from a fixed site and in a fixed orientation. If the mating pairs are physically interrupted at intervals, mapping the time of entry of genes from the donor into the recipient can create a genetic map of the chromosome. The aim of the experiment was to understand these concepts and methods of bacterial genetics by exchanging pieces of E. coli chromosome between different strains by the process of conjugation and using a non-quantitative method to establish the order of some genes relating to amino acid metabolism and sugar catabolism. MethodThe experiment was carried out as laid out in the lab manual with the following detail. The donor strain used was E. coli KL14 (thi-1 Hfr KL14) The bacteria donor and recipient strains were allowed to grow in the shaking water bath at 37 oC for 120 minutes before being mixed together. ResultsFrom the results it is clear that the bacteria could grow earlier on some plates than others. In this case the sample selective media lacking threonine (Plate 6) grew first at time 0minutes (but also had growth on the recipient strain area), then on both plates lacking arginine (plate 1) and with xylose as the sugar (plate 7) at 15 minutes. The sample on plate 3, lacking isoleucine and valine was next to grow then plate 4 lacking leucine at 30 minutes. Plate 5 lacking proline showed growth at 60 minutes and finally plate 2 lacking histidine at 120minutes. The E. coli chromosomal gene order was determined to be: thr, arg and xyl, ilv, leu, pro, hisDiscussionFrom the table of results it is clear that the genes allowing growth to occur by transferring the wild type allele were transferred at different time points for each gene selected. This is due to the process of Hfr conjugation discussed previously in which the chromosome is passed into the recipient in a certain order. As the mating pairs were disrupted at the time intervals given above no further gene transfer could take place in that particular sample and the sample would only grow on a selective plate if the gene allowing synthesis of that particular amino acid or metabolism of the sugar had already been passed into the recipient strain. It was therefore clear that by looking at the time points at which the samples started growing on different plates it was possible to map the order in which the genes had been transferred from the Hfr strain and hence the order in which they are present on the chromosome. Both plate 1 and 7 (lacking arginine and with xylose) showed growth at 15mins and had roughly the same amount of growth at this time and following times. It can therefore be concluded that these genes transferred at times fairly close together between 0 and 15 minutes and are therefore likely to be fairly close together on the chromosome. Plates 3 and 4 (lacking isoleucine/valine and leucine respectively) both showed growth for the first time on the sample plated at 30minutes. However Plate 3 showed more growth at the next sample at 60 minutes (+++) compared to plate 4 (++) and therefore it was concluded that the isoleucine/valine gene would have been the one transferred first probably nearer to the start of the time period (ie. 15minutes) Plate 6, lacking threonine appears to have growth at time 0 and much growth (+ +) at 15 minutes. However this plate also shows growth of the recipient strain alone. This could be due to the mutation in the recipient reverting to the wild type. This is only possible in a point-mutation - a deletion cannot revert as the DNA is missing completely. The mutation in the threonine coding gene in the recipient strain is known to not be a deletion (see materials list in lab manual) and therefore this reversion in possible. This would then invalidate the results for this plate, as this growth does not represent gene transfer. However the growth on plate 6 does still appear to show a gradation increasing at 15 minutes and 30 minutes and therefore there is still a high chance that this is an early gene. Taking all this into account an approximate order of gene transfer from the Hfr strain to the recipient can therefore be determined as follows: thr, arg and xyl, ilv, leu, pro, hisWhere thr = gene coding for threonine, arg = gene coding for arginine, xyl = gene involved in xylose metabolism, ilv = gene coding for isoleucine and valine, leu = gene coding for leucine, pro = gene coding for proline, his = gene coding for histidine. Possible sources of errors in this experiment include the risk of dislodging the mating pairs by causing shaking or jarring whilst removing samples. This would have led to no further gene transfer in the bacteria concerned and hence errors to the results as growth may not have occurred at a time point in which it would have otherwise. The time at which the genes appeared to be transferred was determined by plating onto selective media. It was necessary to not only select for the recombinant (the recipient after it has received genetic material) but also against the donor. This is known as counter selection and here was carried out by including nalidixic acid in the medium as this prevents donor growth but not the recipient. Nalidixic acid will also prevent further gene transfer. If it was not included the donor strain would also grow on the selective media and all samples would show growth at all times due to the presence of the donor. The procedure and accuracy of results could be improved by using a quantitative approach in which the number of recombinant colonies are counted at each time point using a viable count. In conclusion the rough mapping of gene order along the E. coli chromosome can be determined qualitatively by physically interrupting mating pairs of a donor and recipient E. coli strain at certain intervals and noting growth levels after plating on selective media. This highlights the basic concepts and methods of conjugation and recombination in bacteria.","SummaryThe aim of this experiment was to carry out an enzyme assay to study the effects of pH, temperature and product inhibition on an enzyme and calculate the Michaelis constant and order or reaction. This was carried out by assaying alkaline phosphotase under different conditions measuring the extent of reaction with a spectrophotometer. It was discovered that the optimum pH was 9.5 whilst the optimum temperature is 42 oC and activity decreases either side of these optimums due to disruption at the active site or denaturing of the enzyme. Phosphate was found to carry out product inhibition and the reaction was determined as first order. The Michaelis constant was calculated to be 0.0871. IntroductionEnzymes are biological catalysts (they accelerate the rate of a reaction without themselves undergoing any net change) and catalyse most of the chemical reactions taking place in the cell. They carry out this catalysis by providing an alternative reaction pathway with a lower energy transition state. Enzymes are highly specific binding a specific substrate in the active site. The amount of enzyme activity and elements of the kinetics of an enzyme-catalysed reaction can be measured by measuring the rate of appearance of one of the products of the reaction. The effect of a variety of factors on rate of enzyme activity such as pH, temperature, concentration of substrate and enzyme and presence of inhibitors can also be determined using this method. Inhibition of an enzyme involved in an initial step in a metabolic pathway is often carried out by the end product. This is known as feedback inhibition and is an important regulatory strategy. A similar effect, product inhibition occurs when the product of an enzyme catalysed reaction inhibits the enzyme carrying out that reaction. An enzyme catalysed reaction has a reaction rate order. If the rate of the reaction at any instant is proportional to the concentration of the substrate then this is known as first order kinetics. Enzyme kinetics can also be described using the Michaelis-Menten equation. This involves the use of two parameters to describe the kinetic properties of enzymes - V max, the maximum velocity, and K M, the Michaelis Menten constant. K M is related to the affinity of the enzyme for its substrate and is defined as being the concentration of substrate at which the velocity is at half its maximum value. These are linked in the following equation where V O is the rate of formation of product:  FORMULA  Here alkaline phosphotase was used to study principles of enzyme kinetics. Alkaline Phosphotase is a phosphotase (hydrolyses organic esters to inorganic phosphate and alcohol) which is optically active at alkaline pH. It is an excellent enzyme to use in the determination of enzyme kinetics as it is robust and easily assayed. The synthetic substrate p-nitrophenyl phosphate (pNPP) is a convenient choice of substrate to demonstrate the activity of the enzyme as is it colourless until hydrolysed by the enzyme to the products inorganic phosphate and p-nitrophenol (pNP). p-nitrophenol is yellow at alkaline pHs (although colourless below pH7) and so the hydrolysis of pNPP can be followed using a spectrophotometer if the solution is made more alkaline after completion of reaction (eg. By adding NaOH). This also stops the reaction as at a very high pH the enzyme is denatured. pNP is also very stable once the reaction is stopped. The aim of the experiment was to quantitatively assay the activity of alkaline phosphotase under different conditions in order to study the effects of temperature, pH and product inhibition on the enzyme. The kinetic properties of the enzyme were also determined with the aim of calculating the Michaelis constant and the order of reaction. MethodThe experiment was carried out as described in the lab manual with the following details: Effect of pH on enzyme activityThe blank used for spectrophotometer was the second tube at each pH made up with no enzyme present. Effect of temperature on enzyme activityThe blank used was a second tube at each temperature with no enzyme present. Product Inhibition of enzyme actionThe blank used was made up of 2ml glycine buffer, 1ml phosphate, 1ml enzyme and no substrate. A control was made up with 2ml pNPP, 1ml glycine butter, 1ml enzyme and no substrate. The concentrations of phosphate used were (in mM) 1, 10, 20, 40, 60, 80, 100. Please see Appendix (Table 3) for volumes used in making up the concentrations. Determination of Michaelis-Menten constantThe blank used contained no substrate ie. 3ml buffer, 1ml enzyme. The concentrations of substrate (PNP) used were (in mM) 0.5, 1.25, 2.5, 3.75, 5.0 (see Appendix table 4 for volumes used) Order of reaction determinationTubes were made up with 1ml enzyme, 1ml glycine buffer and 2ml pNPP and the time course of the reaction determined by stopping the reaction and measuring the absorbance at a different time for each tube. The times at which each tube was stopped are as follows (in minutes): 0, 0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 5, 6, 8, 10, 14, 18, 20. ResultsThe graph below shows the varying concentrations of product, pNP, as the pH changes. This shows a clear increase in concentration as the pH increases to start with up until pH 9.5 where the graph peaks and then falls away again as the pH continues to increase. The graph is fairly symmetrical around the peak. The graph showing the effect of temperature on the product concentration shows a gradual increase getting sharper towards the peak at about 42 oC. After this point the concentration drops sharply as the temperature is further increased. Boiled enzyme at 37 oC showed only trace absorbance. Graph 3 shows the effect of increasing concentration of phosphate present in relation to the amount of pNP present. It is clear that as the concentration of phosphate is increased the concentration of pNP present falls rapidly. Graph 4 is an Eadie-Hoftsee Plot to calculate the Michaelis constant. This was calculated from the slope of the graph to be 0.0871. (see Appendix Table 5 for calculation). This graph (graph 5) to determine the order of reaction shows a straight line and gives a value of k of 0.013 (see appendix for calculation). DiscussionThe concentration of product can be used as a measure of enzyme activity as all the tubes started off with the same amount of substrate in each part of the experiment. Therefore the tubes with more product at the end must have had more enzyme activity take place in order to produce more product. The effect of pH on enzyme activityThe graph clearly shows that the activity of the enzyme increases up until pH 9.5 before falling away again as the pH continues to increase. The optimum pH for alkaline phosphotase activity is therefore pH 9.5. The reason behind the change in activity with changing pH is to do with the structure of the enzyme and the mechanism with which is works. Firstly enzymes are proteins. Therefore at extremes of pH the enzyme will become denatured - its active site will bend out of shape and no longer function. Therefore enzymes only work over specific pH ranges. The presence of free hydrogen ions or hydroxyl ions can affect the charges on amino acid residues making up the polypeptide chain or chains in the enzyme distorting the three-dimensional shape and causing an irreversible change in the protein's tertiary structure. Secondly enzymes are sensitive to changes in pH due to the great sensitivity of their active site. Even a slight change in pH may upset the chemical arrangement at the active site upsetting substrate binding by disrupting the multiple weak interactions by which the substrate-enzyme complex is formed. The nature of the side chains present in the enzymes structure will therefore play a large part in determining the optimum pH of the enzyme. The effect of temperature on enzyme activityFrom the graph it is clear that the temperature at which alkaline phosphotase carries out the most activity (the optimum temperature) is 42 oC. Below this point the activity of the enzyme is gradually increasing and after the optimum the activity of the enzyme drops sharply. The gradual increase before the peak is as a result of the general effect of increasing temperature on the rate of any chemical reaction - more kinetic energy is present so the molecules can move around faster increasing the chance of a favourable collision. The descending portion of the graph is due to a decrease in catalytic ability as the enzyme molecules start to be denatured. The denaturation occurs because of decreased stability in the structure of the enzyme molecule at high temperatures resulting in changes to the shape of the active site reducing its affinity for substrate and thereby its activity. At very high temperatures all the enzyme molecules present will be denatured and no activity will take place. This is shown by the trace absorbance shown by the boiled enzyme. Virtually no activity has taken place. The small amount of absorbance present may be due to tiny amounts of product formed from the uncatalysed reaction or a few enzyme molecules still active or from contamination from another source. To have a high optimum pH an enzyme must therefore have a more stable structure than those functioning at lower temperatures, as it must be able to withstand these higher temperatures without losing stability. Product Inhibition of Enzyme ActionIt is clear from graph 3 above that increasing the concentration of phosphate (one of the products of the hydrolysis of p-nitrophenyl phosphate) causes a major decrease in the activity of the enzyme - the presence of phosphate is inhibiting the enzyme. As the rate of the reaction started to drop at 10mM phosphate it is clear that the lowest molarity of phosphate which will inhibit alkaline phosphotase lies somewhere between 1mM and 10mM. At 1mM the absorbance is almost the same as the control with no phosphate present and so only a very small amount of inhibition, if any, is occurring. Inhibition of the enzyme by the product of the reaction it catalyses in this way is known as product inhibition and has a regulatory role in many metabolic pathways. As the moles of phosphate being produced in the previous experiments were all a lot smaller than this (all were micromolar quantities) it seems that the phosphate released in these experiments would have been to small to have had an inhibitory effect on the enzyme. Determination of Michaelis constantThe figure arrived at for this constant was 0.0871. However a lot of error could have been introduced into the calculation throughout the experiment. Firstly in making up the test solutions at the start imprecision could have introduced errors. Translating the absorbances into concentrations involved the use of a calibration curve which means the concentrations are really estimates and the use of a trend line also introduces new error all of which could have resulted in an inaccurate result. The introduction of these errors could have occurred in any of the experiments carried out here. Order of reactionThe graph clearly shows a straight line indicating a first order reaction whilst the value of k is less than 1.4 also indicating a first order reaction. This means that the rate of the reaction at any moment is proportional to the concentration of the substrate. Although there are two reactants in the reaction (p-nitrophenyl phosphate and water) the fact that it is still a first order reaction must be due to the mechanism of the reaction. This may be because water is present in such a large excess as pure water is highly concentrated that it is not having an effect on the rate, that is the rate is still only dependent on one concentration - that of p-nitrophenylphosphate and so still only shows first-order kinetics. ConclusionsIn conclusion therefore it was determined that the optimum pH of alkaline phosphatase is 9.5 whilst the optimum temperature is about 42 oC. Activity of the enzyme decreases either side of these optimums. The presence of phosphate, one of the products, inhibits the reaction when in concentration of higher than 1mM with the inhibition being greater as the concentration of phosphate increases. This is known as product inhibition. The Michaelis constant for this reaction has a value of 0.0871 and the reaction is first order.",False
40,"IntroductionThe diagram below shows the generalised amino acid: It can be seen that apart from having an amino group (A terminus) and a carboxyl group (C terminus) the amino acid also has an 'R' group. It is this 'R' group which determines each amino acid and gives it specific characteristics such as charge. A term used to describe an amino acid with respect to it's charge is 'isoelectric point'. This is the pH at which the overall charge on the amino acid is neutral. As amino acids are the subunits of proteins, it is the charge of an amino acid which can determine the three dimensional structure of proteins by influencing the bonding which occurs. If adjacent amino acids are oppositely charged due to the differing properties of their 'R' groups, then electrostatic forces will occur and part of the three dimensional structure of the protein will be formed. The differing charges of amino acids can also be used to identify amino acids in a protein using electrophoresis. This process allows amino acids to migrate towards either the anode or the cathode depending on their charge. Because the distance and direction each amino acid will migrate varies, the composition of the protein can be determined. MethodThe methodologies I followed in order to carry out the two electrophoresis' and to calculate the pH values for each sample are described on pages eighteen and nineteen of part A of the laboratory manual. The only factor of our method that deviated from that set out in the handbook was that the volume of each buffer we prepared was 2.5ml rather than 5.0ml. In order to determine the isoelectric point of casein we firstly had to create a series of buffers (acetic acid) ranging from 0.3M to 1mM. We did this by doubling dilutions. Firstly the initial concentration (0.3M) was created by mixing 1.5ml of 1M acetic acid with 3.5ml of distilled water. After ensuring that this solution was thoroughly mixed, we extracted 2.5ml into a second test tube and added 2.5ml of distilled water, therefore creating a buffer that had a concentration of 0.15M. The process was then repeated so that the following test tube had a concentration of 0.075M, then 0.375M etc. The full calculations used to create each buffer are shown in section one of the appendix. Once the buffers had been prepared, 1ml aliquots of casein (dissolved in 0.1M sodium acetate) were added to each one. As a result the casein precipitated to varying degrees so that the precipitation was greatest in the buffer with the pH closest to the isoelectric point of casein. In order to determine the isoelectric point of casein, the absorbance of each solution was determined using a spectrophotometer. Using a wavelength of 360nm and calibrating using distilled water between each reading, a sample from each of the test tubes was placed in separated cuvettes and the absorbance measured. The distilled water was used as a blank (i.e. it had an absorption of 0) to enable each of the readings to be relative to one another. The samples from test tubes two to five (in which the precipitation was the greatest) had to be diluted before the absorption was calculated as otherwise the values would have been too great. The dilutions were carried out by adding 1ml of the precipitated solution to 4ml of distilled water (i.e. a one in five dilution). The values for the absorption of these four samples were then multiplied by five to make them comparable with the rest of the data. The values for absorption are shown in the table in the results section on the next page. Results(The calculations for pH are shown in section three of the appendix) The graph above shows the relationship between the absorbance and the pH of the nine samples we tested (i.e. the nine different buffer concentrations each with 1ml of casein dissolved in 0.1M sodium acetate). The calculations for pH are shown in section three if the appendix. After adding the casein to each of the buffer solutions it was instantly visible that a precipitation had formed and that the degree of precipitation varied between the different concentrations of buffer solution used. It wasn't however until the absorbance of each sample had been determined that we were able to say which buffer had a pH closest to the isoelectric point of casein. The graph shows that the first buffer solution (0.3M) allowed some of the casein to dissolve leaving the precipitated molecules to produce an absorbency of 0.3. The absorbency then increased to 4.115 at a pH of 4.14 indicating that at this pH less of the casein was able to dissolve. There is then a decline in absorbency between pH 4.14 and 4.74 i.e. more of the casein was dissolving in the buffer solution. The peak in the graph at pH 5.04 indicates that out of the nine precipitate solutions we tested, sample five had a pH that was closest to the isoelectric point of casein. The reasoning behind this is that if we know that at it's isoelectric point casein has a net neutral charge, then it is unable to become dissolved in a solvent (i.e. the buffer solution). As a result the greatest degree of precipitation is formed and therefore this particular sample has the highest absorbance reading. The graph shows that at pH values greater than 5.04 the absorbance decreases. This is due to the fact that as the pH increases there are less hydrogen ions in the buffer. At this point the pH is greater than the pK a (i.e. the pH at which fifty percent of the ionisable groups are charged) and the molecule looses protons to become negatively charged. It is this charge that enabled the casein molecules to partially dissolve in the buffer solution and hence produce a declining absorbency as less and less precipitate formed. The electrophoretogram for haemoglobin and cytochrome c is shown in section four of the appendix. It shows that the mixture sample contained both haemoglobin and cytochrome c and that they were of opposite charges at pH 8.6 (the pH of the buffer-barbitone). By comparing to the charges indicated on the chromatography paper to the direction the haemoglobin and cytochrome c molecules moved, we can determine that at pH 8.6 the haemoglobin had a net negative charge and the cytochrome c had a net positive charge. We can also compare the distances travelled by both samples in order to roughly determine the isoelectric point of each. As the haemoglobin sample moved only one and a half centimetres we can say that it was less affected by the electric field (i.e. it had a smaller potential difference) than cytochrome c, which moved by four centimetres. In other words, the isoelectric point of haemoglobin is closer to pH 8.6 than the isoelectric point of cytochrome c, so that in the experiment haemoglobin had a net charge nearer to neutral. ConclusionThe aim of this experiment was to investigate isoelectric points of different molecules hence determining the isoelectric point of casein, haemoglobin and cytochrome c. Firstly casein. The method we undertook in order to determine the isoelectric point of casein allowed us to use various techniques (the Henderson-Hasselbalch equation, doubling dilutions and spectrophotometry). Whilst the Henderson-Hasselbalch equation and the spectrophotometer were the best methods to determine the pH and the degree of precipitation of each sample, the process of doubling dilutions did not enable us to achieve as many different buffers within the given range (0.3M to 1mM) as were needed to accurately determine the isoelectric point of casein. This method could therefore be improved by determining the degree of precipitation at a greater number of pHs by preparing more buffer solutions. A second way in which the accuracy of this part of the investigation could be improved is by repeating the method at least four times for each different concentration of buffer. In this way the absorbance values could be averaged, ensuring that any anomalies were less prevalent and therefore making the overall results and conclusions more accurate. Secondly haemoglobin and cytochrome c. Although the method of electrophoresis allowed us to determine the net charge of each molecule at pH 8.6 (i.e. the pH of the buffer), it did not allow us to accurately determine their isoelectric points and without any further investigation these values would not be completely accurate. The data we received from this method allows us to conclude that the isoelectric point of haemoglobin is approximately 8.6 minus half the difference between the pH of the buffer and the isoelectric point of cytochrome c (concluded by comparing the relative distances travelled by each molecule). In conclusion our results indicate that the isoelectric points of haemoglobin and cytochrome c are approximately 7.85 and 10.6 respectively. In order to increase the accuracy of these values, the method could be improved by the addition of marker molecules with known isoelectric points at pH 8.6. The comparison between the distance the marker molecules and haemoglobin and cytochrome c migrated could then be used to accurately determine the two unknown isoelectric points. SummaryIn summary, various methods can be used to identify the isoelectric points of different molecules.","IntroductionThe study of growth kinetics is conducted in order to determine the process of bacterial growth in a closed culture. There are many ways in which the cell growth can be monitored, each slightly different in methodology and each producing slightly different sets of results. For example, the cell growth can be monitored by following the change in viable count, measuring biochemical changes within the sample, for example protein concentration or as we aimed to do, by measuring bacterial growth through the change in culture absorbance. In this way we aimed to follow the growth of Serratia marcenscens and collect absorbance data that would enable us to draw a growth curve upon which we would be able to locate the different stages of bacterial growth. In addition we aimed to collect data for nine different species of bacteria in order to determine the composition of a mixed culture. This section of the investigation relied upon common microbiological practises such as the gram stain and the plating of different organisms onto different selective agar plates. MethodThe method for this investigation is described on pages forty-seven to forty-eight of section one, and pages twenty-four and sixty-nine to seventy-one of section two of the laboratory handbook. At no point did we deviate from this method. The calculations for the serial dilutions are shown in section two of the appendix. The results for the identification of the mixed culture were obtained from a previous investigation. The method for this is described on pages forty-four to forty-six of section one and sections nineteen, and twenty-three to twenty-five of section two of the handbook. There were no deviations from this method. ResultsThe graph showing the relationship between absorbance (at 600nm) and time (in minutes) is shown on the additional sheet. The data we obtained to plot this graph (shown in section one of the appendix) clearly show the first five phases of the culture growth. Phase one (the lag period) occurred between 0 and 30 minutes during which time the absorbance increased slowly from 0.05 to 0.07. The second phase is the acceleration phase when the absorbance increased more rapidly from 0.07 to 0.099 before entering the exponential phase between forty minutes and one hundred and twenty minutes. During the exponential phase the change in absorbancy occurred at the greatest rate. It is also during this phase that the specific growth rate was calculated as 0.912 (as shown in section three of the appendix). Our graph also shows the retardation phase when the rate of change in absorbance began to slow and the maximum population phase when the absorbance showed a plateau. These are marked on the graph as phases four and five respectively. The gram stain results for each of the organisms studied last week are shown in figure five of the appendix. In total we identified three gram negative organisms and six gram positive organisms. The appearance of the colonies of each species is also described in section five of the appendix. The results from the plated samples of mixed culture are shown in section six of the appendix. The plate which contained the boiled sample showed evidence of Bacillus pumilis, the plate containing the antibiotic Kanamycin contained Flavobacterium aquatile, the plate containing the antibiotic Nalidixic acid showed E.coli, and the nutrient agar plate contained  Bacillus pumilis, Flavobacterium aquatile, E.coli and Pseudomonas fluorescens. Discussion and conclusionAs shown on the additional sheet and described in the results section, the data we obtained for the growth of Serratia marcescens depicts the typical bacterial growth curve. This theory can therefore be used to explain the changes in absorbance over time. The first phase (the lag phase) occurred in the first thirty minutes of the investigation. This was the time period when the cells in the culture were getting ready to enter the active growth phase and therefore had to synthesis all the enzymes needed for transport and metabolism of the available nutrients, and for the production of new cell products. Although the data we collected showed this phase to be quite inactive, if we had collected data for protein concentration for example, this phase would have shown quite the opposite result. The second phase shown in our results is the acceleration phase (phase two on the graph). The absorbancy in this time period increased as some of the cells began to increase in size and divide. Although the growing cells at this point are growing at the maximum possible speed, the increase in absorbance is still quite slow. This is because due to the natural heterogeneity of the population some of the cells are still in the lag phase and therefore haven't started to grow and divide at this point. The point at which all the viable cells in the culture were growing and dividing is shown by the exponential phase (i.e. between forty minutes and one hundred and twenty minutes on the graph). It is only during this phase that calculations regarding the rate of growth can be calculated (for example the specific growth rate and the doubling time). The values for these two growth rates were calculated to be 0.912 absorbance increase per hour and 0.760 hours respectively. The doubling time value is confirmed from our graph. It took approximately forty minutes for the initial absorbance to double i.e. approximately the same time as the calculated doubling time value of 0.760 hours. The next phase (the retardation phase) is a reflection of the gradual depletion of available nutrients, which slows the cell growth. The rate of growth at this point also decreases as some of the cells stop growing before others until all of the cells stop growing and the absorbance remains constant i.e. the stationary phase. This is shown in section five of our graph on the additional page. The second section of our investigation was to determine the composition of our mixed culture. Firstly, using the plated boiled sample we were able to identify the presence of Bacillus pumilis. This was because the Bacillus pumilis was the only organism to survive the heating due to its production of endospores. The presence of this organism in the mixed culture was also reinforced by the mixed culture motility test which identified motile rods (i.e. Bacillus pumilis) and the gram stain test, which identified gram-positive rods (Bacillus pumilis is gram positive). This organism was also identified to be growing on the nutrient agar plate seeded with the mixed culture. The second organism to be identified in the mixed culture was Flavobacterium aquatile. The identification of this organism was primarily due to the appearance of its colonies as they were a distinctive yellow colour. This same colony was also identified on the mixed culture agar plate. The gram stain of the mixed culture also proved that Flavobacterium aquatile was present as it showed negative rods. The third organism was identified using two methods. Firstly colony comparisons identified the organism growing on the Nalidixic plate to be either E.coli or Pseudomonas. However when a motility test was carried out the organism was shown to be E.coli as it was motile but not highly motile as this would have indicated the presence of Pseudomonas. The presence of this organism in the mixed culture was also identified in the mixed culture gram stain (which showed gram negative rods) and the mixed culture motility test which showed motile rods i.e. E.coli. The final organism was identified from the nutrient agar plate containing the mixed culture, and from the mixed culture motility and gram stain tests. The colonies on the agar plate were small, white and mucoid, identical to those of the pure culture Pseudomonas. The motility test showed the presence of a highly motile rod shaped organism (i.e. again suggesting Pseudomonas) and the gram stain confirmed this by showing gram-negative rods. Therefore in conclusion the mixed culture contained the following organisms, Bacillus pumilis, Flavobacterium aquatile, E.coli and Pseudomonas. SummaryObtaining data to show the growth kinetics of an organism can provide important information that can be used to identify the processes occurring within a culture at any given time. By using the obtained data to plot a growth curve the replication cycle of an organism can be split into phases, hence allowing further study and understanding of the processes involved. Microbiological practices such as gram stains, agar plating and motility tests are very useful in the identification of unknown organisms. The information they give as a group can be used to further prove and therefore aid identification as ideas are backed up with increasing evidence.",True
41,"IntroductionThe study of growth kinetics is conducted in order to determine the process of bacterial growth in a closed culture. There are many ways in which the cell growth can be monitored, each slightly different in methodology and each producing slightly different sets of results. For example, the cell growth can be monitored by following the change in viable count, measuring biochemical changes within the sample, for example protein concentration or as we aimed to do, by measuring bacterial growth through the change in culture absorbance. In this way we aimed to follow the growth of Serratia marcenscens and collect absorbance data that would enable us to draw a growth curve upon which we would be able to locate the different stages of bacterial growth. In addition we aimed to collect data for nine different species of bacteria in order to determine the composition of a mixed culture. This section of the investigation relied upon common microbiological practises such as the gram stain and the plating of different organisms onto different selective agar plates. MethodThe method for this investigation is described on pages forty-seven to forty-eight of section one, and pages twenty-four and sixty-nine to seventy-one of section two of the laboratory handbook. At no point did we deviate from this method. The calculations for the serial dilutions are shown in section two of the appendix. The results for the identification of the mixed culture were obtained from a previous investigation. The method for this is described on pages forty-four to forty-six of section one and sections nineteen, and twenty-three to twenty-five of section two of the handbook. There were no deviations from this method. ResultsThe graph showing the relationship between absorbance (at 600nm) and time (in minutes) is shown on the additional sheet. The data we obtained to plot this graph (shown in section one of the appendix) clearly show the first five phases of the culture growth. Phase one (the lag period) occurred between 0 and 30 minutes during which time the absorbance increased slowly from 0.05 to 0.07. The second phase is the acceleration phase when the absorbance increased more rapidly from 0.07 to 0.099 before entering the exponential phase between forty minutes and one hundred and twenty minutes. During the exponential phase the change in absorbancy occurred at the greatest rate. It is also during this phase that the specific growth rate was calculated as 0.912 (as shown in section three of the appendix). Our graph also shows the retardation phase when the rate of change in absorbance began to slow and the maximum population phase when the absorbance showed a plateau. These are marked on the graph as phases four and five respectively. The gram stain results for each of the organisms studied last week are shown in figure five of the appendix. In total we identified three gram negative organisms and six gram positive organisms. The appearance of the colonies of each species is also described in section five of the appendix. The results from the plated samples of mixed culture are shown in section six of the appendix. The plate which contained the boiled sample showed evidence of Bacillus pumilis, the plate containing the antibiotic Kanamycin contained Flavobacterium aquatile, the plate containing the antibiotic Nalidixic acid showed E.coli, and the nutrient agar plate contained  Bacillus pumilis, Flavobacterium aquatile, E.coli and Pseudomonas fluorescens. Discussion and conclusionAs shown on the additional sheet and described in the results section, the data we obtained for the growth of Serratia marcescens depicts the typical bacterial growth curve. This theory can therefore be used to explain the changes in absorbance over time. The first phase (the lag phase) occurred in the first thirty minutes of the investigation. This was the time period when the cells in the culture were getting ready to enter the active growth phase and therefore had to synthesis all the enzymes needed for transport and metabolism of the available nutrients, and for the production of new cell products. Although the data we collected showed this phase to be quite inactive, if we had collected data for protein concentration for example, this phase would have shown quite the opposite result. The second phase shown in our results is the acceleration phase (phase two on the graph). The absorbancy in this time period increased as some of the cells began to increase in size and divide. Although the growing cells at this point are growing at the maximum possible speed, the increase in absorbance is still quite slow. This is because due to the natural heterogeneity of the population some of the cells are still in the lag phase and therefore haven't started to grow and divide at this point. The point at which all the viable cells in the culture were growing and dividing is shown by the exponential phase (i.e. between forty minutes and one hundred and twenty minutes on the graph). It is only during this phase that calculations regarding the rate of growth can be calculated (for example the specific growth rate and the doubling time). The values for these two growth rates were calculated to be 0.912 absorbance increase per hour and 0.760 hours respectively. The doubling time value is confirmed from our graph. It took approximately forty minutes for the initial absorbance to double i.e. approximately the same time as the calculated doubling time value of 0.760 hours. The next phase (the retardation phase) is a reflection of the gradual depletion of available nutrients, which slows the cell growth. The rate of growth at this point also decreases as some of the cells stop growing before others until all of the cells stop growing and the absorbance remains constant i.e. the stationary phase. This is shown in section five of our graph on the additional page. The second section of our investigation was to determine the composition of our mixed culture. Firstly, using the plated boiled sample we were able to identify the presence of Bacillus pumilis. This was because the Bacillus pumilis was the only organism to survive the heating due to its production of endospores. The presence of this organism in the mixed culture was also reinforced by the mixed culture motility test which identified motile rods (i.e. Bacillus pumilis) and the gram stain test, which identified gram-positive rods (Bacillus pumilis is gram positive). This organism was also identified to be growing on the nutrient agar plate seeded with the mixed culture. The second organism to be identified in the mixed culture was Flavobacterium aquatile. The identification of this organism was primarily due to the appearance of its colonies as they were a distinctive yellow colour. This same colony was also identified on the mixed culture agar plate. The gram stain of the mixed culture also proved that Flavobacterium aquatile was present as it showed negative rods. The third organism was identified using two methods. Firstly colony comparisons identified the organism growing on the Nalidixic plate to be either E.coli or Pseudomonas. However when a motility test was carried out the organism was shown to be E.coli as it was motile but not highly motile as this would have indicated the presence of Pseudomonas. The presence of this organism in the mixed culture was also identified in the mixed culture gram stain (which showed gram negative rods) and the mixed culture motility test which showed motile rods i.e. E.coli. The final organism was identified from the nutrient agar plate containing the mixed culture, and from the mixed culture motility and gram stain tests. The colonies on the agar plate were small, white and mucoid, identical to those of the pure culture Pseudomonas. The motility test showed the presence of a highly motile rod shaped organism (i.e. again suggesting Pseudomonas) and the gram stain confirmed this by showing gram-negative rods. Therefore in conclusion the mixed culture contained the following organisms, Bacillus pumilis, Flavobacterium aquatile, E.coli and Pseudomonas. SummaryObtaining data to show the growth kinetics of an organism can provide important information that can be used to identify the processes occurring within a culture at any given time. By using the obtained data to plot a growth curve the replication cycle of an organism can be split into phases, hence allowing further study and understanding of the processes involved. Microbiological practices such as gram stains, agar plating and motility tests are very useful in the identification of unknown organisms. The information they give as a group can be used to further prove and therefore aid identification as ideas are backed up with increasing evidence.","IntroductionThe diagram below shows the generalised amino acid: It can be seen that apart from having an amino group (A terminus) and a carboxyl group (C terminus) the amino acid also has an 'R' group. It is this 'R' group which determines each amino acid and gives it specific characteristics such as charge. A term used to describe an amino acid with respect to it's charge is 'isoelectric point'. This is the pH at which the overall charge on the amino acid is neutral. As amino acids are the subunits of proteins, it is the charge of an amino acid which can determine the three dimensional structure of proteins by influencing the bonding which occurs. If adjacent amino acids are oppositely charged due to the differing properties of their 'R' groups, then electrostatic forces will occur and part of the three dimensional structure of the protein will be formed. The differing charges of amino acids can also be used to identify amino acids in a protein using electrophoresis. This process allows amino acids to migrate towards either the anode or the cathode depending on their charge. Because the distance and direction each amino acid will migrate varies, the composition of the protein can be determined. MethodThe methodologies I followed in order to carry out the two electrophoresis' and to calculate the pH values for each sample are described on pages eighteen and nineteen of part A of the laboratory manual. The only factor of our method that deviated from that set out in the handbook was that the volume of each buffer we prepared was 2.5ml rather than 5.0ml. In order to determine the isoelectric point of casein we firstly had to create a series of buffers (acetic acid) ranging from 0.3M to 1mM. We did this by doubling dilutions. Firstly the initial concentration (0.3M) was created by mixing 1.5ml of 1M acetic acid with 3.5ml of distilled water. After ensuring that this solution was thoroughly mixed, we extracted 2.5ml into a second test tube and added 2.5ml of distilled water, therefore creating a buffer that had a concentration of 0.15M. The process was then repeated so that the following test tube had a concentration of 0.075M, then 0.375M etc. The full calculations used to create each buffer are shown in section one of the appendix. Once the buffers had been prepared, 1ml aliquots of casein (dissolved in 0.1M sodium acetate) were added to each one. As a result the casein precipitated to varying degrees so that the precipitation was greatest in the buffer with the pH closest to the isoelectric point of casein. In order to determine the isoelectric point of casein, the absorbance of each solution was determined using a spectrophotometer. Using a wavelength of 360nm and calibrating using distilled water between each reading, a sample from each of the test tubes was placed in separated cuvettes and the absorbance measured. The distilled water was used as a blank (i.e. it had an absorption of 0) to enable each of the readings to be relative to one another. The samples from test tubes two to five (in which the precipitation was the greatest) had to be diluted before the absorption was calculated as otherwise the values would have been too great. The dilutions were carried out by adding 1ml of the precipitated solution to 4ml of distilled water (i.e. a one in five dilution). The values for the absorption of these four samples were then multiplied by five to make them comparable with the rest of the data. The values for absorption are shown in the table in the results section on the next page. Results(The calculations for pH are shown in section three of the appendix) The graph above shows the relationship between the absorbance and the pH of the nine samples we tested (i.e. the nine different buffer concentrations each with 1ml of casein dissolved in 0.1M sodium acetate). The calculations for pH are shown in section three if the appendix. After adding the casein to each of the buffer solutions it was instantly visible that a precipitation had formed and that the degree of precipitation varied between the different concentrations of buffer solution used. It wasn't however until the absorbance of each sample had been determined that we were able to say which buffer had a pH closest to the isoelectric point of casein. The graph shows that the first buffer solution (0.3M) allowed some of the casein to dissolve leaving the precipitated molecules to produce an absorbency of 0.3. The absorbency then increased to 4.115 at a pH of 4.14 indicating that at this pH less of the casein was able to dissolve. There is then a decline in absorbency between pH 4.14 and 4.74 i.e. more of the casein was dissolving in the buffer solution. The peak in the graph at pH 5.04 indicates that out of the nine precipitate solutions we tested, sample five had a pH that was closest to the isoelectric point of casein. The reasoning behind this is that if we know that at it's isoelectric point casein has a net neutral charge, then it is unable to become dissolved in a solvent (i.e. the buffer solution). As a result the greatest degree of precipitation is formed and therefore this particular sample has the highest absorbance reading. The graph shows that at pH values greater than 5.04 the absorbance decreases. This is due to the fact that as the pH increases there are less hydrogen ions in the buffer. At this point the pH is greater than the pK a (i.e. the pH at which fifty percent of the ionisable groups are charged) and the molecule looses protons to become negatively charged. It is this charge that enabled the casein molecules to partially dissolve in the buffer solution and hence produce a declining absorbency as less and less precipitate formed. The electrophoretogram for haemoglobin and cytochrome c is shown in section four of the appendix. It shows that the mixture sample contained both haemoglobin and cytochrome c and that they were of opposite charges at pH 8.6 (the pH of the buffer-barbitone). By comparing to the charges indicated on the chromatography paper to the direction the haemoglobin and cytochrome c molecules moved, we can determine that at pH 8.6 the haemoglobin had a net negative charge and the cytochrome c had a net positive charge. We can also compare the distances travelled by both samples in order to roughly determine the isoelectric point of each. As the haemoglobin sample moved only one and a half centimetres we can say that it was less affected by the electric field (i.e. it had a smaller potential difference) than cytochrome c, which moved by four centimetres. In other words, the isoelectric point of haemoglobin is closer to pH 8.6 than the isoelectric point of cytochrome c, so that in the experiment haemoglobin had a net charge nearer to neutral. ConclusionThe aim of this experiment was to investigate isoelectric points of different molecules hence determining the isoelectric point of casein, haemoglobin and cytochrome c. Firstly casein. The method we undertook in order to determine the isoelectric point of casein allowed us to use various techniques (the Henderson-Hasselbalch equation, doubling dilutions and spectrophotometry). Whilst the Henderson-Hasselbalch equation and the spectrophotometer were the best methods to determine the pH and the degree of precipitation of each sample, the process of doubling dilutions did not enable us to achieve as many different buffers within the given range (0.3M to 1mM) as were needed to accurately determine the isoelectric point of casein. This method could therefore be improved by determining the degree of precipitation at a greater number of pHs by preparing more buffer solutions. A second way in which the accuracy of this part of the investigation could be improved is by repeating the method at least four times for each different concentration of buffer. In this way the absorbance values could be averaged, ensuring that any anomalies were less prevalent and therefore making the overall results and conclusions more accurate. Secondly haemoglobin and cytochrome c. Although the method of electrophoresis allowed us to determine the net charge of each molecule at pH 8.6 (i.e. the pH of the buffer), it did not allow us to accurately determine their isoelectric points and without any further investigation these values would not be completely accurate. The data we received from this method allows us to conclude that the isoelectric point of haemoglobin is approximately 8.6 minus half the difference between the pH of the buffer and the isoelectric point of cytochrome c (concluded by comparing the relative distances travelled by each molecule). In conclusion our results indicate that the isoelectric points of haemoglobin and cytochrome c are approximately 7.85 and 10.6 respectively. In order to increase the accuracy of these values, the method could be improved by the addition of marker molecules with known isoelectric points at pH 8.6. The comparison between the distance the marker molecules and haemoglobin and cytochrome c migrated could then be used to accurately determine the two unknown isoelectric points. SummaryIn summary, various methods can be used to identify the isoelectric points of different molecules.",False
42,"IntroductionThe study of growth kinetics is conducted in order to determine the process of bacterial growth in a closed culture. There are many ways in which the cell growth can be monitored, each slightly different in methodology and each producing slightly different sets of results. For example, the cell growth can be monitored by following the change in viable count, measuring biochemical changes within the sample, for example protein concentration or as we aimed to do, by measuring bacterial growth through the change in culture absorbance. In this way we aimed to follow the growth of Serratia marcenscens and collect absorbance data that would enable us to draw a growth curve upon which we would be able to locate the different stages of bacterial growth. In addition we aimed to collect data for nine different species of bacteria in order to determine the composition of a mixed culture. This section of the investigation relied upon common microbiological practises such as the gram stain and the plating of different organisms onto different selective agar plates. MethodThe method for this investigation is described on pages forty-seven to forty-eight of section one, and pages twenty-four and sixty-nine to seventy-one of section two of the laboratory handbook. At no point did we deviate from this method. The calculations for the serial dilutions are shown in section two of the appendix. The results for the identification of the mixed culture were obtained from a previous investigation. The method for this is described on pages forty-four to forty-six of section one and sections nineteen, and twenty-three to twenty-five of section two of the handbook. There were no deviations from this method. ResultsThe graph showing the relationship between absorbance (at 600nm) and time (in minutes) is shown on the additional sheet. The data we obtained to plot this graph (shown in section one of the appendix) clearly show the first five phases of the culture growth. Phase one (the lag period) occurred between 0 and 30 minutes during which time the absorbance increased slowly from 0.05 to 0.07. The second phase is the acceleration phase when the absorbance increased more rapidly from 0.07 to 0.099 before entering the exponential phase between forty minutes and one hundred and twenty minutes. During the exponential phase the change in absorbancy occurred at the greatest rate. It is also during this phase that the specific growth rate was calculated as 0.912 (as shown in section three of the appendix). Our graph also shows the retardation phase when the rate of change in absorbance began to slow and the maximum population phase when the absorbance showed a plateau. These are marked on the graph as phases four and five respectively. The gram stain results for each of the organisms studied last week are shown in figure five of the appendix. In total we identified three gram negative organisms and six gram positive organisms. The appearance of the colonies of each species is also described in section five of the appendix. The results from the plated samples of mixed culture are shown in section six of the appendix. The plate which contained the boiled sample showed evidence of Bacillus pumilis, the plate containing the antibiotic Kanamycin contained Flavobacterium aquatile, the plate containing the antibiotic Nalidixic acid showed E.coli, and the nutrient agar plate contained  Bacillus pumilis, Flavobacterium aquatile, E.coli and Pseudomonas fluorescens. Discussion and conclusionAs shown on the additional sheet and described in the results section, the data we obtained for the growth of Serratia marcescens depicts the typical bacterial growth curve. This theory can therefore be used to explain the changes in absorbance over time. The first phase (the lag phase) occurred in the first thirty minutes of the investigation. This was the time period when the cells in the culture were getting ready to enter the active growth phase and therefore had to synthesis all the enzymes needed for transport and metabolism of the available nutrients, and for the production of new cell products. Although the data we collected showed this phase to be quite inactive, if we had collected data for protein concentration for example, this phase would have shown quite the opposite result. The second phase shown in our results is the acceleration phase (phase two on the graph). The absorbancy in this time period increased as some of the cells began to increase in size and divide. Although the growing cells at this point are growing at the maximum possible speed, the increase in absorbance is still quite slow. This is because due to the natural heterogeneity of the population some of the cells are still in the lag phase and therefore haven't started to grow and divide at this point. The point at which all the viable cells in the culture were growing and dividing is shown by the exponential phase (i.e. between forty minutes and one hundred and twenty minutes on the graph). It is only during this phase that calculations regarding the rate of growth can be calculated (for example the specific growth rate and the doubling time). The values for these two growth rates were calculated to be 0.912 absorbance increase per hour and 0.760 hours respectively. The doubling time value is confirmed from our graph. It took approximately forty minutes for the initial absorbance to double i.e. approximately the same time as the calculated doubling time value of 0.760 hours. The next phase (the retardation phase) is a reflection of the gradual depletion of available nutrients, which slows the cell growth. The rate of growth at this point also decreases as some of the cells stop growing before others until all of the cells stop growing and the absorbance remains constant i.e. the stationary phase. This is shown in section five of our graph on the additional page. The second section of our investigation was to determine the composition of our mixed culture. Firstly, using the plated boiled sample we were able to identify the presence of Bacillus pumilis. This was because the Bacillus pumilis was the only organism to survive the heating due to its production of endospores. The presence of this organism in the mixed culture was also reinforced by the mixed culture motility test which identified motile rods (i.e. Bacillus pumilis) and the gram stain test, which identified gram-positive rods (Bacillus pumilis is gram positive). This organism was also identified to be growing on the nutrient agar plate seeded with the mixed culture. The second organism to be identified in the mixed culture was Flavobacterium aquatile. The identification of this organism was primarily due to the appearance of its colonies as they were a distinctive yellow colour. This same colony was also identified on the mixed culture agar plate. The gram stain of the mixed culture also proved that Flavobacterium aquatile was present as it showed negative rods. The third organism was identified using two methods. Firstly colony comparisons identified the organism growing on the Nalidixic plate to be either E.coli or Pseudomonas. However when a motility test was carried out the organism was shown to be E.coli as it was motile but not highly motile as this would have indicated the presence of Pseudomonas. The presence of this organism in the mixed culture was also identified in the mixed culture gram stain (which showed gram negative rods) and the mixed culture motility test which showed motile rods i.e. E.coli. The final organism was identified from the nutrient agar plate containing the mixed culture, and from the mixed culture motility and gram stain tests. The colonies on the agar plate were small, white and mucoid, identical to those of the pure culture Pseudomonas. The motility test showed the presence of a highly motile rod shaped organism (i.e. again suggesting Pseudomonas) and the gram stain confirmed this by showing gram-negative rods. Therefore in conclusion the mixed culture contained the following organisms, Bacillus pumilis, Flavobacterium aquatile, E.coli and Pseudomonas. SummaryObtaining data to show the growth kinetics of an organism can provide important information that can be used to identify the processes occurring within a culture at any given time. By using the obtained data to plot a growth curve the replication cycle of an organism can be split into phases, hence allowing further study and understanding of the processes involved. Microbiological practices such as gram stains, agar plating and motility tests are very useful in the identification of unknown organisms. The information they give as a group can be used to further prove and therefore aid identification as ideas are backed up with increasing evidence.","Summary (Abstract)Since the emergence and characterisation of  Legionella pneumophila in 1976, much interest has been caused due to the organism's ability to survive and replicate within components of the human immune system that normally combat such an infection. In recent years the incidence of Legionnaires' disease has increased. This essay aims to understand the characteristics of L. pneumophila that allow infection, survival and replication within both amoebae in the natural environment, and human alveolar macrophages leading to the development of the pneumonia-like Legionnaires' disease. IntroductionLegionella pneumophila is a motile, gram negative, aerobic rod commonly found in fresh water supplies where it prevails as an intracellular parasite of amoebae (1). This organism has complex nutritional requirements, including an unusually high iron and cysteine requirement, low levels of sodium and aromatic compounds (3). Unlike other waterborne pathogens (which are often transmitted through drinking or recreational water or from person to person), the spread of L. pneumophila occurs via infected aerosolised water droplets that are subsequently inhaled. It wasn't until 1976 when an outbreak of Legionnaires' disease struck an American Legion convention killing 29 people, that L. pneumophila was first established as the causative agent of this pneumonia-like disease (2, reviewed in 1). It is now recognised that L. pneumophila is a classic opportunistic pathogen, infecting and causing disease in immunocompromised individuals, for example the elderly, individuals with damaged lungs or lung-related disease (1). In addition to the generation of Legionnaires' disease, L. pneumophila also causes a much milder, flu-like self-limiting disease called Pontiacs Fever (3). Unlike the symptoms exhibited by Legionnaires' disease (fever, disorientation, lethargy and lung damage), symptoms of Pontiacs Fever are less severe and include fever, chills and headaches. To date evidence of Pontiacs Fever has only been noted during Legionnaires' outbreaks and in comparison to Legionnaires' disease there have been no recorded mortalities from this form of the infection. The role of amoebae in the survival of Legionella pneumophila within the natural environmentThe spread of L. pneumophila is dependent upon the aerosolization of contaminated water droplets from contaminated water storage systems. Until the development of water reservoirs (e.g. cooling towers) and evaporative condensers of air conditioning systems, L. pneumophila posed no risk to humans (4, reviewed in 1). However, in recent years the number of Legionnaires' outbreaks has increased, the majority occurring during the summer months when use of air conditioning is generally greater. By examining the processes occurring within such water storage and distribution systems we can gain an increased understanding of why more and more outbreaks are being recorded. Firstly, as with any surface, biofilms form within water storage systems. A microbial biofilm consists of bacterial colonies attached to each other and their surface by the secretion of adhesive polysaccharides. In adverse environments, i.e. nutritionally deficient conditions, biofilms trap essential nutrients (for example iron), providing a rich environment for bacterial growth. As the number of large-scale water storage and distribution systems has increased, more buildings now contain reservoirs of L. pneumophila. In practice, these reservoirs cause an increased threat to humans due to inadequate cleaning and water treatment standards which don't remove the organisms before they have chance to replicate and spread to potential hosts. A second factor affecting the increase of Legionnaires' outbreaks is the presence of amoebae in aquatic systems. This organism feeds on microorganisms present in biofilms and hence may ingest L. pneumophila by whirling phagocytosis (a process characteristic of L. pneumophila). Of the thirteen species of amoebae, only two support the replication of L. pneumophila (5) and therefore act as reservoirs for L. pneumophila in the natural environment. These two species are Acanthamoeba and  Naeglaria (3). Within the amoebae, L. pneumophila undergoes high levels of replication and is protected to a much greater extent than free bacteria in the water would be from water treatment processes such as chlorination and Kathon CG. As a result, controlling the presence and replication of L. pneumophila in potable water supplies is very difficult, as even residual treatment which may kill free bacteria cannot remove cells carried by the highly tolerant amoebae. These facts can be shown by a study using water samples obtained from five hospitals in Paris. Of the samples taken, 71% contained amoebae and 47% showed presence of L. pneumophila (6, reviewed in 1). When these water samples were incubated, bacterial replication occurred, providing the sample contained amoebae (1). The role of amoebae in L. pneumophila virulence factorsIn addition to protecting L. pneumophila from water treatment processes and allowing replication, colonisation of amoebae has also accounted for the evolution of virulence factors essential for infection and replication within human alveolar macrophages. There are numerous similarities between the life cycles of L. pneumophila within both amoebae and alveolar macrophages (for example in both amoebae and macrophages, coiling phagocytosis occurs) but moreover genetic studies have shown that the defects in three virulence factors essential for bacterial replication and survival in monocytic U937 cells also inhibits these same processes in amoebae. The effects of such research is summarised in figure 1: This research therefore indicates that the strategies evolved by L. pneumophila to grow in amoebae also work in the human alveolar macrophage. In addition to the research shown in figure 1, there are other virulence factors of L. pneumophila that are established in amoebae and not only enable growth in macrophages, but in fact increase the incidence of the organism and the rate at which complications with the disease are created. Examples of such factors are summarised in figure 2. The increased ability of Legionella pneumophila grown in phagocytes to withstand chemical biocides and antibiotics was demonstrated as 71% of bacteria obtained from Acanthamoeba polyphaga survived a 24-hour exposure to 5μg/ml riampin. This value of 71% is significant as when the same conditions were applied to bacteria cultured in broth, more than 99.9% killing was observed (12, reviewed in 1). This therefore clearly shows that replication in amoebae increases the virulence of Legionella pneumophila and hence the ability of this organism to cause infection and disease. The host immune response to L. pneumophilaIn general, when a host organism becomes infected, it is the job of both the innate and humoral immune responses to clear that infection. L. pneumophila infections however do not follow this rule as this organism demonstrates relatively high resistance to humoral responses such as complement-mediated killing even though the complement component C3 is readily bound (13, reviewed in 1). Even with the presence of specific antibodies there is little increase of monocytic bactericidal activity. Horwitzal. (13, reviewed in 1) demonstrated this by incubating serum- resistant encapsulated E.coli for one hour with 10% fresh normal serum, L. pneumophila specific antiserum and polymorphonuclear cells. It was observed that the number of colony forming units (CFUs) decreased by 2.5 logs. Another similar treatment reduced L. pneumophila CFUs by only 0.5 logs. These data therefore indicate that complement, specific antibodies and polymorphonuclear cells cannot resolve infection and therefore partly account for the ability of L. pneumophila to enter and replicate within human alveolar macrophages. Due to the inability of innate and humoral immune responses to clear L. pneumophila infections, this role therefore becomes the responsibility of cell-mediated immune responses (14, reviewed in1). However, as L. pneumophila is an opportunistic pathogen such responses in immunocompromised individuals fails and hence allow the infection to prevail. Horwitzal. (15, reviewed in 1) studied the role of the cell-mediated response in non-susceptible hosts. The activity of peripheral blood mononuclear cells from recovered individuals in comparison to age and sex matched controls. The findings of this research are summarised in figure 3 and show that clearly a specific T-cell response has occurred in the recovered patients. In addition to the data described on the previous page, the supernatants of samples of patient cells exposed to L. pneumophila contained activated naïve mononuclear cells ready to inhibit replication of this bacterial species. This therefore supports the idea of a cell-mediated immune response. There are many stages to the cell-mediated response, however as previously discussed these are only feasible in non-compromised host cells. A critical activator is interferon-γ (IFN-γ), a cytokine that activates macrophages. Bhardwaj et al (16, reviewed in 1) demonstrated that IFN-γ doesn't enhance L. pneumophila killing or prevent the formation of replication vesicles but instead prevents bacterial growth via an iron-dependant mechanism. The ability of IFN-γ activated macrophages to prevent growth was reversed when samples were supplemented with iron transferrin (17, reviewed in1). This therefore indicates that the pool of iron in activated macrophages may limit the growth of L. pneumophila and has been further proved by Byrdal. (18, reviewed in 1) in research showing that IFN-γ activated blood monocytes contained 73% fewer transferrin-binding sites than non-activated control cells. Because of the ability of activated macrophages in non-susceptible hosts to terminate infections by L. pneumophila, we can begin to understand the factors that allow this organism to evade immune responses in the compromised individual and hence cause infection. Phagocyte interactionsAlthough the role of the host immune system is to protect and rid the body of foreign organisms, in the case of L. pneumophila, factors of the immune system in immunocompromised individuals actually aid the replication and survival of this organism within alveolar macrophages. As described in the previous section, components of complement (e.g. C3 and specific antibodies) opsonize the bacterium in an attempt to remove it from the body (13, reviewed in 1). However, upon failure of complement-mediated killing, receptors on the surface of macrophages recognise the opsonins and allow a special whirling phagocytosis event to occur, characteristic of L. pneumophila infections (19, reviewed in 1). In the absence of opsonization, the Mip protein (macrophages invasion protein) found on the L. pneumophila outer membrane, potentiates the invasion of human mononuclear phagocytes, again via whirling phagocytosis (3). This Mip protein is only synthesised in response to low levels of amino acids and the consequent presence of uncharged transfer RNAs (tRNAs), which activates Rel A causing the accumulation of guanosine 3'5' bispyrophosphate synthetases (ppGpp). This accumulation results in entry into the stationary phase of replication and the expression of virulence traits that promote transmission e.g. the Mip protein. This example therefore illustrates how the growth phase of L. pneumophila determines its phenotype (20, reviewed in 1). The production of an isolated phagosome in macrophage survivalOnce inside the macrophage, vesicles containing L. pneumophila must act quickly to become isolated from the lysosomal pathway and hence survive and replicate within their host macrophage. The main virulence system employed L. pneumophila is the dot/icm gene system (defective organelle trafficking and intracellular replication genes). As part of the type IV secretion system these two genes were originally designated due to their ability to transfer plasmid DNA to a recipient cell (21, reviewed in 1). However, immediately after phagocytosis of L. pneumophila by alveolar macrophages, these genes establish a different role, by assembling and activating a secretion system that integrates pores into the phagosomal membrane and hence remodels the phagosome membrane. It is through these pores that effector proteins (which would otherwise be used to traffic the phagosome to the endosome) are then transported to the host cytoplasm (22, reviewed in 1). Royal. (23, reviewed in 1) demonstrated the importance of the dot/icm genes in this process by using Electron Microscopy (EM). Results showed that in contrast to wild type cells, dot/icm mutants were delivered to the lysosomal compartment between five and thirty minutes after infection. Examples of proteins lost from the phagosome during membrane remodelling include the lysosomal marker LAMP-1 as identified by fluorescence microscopic assays (25, reviewed in 1). The loss of this guanosine triphosphate (GTP) binding protein from the membrane prohibits the promotion of fusion between early and late endosomal compartments (26 reviewed in 1). Unpublished observations also indicate that the loss of transferrin receptors from the phagosomal membrane stops virulent L. pneumophila vacuoles from interacting with the early endosomal compartment (27, reviewed in 1). Whilst the dot/icm genes can explain phagosome remodelling and the evasion of the endosomal pathway, other studies suggest that these processes could be accounted for by the presence of smooth vesicles on the cytoplasmic phagosome membrane within 15-20 minutes of infection (24, reviewed in 1). Horwitzal. (24) used kinetic studies to show that such vesicles budded from the phagosome taking with them proteins that would otherwise activate the endocytic pathway. In this way the maturation of the L. pneumophila-containing phagosome was hindered. The evidence presented to support both dot/icm and vesicle remodelling of the phagosome membrane illustrates the importance of this step in the intracellular survival of L. pneumophila. By measuring the release of p-nitrophenyl from the synthetic substrate p-nitrophenylphosphorylcholine (29, reviewed in 1) the activity of phospholipases has also been identified as interacting with the phagosome membrane and hence preventing phagolysosome formation. Two other molecules are also known to disrupt phagolysosome formation. These are the lectin, concanavalin A, and ammonium chloride. Although neither of these two molecules have been linked to pathogenesis, the lectin can cross-link membrane glycoproteins forming vacuoles that can't fuse with lysosomes (29, reviewed in 1), whilst ammonium chloride raises vacuolar pH hence interfering with phagolysosome formation (30, reviewed in 1). Replication of L. pneumophilaAs previously described, the dot/icm type IV secretion system is one of the primary mechanisms by which ingested L. pneumophila avoid immediate lysosomal delivery. Research has shown that this same set of genes is also involved in the recruitment of the endoplasmic reticulum (ER) and the formation of a replication vacuole. (31, reviewed in 32). Within one hour after the isolated phagosome is established, numerous mitochondria are recruited to its side and approximately four hours after isolation the cytoplasmic face interacts with the host cell ER, hence establishing the site of replication (24, reviewed in 1). After fusing with the ER, L. pneumophila becomes replicative, acid tolerant, and stops the expression of virulence traits such as those involved in the blocking of membrane fusion. Once these important transformations have occurred the  L. pneumophila exploits the natural function of the ER, and is delivered by autophagy to the harsh environment of the lysosomal compartment. It is then within the nutrient-rich environment of the lysosome that the L. pneumophila really exploits its host cell by replicating exponentially until all possible nutrients have been exhausted. Without the ability to replicate so profusely at the expense of the host macrophage, infection by L. pneumophila would have been fruitless and all the mechanisms evolved to evade host protection mechanisms would have gone to waste. As the available nutrient levels gradually decline, the L. pneumophila stringent paradigm response is triggered, coordinating the expression of virulence and entry into the stationary phase of replication. The mechanisms by which this response occurs is shown in figure 4: When amino acids are limiting uncharged tRNAs activate RelA, a guanosine 3'5'- bispyrophosphate synthetases. Accumulation of ppGpp coordinates entry into the stationary phase with expression of virulence traits that promote transmission to a new host. Some effectors may be substrates for type II and IV secretion systems whilst others such as flagellin (essential in providing motility for finding a new host), the development of osmotic resistance (essential for surviving in the natural environment) and the secretion of cytotoxins (promotes escape from the spent host by lysis) are more easily categorised. Prevention of Legionnaires' diseaseThe most obvious way to prevent further incidence of Legionnaires' disease would be to eradicate L. pneumophila from all water systems that may produce contaminated aerosols. As previously discussed this would be an extremely difficult in practice due to the resistance of bacteria within amoebae and biofilms to biocides. Other method such as UV radiation and heating to 60°C can be used to kill L. pneumophila within both biofilms and amoebae, but in practice the costs involved with treating such large volumes of water in this way make these treatments impractical (3). Therefore the majority of recent research has been focused on the development of biocides for water treatment that have no detrimental effects on humans, but kill L. pneumophila living in biofilms and amoebae. In addition, research has also focused on the development of a vaccine that would involve the activation of macrophages and recruitment of cytotoxic T cells to help kill the bacterium-infected macrophages (3). However, at this point in time no such advances have been made and therefore the main method used to prevent outbreaks of Legionnaires' disease remains the cleaning and maintenance of water systems such that microbial life is limited or prevented. Regular chemical and microbial testing of potential L. pneumophila water systems should monitor these processes. ConclusionThe increased incidence of Legionnaires' disease in recent years is primarily due to the increased need to store water in large quantities and the development and increased usage of aerosolizing technology for example whirlpools, showers and air conditioning systems. With inadequate cleaning and the stringent water treatment policies implemented by many countries, many pathogens are not removed from such water systems and instead find themselves in the perfect niche for colonisation and transmission. Within the natural environment, L. pneumophila can survive and replicate within amoebae. The colonisation of this organism has enabled L. pneumophila to remain in aquatic systems at a higher incidence than if the bacteria were just free-living. This is due to the resistance and protection conferred by the amoebae during water treatment processes and other adverse environmental conditions. In addition, the colonisation of amoebae has also led to the evolution of specific virulence factors essential for infection, survival and replication within human alveolar macrophages such that when a susceptible host inhales a L. pneumophila-contaminated aerosol and becomes infected, this component of the immune system is exploited. A greater understanding of the virulence factors and the characteristics of L. pneumophila which enable it to be such an efficient human parasite will help prevent outbreaks of Legionnaires' disease in the future.",True
43,"Summary (Abstract)Since the emergence and characterisation of  Legionella pneumophila in 1976, much interest has been caused due to the organism's ability to survive and replicate within components of the human immune system that normally combat such an infection. In recent years the incidence of Legionnaires' disease has increased. This essay aims to understand the characteristics of L. pneumophila that allow infection, survival and replication within both amoebae in the natural environment, and human alveolar macrophages leading to the development of the pneumonia-like Legionnaires' disease. IntroductionLegionella pneumophila is a motile, gram negative, aerobic rod commonly found in fresh water supplies where it prevails as an intracellular parasite of amoebae (1). This organism has complex nutritional requirements, including an unusually high iron and cysteine requirement, low levels of sodium and aromatic compounds (3). Unlike other waterborne pathogens (which are often transmitted through drinking or recreational water or from person to person), the spread of L. pneumophila occurs via infected aerosolised water droplets that are subsequently inhaled. It wasn't until 1976 when an outbreak of Legionnaires' disease struck an American Legion convention killing 29 people, that L. pneumophila was first established as the causative agent of this pneumonia-like disease (2, reviewed in 1). It is now recognised that L. pneumophila is a classic opportunistic pathogen, infecting and causing disease in immunocompromised individuals, for example the elderly, individuals with damaged lungs or lung-related disease (1). In addition to the generation of Legionnaires' disease, L. pneumophila also causes a much milder, flu-like self-limiting disease called Pontiacs Fever (3). Unlike the symptoms exhibited by Legionnaires' disease (fever, disorientation, lethargy and lung damage), symptoms of Pontiacs Fever are less severe and include fever, chills and headaches. To date evidence of Pontiacs Fever has only been noted during Legionnaires' outbreaks and in comparison to Legionnaires' disease there have been no recorded mortalities from this form of the infection. The role of amoebae in the survival of Legionella pneumophila within the natural environmentThe spread of L. pneumophila is dependent upon the aerosolization of contaminated water droplets from contaminated water storage systems. Until the development of water reservoirs (e.g. cooling towers) and evaporative condensers of air conditioning systems, L. pneumophila posed no risk to humans (4, reviewed in 1). However, in recent years the number of Legionnaires' outbreaks has increased, the majority occurring during the summer months when use of air conditioning is generally greater. By examining the processes occurring within such water storage and distribution systems we can gain an increased understanding of why more and more outbreaks are being recorded. Firstly, as with any surface, biofilms form within water storage systems. A microbial biofilm consists of bacterial colonies attached to each other and their surface by the secretion of adhesive polysaccharides. In adverse environments, i.e. nutritionally deficient conditions, biofilms trap essential nutrients (for example iron), providing a rich environment for bacterial growth. As the number of large-scale water storage and distribution systems has increased, more buildings now contain reservoirs of L. pneumophila. In practice, these reservoirs cause an increased threat to humans due to inadequate cleaning and water treatment standards which don't remove the organisms before they have chance to replicate and spread to potential hosts. A second factor affecting the increase of Legionnaires' outbreaks is the presence of amoebae in aquatic systems. This organism feeds on microorganisms present in biofilms and hence may ingest L. pneumophila by whirling phagocytosis (a process characteristic of L. pneumophila). Of the thirteen species of amoebae, only two support the replication of L. pneumophila (5) and therefore act as reservoirs for L. pneumophila in the natural environment. These two species are Acanthamoeba and  Naeglaria (3). Within the amoebae, L. pneumophila undergoes high levels of replication and is protected to a much greater extent than free bacteria in the water would be from water treatment processes such as chlorination and Kathon CG. As a result, controlling the presence and replication of L. pneumophila in potable water supplies is very difficult, as even residual treatment which may kill free bacteria cannot remove cells carried by the highly tolerant amoebae. These facts can be shown by a study using water samples obtained from five hospitals in Paris. Of the samples taken, 71% contained amoebae and 47% showed presence of L. pneumophila (6, reviewed in 1). When these water samples were incubated, bacterial replication occurred, providing the sample contained amoebae (1). The role of amoebae in L. pneumophila virulence factorsIn addition to protecting L. pneumophila from water treatment processes and allowing replication, colonisation of amoebae has also accounted for the evolution of virulence factors essential for infection and replication within human alveolar macrophages. There are numerous similarities between the life cycles of L. pneumophila within both amoebae and alveolar macrophages (for example in both amoebae and macrophages, coiling phagocytosis occurs) but moreover genetic studies have shown that the defects in three virulence factors essential for bacterial replication and survival in monocytic U937 cells also inhibits these same processes in amoebae. The effects of such research is summarised in figure 1: This research therefore indicates that the strategies evolved by L. pneumophila to grow in amoebae also work in the human alveolar macrophage. In addition to the research shown in figure 1, there are other virulence factors of L. pneumophila that are established in amoebae and not only enable growth in macrophages, but in fact increase the incidence of the organism and the rate at which complications with the disease are created. Examples of such factors are summarised in figure 2. The increased ability of Legionella pneumophila grown in phagocytes to withstand chemical biocides and antibiotics was demonstrated as 71% of bacteria obtained from Acanthamoeba polyphaga survived a 24-hour exposure to 5μg/ml riampin. This value of 71% is significant as when the same conditions were applied to bacteria cultured in broth, more than 99.9% killing was observed (12, reviewed in 1). This therefore clearly shows that replication in amoebae increases the virulence of Legionella pneumophila and hence the ability of this organism to cause infection and disease. The host immune response to L. pneumophilaIn general, when a host organism becomes infected, it is the job of both the innate and humoral immune responses to clear that infection. L. pneumophila infections however do not follow this rule as this organism demonstrates relatively high resistance to humoral responses such as complement-mediated killing even though the complement component C3 is readily bound (13, reviewed in 1). Even with the presence of specific antibodies there is little increase of monocytic bactericidal activity. Horwitzal. (13, reviewed in 1) demonstrated this by incubating serum- resistant encapsulated E.coli for one hour with 10% fresh normal serum, L. pneumophila specific antiserum and polymorphonuclear cells. It was observed that the number of colony forming units (CFUs) decreased by 2.5 logs. Another similar treatment reduced L. pneumophila CFUs by only 0.5 logs. These data therefore indicate that complement, specific antibodies and polymorphonuclear cells cannot resolve infection and therefore partly account for the ability of L. pneumophila to enter and replicate within human alveolar macrophages. Due to the inability of innate and humoral immune responses to clear L. pneumophila infections, this role therefore becomes the responsibility of cell-mediated immune responses (14, reviewed in1). However, as L. pneumophila is an opportunistic pathogen such responses in immunocompromised individuals fails and hence allow the infection to prevail. Horwitzal. (15, reviewed in 1) studied the role of the cell-mediated response in non-susceptible hosts. The activity of peripheral blood mononuclear cells from recovered individuals in comparison to age and sex matched controls. The findings of this research are summarised in figure 3 and show that clearly a specific T-cell response has occurred in the recovered patients. In addition to the data described on the previous page, the supernatants of samples of patient cells exposed to L. pneumophila contained activated naïve mononuclear cells ready to inhibit replication of this bacterial species. This therefore supports the idea of a cell-mediated immune response. There are many stages to the cell-mediated response, however as previously discussed these are only feasible in non-compromised host cells. A critical activator is interferon-γ (IFN-γ), a cytokine that activates macrophages. Bhardwaj et al (16, reviewed in 1) demonstrated that IFN-γ doesn't enhance L. pneumophila killing or prevent the formation of replication vesicles but instead prevents bacterial growth via an iron-dependant mechanism. The ability of IFN-γ activated macrophages to prevent growth was reversed when samples were supplemented with iron transferrin (17, reviewed in1). This therefore indicates that the pool of iron in activated macrophages may limit the growth of L. pneumophila and has been further proved by Byrdal. (18, reviewed in 1) in research showing that IFN-γ activated blood monocytes contained 73% fewer transferrin-binding sites than non-activated control cells. Because of the ability of activated macrophages in non-susceptible hosts to terminate infections by L. pneumophila, we can begin to understand the factors that allow this organism to evade immune responses in the compromised individual and hence cause infection. Phagocyte interactionsAlthough the role of the host immune system is to protect and rid the body of foreign organisms, in the case of L. pneumophila, factors of the immune system in immunocompromised individuals actually aid the replication and survival of this organism within alveolar macrophages. As described in the previous section, components of complement (e.g. C3 and specific antibodies) opsonize the bacterium in an attempt to remove it from the body (13, reviewed in 1). However, upon failure of complement-mediated killing, receptors on the surface of macrophages recognise the opsonins and allow a special whirling phagocytosis event to occur, characteristic of L. pneumophila infections (19, reviewed in 1). In the absence of opsonization, the Mip protein (macrophages invasion protein) found on the L. pneumophila outer membrane, potentiates the invasion of human mononuclear phagocytes, again via whirling phagocytosis (3). This Mip protein is only synthesised in response to low levels of amino acids and the consequent presence of uncharged transfer RNAs (tRNAs), which activates Rel A causing the accumulation of guanosine 3'5' bispyrophosphate synthetases (ppGpp). This accumulation results in entry into the stationary phase of replication and the expression of virulence traits that promote transmission e.g. the Mip protein. This example therefore illustrates how the growth phase of L. pneumophila determines its phenotype (20, reviewed in 1). The production of an isolated phagosome in macrophage survivalOnce inside the macrophage, vesicles containing L. pneumophila must act quickly to become isolated from the lysosomal pathway and hence survive and replicate within their host macrophage. The main virulence system employed L. pneumophila is the dot/icm gene system (defective organelle trafficking and intracellular replication genes). As part of the type IV secretion system these two genes were originally designated due to their ability to transfer plasmid DNA to a recipient cell (21, reviewed in 1). However, immediately after phagocytosis of L. pneumophila by alveolar macrophages, these genes establish a different role, by assembling and activating a secretion system that integrates pores into the phagosomal membrane and hence remodels the phagosome membrane. It is through these pores that effector proteins (which would otherwise be used to traffic the phagosome to the endosome) are then transported to the host cytoplasm (22, reviewed in 1). Royal. (23, reviewed in 1) demonstrated the importance of the dot/icm genes in this process by using Electron Microscopy (EM). Results showed that in contrast to wild type cells, dot/icm mutants were delivered to the lysosomal compartment between five and thirty minutes after infection. Examples of proteins lost from the phagosome during membrane remodelling include the lysosomal marker LAMP-1 as identified by fluorescence microscopic assays (25, reviewed in 1). The loss of this guanosine triphosphate (GTP) binding protein from the membrane prohibits the promotion of fusion between early and late endosomal compartments (26 reviewed in 1). Unpublished observations also indicate that the loss of transferrin receptors from the phagosomal membrane stops virulent L. pneumophila vacuoles from interacting with the early endosomal compartment (27, reviewed in 1). Whilst the dot/icm genes can explain phagosome remodelling and the evasion of the endosomal pathway, other studies suggest that these processes could be accounted for by the presence of smooth vesicles on the cytoplasmic phagosome membrane within 15-20 minutes of infection (24, reviewed in 1). Horwitzal. (24) used kinetic studies to show that such vesicles budded from the phagosome taking with them proteins that would otherwise activate the endocytic pathway. In this way the maturation of the L. pneumophila-containing phagosome was hindered. The evidence presented to support both dot/icm and vesicle remodelling of the phagosome membrane illustrates the importance of this step in the intracellular survival of L. pneumophila. By measuring the release of p-nitrophenyl from the synthetic substrate p-nitrophenylphosphorylcholine (29, reviewed in 1) the activity of phospholipases has also been identified as interacting with the phagosome membrane and hence preventing phagolysosome formation. Two other molecules are also known to disrupt phagolysosome formation. These are the lectin, concanavalin A, and ammonium chloride. Although neither of these two molecules have been linked to pathogenesis, the lectin can cross-link membrane glycoproteins forming vacuoles that can't fuse with lysosomes (29, reviewed in 1), whilst ammonium chloride raises vacuolar pH hence interfering with phagolysosome formation (30, reviewed in 1). Replication of L. pneumophilaAs previously described, the dot/icm type IV secretion system is one of the primary mechanisms by which ingested L. pneumophila avoid immediate lysosomal delivery. Research has shown that this same set of genes is also involved in the recruitment of the endoplasmic reticulum (ER) and the formation of a replication vacuole. (31, reviewed in 32). Within one hour after the isolated phagosome is established, numerous mitochondria are recruited to its side and approximately four hours after isolation the cytoplasmic face interacts with the host cell ER, hence establishing the site of replication (24, reviewed in 1). After fusing with the ER, L. pneumophila becomes replicative, acid tolerant, and stops the expression of virulence traits such as those involved in the blocking of membrane fusion. Once these important transformations have occurred the  L. pneumophila exploits the natural function of the ER, and is delivered by autophagy to the harsh environment of the lysosomal compartment. It is then within the nutrient-rich environment of the lysosome that the L. pneumophila really exploits its host cell by replicating exponentially until all possible nutrients have been exhausted. Without the ability to replicate so profusely at the expense of the host macrophage, infection by L. pneumophila would have been fruitless and all the mechanisms evolved to evade host protection mechanisms would have gone to waste. As the available nutrient levels gradually decline, the L. pneumophila stringent paradigm response is triggered, coordinating the expression of virulence and entry into the stationary phase of replication. The mechanisms by which this response occurs is shown in figure 4: When amino acids are limiting uncharged tRNAs activate RelA, a guanosine 3'5'- bispyrophosphate synthetases. Accumulation of ppGpp coordinates entry into the stationary phase with expression of virulence traits that promote transmission to a new host. Some effectors may be substrates for type II and IV secretion systems whilst others such as flagellin (essential in providing motility for finding a new host), the development of osmotic resistance (essential for surviving in the natural environment) and the secretion of cytotoxins (promotes escape from the spent host by lysis) are more easily categorised. Prevention of Legionnaires' diseaseThe most obvious way to prevent further incidence of Legionnaires' disease would be to eradicate L. pneumophila from all water systems that may produce contaminated aerosols. As previously discussed this would be an extremely difficult in practice due to the resistance of bacteria within amoebae and biofilms to biocides. Other method such as UV radiation and heating to 60°C can be used to kill L. pneumophila within both biofilms and amoebae, but in practice the costs involved with treating such large volumes of water in this way make these treatments impractical (3). Therefore the majority of recent research has been focused on the development of biocides for water treatment that have no detrimental effects on humans, but kill L. pneumophila living in biofilms and amoebae. In addition, research has also focused on the development of a vaccine that would involve the activation of macrophages and recruitment of cytotoxic T cells to help kill the bacterium-infected macrophages (3). However, at this point in time no such advances have been made and therefore the main method used to prevent outbreaks of Legionnaires' disease remains the cleaning and maintenance of water systems such that microbial life is limited or prevented. Regular chemical and microbial testing of potential L. pneumophila water systems should monitor these processes. ConclusionThe increased incidence of Legionnaires' disease in recent years is primarily due to the increased need to store water in large quantities and the development and increased usage of aerosolizing technology for example whirlpools, showers and air conditioning systems. With inadequate cleaning and the stringent water treatment policies implemented by many countries, many pathogens are not removed from such water systems and instead find themselves in the perfect niche for colonisation and transmission. Within the natural environment, L. pneumophila can survive and replicate within amoebae. The colonisation of this organism has enabled L. pneumophila to remain in aquatic systems at a higher incidence than if the bacteria were just free-living. This is due to the resistance and protection conferred by the amoebae during water treatment processes and other adverse environmental conditions. In addition, the colonisation of amoebae has also led to the evolution of specific virulence factors essential for infection, survival and replication within human alveolar macrophages such that when a susceptible host inhales a L. pneumophila-contaminated aerosol and becomes infected, this component of the immune system is exploited. A greater understanding of the virulence factors and the characteristics of L. pneumophila which enable it to be such an efficient human parasite will help prevent outbreaks of Legionnaires' disease in the future.","IntroductionThe study of growth kinetics is conducted in order to determine the process of bacterial growth in a closed culture. There are many ways in which the cell growth can be monitored, each slightly different in methodology and each producing slightly different sets of results. For example, the cell growth can be monitored by following the change in viable count, measuring biochemical changes within the sample, for example protein concentration or as we aimed to do, by measuring bacterial growth through the change in culture absorbance. In this way we aimed to follow the growth of Serratia marcenscens and collect absorbance data that would enable us to draw a growth curve upon which we would be able to locate the different stages of bacterial growth. In addition we aimed to collect data for nine different species of bacteria in order to determine the composition of a mixed culture. This section of the investigation relied upon common microbiological practises such as the gram stain and the plating of different organisms onto different selective agar plates. MethodThe method for this investigation is described on pages forty-seven to forty-eight of section one, and pages twenty-four and sixty-nine to seventy-one of section two of the laboratory handbook. At no point did we deviate from this method. The calculations for the serial dilutions are shown in section two of the appendix. The results for the identification of the mixed culture were obtained from a previous investigation. The method for this is described on pages forty-four to forty-six of section one and sections nineteen, and twenty-three to twenty-five of section two of the handbook. There were no deviations from this method. ResultsThe graph showing the relationship between absorbance (at 600nm) and time (in minutes) is shown on the additional sheet. The data we obtained to plot this graph (shown in section one of the appendix) clearly show the first five phases of the culture growth. Phase one (the lag period) occurred between 0 and 30 minutes during which time the absorbance increased slowly from 0.05 to 0.07. The second phase is the acceleration phase when the absorbance increased more rapidly from 0.07 to 0.099 before entering the exponential phase between forty minutes and one hundred and twenty minutes. During the exponential phase the change in absorbancy occurred at the greatest rate. It is also during this phase that the specific growth rate was calculated as 0.912 (as shown in section three of the appendix). Our graph also shows the retardation phase when the rate of change in absorbance began to slow and the maximum population phase when the absorbance showed a plateau. These are marked on the graph as phases four and five respectively. The gram stain results for each of the organisms studied last week are shown in figure five of the appendix. In total we identified three gram negative organisms and six gram positive organisms. The appearance of the colonies of each species is also described in section five of the appendix. The results from the plated samples of mixed culture are shown in section six of the appendix. The plate which contained the boiled sample showed evidence of Bacillus pumilis, the plate containing the antibiotic Kanamycin contained Flavobacterium aquatile, the plate containing the antibiotic Nalidixic acid showed E.coli, and the nutrient agar plate contained  Bacillus pumilis, Flavobacterium aquatile, E.coli and Pseudomonas fluorescens. Discussion and conclusionAs shown on the additional sheet and described in the results section, the data we obtained for the growth of Serratia marcescens depicts the typical bacterial growth curve. This theory can therefore be used to explain the changes in absorbance over time. The first phase (the lag phase) occurred in the first thirty minutes of the investigation. This was the time period when the cells in the culture were getting ready to enter the active growth phase and therefore had to synthesis all the enzymes needed for transport and metabolism of the available nutrients, and for the production of new cell products. Although the data we collected showed this phase to be quite inactive, if we had collected data for protein concentration for example, this phase would have shown quite the opposite result. The second phase shown in our results is the acceleration phase (phase two on the graph). The absorbancy in this time period increased as some of the cells began to increase in size and divide. Although the growing cells at this point are growing at the maximum possible speed, the increase in absorbance is still quite slow. This is because due to the natural heterogeneity of the population some of the cells are still in the lag phase and therefore haven't started to grow and divide at this point. The point at which all the viable cells in the culture were growing and dividing is shown by the exponential phase (i.e. between forty minutes and one hundred and twenty minutes on the graph). It is only during this phase that calculations regarding the rate of growth can be calculated (for example the specific growth rate and the doubling time). The values for these two growth rates were calculated to be 0.912 absorbance increase per hour and 0.760 hours respectively. The doubling time value is confirmed from our graph. It took approximately forty minutes for the initial absorbance to double i.e. approximately the same time as the calculated doubling time value of 0.760 hours. The next phase (the retardation phase) is a reflection of the gradual depletion of available nutrients, which slows the cell growth. The rate of growth at this point also decreases as some of the cells stop growing before others until all of the cells stop growing and the absorbance remains constant i.e. the stationary phase. This is shown in section five of our graph on the additional page. The second section of our investigation was to determine the composition of our mixed culture. Firstly, using the plated boiled sample we were able to identify the presence of Bacillus pumilis. This was because the Bacillus pumilis was the only organism to survive the heating due to its production of endospores. The presence of this organism in the mixed culture was also reinforced by the mixed culture motility test which identified motile rods (i.e. Bacillus pumilis) and the gram stain test, which identified gram-positive rods (Bacillus pumilis is gram positive). This organism was also identified to be growing on the nutrient agar plate seeded with the mixed culture. The second organism to be identified in the mixed culture was Flavobacterium aquatile. The identification of this organism was primarily due to the appearance of its colonies as they were a distinctive yellow colour. This same colony was also identified on the mixed culture agar plate. The gram stain of the mixed culture also proved that Flavobacterium aquatile was present as it showed negative rods. The third organism was identified using two methods. Firstly colony comparisons identified the organism growing on the Nalidixic plate to be either E.coli or Pseudomonas. However when a motility test was carried out the organism was shown to be E.coli as it was motile but not highly motile as this would have indicated the presence of Pseudomonas. The presence of this organism in the mixed culture was also identified in the mixed culture gram stain (which showed gram negative rods) and the mixed culture motility test which showed motile rods i.e. E.coli. The final organism was identified from the nutrient agar plate containing the mixed culture, and from the mixed culture motility and gram stain tests. The colonies on the agar plate were small, white and mucoid, identical to those of the pure culture Pseudomonas. The motility test showed the presence of a highly motile rod shaped organism (i.e. again suggesting Pseudomonas) and the gram stain confirmed this by showing gram-negative rods. Therefore in conclusion the mixed culture contained the following organisms, Bacillus pumilis, Flavobacterium aquatile, E.coli and Pseudomonas. SummaryObtaining data to show the growth kinetics of an organism can provide important information that can be used to identify the processes occurring within a culture at any given time. By using the obtained data to plot a growth curve the replication cycle of an organism can be split into phases, hence allowing further study and understanding of the processes involved. Microbiological practices such as gram stains, agar plating and motility tests are very useful in the identification of unknown organisms. The information they give as a group can be used to further prove and therefore aid identification as ideas are backed up with increasing evidence.",False
44,"Summary (Abstract)Since the emergence and characterisation of  Legionella pneumophila in 1976, much interest has been caused due to the organism's ability to survive and replicate within components of the human immune system that normally combat such an infection. In recent years the incidence of Legionnaires' disease has increased. This essay aims to understand the characteristics of L. pneumophila that allow infection, survival and replication within both amoebae in the natural environment, and human alveolar macrophages leading to the development of the pneumonia-like Legionnaires' disease. IntroductionLegionella pneumophila is a motile, gram negative, aerobic rod commonly found in fresh water supplies where it prevails as an intracellular parasite of amoebae (1). This organism has complex nutritional requirements, including an unusually high iron and cysteine requirement, low levels of sodium and aromatic compounds (3). Unlike other waterborne pathogens (which are often transmitted through drinking or recreational water or from person to person), the spread of L. pneumophila occurs via infected aerosolised water droplets that are subsequently inhaled. It wasn't until 1976 when an outbreak of Legionnaires' disease struck an American Legion convention killing 29 people, that L. pneumophila was first established as the causative agent of this pneumonia-like disease (2, reviewed in 1). It is now recognised that L. pneumophila is a classic opportunistic pathogen, infecting and causing disease in immunocompromised individuals, for example the elderly, individuals with damaged lungs or lung-related disease (1). In addition to the generation of Legionnaires' disease, L. pneumophila also causes a much milder, flu-like self-limiting disease called Pontiacs Fever (3). Unlike the symptoms exhibited by Legionnaires' disease (fever, disorientation, lethargy and lung damage), symptoms of Pontiacs Fever are less severe and include fever, chills and headaches. To date evidence of Pontiacs Fever has only been noted during Legionnaires' outbreaks and in comparison to Legionnaires' disease there have been no recorded mortalities from this form of the infection. The role of amoebae in the survival of Legionella pneumophila within the natural environmentThe spread of L. pneumophila is dependent upon the aerosolization of contaminated water droplets from contaminated water storage systems. Until the development of water reservoirs (e.g. cooling towers) and evaporative condensers of air conditioning systems, L. pneumophila posed no risk to humans (4, reviewed in 1). However, in recent years the number of Legionnaires' outbreaks has increased, the majority occurring during the summer months when use of air conditioning is generally greater. By examining the processes occurring within such water storage and distribution systems we can gain an increased understanding of why more and more outbreaks are being recorded. Firstly, as with any surface, biofilms form within water storage systems. A microbial biofilm consists of bacterial colonies attached to each other and their surface by the secretion of adhesive polysaccharides. In adverse environments, i.e. nutritionally deficient conditions, biofilms trap essential nutrients (for example iron), providing a rich environment for bacterial growth. As the number of large-scale water storage and distribution systems has increased, more buildings now contain reservoirs of L. pneumophila. In practice, these reservoirs cause an increased threat to humans due to inadequate cleaning and water treatment standards which don't remove the organisms before they have chance to replicate and spread to potential hosts. A second factor affecting the increase of Legionnaires' outbreaks is the presence of amoebae in aquatic systems. This organism feeds on microorganisms present in biofilms and hence may ingest L. pneumophila by whirling phagocytosis (a process characteristic of L. pneumophila). Of the thirteen species of amoebae, only two support the replication of L. pneumophila (5) and therefore act as reservoirs for L. pneumophila in the natural environment. These two species are Acanthamoeba and  Naeglaria (3). Within the amoebae, L. pneumophila undergoes high levels of replication and is protected to a much greater extent than free bacteria in the water would be from water treatment processes such as chlorination and Kathon CG. As a result, controlling the presence and replication of L. pneumophila in potable water supplies is very difficult, as even residual treatment which may kill free bacteria cannot remove cells carried by the highly tolerant amoebae. These facts can be shown by a study using water samples obtained from five hospitals in Paris. Of the samples taken, 71% contained amoebae and 47% showed presence of L. pneumophila (6, reviewed in 1). When these water samples were incubated, bacterial replication occurred, providing the sample contained amoebae (1). The role of amoebae in L. pneumophila virulence factorsIn addition to protecting L. pneumophila from water treatment processes and allowing replication, colonisation of amoebae has also accounted for the evolution of virulence factors essential for infection and replication within human alveolar macrophages. There are numerous similarities between the life cycles of L. pneumophila within both amoebae and alveolar macrophages (for example in both amoebae and macrophages, coiling phagocytosis occurs) but moreover genetic studies have shown that the defects in three virulence factors essential for bacterial replication and survival in monocytic U937 cells also inhibits these same processes in amoebae. The effects of such research is summarised in figure 1: This research therefore indicates that the strategies evolved by L. pneumophila to grow in amoebae also work in the human alveolar macrophage. In addition to the research shown in figure 1, there are other virulence factors of L. pneumophila that are established in amoebae and not only enable growth in macrophages, but in fact increase the incidence of the organism and the rate at which complications with the disease are created. Examples of such factors are summarised in figure 2. The increased ability of Legionella pneumophila grown in phagocytes to withstand chemical biocides and antibiotics was demonstrated as 71% of bacteria obtained from Acanthamoeba polyphaga survived a 24-hour exposure to 5μg/ml riampin. This value of 71% is significant as when the same conditions were applied to bacteria cultured in broth, more than 99.9% killing was observed (12, reviewed in 1). This therefore clearly shows that replication in amoebae increases the virulence of Legionella pneumophila and hence the ability of this organism to cause infection and disease. The host immune response to L. pneumophilaIn general, when a host organism becomes infected, it is the job of both the innate and humoral immune responses to clear that infection. L. pneumophila infections however do not follow this rule as this organism demonstrates relatively high resistance to humoral responses such as complement-mediated killing even though the complement component C3 is readily bound (13, reviewed in 1). Even with the presence of specific antibodies there is little increase of monocytic bactericidal activity. Horwitzal. (13, reviewed in 1) demonstrated this by incubating serum- resistant encapsulated E.coli for one hour with 10% fresh normal serum, L. pneumophila specific antiserum and polymorphonuclear cells. It was observed that the number of colony forming units (CFUs) decreased by 2.5 logs. Another similar treatment reduced L. pneumophila CFUs by only 0.5 logs. These data therefore indicate that complement, specific antibodies and polymorphonuclear cells cannot resolve infection and therefore partly account for the ability of L. pneumophila to enter and replicate within human alveolar macrophages. Due to the inability of innate and humoral immune responses to clear L. pneumophila infections, this role therefore becomes the responsibility of cell-mediated immune responses (14, reviewed in1). However, as L. pneumophila is an opportunistic pathogen such responses in immunocompromised individuals fails and hence allow the infection to prevail. Horwitzal. (15, reviewed in 1) studied the role of the cell-mediated response in non-susceptible hosts. The activity of peripheral blood mononuclear cells from recovered individuals in comparison to age and sex matched controls. The findings of this research are summarised in figure 3 and show that clearly a specific T-cell response has occurred in the recovered patients. In addition to the data described on the previous page, the supernatants of samples of patient cells exposed to L. pneumophila contained activated naïve mononuclear cells ready to inhibit replication of this bacterial species. This therefore supports the idea of a cell-mediated immune response. There are many stages to the cell-mediated response, however as previously discussed these are only feasible in non-compromised host cells. A critical activator is interferon-γ (IFN-γ), a cytokine that activates macrophages. Bhardwaj et al (16, reviewed in 1) demonstrated that IFN-γ doesn't enhance L. pneumophila killing or prevent the formation of replication vesicles but instead prevents bacterial growth via an iron-dependant mechanism. The ability of IFN-γ activated macrophages to prevent growth was reversed when samples were supplemented with iron transferrin (17, reviewed in1). This therefore indicates that the pool of iron in activated macrophages may limit the growth of L. pneumophila and has been further proved by Byrdal. (18, reviewed in 1) in research showing that IFN-γ activated blood monocytes contained 73% fewer transferrin-binding sites than non-activated control cells. Because of the ability of activated macrophages in non-susceptible hosts to terminate infections by L. pneumophila, we can begin to understand the factors that allow this organism to evade immune responses in the compromised individual and hence cause infection. Phagocyte interactionsAlthough the role of the host immune system is to protect and rid the body of foreign organisms, in the case of L. pneumophila, factors of the immune system in immunocompromised individuals actually aid the replication and survival of this organism within alveolar macrophages. As described in the previous section, components of complement (e.g. C3 and specific antibodies) opsonize the bacterium in an attempt to remove it from the body (13, reviewed in 1). However, upon failure of complement-mediated killing, receptors on the surface of macrophages recognise the opsonins and allow a special whirling phagocytosis event to occur, characteristic of L. pneumophila infections (19, reviewed in 1). In the absence of opsonization, the Mip protein (macrophages invasion protein) found on the L. pneumophila outer membrane, potentiates the invasion of human mononuclear phagocytes, again via whirling phagocytosis (3). This Mip protein is only synthesised in response to low levels of amino acids and the consequent presence of uncharged transfer RNAs (tRNAs), which activates Rel A causing the accumulation of guanosine 3'5' bispyrophosphate synthetases (ppGpp). This accumulation results in entry into the stationary phase of replication and the expression of virulence traits that promote transmission e.g. the Mip protein. This example therefore illustrates how the growth phase of L. pneumophila determines its phenotype (20, reviewed in 1). The production of an isolated phagosome in macrophage survivalOnce inside the macrophage, vesicles containing L. pneumophila must act quickly to become isolated from the lysosomal pathway and hence survive and replicate within their host macrophage. The main virulence system employed L. pneumophila is the dot/icm gene system (defective organelle trafficking and intracellular replication genes). As part of the type IV secretion system these two genes were originally designated due to their ability to transfer plasmid DNA to a recipient cell (21, reviewed in 1). However, immediately after phagocytosis of L. pneumophila by alveolar macrophages, these genes establish a different role, by assembling and activating a secretion system that integrates pores into the phagosomal membrane and hence remodels the phagosome membrane. It is through these pores that effector proteins (which would otherwise be used to traffic the phagosome to the endosome) are then transported to the host cytoplasm (22, reviewed in 1). Royal. (23, reviewed in 1) demonstrated the importance of the dot/icm genes in this process by using Electron Microscopy (EM). Results showed that in contrast to wild type cells, dot/icm mutants were delivered to the lysosomal compartment between five and thirty minutes after infection. Examples of proteins lost from the phagosome during membrane remodelling include the lysosomal marker LAMP-1 as identified by fluorescence microscopic assays (25, reviewed in 1). The loss of this guanosine triphosphate (GTP) binding protein from the membrane prohibits the promotion of fusion between early and late endosomal compartments (26 reviewed in 1). Unpublished observations also indicate that the loss of transferrin receptors from the phagosomal membrane stops virulent L. pneumophila vacuoles from interacting with the early endosomal compartment (27, reviewed in 1). Whilst the dot/icm genes can explain phagosome remodelling and the evasion of the endosomal pathway, other studies suggest that these processes could be accounted for by the presence of smooth vesicles on the cytoplasmic phagosome membrane within 15-20 minutes of infection (24, reviewed in 1). Horwitzal. (24) used kinetic studies to show that such vesicles budded from the phagosome taking with them proteins that would otherwise activate the endocytic pathway. In this way the maturation of the L. pneumophila-containing phagosome was hindered. The evidence presented to support both dot/icm and vesicle remodelling of the phagosome membrane illustrates the importance of this step in the intracellular survival of L. pneumophila. By measuring the release of p-nitrophenyl from the synthetic substrate p-nitrophenylphosphorylcholine (29, reviewed in 1) the activity of phospholipases has also been identified as interacting with the phagosome membrane and hence preventing phagolysosome formation. Two other molecules are also known to disrupt phagolysosome formation. These are the lectin, concanavalin A, and ammonium chloride. Although neither of these two molecules have been linked to pathogenesis, the lectin can cross-link membrane glycoproteins forming vacuoles that can't fuse with lysosomes (29, reviewed in 1), whilst ammonium chloride raises vacuolar pH hence interfering with phagolysosome formation (30, reviewed in 1). Replication of L. pneumophilaAs previously described, the dot/icm type IV secretion system is one of the primary mechanisms by which ingested L. pneumophila avoid immediate lysosomal delivery. Research has shown that this same set of genes is also involved in the recruitment of the endoplasmic reticulum (ER) and the formation of a replication vacuole. (31, reviewed in 32). Within one hour after the isolated phagosome is established, numerous mitochondria are recruited to its side and approximately four hours after isolation the cytoplasmic face interacts with the host cell ER, hence establishing the site of replication (24, reviewed in 1). After fusing with the ER, L. pneumophila becomes replicative, acid tolerant, and stops the expression of virulence traits such as those involved in the blocking of membrane fusion. Once these important transformations have occurred the  L. pneumophila exploits the natural function of the ER, and is delivered by autophagy to the harsh environment of the lysosomal compartment. It is then within the nutrient-rich environment of the lysosome that the L. pneumophila really exploits its host cell by replicating exponentially until all possible nutrients have been exhausted. Without the ability to replicate so profusely at the expense of the host macrophage, infection by L. pneumophila would have been fruitless and all the mechanisms evolved to evade host protection mechanisms would have gone to waste. As the available nutrient levels gradually decline, the L. pneumophila stringent paradigm response is triggered, coordinating the expression of virulence and entry into the stationary phase of replication. The mechanisms by which this response occurs is shown in figure 4: When amino acids are limiting uncharged tRNAs activate RelA, a guanosine 3'5'- bispyrophosphate synthetases. Accumulation of ppGpp coordinates entry into the stationary phase with expression of virulence traits that promote transmission to a new host. Some effectors may be substrates for type II and IV secretion systems whilst others such as flagellin (essential in providing motility for finding a new host), the development of osmotic resistance (essential for surviving in the natural environment) and the secretion of cytotoxins (promotes escape from the spent host by lysis) are more easily categorised. Prevention of Legionnaires' diseaseThe most obvious way to prevent further incidence of Legionnaires' disease would be to eradicate L. pneumophila from all water systems that may produce contaminated aerosols. As previously discussed this would be an extremely difficult in practice due to the resistance of bacteria within amoebae and biofilms to biocides. Other method such as UV radiation and heating to 60°C can be used to kill L. pneumophila within both biofilms and amoebae, but in practice the costs involved with treating such large volumes of water in this way make these treatments impractical (3). Therefore the majority of recent research has been focused on the development of biocides for water treatment that have no detrimental effects on humans, but kill L. pneumophila living in biofilms and amoebae. In addition, research has also focused on the development of a vaccine that would involve the activation of macrophages and recruitment of cytotoxic T cells to help kill the bacterium-infected macrophages (3). However, at this point in time no such advances have been made and therefore the main method used to prevent outbreaks of Legionnaires' disease remains the cleaning and maintenance of water systems such that microbial life is limited or prevented. Regular chemical and microbial testing of potential L. pneumophila water systems should monitor these processes. ConclusionThe increased incidence of Legionnaires' disease in recent years is primarily due to the increased need to store water in large quantities and the development and increased usage of aerosolizing technology for example whirlpools, showers and air conditioning systems. With inadequate cleaning and the stringent water treatment policies implemented by many countries, many pathogens are not removed from such water systems and instead find themselves in the perfect niche for colonisation and transmission. Within the natural environment, L. pneumophila can survive and replicate within amoebae. The colonisation of this organism has enabled L. pneumophila to remain in aquatic systems at a higher incidence than if the bacteria were just free-living. This is due to the resistance and protection conferred by the amoebae during water treatment processes and other adverse environmental conditions. In addition, the colonisation of amoebae has also led to the evolution of specific virulence factors essential for infection, survival and replication within human alveolar macrophages such that when a susceptible host inhales a L. pneumophila-contaminated aerosol and becomes infected, this component of the immune system is exploited. A greater understanding of the virulence factors and the characteristics of L. pneumophila which enable it to be such an efficient human parasite will help prevent outbreaks of Legionnaires' disease in the future.","Agrobacterium-mediated transformation of Arabidopsis by floral dip, and selection of transformantsIntroductionTransformation as a process of genetic exchange is widely used for producing transgenetic plants. Like many bacteria Agrobacterium possess DNA in the form of a plasmid in addition to its main genome. Under normal circumstances this plasmid contains the vir genes which causes part of the plasmid to self-replicate and insert itself into the genome of the plant host cells. This then alters hormone production and causes the development of galls or tumours in the plants. The bacterial T-DNA i.e. the portion of the plasmid that becomes integrated in the plant genome can be modified such that its incorporation does not cause tumour formation, but instead confers foreign genes (such as those for antibiotic resistance) upon the plants phenotype. In our investigation we aimed to transform Arabidopsis plants with a genetic construct that allowed selection of transformed plants due to Kanamycin resistance and in addition allowed the uptake of the plasmid DNA to be visualised using the fluorescent reporter gene green fluorescent protein (GFP). As the uptake and incorporation of the plasmid DNA is a rare event we aimed to screen the progeny of plants subjected to the floral dip and select for stable transformants and hence calculate a value for transformation frequency. MethodsThe method for this investigation is described on pages five and six of the laboratory manual. There were no alterations to this method. ResultsFor the first part of our investigation i.e. the identification of plants transformed by floral dipping we used a UV lamp to determine that the control plant i.e. the plants that had not been transformed showed only red fluorescence characteristic of chlorophyll and in addition yellow spots in dying tissues where no chlorophyll was present. In contrast the transformed plants showed green fluorescence in parts of the stems and leaves, and in some cases the parts of the flowers were also green. DiscussionThe fact that only the transformed plants fluoresced green enabled us to determine that the activity of GFP was as a result of transformation. In addition, by using the viral promoter 35S we were able to ensure that expression of the reporter gene could have occurred in any cell in the plant and therefore again the expression of GFP was dependant only upon successful transformation. In most cases it is probable that the uptake of the plasmid was only transient i.e. the GFP reporter gene did not incorporate into the host cells genome and therefore would be lost after cell division. However as we detected some green fluorescence in parts of the flowers there was the chance that the plants germ cells had been infected and therefore any progeny may carry GFP in their genomes. GUS staining of Arabidopsis seedlings transformed with a promoter trap constructIntroductionThe use of promoter traps in genetics has been greatly exploited in recent years. This technique involves transformation and random integration of reporter gene sequences that lack cis-acting sequences required for expression. Once inserted in the genome if the reporter gene has landed downstream of a promoter for another gene, this promoter can act upon the reporter gene to allow its expression. If during transposition the reporter gene lands within another gene hence disrupting and replacing its expression, then patterns of reporter gene expression can be used to identify which gene has been disrupted, where this gene is located within the genome and more importantly where this gene is expressed and how it affects the plants phenotype. In this investigation we aimed to use the reporter gene Gus in a promoter trap to identify and characterise mutant plant phenotypes generated by transformation. MethodThe method for this investigation is described on pages seven and eight of the laboratory handbook, we made no alterations to this method. ResultsThe three positive controls we used for this investigation were LAX2:Gus GL2:Gus and CHS:Gus in which we expected to find blue staining in the auxin transport tissues, trichome cells in the root and in all tissues respectively. As the seedlings we used for the investigation were so small, when we looked at our LAX2:Gus plants there was no staining as no germination had occurred. In the case of the GL2:Gus plants only a small part of the seed coat had stained blue, and for the CHS:Gus plants there was no obvious root staining however the hypocotyls and cotyledons had stained deep blue. Similarly to the LAX2:Gus control plants, no germination had occurred in any of the plants in our three promoter trap lines (N4567, N4568 and N4554) and therefore no staining was observed. DiscussionThe fact that the seedlings used in this investigation were so premature was a major limiting factor and therefore we would need to repeat this investigation using more developed plants before any staining of our promoter traps could be observed. The three positive controls (with the exception of LAX2:gus) although premature in their development showed the expected patterns of Gus expression. The reporter gene Gus had been inserted downstream of each of the specified promoters and therefore had been expressed in the same tissues as the native gene. It was because of this that Gus staining was observed only in the hypocotyls and cotyledons in our CHS:Gus plants, as CHS drives expression of chalcone synthase, an enzyme involved in protecting the plant from light damage, and this protein is expressed only in the tissues of the plant which are in contact with light (i.e. the hypocotyls and cotyledons). If we had used more developed LAX2:Gus controls then we would have expected to find staining in the auxin transport tissues as the presence of Gus downstream of the LAX2 promoter would have allowed expression of Gus in those tissues where the native gene is normally expressed. Our final control promoter GL2 drives expression of factors needed for trichome morphogenesis such as cell expansion and maturation of the cell. As the plants we studied were so small and lacked any roots, our results showed that only part of the seed coat was stained, most probably the cells which would later develop into the roots, as this is where trichome cells are found. If any of our promoter trap plants had been developed enough to observe staining, then we would have been able to locate those tissues where Gus was being expressed. As a result we would then have been able to identify possible genes disrupted by the transposition and insertion of the reporter gene, and in addition we could have analysed the patterns of gus expression to determine the expression patterns of the disrupted gene. As we did not obtain any data for this investigation we would need to repeat the method set out on pages seven and eight using larger and more developed seedlings. Luciferase as a dynamic reporter for gene expression in  ArabidopsisIntroductionThe protein luciferase is an enzyme which produces light in the presence of luciferin, ATP and oxygen. In our investigation transgenetic plants were produced which contained the reporter gene luciferin under the control of four different promoters, chs from which chalcone synthase is transcribed, the chlorophyll a/b binding protein promoter cab, phyA from which phytochrome A is transcribed and 35S, a viral promoter. As luciferase activity can be measured with great accuracy (due to the use of highly sensitive light-detectors), we aimed to quantify and compare the expression of luciferase in the roots and leaves of Arabidopsis when driven by the four different promoters. One of the advantages of using luciferase as a reporter gene is that it is unstable and hence as the levels of active protein change over time, it can be used to study the dynamic changes of induction of transcription from a specific promoter. As previously mentioned the chlorophyll a/b binding protein is transcribed from the promoter cab, therefore using luciferase as a reporter gene transcribed from this promoter, we aimed to study the changing expression of the chlorophyll a/b binding protein over time after exposure to red light for five minutes. MethodThe method for this investigation is described on pages nine and ten of the laboratory manual. There were no changes made to this method. ResultsThe first part of our investigation was to quantify the expression levels of luciferase driven from four different promoters in the roots and leaves of Arabidopsis plants that were either grown under constant light or adapted to the darkness for 48 hours. This data is shown in Figure 1. The data in Figure 1 shows that the different promoters allowed different patterns of expression under the different conditions. When luciferase was transcribed from the CHS promoter, there was little expression in the roots or the leaves of plants grown in the light and no expression at all when the plants were grown in the dark. As shown in Figure 1, we only studied PHYA plants that had been grown in the dark and found that the level of luciferase activity was the highest of all the different constructs in the leaves of these plants. When transcribed from the promoter CAB the data in Figure 1 shows that there was no expression of luciferase in the roots of either the light or dark plants, however expression in the leaves occurred in both the light and dark grown plants with a greater pixel intensity generated in the light grown plants. Our final promoter 35S allowed similar expression of luciferase in the light and the dark, however the pixel intensity from the leaves was slightly higher than from the roots. The second part of our investigation was to use the dynamic change in luciferase expression to study the response of the CAB promoter to light. We firstly studied the changing patterns of expression in wild type dark and light grown plants. This data is shown in Figure 2. As shown in Figure 2, the plants grown in the dark did not show any change in luciferase expression (and hence luciferase activity) when there was no light stimulus applied. In contrast the plants which had been given a light stimulus at time zero showed a sharp increase in photon emission followed by a gradual decline back to the start level after sixteen hours. Before we could screen for plants that were potentially mutant in their ability to respond to a light stimulus we had to investigate the range of responses in the wild type light plant. This data is shown in Figure 3. The data presented in Figure 3 show that all the wild type plants were able to respond to light in a similar manner i.e. they all showed an increase in luciferase activity after the light stimulus at time zero, after which time the level of luciferase activity gradually declined over a period of between five and sixteen hours to the same level of activity as at time minus six hours. There was however variation between the twenty-four plants screened for example plant C6 produced the highest response but also had the highest starting photon emission. Figures 4 and 5 show the mutagenised plants that we selected as being potentially defective in their ability to sense and respond to the light stimulus. There are three potential light mutants shown in Figure 4, plants E6, G11 and F11. In comparison to the average wild type plant, plant E6 emitted a much greater number of photons at the start of the experiment. This value then declined steeply before rising again at time zero. Between five and sixteen hours the photons emitted gradually declined, such that by the end of the experiment their levels were approximately the same as at time zero. Plant G11 showed the same pattern in its response to light as the average wild type, however there was a much shorter decline in photon emission (between about three and five hours) after which time emission gradually increased again. Finally plant F11. This plant also showed a similar pattern in response to the light as did the wild type plant however similarly to plant G11 the decline in photon emission was shorter than that of the wild type (between about one and eleven hours), and after about eleven hours emission increased once again. As shown in Figure 5, the amount of photons emitted by plant H12 was much higher than those emitted by the wild type plant. In addition whereas the emissions by the wild type plant only occurred after the light stimulus at time zero and then gradually declined back to the initial level, the amount of photons emitted by plant H12 gradually increased from the start of the investigation before declining between four and ten hours and then gradually increasing again. DiscussionAs shown in Figure 1, the expression patterns of the reporter gene luciferase varied dependent upon the promoter from which it was transcribed. The viral promoter 35S was selected to allow constitutive expression of the reporter gene in all areas of the plant. However as viruses are obligate intracellular parasites and use the host machinery for protein synthesis, transcription from this promoter would have required items such as transcription factors which may not have been ubiquitous throughout the plant and in both light and dark situations. This may therefore account for why luciferase expression was higher in the leaves than the roots. Our second promoter CAB is normally drives expression of the chlorophyll a/b binding protein, a component of photosystem II in the chloroplasts. As such, expression of this protein only occurs in the leaves of Arabidopsis i.e. where the light absorbing and photosynthetic apparatus of the plants are located and hence where this protein is needed. This is shown in our findings as when luciferase was transcribed from this promoter there was no expression at all in the roots. Our data also showed that there was more expression of luciferase in the leaves of plants grown in the light than those grown in the dark. This is because the chlorophyll binding protein is only needed when the plant is in light environments and therefore the plant has sensory mechanisms to ensure that when this is the case, greater expression of the binding protein or in our case the reporter gene luciferase occurs. There was however still a significant amount of expression from this promoter of the plants that were dark-adapted. Although our investigation didn't' allow us to determine why this occurred; however it may have been a result of the normal circadian rhythm of the plant. This process controls the plants 24 hour clock and so even when the plant was adapted to a dark environment, if the circadian clock was telling it that it was daytime and that there was the chance of a light environment developing then expression of this protein would still occur, however at reduced levels. This is shown in our findings in Figure 1. The third promoter we studied was phyA from which phytochrome A, a red light photoreceptor is transcribed in wild type plants. This protein is part of the plants machinery to detect and integrate light signals into pathways that initiate and regulate plant growth and development. As a result, phytochrome A is predominantly found in the leaves, as these are the parts of the plant which will be able to detect the light if it is present. This is shown in our results as we found that when luciferase was transcribed from the phyA promoter this expression occurred at much greater levels in the leaves than in the roots. In addition, although we did not study the expression of luciferase when transcribed from phyA light-grown plants, we would expect to find that the levels of luciferase would have been much lower than in the dark-grown plant. This would have been because once in light, the expression of the natural phytochrome A protein is down regulated at both the transcriptional and post-translational levels, as there is no longer a need to detect whether or not light is present in the growth environment. The final promoter we studied was chs. The data we obtained (as shown in Figure 1) showed that there was no expression of luciferase from chs in plants that had been grown in the dark, and in those grown in the light, there was higher expression in the leaves than in the roots. This is because the protein product normally transcribed from this promoter (chalcone synthase) is involved in the production of photoprotective pigments i.e. pigments that protect the plant from light damage. As a result, the plant only produces chalcone synthase in conditions where light damage may occur i.e. in light grown conditions and in addition modifies the expression of this enzyme such that tissues with a lesser chance of being exposed to light e.g. the roots express lower levels of chalcone synthase. As our reporter gene was being expressed in place of chalcone synthase, the data we obtained for this construct can be understood in this way. The first part of our study surrounding the dynamic changes of luciferase expression after a given light stimulus Figure 2 shows that, as there was no change in light emission in the dark plant in comparison to the light plant, the changes luciferase activity occurred as a direct consequence of the light stimulus. The data in Figure 3 allowed us to determine the range over which the wild type plants responded to the light stimulus and as a result helped us to identify four potential CAB: luc mutants as their response differed from that of the wild type. As described in the results section we identified four mutagenised plants which showed different patterns (from the wild type plants) in their response to a given light stimulus. It may be the case that these plants are mutant in their ability to respond to light and therefore they should be kept for further analysis. Plant H12 didn't show any light response and therefore may be a possible det mutant. There was also evidence that expression of luciferase from the CAB promoter in plant H12 was subject to regulation by circadian rhythms as the photons emitted declined and then increased again in the absence of a light stimulus. Plants G11 and F11 showed very similar patterns of luciferase expression, however the amount expressed from F11 was slightly higher. Similarly to plant H12, both G11 and F11 showed evidence that luciferase expression was being modulated by the plants normal circadian rhythms as although both plants responded to the light stimulus, the expression of luciferase increased again in the absence of a second light stimulus. Our final possible mutant, plant E6 showed evidence of a light response as the expression of luciferase increased after the light stimulus however it also showed evidence of circadian regulation as expression at the start of the investigation i.e. before the light stimulus was high. ConclusionReporter genes are genes with easily assayable products for example lacZ which encodes for β-galactosidase. These exogenous genes can be inserted into different genetic constructs to allow the activity of different genetic factors such as promoters, repressors etc. to be studied. The expression of the gene product is measured to give an accurate representation of the activity of the factors influencing expression. In our investigation we used three different reporter genes, green fluorescent protein (GFP), β-glucoronidase (GUS) and luciferase (luc). There are advantages and disadvantages for using each reporter gene and it is essential that each factor is taken into consideration when planning an experiment and deciding which gene to use. Firstly in comparison to many reporter genes, using GFP is relatively cheap. This is because there is no substrate needed and no specialised equipment is needed to observe the expression of the gene (only a UV lamp is needed). In addition the expression of GFP is easily observed under UV light and the visualisation of GFP is vivo allows changes in expression to be studied over time. On the other hand, as expression patterns are observed using UV light, specialist photographic equipment is needed to record these findings. In addition the red fluorescence of chlorophyll may interfere with the expression patterns of GFP. Secondly unlike GFP, when using GUS patterns of expression can be observed using the naked eye and standard photographic equipment can be used to record this finding. On the down side however, the substrate needed to observe GUS expression is very expensive, the method needed to carry out this procedure is time consuming and there is no scope to analyse expression in vivo as the plants need to be killed before expression patterns can be observed. Finally in contrast to both GUS and GFP using luciferase as a reporter gene requires using both an expensive substrate and expensive equipment. The use of this reporter gene does however have more positive aspects than the other two as it can be used to give a quantitative measure of gene expression and can also be carried out in vivo so any mutant plants can be used for further research. In addition the equipment used to measure luciferase activity is very sensitive and therefore even the expression from very weak promoters can be detected. Finally and most importantly, the enzyme luciferase is unstable and therefore it loses activity with a half-life of around thirty minutes. This therefore makes luciferase useful for studying dynamic changes in gene expression over time.",True
45,"Agrobacterium-mediated transformation of Arabidopsis by floral dip, and selection of transformantsIntroductionTransformation as a process of genetic exchange is widely used for producing transgenetic plants. Like many bacteria Agrobacterium possess DNA in the form of a plasmid in addition to its main genome. Under normal circumstances this plasmid contains the vir genes which causes part of the plasmid to self-replicate and insert itself into the genome of the plant host cells. This then alters hormone production and causes the development of galls or tumours in the plants. The bacterial T-DNA i.e. the portion of the plasmid that becomes integrated in the plant genome can be modified such that its incorporation does not cause tumour formation, but instead confers foreign genes (such as those for antibiotic resistance) upon the plants phenotype. In our investigation we aimed to transform Arabidopsis plants with a genetic construct that allowed selection of transformed plants due to Kanamycin resistance and in addition allowed the uptake of the plasmid DNA to be visualised using the fluorescent reporter gene green fluorescent protein (GFP). As the uptake and incorporation of the plasmid DNA is a rare event we aimed to screen the progeny of plants subjected to the floral dip and select for stable transformants and hence calculate a value for transformation frequency. MethodsThe method for this investigation is described on pages five and six of the laboratory manual. There were no alterations to this method. ResultsFor the first part of our investigation i.e. the identification of plants transformed by floral dipping we used a UV lamp to determine that the control plant i.e. the plants that had not been transformed showed only red fluorescence characteristic of chlorophyll and in addition yellow spots in dying tissues where no chlorophyll was present. In contrast the transformed plants showed green fluorescence in parts of the stems and leaves, and in some cases the parts of the flowers were also green. DiscussionThe fact that only the transformed plants fluoresced green enabled us to determine that the activity of GFP was as a result of transformation. In addition, by using the viral promoter 35S we were able to ensure that expression of the reporter gene could have occurred in any cell in the plant and therefore again the expression of GFP was dependant only upon successful transformation. In most cases it is probable that the uptake of the plasmid was only transient i.e. the GFP reporter gene did not incorporate into the host cells genome and therefore would be lost after cell division. However as we detected some green fluorescence in parts of the flowers there was the chance that the plants germ cells had been infected and therefore any progeny may carry GFP in their genomes. GUS staining of Arabidopsis seedlings transformed with a promoter trap constructIntroductionThe use of promoter traps in genetics has been greatly exploited in recent years. This technique involves transformation and random integration of reporter gene sequences that lack cis-acting sequences required for expression. Once inserted in the genome if the reporter gene has landed downstream of a promoter for another gene, this promoter can act upon the reporter gene to allow its expression. If during transposition the reporter gene lands within another gene hence disrupting and replacing its expression, then patterns of reporter gene expression can be used to identify which gene has been disrupted, where this gene is located within the genome and more importantly where this gene is expressed and how it affects the plants phenotype. In this investigation we aimed to use the reporter gene Gus in a promoter trap to identify and characterise mutant plant phenotypes generated by transformation. MethodThe method for this investigation is described on pages seven and eight of the laboratory handbook, we made no alterations to this method. ResultsThe three positive controls we used for this investigation were LAX2:Gus GL2:Gus and CHS:Gus in which we expected to find blue staining in the auxin transport tissues, trichome cells in the root and in all tissues respectively. As the seedlings we used for the investigation were so small, when we looked at our LAX2:Gus plants there was no staining as no germination had occurred. In the case of the GL2:Gus plants only a small part of the seed coat had stained blue, and for the CHS:Gus plants there was no obvious root staining however the hypocotyls and cotyledons had stained deep blue. Similarly to the LAX2:Gus control plants, no germination had occurred in any of the plants in our three promoter trap lines (N4567, N4568 and N4554) and therefore no staining was observed. DiscussionThe fact that the seedlings used in this investigation were so premature was a major limiting factor and therefore we would need to repeat this investigation using more developed plants before any staining of our promoter traps could be observed. The three positive controls (with the exception of LAX2:gus) although premature in their development showed the expected patterns of Gus expression. The reporter gene Gus had been inserted downstream of each of the specified promoters and therefore had been expressed in the same tissues as the native gene. It was because of this that Gus staining was observed only in the hypocotyls and cotyledons in our CHS:Gus plants, as CHS drives expression of chalcone synthase, an enzyme involved in protecting the plant from light damage, and this protein is expressed only in the tissues of the plant which are in contact with light (i.e. the hypocotyls and cotyledons). If we had used more developed LAX2:Gus controls then we would have expected to find staining in the auxin transport tissues as the presence of Gus downstream of the LAX2 promoter would have allowed expression of Gus in those tissues where the native gene is normally expressed. Our final control promoter GL2 drives expression of factors needed for trichome morphogenesis such as cell expansion and maturation of the cell. As the plants we studied were so small and lacked any roots, our results showed that only part of the seed coat was stained, most probably the cells which would later develop into the roots, as this is where trichome cells are found. If any of our promoter trap plants had been developed enough to observe staining, then we would have been able to locate those tissues where Gus was being expressed. As a result we would then have been able to identify possible genes disrupted by the transposition and insertion of the reporter gene, and in addition we could have analysed the patterns of gus expression to determine the expression patterns of the disrupted gene. As we did not obtain any data for this investigation we would need to repeat the method set out on pages seven and eight using larger and more developed seedlings. Luciferase as a dynamic reporter for gene expression in  ArabidopsisIntroductionThe protein luciferase is an enzyme which produces light in the presence of luciferin, ATP and oxygen. In our investigation transgenetic plants were produced which contained the reporter gene luciferin under the control of four different promoters, chs from which chalcone synthase is transcribed, the chlorophyll a/b binding protein promoter cab, phyA from which phytochrome A is transcribed and 35S, a viral promoter. As luciferase activity can be measured with great accuracy (due to the use of highly sensitive light-detectors), we aimed to quantify and compare the expression of luciferase in the roots and leaves of Arabidopsis when driven by the four different promoters. One of the advantages of using luciferase as a reporter gene is that it is unstable and hence as the levels of active protein change over time, it can be used to study the dynamic changes of induction of transcription from a specific promoter. As previously mentioned the chlorophyll a/b binding protein is transcribed from the promoter cab, therefore using luciferase as a reporter gene transcribed from this promoter, we aimed to study the changing expression of the chlorophyll a/b binding protein over time after exposure to red light for five minutes. MethodThe method for this investigation is described on pages nine and ten of the laboratory manual. There were no changes made to this method. ResultsThe first part of our investigation was to quantify the expression levels of luciferase driven from four different promoters in the roots and leaves of Arabidopsis plants that were either grown under constant light or adapted to the darkness for 48 hours. This data is shown in Figure 1. The data in Figure 1 shows that the different promoters allowed different patterns of expression under the different conditions. When luciferase was transcribed from the CHS promoter, there was little expression in the roots or the leaves of plants grown in the light and no expression at all when the plants were grown in the dark. As shown in Figure 1, we only studied PHYA plants that had been grown in the dark and found that the level of luciferase activity was the highest of all the different constructs in the leaves of these plants. When transcribed from the promoter CAB the data in Figure 1 shows that there was no expression of luciferase in the roots of either the light or dark plants, however expression in the leaves occurred in both the light and dark grown plants with a greater pixel intensity generated in the light grown plants. Our final promoter 35S allowed similar expression of luciferase in the light and the dark, however the pixel intensity from the leaves was slightly higher than from the roots. The second part of our investigation was to use the dynamic change in luciferase expression to study the response of the CAB promoter to light. We firstly studied the changing patterns of expression in wild type dark and light grown plants. This data is shown in Figure 2. As shown in Figure 2, the plants grown in the dark did not show any change in luciferase expression (and hence luciferase activity) when there was no light stimulus applied. In contrast the plants which had been given a light stimulus at time zero showed a sharp increase in photon emission followed by a gradual decline back to the start level after sixteen hours. Before we could screen for plants that were potentially mutant in their ability to respond to a light stimulus we had to investigate the range of responses in the wild type light plant. This data is shown in Figure 3. The data presented in Figure 3 show that all the wild type plants were able to respond to light in a similar manner i.e. they all showed an increase in luciferase activity after the light stimulus at time zero, after which time the level of luciferase activity gradually declined over a period of between five and sixteen hours to the same level of activity as at time minus six hours. There was however variation between the twenty-four plants screened for example plant C6 produced the highest response but also had the highest starting photon emission. Figures 4 and 5 show the mutagenised plants that we selected as being potentially defective in their ability to sense and respond to the light stimulus. There are three potential light mutants shown in Figure 4, plants E6, G11 and F11. In comparison to the average wild type plant, plant E6 emitted a much greater number of photons at the start of the experiment. This value then declined steeply before rising again at time zero. Between five and sixteen hours the photons emitted gradually declined, such that by the end of the experiment their levels were approximately the same as at time zero. Plant G11 showed the same pattern in its response to light as the average wild type, however there was a much shorter decline in photon emission (between about three and five hours) after which time emission gradually increased again. Finally plant F11. This plant also showed a similar pattern in response to the light as did the wild type plant however similarly to plant G11 the decline in photon emission was shorter than that of the wild type (between about one and eleven hours), and after about eleven hours emission increased once again. As shown in Figure 5, the amount of photons emitted by plant H12 was much higher than those emitted by the wild type plant. In addition whereas the emissions by the wild type plant only occurred after the light stimulus at time zero and then gradually declined back to the initial level, the amount of photons emitted by plant H12 gradually increased from the start of the investigation before declining between four and ten hours and then gradually increasing again. DiscussionAs shown in Figure 1, the expression patterns of the reporter gene luciferase varied dependent upon the promoter from which it was transcribed. The viral promoter 35S was selected to allow constitutive expression of the reporter gene in all areas of the plant. However as viruses are obligate intracellular parasites and use the host machinery for protein synthesis, transcription from this promoter would have required items such as transcription factors which may not have been ubiquitous throughout the plant and in both light and dark situations. This may therefore account for why luciferase expression was higher in the leaves than the roots. Our second promoter CAB is normally drives expression of the chlorophyll a/b binding protein, a component of photosystem II in the chloroplasts. As such, expression of this protein only occurs in the leaves of Arabidopsis i.e. where the light absorbing and photosynthetic apparatus of the plants are located and hence where this protein is needed. This is shown in our findings as when luciferase was transcribed from this promoter there was no expression at all in the roots. Our data also showed that there was more expression of luciferase in the leaves of plants grown in the light than those grown in the dark. This is because the chlorophyll binding protein is only needed when the plant is in light environments and therefore the plant has sensory mechanisms to ensure that when this is the case, greater expression of the binding protein or in our case the reporter gene luciferase occurs. There was however still a significant amount of expression from this promoter of the plants that were dark-adapted. Although our investigation didn't' allow us to determine why this occurred; however it may have been a result of the normal circadian rhythm of the plant. This process controls the plants 24 hour clock and so even when the plant was adapted to a dark environment, if the circadian clock was telling it that it was daytime and that there was the chance of a light environment developing then expression of this protein would still occur, however at reduced levels. This is shown in our findings in Figure 1. The third promoter we studied was phyA from which phytochrome A, a red light photoreceptor is transcribed in wild type plants. This protein is part of the plants machinery to detect and integrate light signals into pathways that initiate and regulate plant growth and development. As a result, phytochrome A is predominantly found in the leaves, as these are the parts of the plant which will be able to detect the light if it is present. This is shown in our results as we found that when luciferase was transcribed from the phyA promoter this expression occurred at much greater levels in the leaves than in the roots. In addition, although we did not study the expression of luciferase when transcribed from phyA light-grown plants, we would expect to find that the levels of luciferase would have been much lower than in the dark-grown plant. This would have been because once in light, the expression of the natural phytochrome A protein is down regulated at both the transcriptional and post-translational levels, as there is no longer a need to detect whether or not light is present in the growth environment. The final promoter we studied was chs. The data we obtained (as shown in Figure 1) showed that there was no expression of luciferase from chs in plants that had been grown in the dark, and in those grown in the light, there was higher expression in the leaves than in the roots. This is because the protein product normally transcribed from this promoter (chalcone synthase) is involved in the production of photoprotective pigments i.e. pigments that protect the plant from light damage. As a result, the plant only produces chalcone synthase in conditions where light damage may occur i.e. in light grown conditions and in addition modifies the expression of this enzyme such that tissues with a lesser chance of being exposed to light e.g. the roots express lower levels of chalcone synthase. As our reporter gene was being expressed in place of chalcone synthase, the data we obtained for this construct can be understood in this way. The first part of our study surrounding the dynamic changes of luciferase expression after a given light stimulus Figure 2 shows that, as there was no change in light emission in the dark plant in comparison to the light plant, the changes luciferase activity occurred as a direct consequence of the light stimulus. The data in Figure 3 allowed us to determine the range over which the wild type plants responded to the light stimulus and as a result helped us to identify four potential CAB: luc mutants as their response differed from that of the wild type. As described in the results section we identified four mutagenised plants which showed different patterns (from the wild type plants) in their response to a given light stimulus. It may be the case that these plants are mutant in their ability to respond to light and therefore they should be kept for further analysis. Plant H12 didn't show any light response and therefore may be a possible det mutant. There was also evidence that expression of luciferase from the CAB promoter in plant H12 was subject to regulation by circadian rhythms as the photons emitted declined and then increased again in the absence of a light stimulus. Plants G11 and F11 showed very similar patterns of luciferase expression, however the amount expressed from F11 was slightly higher. Similarly to plant H12, both G11 and F11 showed evidence that luciferase expression was being modulated by the plants normal circadian rhythms as although both plants responded to the light stimulus, the expression of luciferase increased again in the absence of a second light stimulus. Our final possible mutant, plant E6 showed evidence of a light response as the expression of luciferase increased after the light stimulus however it also showed evidence of circadian regulation as expression at the start of the investigation i.e. before the light stimulus was high. ConclusionReporter genes are genes with easily assayable products for example lacZ which encodes for β-galactosidase. These exogenous genes can be inserted into different genetic constructs to allow the activity of different genetic factors such as promoters, repressors etc. to be studied. The expression of the gene product is measured to give an accurate representation of the activity of the factors influencing expression. In our investigation we used three different reporter genes, green fluorescent protein (GFP), β-glucoronidase (GUS) and luciferase (luc). There are advantages and disadvantages for using each reporter gene and it is essential that each factor is taken into consideration when planning an experiment and deciding which gene to use. Firstly in comparison to many reporter genes, using GFP is relatively cheap. This is because there is no substrate needed and no specialised equipment is needed to observe the expression of the gene (only a UV lamp is needed). In addition the expression of GFP is easily observed under UV light and the visualisation of GFP is vivo allows changes in expression to be studied over time. On the other hand, as expression patterns are observed using UV light, specialist photographic equipment is needed to record these findings. In addition the red fluorescence of chlorophyll may interfere with the expression patterns of GFP. Secondly unlike GFP, when using GUS patterns of expression can be observed using the naked eye and standard photographic equipment can be used to record this finding. On the down side however, the substrate needed to observe GUS expression is very expensive, the method needed to carry out this procedure is time consuming and there is no scope to analyse expression in vivo as the plants need to be killed before expression patterns can be observed. Finally in contrast to both GUS and GFP using luciferase as a reporter gene requires using both an expensive substrate and expensive equipment. The use of this reporter gene does however have more positive aspects than the other two as it can be used to give a quantitative measure of gene expression and can also be carried out in vivo so any mutant plants can be used for further research. In addition the equipment used to measure luciferase activity is very sensitive and therefore even the expression from very weak promoters can be detected. Finally and most importantly, the enzyme luciferase is unstable and therefore it loses activity with a half-life of around thirty minutes. This therefore makes luciferase useful for studying dynamic changes in gene expression over time.","Summary (Abstract)Since the emergence and characterisation of  Legionella pneumophila in 1976, much interest has been caused due to the organism's ability to survive and replicate within components of the human immune system that normally combat such an infection. In recent years the incidence of Legionnaires' disease has increased. This essay aims to understand the characteristics of L. pneumophila that allow infection, survival and replication within both amoebae in the natural environment, and human alveolar macrophages leading to the development of the pneumonia-like Legionnaires' disease. IntroductionLegionella pneumophila is a motile, gram negative, aerobic rod commonly found in fresh water supplies where it prevails as an intracellular parasite of amoebae (1). This organism has complex nutritional requirements, including an unusually high iron and cysteine requirement, low levels of sodium and aromatic compounds (3). Unlike other waterborne pathogens (which are often transmitted through drinking or recreational water or from person to person), the spread of L. pneumophila occurs via infected aerosolised water droplets that are subsequently inhaled. It wasn't until 1976 when an outbreak of Legionnaires' disease struck an American Legion convention killing 29 people, that L. pneumophila was first established as the causative agent of this pneumonia-like disease (2, reviewed in 1). It is now recognised that L. pneumophila is a classic opportunistic pathogen, infecting and causing disease in immunocompromised individuals, for example the elderly, individuals with damaged lungs or lung-related disease (1). In addition to the generation of Legionnaires' disease, L. pneumophila also causes a much milder, flu-like self-limiting disease called Pontiacs Fever (3). Unlike the symptoms exhibited by Legionnaires' disease (fever, disorientation, lethargy and lung damage), symptoms of Pontiacs Fever are less severe and include fever, chills and headaches. To date evidence of Pontiacs Fever has only been noted during Legionnaires' outbreaks and in comparison to Legionnaires' disease there have been no recorded mortalities from this form of the infection. The role of amoebae in the survival of Legionella pneumophila within the natural environmentThe spread of L. pneumophila is dependent upon the aerosolization of contaminated water droplets from contaminated water storage systems. Until the development of water reservoirs (e.g. cooling towers) and evaporative condensers of air conditioning systems, L. pneumophila posed no risk to humans (4, reviewed in 1). However, in recent years the number of Legionnaires' outbreaks has increased, the majority occurring during the summer months when use of air conditioning is generally greater. By examining the processes occurring within such water storage and distribution systems we can gain an increased understanding of why more and more outbreaks are being recorded. Firstly, as with any surface, biofilms form within water storage systems. A microbial biofilm consists of bacterial colonies attached to each other and their surface by the secretion of adhesive polysaccharides. In adverse environments, i.e. nutritionally deficient conditions, biofilms trap essential nutrients (for example iron), providing a rich environment for bacterial growth. As the number of large-scale water storage and distribution systems has increased, more buildings now contain reservoirs of L. pneumophila. In practice, these reservoirs cause an increased threat to humans due to inadequate cleaning and water treatment standards which don't remove the organisms before they have chance to replicate and spread to potential hosts. A second factor affecting the increase of Legionnaires' outbreaks is the presence of amoebae in aquatic systems. This organism feeds on microorganisms present in biofilms and hence may ingest L. pneumophila by whirling phagocytosis (a process characteristic of L. pneumophila). Of the thirteen species of amoebae, only two support the replication of L. pneumophila (5) and therefore act as reservoirs for L. pneumophila in the natural environment. These two species are Acanthamoeba and  Naeglaria (3). Within the amoebae, L. pneumophila undergoes high levels of replication and is protected to a much greater extent than free bacteria in the water would be from water treatment processes such as chlorination and Kathon CG. As a result, controlling the presence and replication of L. pneumophila in potable water supplies is very difficult, as even residual treatment which may kill free bacteria cannot remove cells carried by the highly tolerant amoebae. These facts can be shown by a study using water samples obtained from five hospitals in Paris. Of the samples taken, 71% contained amoebae and 47% showed presence of L. pneumophila (6, reviewed in 1). When these water samples were incubated, bacterial replication occurred, providing the sample contained amoebae (1). The role of amoebae in L. pneumophila virulence factorsIn addition to protecting L. pneumophila from water treatment processes and allowing replication, colonisation of amoebae has also accounted for the evolution of virulence factors essential for infection and replication within human alveolar macrophages. There are numerous similarities between the life cycles of L. pneumophila within both amoebae and alveolar macrophages (for example in both amoebae and macrophages, coiling phagocytosis occurs) but moreover genetic studies have shown that the defects in three virulence factors essential for bacterial replication and survival in monocytic U937 cells also inhibits these same processes in amoebae. The effects of such research is summarised in figure 1: This research therefore indicates that the strategies evolved by L. pneumophila to grow in amoebae also work in the human alveolar macrophage. In addition to the research shown in figure 1, there are other virulence factors of L. pneumophila that are established in amoebae and not only enable growth in macrophages, but in fact increase the incidence of the organism and the rate at which complications with the disease are created. Examples of such factors are summarised in figure 2. The increased ability of Legionella pneumophila grown in phagocytes to withstand chemical biocides and antibiotics was demonstrated as 71% of bacteria obtained from Acanthamoeba polyphaga survived a 24-hour exposure to 5μg/ml riampin. This value of 71% is significant as when the same conditions were applied to bacteria cultured in broth, more than 99.9% killing was observed (12, reviewed in 1). This therefore clearly shows that replication in amoebae increases the virulence of Legionella pneumophila and hence the ability of this organism to cause infection and disease. The host immune response to L. pneumophilaIn general, when a host organism becomes infected, it is the job of both the innate and humoral immune responses to clear that infection. L. pneumophila infections however do not follow this rule as this organism demonstrates relatively high resistance to humoral responses such as complement-mediated killing even though the complement component C3 is readily bound (13, reviewed in 1). Even with the presence of specific antibodies there is little increase of monocytic bactericidal activity. Horwitzal. (13, reviewed in 1) demonstrated this by incubating serum- resistant encapsulated E.coli for one hour with 10% fresh normal serum, L. pneumophila specific antiserum and polymorphonuclear cells. It was observed that the number of colony forming units (CFUs) decreased by 2.5 logs. Another similar treatment reduced L. pneumophila CFUs by only 0.5 logs. These data therefore indicate that complement, specific antibodies and polymorphonuclear cells cannot resolve infection and therefore partly account for the ability of L. pneumophila to enter and replicate within human alveolar macrophages. Due to the inability of innate and humoral immune responses to clear L. pneumophila infections, this role therefore becomes the responsibility of cell-mediated immune responses (14, reviewed in1). However, as L. pneumophila is an opportunistic pathogen such responses in immunocompromised individuals fails and hence allow the infection to prevail. Horwitzal. (15, reviewed in 1) studied the role of the cell-mediated response in non-susceptible hosts. The activity of peripheral blood mononuclear cells from recovered individuals in comparison to age and sex matched controls. The findings of this research are summarised in figure 3 and show that clearly a specific T-cell response has occurred in the recovered patients. In addition to the data described on the previous page, the supernatants of samples of patient cells exposed to L. pneumophila contained activated naïve mononuclear cells ready to inhibit replication of this bacterial species. This therefore supports the idea of a cell-mediated immune response. There are many stages to the cell-mediated response, however as previously discussed these are only feasible in non-compromised host cells. A critical activator is interferon-γ (IFN-γ), a cytokine that activates macrophages. Bhardwaj et al (16, reviewed in 1) demonstrated that IFN-γ doesn't enhance L. pneumophila killing or prevent the formation of replication vesicles but instead prevents bacterial growth via an iron-dependant mechanism. The ability of IFN-γ activated macrophages to prevent growth was reversed when samples were supplemented with iron transferrin (17, reviewed in1). This therefore indicates that the pool of iron in activated macrophages may limit the growth of L. pneumophila and has been further proved by Byrdal. (18, reviewed in 1) in research showing that IFN-γ activated blood monocytes contained 73% fewer transferrin-binding sites than non-activated control cells. Because of the ability of activated macrophages in non-susceptible hosts to terminate infections by L. pneumophila, we can begin to understand the factors that allow this organism to evade immune responses in the compromised individual and hence cause infection. Phagocyte interactionsAlthough the role of the host immune system is to protect and rid the body of foreign organisms, in the case of L. pneumophila, factors of the immune system in immunocompromised individuals actually aid the replication and survival of this organism within alveolar macrophages. As described in the previous section, components of complement (e.g. C3 and specific antibodies) opsonize the bacterium in an attempt to remove it from the body (13, reviewed in 1). However, upon failure of complement-mediated killing, receptors on the surface of macrophages recognise the opsonins and allow a special whirling phagocytosis event to occur, characteristic of L. pneumophila infections (19, reviewed in 1). In the absence of opsonization, the Mip protein (macrophages invasion protein) found on the L. pneumophila outer membrane, potentiates the invasion of human mononuclear phagocytes, again via whirling phagocytosis (3). This Mip protein is only synthesised in response to low levels of amino acids and the consequent presence of uncharged transfer RNAs (tRNAs), which activates Rel A causing the accumulation of guanosine 3'5' bispyrophosphate synthetases (ppGpp). This accumulation results in entry into the stationary phase of replication and the expression of virulence traits that promote transmission e.g. the Mip protein. This example therefore illustrates how the growth phase of L. pneumophila determines its phenotype (20, reviewed in 1). The production of an isolated phagosome in macrophage survivalOnce inside the macrophage, vesicles containing L. pneumophila must act quickly to become isolated from the lysosomal pathway and hence survive and replicate within their host macrophage. The main virulence system employed L. pneumophila is the dot/icm gene system (defective organelle trafficking and intracellular replication genes). As part of the type IV secretion system these two genes were originally designated due to their ability to transfer plasmid DNA to a recipient cell (21, reviewed in 1). However, immediately after phagocytosis of L. pneumophila by alveolar macrophages, these genes establish a different role, by assembling and activating a secretion system that integrates pores into the phagosomal membrane and hence remodels the phagosome membrane. It is through these pores that effector proteins (which would otherwise be used to traffic the phagosome to the endosome) are then transported to the host cytoplasm (22, reviewed in 1). Royal. (23, reviewed in 1) demonstrated the importance of the dot/icm genes in this process by using Electron Microscopy (EM). Results showed that in contrast to wild type cells, dot/icm mutants were delivered to the lysosomal compartment between five and thirty minutes after infection. Examples of proteins lost from the phagosome during membrane remodelling include the lysosomal marker LAMP-1 as identified by fluorescence microscopic assays (25, reviewed in 1). The loss of this guanosine triphosphate (GTP) binding protein from the membrane prohibits the promotion of fusion between early and late endosomal compartments (26 reviewed in 1). Unpublished observations also indicate that the loss of transferrin receptors from the phagosomal membrane stops virulent L. pneumophila vacuoles from interacting with the early endosomal compartment (27, reviewed in 1). Whilst the dot/icm genes can explain phagosome remodelling and the evasion of the endosomal pathway, other studies suggest that these processes could be accounted for by the presence of smooth vesicles on the cytoplasmic phagosome membrane within 15-20 minutes of infection (24, reviewed in 1). Horwitzal. (24) used kinetic studies to show that such vesicles budded from the phagosome taking with them proteins that would otherwise activate the endocytic pathway. In this way the maturation of the L. pneumophila-containing phagosome was hindered. The evidence presented to support both dot/icm and vesicle remodelling of the phagosome membrane illustrates the importance of this step in the intracellular survival of L. pneumophila. By measuring the release of p-nitrophenyl from the synthetic substrate p-nitrophenylphosphorylcholine (29, reviewed in 1) the activity of phospholipases has also been identified as interacting with the phagosome membrane and hence preventing phagolysosome formation. Two other molecules are also known to disrupt phagolysosome formation. These are the lectin, concanavalin A, and ammonium chloride. Although neither of these two molecules have been linked to pathogenesis, the lectin can cross-link membrane glycoproteins forming vacuoles that can't fuse with lysosomes (29, reviewed in 1), whilst ammonium chloride raises vacuolar pH hence interfering with phagolysosome formation (30, reviewed in 1). Replication of L. pneumophilaAs previously described, the dot/icm type IV secretion system is one of the primary mechanisms by which ingested L. pneumophila avoid immediate lysosomal delivery. Research has shown that this same set of genes is also involved in the recruitment of the endoplasmic reticulum (ER) and the formation of a replication vacuole. (31, reviewed in 32). Within one hour after the isolated phagosome is established, numerous mitochondria are recruited to its side and approximately four hours after isolation the cytoplasmic face interacts with the host cell ER, hence establishing the site of replication (24, reviewed in 1). After fusing with the ER, L. pneumophila becomes replicative, acid tolerant, and stops the expression of virulence traits such as those involved in the blocking of membrane fusion. Once these important transformations have occurred the  L. pneumophila exploits the natural function of the ER, and is delivered by autophagy to the harsh environment of the lysosomal compartment. It is then within the nutrient-rich environment of the lysosome that the L. pneumophila really exploits its host cell by replicating exponentially until all possible nutrients have been exhausted. Without the ability to replicate so profusely at the expense of the host macrophage, infection by L. pneumophila would have been fruitless and all the mechanisms evolved to evade host protection mechanisms would have gone to waste. As the available nutrient levels gradually decline, the L. pneumophila stringent paradigm response is triggered, coordinating the expression of virulence and entry into the stationary phase of replication. The mechanisms by which this response occurs is shown in figure 4: When amino acids are limiting uncharged tRNAs activate RelA, a guanosine 3'5'- bispyrophosphate synthetases. Accumulation of ppGpp coordinates entry into the stationary phase with expression of virulence traits that promote transmission to a new host. Some effectors may be substrates for type II and IV secretion systems whilst others such as flagellin (essential in providing motility for finding a new host), the development of osmotic resistance (essential for surviving in the natural environment) and the secretion of cytotoxins (promotes escape from the spent host by lysis) are more easily categorised. Prevention of Legionnaires' diseaseThe most obvious way to prevent further incidence of Legionnaires' disease would be to eradicate L. pneumophila from all water systems that may produce contaminated aerosols. As previously discussed this would be an extremely difficult in practice due to the resistance of bacteria within amoebae and biofilms to biocides. Other method such as UV radiation and heating to 60°C can be used to kill L. pneumophila within both biofilms and amoebae, but in practice the costs involved with treating such large volumes of water in this way make these treatments impractical (3). Therefore the majority of recent research has been focused on the development of biocides for water treatment that have no detrimental effects on humans, but kill L. pneumophila living in biofilms and amoebae. In addition, research has also focused on the development of a vaccine that would involve the activation of macrophages and recruitment of cytotoxic T cells to help kill the bacterium-infected macrophages (3). However, at this point in time no such advances have been made and therefore the main method used to prevent outbreaks of Legionnaires' disease remains the cleaning and maintenance of water systems such that microbial life is limited or prevented. Regular chemical and microbial testing of potential L. pneumophila water systems should monitor these processes. ConclusionThe increased incidence of Legionnaires' disease in recent years is primarily due to the increased need to store water in large quantities and the development and increased usage of aerosolizing technology for example whirlpools, showers and air conditioning systems. With inadequate cleaning and the stringent water treatment policies implemented by many countries, many pathogens are not removed from such water systems and instead find themselves in the perfect niche for colonisation and transmission. Within the natural environment, L. pneumophila can survive and replicate within amoebae. The colonisation of this organism has enabled L. pneumophila to remain in aquatic systems at a higher incidence than if the bacteria were just free-living. This is due to the resistance and protection conferred by the amoebae during water treatment processes and other adverse environmental conditions. In addition, the colonisation of amoebae has also led to the evolution of specific virulence factors essential for infection, survival and replication within human alveolar macrophages such that when a susceptible host inhales a L. pneumophila-contaminated aerosol and becomes infected, this component of the immune system is exploited. A greater understanding of the virulence factors and the characteristics of L. pneumophila which enable it to be such an efficient human parasite will help prevent outbreaks of Legionnaires' disease in the future.",False
46,"Agrobacterium-mediated transformation of Arabidopsis by floral dip, and selection of transformantsIntroductionTransformation as a process of genetic exchange is widely used for producing transgenetic plants. Like many bacteria Agrobacterium possess DNA in the form of a plasmid in addition to its main genome. Under normal circumstances this plasmid contains the vir genes which causes part of the plasmid to self-replicate and insert itself into the genome of the plant host cells. This then alters hormone production and causes the development of galls or tumours in the plants. The bacterial T-DNA i.e. the portion of the plasmid that becomes integrated in the plant genome can be modified such that its incorporation does not cause tumour formation, but instead confers foreign genes (such as those for antibiotic resistance) upon the plants phenotype. In our investigation we aimed to transform Arabidopsis plants with a genetic construct that allowed selection of transformed plants due to Kanamycin resistance and in addition allowed the uptake of the plasmid DNA to be visualised using the fluorescent reporter gene green fluorescent protein (GFP). As the uptake and incorporation of the plasmid DNA is a rare event we aimed to screen the progeny of plants subjected to the floral dip and select for stable transformants and hence calculate a value for transformation frequency. MethodsThe method for this investigation is described on pages five and six of the laboratory manual. There were no alterations to this method. ResultsFor the first part of our investigation i.e. the identification of plants transformed by floral dipping we used a UV lamp to determine that the control plant i.e. the plants that had not been transformed showed only red fluorescence characteristic of chlorophyll and in addition yellow spots in dying tissues where no chlorophyll was present. In contrast the transformed plants showed green fluorescence in parts of the stems and leaves, and in some cases the parts of the flowers were also green. DiscussionThe fact that only the transformed plants fluoresced green enabled us to determine that the activity of GFP was as a result of transformation. In addition, by using the viral promoter 35S we were able to ensure that expression of the reporter gene could have occurred in any cell in the plant and therefore again the expression of GFP was dependant only upon successful transformation. In most cases it is probable that the uptake of the plasmid was only transient i.e. the GFP reporter gene did not incorporate into the host cells genome and therefore would be lost after cell division. However as we detected some green fluorescence in parts of the flowers there was the chance that the plants germ cells had been infected and therefore any progeny may carry GFP in their genomes. GUS staining of Arabidopsis seedlings transformed with a promoter trap constructIntroductionThe use of promoter traps in genetics has been greatly exploited in recent years. This technique involves transformation and random integration of reporter gene sequences that lack cis-acting sequences required for expression. Once inserted in the genome if the reporter gene has landed downstream of a promoter for another gene, this promoter can act upon the reporter gene to allow its expression. If during transposition the reporter gene lands within another gene hence disrupting and replacing its expression, then patterns of reporter gene expression can be used to identify which gene has been disrupted, where this gene is located within the genome and more importantly where this gene is expressed and how it affects the plants phenotype. In this investigation we aimed to use the reporter gene Gus in a promoter trap to identify and characterise mutant plant phenotypes generated by transformation. MethodThe method for this investigation is described on pages seven and eight of the laboratory handbook, we made no alterations to this method. ResultsThe three positive controls we used for this investigation were LAX2:Gus GL2:Gus and CHS:Gus in which we expected to find blue staining in the auxin transport tissues, trichome cells in the root and in all tissues respectively. As the seedlings we used for the investigation were so small, when we looked at our LAX2:Gus plants there was no staining as no germination had occurred. In the case of the GL2:Gus plants only a small part of the seed coat had stained blue, and for the CHS:Gus plants there was no obvious root staining however the hypocotyls and cotyledons had stained deep blue. Similarly to the LAX2:Gus control plants, no germination had occurred in any of the plants in our three promoter trap lines (N4567, N4568 and N4554) and therefore no staining was observed. DiscussionThe fact that the seedlings used in this investigation were so premature was a major limiting factor and therefore we would need to repeat this investigation using more developed plants before any staining of our promoter traps could be observed. The three positive controls (with the exception of LAX2:gus) although premature in their development showed the expected patterns of Gus expression. The reporter gene Gus had been inserted downstream of each of the specified promoters and therefore had been expressed in the same tissues as the native gene. It was because of this that Gus staining was observed only in the hypocotyls and cotyledons in our CHS:Gus plants, as CHS drives expression of chalcone synthase, an enzyme involved in protecting the plant from light damage, and this protein is expressed only in the tissues of the plant which are in contact with light (i.e. the hypocotyls and cotyledons). If we had used more developed LAX2:Gus controls then we would have expected to find staining in the auxin transport tissues as the presence of Gus downstream of the LAX2 promoter would have allowed expression of Gus in those tissues where the native gene is normally expressed. Our final control promoter GL2 drives expression of factors needed for trichome morphogenesis such as cell expansion and maturation of the cell. As the plants we studied were so small and lacked any roots, our results showed that only part of the seed coat was stained, most probably the cells which would later develop into the roots, as this is where trichome cells are found. If any of our promoter trap plants had been developed enough to observe staining, then we would have been able to locate those tissues where Gus was being expressed. As a result we would then have been able to identify possible genes disrupted by the transposition and insertion of the reporter gene, and in addition we could have analysed the patterns of gus expression to determine the expression patterns of the disrupted gene. As we did not obtain any data for this investigation we would need to repeat the method set out on pages seven and eight using larger and more developed seedlings. Luciferase as a dynamic reporter for gene expression in  ArabidopsisIntroductionThe protein luciferase is an enzyme which produces light in the presence of luciferin, ATP and oxygen. In our investigation transgenetic plants were produced which contained the reporter gene luciferin under the control of four different promoters, chs from which chalcone synthase is transcribed, the chlorophyll a/b binding protein promoter cab, phyA from which phytochrome A is transcribed and 35S, a viral promoter. As luciferase activity can be measured with great accuracy (due to the use of highly sensitive light-detectors), we aimed to quantify and compare the expression of luciferase in the roots and leaves of Arabidopsis when driven by the four different promoters. One of the advantages of using luciferase as a reporter gene is that it is unstable and hence as the levels of active protein change over time, it can be used to study the dynamic changes of induction of transcription from a specific promoter. As previously mentioned the chlorophyll a/b binding protein is transcribed from the promoter cab, therefore using luciferase as a reporter gene transcribed from this promoter, we aimed to study the changing expression of the chlorophyll a/b binding protein over time after exposure to red light for five minutes. MethodThe method for this investigation is described on pages nine and ten of the laboratory manual. There were no changes made to this method. ResultsThe first part of our investigation was to quantify the expression levels of luciferase driven from four different promoters in the roots and leaves of Arabidopsis plants that were either grown under constant light or adapted to the darkness for 48 hours. This data is shown in Figure 1. The data in Figure 1 shows that the different promoters allowed different patterns of expression under the different conditions. When luciferase was transcribed from the CHS promoter, there was little expression in the roots or the leaves of plants grown in the light and no expression at all when the plants were grown in the dark. As shown in Figure 1, we only studied PHYA plants that had been grown in the dark and found that the level of luciferase activity was the highest of all the different constructs in the leaves of these plants. When transcribed from the promoter CAB the data in Figure 1 shows that there was no expression of luciferase in the roots of either the light or dark plants, however expression in the leaves occurred in both the light and dark grown plants with a greater pixel intensity generated in the light grown plants. Our final promoter 35S allowed similar expression of luciferase in the light and the dark, however the pixel intensity from the leaves was slightly higher than from the roots. The second part of our investigation was to use the dynamic change in luciferase expression to study the response of the CAB promoter to light. We firstly studied the changing patterns of expression in wild type dark and light grown plants. This data is shown in Figure 2. As shown in Figure 2, the plants grown in the dark did not show any change in luciferase expression (and hence luciferase activity) when there was no light stimulus applied. In contrast the plants which had been given a light stimulus at time zero showed a sharp increase in photon emission followed by a gradual decline back to the start level after sixteen hours. Before we could screen for plants that were potentially mutant in their ability to respond to a light stimulus we had to investigate the range of responses in the wild type light plant. This data is shown in Figure 3. The data presented in Figure 3 show that all the wild type plants were able to respond to light in a similar manner i.e. they all showed an increase in luciferase activity after the light stimulus at time zero, after which time the level of luciferase activity gradually declined over a period of between five and sixteen hours to the same level of activity as at time minus six hours. There was however variation between the twenty-four plants screened for example plant C6 produced the highest response but also had the highest starting photon emission. Figures 4 and 5 show the mutagenised plants that we selected as being potentially defective in their ability to sense and respond to the light stimulus. There are three potential light mutants shown in Figure 4, plants E6, G11 and F11. In comparison to the average wild type plant, plant E6 emitted a much greater number of photons at the start of the experiment. This value then declined steeply before rising again at time zero. Between five and sixteen hours the photons emitted gradually declined, such that by the end of the experiment their levels were approximately the same as at time zero. Plant G11 showed the same pattern in its response to light as the average wild type, however there was a much shorter decline in photon emission (between about three and five hours) after which time emission gradually increased again. Finally plant F11. This plant also showed a similar pattern in response to the light as did the wild type plant however similarly to plant G11 the decline in photon emission was shorter than that of the wild type (between about one and eleven hours), and after about eleven hours emission increased once again. As shown in Figure 5, the amount of photons emitted by plant H12 was much higher than those emitted by the wild type plant. In addition whereas the emissions by the wild type plant only occurred after the light stimulus at time zero and then gradually declined back to the initial level, the amount of photons emitted by plant H12 gradually increased from the start of the investigation before declining between four and ten hours and then gradually increasing again. DiscussionAs shown in Figure 1, the expression patterns of the reporter gene luciferase varied dependent upon the promoter from which it was transcribed. The viral promoter 35S was selected to allow constitutive expression of the reporter gene in all areas of the plant. However as viruses are obligate intracellular parasites and use the host machinery for protein synthesis, transcription from this promoter would have required items such as transcription factors which may not have been ubiquitous throughout the plant and in both light and dark situations. This may therefore account for why luciferase expression was higher in the leaves than the roots. Our second promoter CAB is normally drives expression of the chlorophyll a/b binding protein, a component of photosystem II in the chloroplasts. As such, expression of this protein only occurs in the leaves of Arabidopsis i.e. where the light absorbing and photosynthetic apparatus of the plants are located and hence where this protein is needed. This is shown in our findings as when luciferase was transcribed from this promoter there was no expression at all in the roots. Our data also showed that there was more expression of luciferase in the leaves of plants grown in the light than those grown in the dark. This is because the chlorophyll binding protein is only needed when the plant is in light environments and therefore the plant has sensory mechanisms to ensure that when this is the case, greater expression of the binding protein or in our case the reporter gene luciferase occurs. There was however still a significant amount of expression from this promoter of the plants that were dark-adapted. Although our investigation didn't' allow us to determine why this occurred; however it may have been a result of the normal circadian rhythm of the plant. This process controls the plants 24 hour clock and so even when the plant was adapted to a dark environment, if the circadian clock was telling it that it was daytime and that there was the chance of a light environment developing then expression of this protein would still occur, however at reduced levels. This is shown in our findings in Figure 1. The third promoter we studied was phyA from which phytochrome A, a red light photoreceptor is transcribed in wild type plants. This protein is part of the plants machinery to detect and integrate light signals into pathways that initiate and regulate plant growth and development. As a result, phytochrome A is predominantly found in the leaves, as these are the parts of the plant which will be able to detect the light if it is present. This is shown in our results as we found that when luciferase was transcribed from the phyA promoter this expression occurred at much greater levels in the leaves than in the roots. In addition, although we did not study the expression of luciferase when transcribed from phyA light-grown plants, we would expect to find that the levels of luciferase would have been much lower than in the dark-grown plant. This would have been because once in light, the expression of the natural phytochrome A protein is down regulated at both the transcriptional and post-translational levels, as there is no longer a need to detect whether or not light is present in the growth environment. The final promoter we studied was chs. The data we obtained (as shown in Figure 1) showed that there was no expression of luciferase from chs in plants that had been grown in the dark, and in those grown in the light, there was higher expression in the leaves than in the roots. This is because the protein product normally transcribed from this promoter (chalcone synthase) is involved in the production of photoprotective pigments i.e. pigments that protect the plant from light damage. As a result, the plant only produces chalcone synthase in conditions where light damage may occur i.e. in light grown conditions and in addition modifies the expression of this enzyme such that tissues with a lesser chance of being exposed to light e.g. the roots express lower levels of chalcone synthase. As our reporter gene was being expressed in place of chalcone synthase, the data we obtained for this construct can be understood in this way. The first part of our study surrounding the dynamic changes of luciferase expression after a given light stimulus Figure 2 shows that, as there was no change in light emission in the dark plant in comparison to the light plant, the changes luciferase activity occurred as a direct consequence of the light stimulus. The data in Figure 3 allowed us to determine the range over which the wild type plants responded to the light stimulus and as a result helped us to identify four potential CAB: luc mutants as their response differed from that of the wild type. As described in the results section we identified four mutagenised plants which showed different patterns (from the wild type plants) in their response to a given light stimulus. It may be the case that these plants are mutant in their ability to respond to light and therefore they should be kept for further analysis. Plant H12 didn't show any light response and therefore may be a possible det mutant. There was also evidence that expression of luciferase from the CAB promoter in plant H12 was subject to regulation by circadian rhythms as the photons emitted declined and then increased again in the absence of a light stimulus. Plants G11 and F11 showed very similar patterns of luciferase expression, however the amount expressed from F11 was slightly higher. Similarly to plant H12, both G11 and F11 showed evidence that luciferase expression was being modulated by the plants normal circadian rhythms as although both plants responded to the light stimulus, the expression of luciferase increased again in the absence of a second light stimulus. Our final possible mutant, plant E6 showed evidence of a light response as the expression of luciferase increased after the light stimulus however it also showed evidence of circadian regulation as expression at the start of the investigation i.e. before the light stimulus was high. ConclusionReporter genes are genes with easily assayable products for example lacZ which encodes for β-galactosidase. These exogenous genes can be inserted into different genetic constructs to allow the activity of different genetic factors such as promoters, repressors etc. to be studied. The expression of the gene product is measured to give an accurate representation of the activity of the factors influencing expression. In our investigation we used three different reporter genes, green fluorescent protein (GFP), β-glucoronidase (GUS) and luciferase (luc). There are advantages and disadvantages for using each reporter gene and it is essential that each factor is taken into consideration when planning an experiment and deciding which gene to use. Firstly in comparison to many reporter genes, using GFP is relatively cheap. This is because there is no substrate needed and no specialised equipment is needed to observe the expression of the gene (only a UV lamp is needed). In addition the expression of GFP is easily observed under UV light and the visualisation of GFP is vivo allows changes in expression to be studied over time. On the other hand, as expression patterns are observed using UV light, specialist photographic equipment is needed to record these findings. In addition the red fluorescence of chlorophyll may interfere with the expression patterns of GFP. Secondly unlike GFP, when using GUS patterns of expression can be observed using the naked eye and standard photographic equipment can be used to record this finding. On the down side however, the substrate needed to observe GUS expression is very expensive, the method needed to carry out this procedure is time consuming and there is no scope to analyse expression in vivo as the plants need to be killed before expression patterns can be observed. Finally in contrast to both GUS and GFP using luciferase as a reporter gene requires using both an expensive substrate and expensive equipment. The use of this reporter gene does however have more positive aspects than the other two as it can be used to give a quantitative measure of gene expression and can also be carried out in vivo so any mutant plants can be used for further research. In addition the equipment used to measure luciferase activity is very sensitive and therefore even the expression from very weak promoters can be detected. Finally and most importantly, the enzyme luciferase is unstable and therefore it loses activity with a half-life of around thirty minutes. This therefore makes luciferase useful for studying dynamic changes in gene expression over time.","AbstractThis essay investigates the different types of stem cells, their origins and properties and why these characteristics have generated so much hope that stem cells can offer solutions to numerous different diseases and medical conditions. IntroductionStem cells are a unique population of cells found in both adult and foetal tissues which have the ability to renew themselves, to remain undifferentiated and to respond to specific signals and conditions, triggering differentiation. Within adult tissues (such as bone marrow, the brain and muscle), stem cells act to replace cells lost naturally, by disease or through injury. As such, adult stem cells are multipotent and can renew their own population and differentiate to yield the specific cells of their originating tissue. An example of this is shown in Figure 1 where haematopoietic stem cells (HSC) found in the bone marrow may divide symmetrically to regenerate their own population, or asymmetrically forming a further HSC and a daughter cell, the latter of which is committed to one of the two major haematopoietic lineages and hence replenishes the body with all the different blood cells (1). In contrast to adult stem cells, those obtained from embryos are pluripotent and have no fixed developmental process. The fact that pluripotent stem cells can theoretically differentiate into any cell type in the body has generated much research into developing methods to use pluripotent stem cells for cell-based therapy, and hence repairing tissues damaged by disease or injury. The origin of pluripotent stem cellsPluripotent stem cells i.e. distinct cells with complete developmental plasticity can be found in three different sources; embyronal carcinoma (EC) cells, embryonic stem (ES) cells and embryonic germ (EG) cells. The ability of stem cells to differentiate into such a wide variety of different cells was first recognised in testicular cancers. It was noticed that weird tumours formed that contained a wide variety of different tissue types, for example squamous epithelium and muscle, all of which were derived from pluripotent EC cells which themselves were derived from the primordial germ cells (2). The extent of tissue differentiation in this finding was examined using artificially cultured EC cell lines derived from the tumours, in the presence and absence of a mitotically inactive layer of fibroblasts, the feeder layer (Kahan et al. referenced in 2). The second category of pluripotent cells ES cells, are derived from the three to five day old embryo known as the blastocyst which consists of a hallow microscopic ball of around 150 cells (3). This pre-implantation embryo contains around 30 pluripotent cells in its centre known as the inner cell mass (ICM) which, as demonstrated by outgrowth cultures, can differentiate to yield many different specific cell types. During the outgrowth of ES cells, it was noted that some of the cells remained undifferentiated and if sustained on a feeder layer to which they could adhere and obtain nutrients from, they could be expanded to produce a seemingly immortal ES line (2). Finally, EG cells are derived from the primordial germ cells (PGC), the embryonic precursors of gametes and the same cells from which EC cells derive (4). This final group of cells were identified as having stem cell properties after growth upon feeder layers supplemented with serum and growth factors produced clusters of colonies that were morphologically indistinguishable from EC or ES grown under the same conditions (2). Culturing embryonic stem cellsIn recent years the potential uses of stem cells in medicine have gained increased appreciation and as a result have driven the need to understand the factors regulating and controlling stem cells in their natural environment. However, the challenge of producing different types of differentiated cells from pluripotent stem cells first needed to be overcome. Many investigations focussed on changing growth conditions, for example cell density or adding specific growth factors, but as such changes were dictated by trial and error the cellular differentiation achieved in this way was extremely haphazard and varied (2). In contrast, other studies where ES were grown in a development culture demonstrated that the cells began to differentiate into multicellular aggregates called embryoid bodies- clusters of cells that due to the variety of tissues formed represent early embryos (1,2,5). If these embryoid bodies were then transferred to a solid medium they developed into many different cell types for example neural cells and cardiomyocytes which could be separated for further study by methods such as fluorescence activated cells sorting (FACS) (6). However, the generation of embryoid bodies in this way could not be used in a controlled manner to produce specific cell types as it was due to spontaneous differentiation (5). As a result other molecular methods such as those shown in Figure 2 have been used to determine protocols for directed differentiation of ES cells into certain cell types. If the differentiation of ES can be developed reliably as suggested in Figure 2, then stem cells have the potential to become an important tool in treating some of the major diseases such as diabetes and traumatic spinal chord injury. However at the present time this ability is only available to a very limited number of cell types. Stem cells in medicine and developmentThe use of stem cells in medicine is not only relevant for curing diseases such as diabetes and muscular dystrophy, but also in gaining an increased understanding into the development of our own species. By using ES cells to understand why developmental points such as cell formation and proliferation occur and how these process impact upon embryogenesis, we can gain an increased understanding as to why conditions such as teratogens (which cause foetal malformations) are generated, and in addition possibly develop new pre-natal screens to detect such conditions (7). However, the main potential of pluripotent stem cells lies in the development of patient-specific tissues for transplantation and restoration of damaged or diseased tissues. This process of therapeutic cloning would involve taking samples of the patients ES cells and manipulating them to differentiate into the large numbers of cells needed to cure disease or restore tissue function. Figure 3 gives an outline of the range of conditions ES could be used to treat. Many success stories have been documented (8) however a major factor holding back the development of such techniques in many countries (for example the United States of America), are the ethical issues surrounding the cloning of embryos for use in stem cell research. As such, research is also being undertaken to investigate the potential for adult stem cells in medicine. Adult stem cellsIn comparison to embryo-obtained pluripotent stem cells, which are obtained from the PGC, multipotent stem cells from adults, are derived from soma i.e. cells which are no longer capable of making germ cells due to developmental totipotency (2). Because of this it was thought that adult stem cells had only very small differentiation ability. However in contrast to this belief, a plethora of research such as that carried out by Ferrari et al. (referenced in 9) showed that cells derived from adult bone marrow recent could differentiate into muscle cells i.e. differentiate into cells that were from a completely different tissue to the one from which they were isolated (9). In addition to this work, other studies such as that conducted by Frisen et al. (10) found that adult stem cells could even cross from one classic germ layer to another for example as CNS stem cells were shown to behave like ES cells when introduced to blastocytes, hence forming cells of all three primary germ layers. Although it is unlikely that adult stem cells will ever be able to produce the same range of differentiated cells types as pluripotent embryo-derived stem cells, with such findings of adult stem cell differentiation becoming more frequently documented, further research is needed to establish how adult stem cells could be used grow specific cell types to treat some human diseases and potentially relieve the need to use politically sensitive ES cells. Problems associated with developing stem cells to treat diseaseAlthough there has been great insight into the potential use of stem cells in medicine, there still remain a large number of issues which need to be overcome before the theory of cell-based therapies can be made a reality. Primarily stem cells would be needed in large quantities and their differentiation would need to be controlled in order to form a homologous population of cells (2). At present the controlled generation of specific cell types differentiated from stem cells has only been documented in a limited amount of cases and therefore the lack of knowledge in this area represents a real challenge before the full potential of cell-based therapy can be realised. In addition, at present there are only a limited number of human pluripotent stem cell lines and only a few of these have been successfully accredited. It is likely however that of those accredited an even smaller number will actually prove to be useful in clinical practices and many of those may be unavailable due to patents and material transfer agreements (2). The long-term survival of some cultured stem cell lines is also a worry as those cells which survive may do so as a result of gene mutations or chromosome abnormalities (2). One of the major safety issues is the generation of histocompatible tissues for transplantation. With such a huge genetic diversity within the human population the problem of tissue rejection must be discussed. Two solutions to this problem would be to either suppress the immune system of the patient or induce tolerance. However, these solutions are only short-term and the ideal situation would be to make pluripotent cells from embryos that would be compatible with any individual (2). This could be achieved in two ways. Firstly, samples of nuclei taken from the patient's soma could be used to reprogram the cytoplasm of an oocyte, such that when the embryo develops it produces stem cells that are genetically identical to the patient i.e. therapeutic cloning. Secondly, homologous recombination could be carried out in existing stem cells to create a line which is compatible with the patient. Both of these techniques however are difficult to conduct and therefore at present the problem of histocompatibility remains. A second safety issue is the improper differentiation of implanted cells or development of tumours after transplantation. This concern has come from research such as that carried out by Tada et al. in which EG cells were used to study the changed in epigenetic modifications in the germ line by analysing the effects on a somatic nucleus (11). Analysis found that the presence of EG cells caused 'striking changes in methylation of the somatic nucleus, resulting in demethylation of several imprinted and non-imprinted genes' (11). Such modifications were shown to be heritable and affected gene expression (11). This and other findings of a similar nature therefore reinforces the idea that differentiated cells rather than stem cells may be more suited to transplantation. The final issue surrounding the use of stem cells in medicine is the worry that patients may acquire infectious agents from infected embryo-derived pluripotent stem cells or the bovine serum feeder layers required for growth (2). As a result, work has been carried out to develop murine ES cells that do not require feeder cells for growth and human ES cells which require either foetal calf serum or conditioned medium processed by mouse feeder cells (Nichols et al. and Xu et al. respectively as referenced in 2). However in order to eliminate the concern of infectious agents in stem-cell derived transplantation materials, a serum-free medium with purified recombinant growth factors on a defined extracellular matrix will need to be developed. Stem cells and diabetesDiabetes is a condition characterised by abnormally high levels of glucose in the bloodstream which reduces life expectancy by a median of twelve years (Mangel et al. referenced in 13) and affects sixteen million people in the USA (12). The development of this disease can be classified as either type one or type two (1). Type one diabetes, or juvenile-onset diabetes, is an autoimmune condition in which self-pancreatic cells are targeted by immune cells and are destroyed. As a result the islets of the pancreas are unable to secrete insulin in the presence of glucose. Type two diabetes is generally associated with older, overweight individuals and develops when the body cannot use insulin effectively (1). In both cases the result of this disease causes excessive glucose accumulation in the blood which can lead to further complications such as blindness and heart failure. At present the well-established treatment for diabetes, the use of exogenous insulin, is very effective and safe for self-treatment by patients (12). However the side effects of this treatment are not well understood and although the Diabetes Control and Complications Trial Research Group (referenced in 13) have shown that intensive treatment in this way controls hyperglycaemia and reduces long-term complications, there is still room for improvement. The childhood onset of type 1 diabetes predisposes the patient to complicated secondary conditions and therefore this population would certainly benefit from alternative treatment. In addition, intensive insulin therapy treatment doesn't achieve the normal haemoglobin A1c levels which is important in the generation of diabetic organ damage and therefore this problem could be combated using stem cell therapy. As a result, if stem cells were to be used in the treatment of diabetes, they would have to compete against the well-established exogenous insulin treatment and be able to offer significant advantages over this system before the risks associated with transplantation could be justified (13). At present two milestones have been achieved in the development of ES for treating diabetes. Firstly ES cells have been directed to differentiate to form insulin-producing cells in vitro (13). McKay et al. (12) first demonstrated this showing that mouse ES cells self-assembled into structures that both physically and functionally resembled normal pancreatic islets. These engineered cells could even secrete insulin when triggered with glucose in a mechanism similar to that seen in vivo (12). The protocol used to differentiate such cells was based upon an already established method for producing neurons from ES and is outlined in Figure 4. Tests carried out by Csernus et al. (referenced in 13) measured the level of glucose-dependant insulin release in the engineered cells. They found that at the end of stage five (as shown in Figure 4) the cells released insulin in a 'dose-dependant manner with fast kinetics characteristic of primary pancreatic islets in vitro'. However, it was calculated that the cultured cells contained fifty times less insulin than normal islet cells, a figure which is likely to increase with further research. The mechanism of insulin secretion was also investigated by detecting the effects of several agonists (e.g. tolbutamide, a sulfonylurea inhibitor of ATP-dependant K+ channels) and several antagonists (e.g. diazoxide, an activator of ATP-dependant K+ channels). It was found that similarly to in vivo, the agonists all stimulated the secretion of insulin whilst the antagonists inhibited insulin secretion. This therefore suggested that the insulin-positive cells generated from ES cells used normal pancreatic machinery to secrete insulin (12). The second milestone was demonstrated by McKay et al. and showed that engineered insulin-producing cells have transplantation ability (12). When injected into diabetic mice the cells rapidly self-assembled by vascularizing and maintaining a clustered, islet-like organisation. This is shown in Figure 5. The experiments by McKay et al. were unable to demonstrate that the engineered insulin-secreting cells could overcome hyperglycaemia in diabetic mice, however this was not unexpected as it was known through previous investigations that the engineered cells could not produce as much insulin as native pancreatic islets (12). It is clear that engineering ES cells in this way holds great prospects for generating immunocompatible tissues for transplantation in the treatment and potential cure of diabetes. It needs to be remembered however, that although the research into this area is moving forwards the clinical approach to compete with exogenous insulin use is still a long way off. Engineering skin from stem cellsThe process of tissue engineering refers to the production of spare parts to replace damages or lost organs using postnatal stem cells. The skin is a well-understood organ and as such there has been much research into the ability of stem cells to generate skin grafts for patient-specific transplantation. The precursor skin cells are ranked into a hierarchy and under clonogenic conditions form three different types of cell types, holoclones, meroclones and paraclones (15). Of these three lines only holoclones have the ability to self-renewal and therefore are the only product of a true stem cell. Because of this, a small pure population of holoclones could be used to generate an epidermal graft (15). The production of skin autographs in vitro is described in Figure 6. The results of such experimentation have been varied. It was found that if the experimental conditions were not correctly set, the holoclone-generating compartments in the cell cultures became depleted in a manner similar to when part of the epidermal stem cell compartment becomes depleted during ageing. Secondly, graft failure was also noted (15). As a result, this study demonstrated that the experimental design and potential uses of using stem cells in tissue engineering are present, but putting them into practice is a long way off. Currently only a very small number of areas have seen the theory translated into clinical practice. ConclusionThe use of mouse ES cells during the past twenty years has revolutionized our understanding of embryonic, foetal and postnatal development in mammals. The availability of human pluripotent stem cells has now opened up many exciting possibilities for the future. In the next few years, new molecular advances may aid the identification of critical genes involved in stem cell differentiation and renewal, whilst others may develop methods to manipulate stem cells in vivo. Although there have been many success stories of different cell lines that have been developed from pluripotent cells and transplanted into animal models, we may find that there are some cell types which are impossible to develop, whilst others may not be suited for human transplantation. With so many unanswered questions and endless possible uses for stem cells in medicine, it is without doubt that stem cell research will remain highly active for many years to come and in doing so will hopefully provide many answers for diseases such as diabetes and muscular dystrophy which affect thousands of people throughout the world every day.",True
47,"AbstractThis essay investigates the different types of stem cells, their origins and properties and why these characteristics have generated so much hope that stem cells can offer solutions to numerous different diseases and medical conditions. IntroductionStem cells are a unique population of cells found in both adult and foetal tissues which have the ability to renew themselves, to remain undifferentiated and to respond to specific signals and conditions, triggering differentiation. Within adult tissues (such as bone marrow, the brain and muscle), stem cells act to replace cells lost naturally, by disease or through injury. As such, adult stem cells are multipotent and can renew their own population and differentiate to yield the specific cells of their originating tissue. An example of this is shown in Figure 1 where haematopoietic stem cells (HSC) found in the bone marrow may divide symmetrically to regenerate their own population, or asymmetrically forming a further HSC and a daughter cell, the latter of which is committed to one of the two major haematopoietic lineages and hence replenishes the body with all the different blood cells (1). In contrast to adult stem cells, those obtained from embryos are pluripotent and have no fixed developmental process. The fact that pluripotent stem cells can theoretically differentiate into any cell type in the body has generated much research into developing methods to use pluripotent stem cells for cell-based therapy, and hence repairing tissues damaged by disease or injury. The origin of pluripotent stem cellsPluripotent stem cells i.e. distinct cells with complete developmental plasticity can be found in three different sources; embyronal carcinoma (EC) cells, embryonic stem (ES) cells and embryonic germ (EG) cells. The ability of stem cells to differentiate into such a wide variety of different cells was first recognised in testicular cancers. It was noticed that weird tumours formed that contained a wide variety of different tissue types, for example squamous epithelium and muscle, all of which were derived from pluripotent EC cells which themselves were derived from the primordial germ cells (2). The extent of tissue differentiation in this finding was examined using artificially cultured EC cell lines derived from the tumours, in the presence and absence of a mitotically inactive layer of fibroblasts, the feeder layer (Kahan et al. referenced in 2). The second category of pluripotent cells ES cells, are derived from the three to five day old embryo known as the blastocyst which consists of a hallow microscopic ball of around 150 cells (3). This pre-implantation embryo contains around 30 pluripotent cells in its centre known as the inner cell mass (ICM) which, as demonstrated by outgrowth cultures, can differentiate to yield many different specific cell types. During the outgrowth of ES cells, it was noted that some of the cells remained undifferentiated and if sustained on a feeder layer to which they could adhere and obtain nutrients from, they could be expanded to produce a seemingly immortal ES line (2). Finally, EG cells are derived from the primordial germ cells (PGC), the embryonic precursors of gametes and the same cells from which EC cells derive (4). This final group of cells were identified as having stem cell properties after growth upon feeder layers supplemented with serum and growth factors produced clusters of colonies that were morphologically indistinguishable from EC or ES grown under the same conditions (2). Culturing embryonic stem cellsIn recent years the potential uses of stem cells in medicine have gained increased appreciation and as a result have driven the need to understand the factors regulating and controlling stem cells in their natural environment. However, the challenge of producing different types of differentiated cells from pluripotent stem cells first needed to be overcome. Many investigations focussed on changing growth conditions, for example cell density or adding specific growth factors, but as such changes were dictated by trial and error the cellular differentiation achieved in this way was extremely haphazard and varied (2). In contrast, other studies where ES were grown in a development culture demonstrated that the cells began to differentiate into multicellular aggregates called embryoid bodies- clusters of cells that due to the variety of tissues formed represent early embryos (1,2,5). If these embryoid bodies were then transferred to a solid medium they developed into many different cell types for example neural cells and cardiomyocytes which could be separated for further study by methods such as fluorescence activated cells sorting (FACS) (6). However, the generation of embryoid bodies in this way could not be used in a controlled manner to produce specific cell types as it was due to spontaneous differentiation (5). As a result other molecular methods such as those shown in Figure 2 have been used to determine protocols for directed differentiation of ES cells into certain cell types. If the differentiation of ES can be developed reliably as suggested in Figure 2, then stem cells have the potential to become an important tool in treating some of the major diseases such as diabetes and traumatic spinal chord injury. However at the present time this ability is only available to a very limited number of cell types. Stem cells in medicine and developmentThe use of stem cells in medicine is not only relevant for curing diseases such as diabetes and muscular dystrophy, but also in gaining an increased understanding into the development of our own species. By using ES cells to understand why developmental points such as cell formation and proliferation occur and how these process impact upon embryogenesis, we can gain an increased understanding as to why conditions such as teratogens (which cause foetal malformations) are generated, and in addition possibly develop new pre-natal screens to detect such conditions (7). However, the main potential of pluripotent stem cells lies in the development of patient-specific tissues for transplantation and restoration of damaged or diseased tissues. This process of therapeutic cloning would involve taking samples of the patients ES cells and manipulating them to differentiate into the large numbers of cells needed to cure disease or restore tissue function. Figure 3 gives an outline of the range of conditions ES could be used to treat. Many success stories have been documented (8) however a major factor holding back the development of such techniques in many countries (for example the United States of America), are the ethical issues surrounding the cloning of embryos for use in stem cell research. As such, research is also being undertaken to investigate the potential for adult stem cells in medicine. Adult stem cellsIn comparison to embryo-obtained pluripotent stem cells, which are obtained from the PGC, multipotent stem cells from adults, are derived from soma i.e. cells which are no longer capable of making germ cells due to developmental totipotency (2). Because of this it was thought that adult stem cells had only very small differentiation ability. However in contrast to this belief, a plethora of research such as that carried out by Ferrari et al. (referenced in 9) showed that cells derived from adult bone marrow recent could differentiate into muscle cells i.e. differentiate into cells that were from a completely different tissue to the one from which they were isolated (9). In addition to this work, other studies such as that conducted by Frisen et al. (10) found that adult stem cells could even cross from one classic germ layer to another for example as CNS stem cells were shown to behave like ES cells when introduced to blastocytes, hence forming cells of all three primary germ layers. Although it is unlikely that adult stem cells will ever be able to produce the same range of differentiated cells types as pluripotent embryo-derived stem cells, with such findings of adult stem cell differentiation becoming more frequently documented, further research is needed to establish how adult stem cells could be used grow specific cell types to treat some human diseases and potentially relieve the need to use politically sensitive ES cells. Problems associated with developing stem cells to treat diseaseAlthough there has been great insight into the potential use of stem cells in medicine, there still remain a large number of issues which need to be overcome before the theory of cell-based therapies can be made a reality. Primarily stem cells would be needed in large quantities and their differentiation would need to be controlled in order to form a homologous population of cells (2). At present the controlled generation of specific cell types differentiated from stem cells has only been documented in a limited amount of cases and therefore the lack of knowledge in this area represents a real challenge before the full potential of cell-based therapy can be realised. In addition, at present there are only a limited number of human pluripotent stem cell lines and only a few of these have been successfully accredited. It is likely however that of those accredited an even smaller number will actually prove to be useful in clinical practices and many of those may be unavailable due to patents and material transfer agreements (2). The long-term survival of some cultured stem cell lines is also a worry as those cells which survive may do so as a result of gene mutations or chromosome abnormalities (2). One of the major safety issues is the generation of histocompatible tissues for transplantation. With such a huge genetic diversity within the human population the problem of tissue rejection must be discussed. Two solutions to this problem would be to either suppress the immune system of the patient or induce tolerance. However, these solutions are only short-term and the ideal situation would be to make pluripotent cells from embryos that would be compatible with any individual (2). This could be achieved in two ways. Firstly, samples of nuclei taken from the patient's soma could be used to reprogram the cytoplasm of an oocyte, such that when the embryo develops it produces stem cells that are genetically identical to the patient i.e. therapeutic cloning. Secondly, homologous recombination could be carried out in existing stem cells to create a line which is compatible with the patient. Both of these techniques however are difficult to conduct and therefore at present the problem of histocompatibility remains. A second safety issue is the improper differentiation of implanted cells or development of tumours after transplantation. This concern has come from research such as that carried out by Tada et al. in which EG cells were used to study the changed in epigenetic modifications in the germ line by analysing the effects on a somatic nucleus (11). Analysis found that the presence of EG cells caused 'striking changes in methylation of the somatic nucleus, resulting in demethylation of several imprinted and non-imprinted genes' (11). Such modifications were shown to be heritable and affected gene expression (11). This and other findings of a similar nature therefore reinforces the idea that differentiated cells rather than stem cells may be more suited to transplantation. The final issue surrounding the use of stem cells in medicine is the worry that patients may acquire infectious agents from infected embryo-derived pluripotent stem cells or the bovine serum feeder layers required for growth (2). As a result, work has been carried out to develop murine ES cells that do not require feeder cells for growth and human ES cells which require either foetal calf serum or conditioned medium processed by mouse feeder cells (Nichols et al. and Xu et al. respectively as referenced in 2). However in order to eliminate the concern of infectious agents in stem-cell derived transplantation materials, a serum-free medium with purified recombinant growth factors on a defined extracellular matrix will need to be developed. Stem cells and diabetesDiabetes is a condition characterised by abnormally high levels of glucose in the bloodstream which reduces life expectancy by a median of twelve years (Mangel et al. referenced in 13) and affects sixteen million people in the USA (12). The development of this disease can be classified as either type one or type two (1). Type one diabetes, or juvenile-onset diabetes, is an autoimmune condition in which self-pancreatic cells are targeted by immune cells and are destroyed. As a result the islets of the pancreas are unable to secrete insulin in the presence of glucose. Type two diabetes is generally associated with older, overweight individuals and develops when the body cannot use insulin effectively (1). In both cases the result of this disease causes excessive glucose accumulation in the blood which can lead to further complications such as blindness and heart failure. At present the well-established treatment for diabetes, the use of exogenous insulin, is very effective and safe for self-treatment by patients (12). However the side effects of this treatment are not well understood and although the Diabetes Control and Complications Trial Research Group (referenced in 13) have shown that intensive treatment in this way controls hyperglycaemia and reduces long-term complications, there is still room for improvement. The childhood onset of type 1 diabetes predisposes the patient to complicated secondary conditions and therefore this population would certainly benefit from alternative treatment. In addition, intensive insulin therapy treatment doesn't achieve the normal haemoglobin A1c levels which is important in the generation of diabetic organ damage and therefore this problem could be combated using stem cell therapy. As a result, if stem cells were to be used in the treatment of diabetes, they would have to compete against the well-established exogenous insulin treatment and be able to offer significant advantages over this system before the risks associated with transplantation could be justified (13). At present two milestones have been achieved in the development of ES for treating diabetes. Firstly ES cells have been directed to differentiate to form insulin-producing cells in vitro (13). McKay et al. (12) first demonstrated this showing that mouse ES cells self-assembled into structures that both physically and functionally resembled normal pancreatic islets. These engineered cells could even secrete insulin when triggered with glucose in a mechanism similar to that seen in vivo (12). The protocol used to differentiate such cells was based upon an already established method for producing neurons from ES and is outlined in Figure 4. Tests carried out by Csernus et al. (referenced in 13) measured the level of glucose-dependant insulin release in the engineered cells. They found that at the end of stage five (as shown in Figure 4) the cells released insulin in a 'dose-dependant manner with fast kinetics characteristic of primary pancreatic islets in vitro'. However, it was calculated that the cultured cells contained fifty times less insulin than normal islet cells, a figure which is likely to increase with further research. The mechanism of insulin secretion was also investigated by detecting the effects of several agonists (e.g. tolbutamide, a sulfonylurea inhibitor of ATP-dependant K+ channels) and several antagonists (e.g. diazoxide, an activator of ATP-dependant K+ channels). It was found that similarly to in vivo, the agonists all stimulated the secretion of insulin whilst the antagonists inhibited insulin secretion. This therefore suggested that the insulin-positive cells generated from ES cells used normal pancreatic machinery to secrete insulin (12). The second milestone was demonstrated by McKay et al. and showed that engineered insulin-producing cells have transplantation ability (12). When injected into diabetic mice the cells rapidly self-assembled by vascularizing and maintaining a clustered, islet-like organisation. This is shown in Figure 5. The experiments by McKay et al. were unable to demonstrate that the engineered insulin-secreting cells could overcome hyperglycaemia in diabetic mice, however this was not unexpected as it was known through previous investigations that the engineered cells could not produce as much insulin as native pancreatic islets (12). It is clear that engineering ES cells in this way holds great prospects for generating immunocompatible tissues for transplantation in the treatment and potential cure of diabetes. It needs to be remembered however, that although the research into this area is moving forwards the clinical approach to compete with exogenous insulin use is still a long way off. Engineering skin from stem cellsThe process of tissue engineering refers to the production of spare parts to replace damages or lost organs using postnatal stem cells. The skin is a well-understood organ and as such there has been much research into the ability of stem cells to generate skin grafts for patient-specific transplantation. The precursor skin cells are ranked into a hierarchy and under clonogenic conditions form three different types of cell types, holoclones, meroclones and paraclones (15). Of these three lines only holoclones have the ability to self-renewal and therefore are the only product of a true stem cell. Because of this, a small pure population of holoclones could be used to generate an epidermal graft (15). The production of skin autographs in vitro is described in Figure 6. The results of such experimentation have been varied. It was found that if the experimental conditions were not correctly set, the holoclone-generating compartments in the cell cultures became depleted in a manner similar to when part of the epidermal stem cell compartment becomes depleted during ageing. Secondly, graft failure was also noted (15). As a result, this study demonstrated that the experimental design and potential uses of using stem cells in tissue engineering are present, but putting them into practice is a long way off. Currently only a very small number of areas have seen the theory translated into clinical practice. ConclusionThe use of mouse ES cells during the past twenty years has revolutionized our understanding of embryonic, foetal and postnatal development in mammals. The availability of human pluripotent stem cells has now opened up many exciting possibilities for the future. In the next few years, new molecular advances may aid the identification of critical genes involved in stem cell differentiation and renewal, whilst others may develop methods to manipulate stem cells in vivo. Although there have been many success stories of different cell lines that have been developed from pluripotent cells and transplanted into animal models, we may find that there are some cell types which are impossible to develop, whilst others may not be suited for human transplantation. With so many unanswered questions and endless possible uses for stem cells in medicine, it is without doubt that stem cell research will remain highly active for many years to come and in doing so will hopefully provide many answers for diseases such as diabetes and muscular dystrophy which affect thousands of people throughout the world every day.","Agrobacterium-mediated transformation of Arabidopsis by floral dip, and selection of transformantsIntroductionTransformation as a process of genetic exchange is widely used for producing transgenetic plants. Like many bacteria Agrobacterium possess DNA in the form of a plasmid in addition to its main genome. Under normal circumstances this plasmid contains the vir genes which causes part of the plasmid to self-replicate and insert itself into the genome of the plant host cells. This then alters hormone production and causes the development of galls or tumours in the plants. The bacterial T-DNA i.e. the portion of the plasmid that becomes integrated in the plant genome can be modified such that its incorporation does not cause tumour formation, but instead confers foreign genes (such as those for antibiotic resistance) upon the plants phenotype. In our investigation we aimed to transform Arabidopsis plants with a genetic construct that allowed selection of transformed plants due to Kanamycin resistance and in addition allowed the uptake of the plasmid DNA to be visualised using the fluorescent reporter gene green fluorescent protein (GFP). As the uptake and incorporation of the plasmid DNA is a rare event we aimed to screen the progeny of plants subjected to the floral dip and select for stable transformants and hence calculate a value for transformation frequency. MethodsThe method for this investigation is described on pages five and six of the laboratory manual. There were no alterations to this method. ResultsFor the first part of our investigation i.e. the identification of plants transformed by floral dipping we used a UV lamp to determine that the control plant i.e. the plants that had not been transformed showed only red fluorescence characteristic of chlorophyll and in addition yellow spots in dying tissues where no chlorophyll was present. In contrast the transformed plants showed green fluorescence in parts of the stems and leaves, and in some cases the parts of the flowers were also green. DiscussionThe fact that only the transformed plants fluoresced green enabled us to determine that the activity of GFP was as a result of transformation. In addition, by using the viral promoter 35S we were able to ensure that expression of the reporter gene could have occurred in any cell in the plant and therefore again the expression of GFP was dependant only upon successful transformation. In most cases it is probable that the uptake of the plasmid was only transient i.e. the GFP reporter gene did not incorporate into the host cells genome and therefore would be lost after cell division. However as we detected some green fluorescence in parts of the flowers there was the chance that the plants germ cells had been infected and therefore any progeny may carry GFP in their genomes. GUS staining of Arabidopsis seedlings transformed with a promoter trap constructIntroductionThe use of promoter traps in genetics has been greatly exploited in recent years. This technique involves transformation and random integration of reporter gene sequences that lack cis-acting sequences required for expression. Once inserted in the genome if the reporter gene has landed downstream of a promoter for another gene, this promoter can act upon the reporter gene to allow its expression. If during transposition the reporter gene lands within another gene hence disrupting and replacing its expression, then patterns of reporter gene expression can be used to identify which gene has been disrupted, where this gene is located within the genome and more importantly where this gene is expressed and how it affects the plants phenotype. In this investigation we aimed to use the reporter gene Gus in a promoter trap to identify and characterise mutant plant phenotypes generated by transformation. MethodThe method for this investigation is described on pages seven and eight of the laboratory handbook, we made no alterations to this method. ResultsThe three positive controls we used for this investigation were LAX2:Gus GL2:Gus and CHS:Gus in which we expected to find blue staining in the auxin transport tissues, trichome cells in the root and in all tissues respectively. As the seedlings we used for the investigation were so small, when we looked at our LAX2:Gus plants there was no staining as no germination had occurred. In the case of the GL2:Gus plants only a small part of the seed coat had stained blue, and for the CHS:Gus plants there was no obvious root staining however the hypocotyls and cotyledons had stained deep blue. Similarly to the LAX2:Gus control plants, no germination had occurred in any of the plants in our three promoter trap lines (N4567, N4568 and N4554) and therefore no staining was observed. DiscussionThe fact that the seedlings used in this investigation were so premature was a major limiting factor and therefore we would need to repeat this investigation using more developed plants before any staining of our promoter traps could be observed. The three positive controls (with the exception of LAX2:gus) although premature in their development showed the expected patterns of Gus expression. The reporter gene Gus had been inserted downstream of each of the specified promoters and therefore had been expressed in the same tissues as the native gene. It was because of this that Gus staining was observed only in the hypocotyls and cotyledons in our CHS:Gus plants, as CHS drives expression of chalcone synthase, an enzyme involved in protecting the plant from light damage, and this protein is expressed only in the tissues of the plant which are in contact with light (i.e. the hypocotyls and cotyledons). If we had used more developed LAX2:Gus controls then we would have expected to find staining in the auxin transport tissues as the presence of Gus downstream of the LAX2 promoter would have allowed expression of Gus in those tissues where the native gene is normally expressed. Our final control promoter GL2 drives expression of factors needed for trichome morphogenesis such as cell expansion and maturation of the cell. As the plants we studied were so small and lacked any roots, our results showed that only part of the seed coat was stained, most probably the cells which would later develop into the roots, as this is where trichome cells are found. If any of our promoter trap plants had been developed enough to observe staining, then we would have been able to locate those tissues where Gus was being expressed. As a result we would then have been able to identify possible genes disrupted by the transposition and insertion of the reporter gene, and in addition we could have analysed the patterns of gus expression to determine the expression patterns of the disrupted gene. As we did not obtain any data for this investigation we would need to repeat the method set out on pages seven and eight using larger and more developed seedlings. Luciferase as a dynamic reporter for gene expression in  ArabidopsisIntroductionThe protein luciferase is an enzyme which produces light in the presence of luciferin, ATP and oxygen. In our investigation transgenetic plants were produced which contained the reporter gene luciferin under the control of four different promoters, chs from which chalcone synthase is transcribed, the chlorophyll a/b binding protein promoter cab, phyA from which phytochrome A is transcribed and 35S, a viral promoter. As luciferase activity can be measured with great accuracy (due to the use of highly sensitive light-detectors), we aimed to quantify and compare the expression of luciferase in the roots and leaves of Arabidopsis when driven by the four different promoters. One of the advantages of using luciferase as a reporter gene is that it is unstable and hence as the levels of active protein change over time, it can be used to study the dynamic changes of induction of transcription from a specific promoter. As previously mentioned the chlorophyll a/b binding protein is transcribed from the promoter cab, therefore using luciferase as a reporter gene transcribed from this promoter, we aimed to study the changing expression of the chlorophyll a/b binding protein over time after exposure to red light for five minutes. MethodThe method for this investigation is described on pages nine and ten of the laboratory manual. There were no changes made to this method. ResultsThe first part of our investigation was to quantify the expression levels of luciferase driven from four different promoters in the roots and leaves of Arabidopsis plants that were either grown under constant light or adapted to the darkness for 48 hours. This data is shown in Figure 1. The data in Figure 1 shows that the different promoters allowed different patterns of expression under the different conditions. When luciferase was transcribed from the CHS promoter, there was little expression in the roots or the leaves of plants grown in the light and no expression at all when the plants were grown in the dark. As shown in Figure 1, we only studied PHYA plants that had been grown in the dark and found that the level of luciferase activity was the highest of all the different constructs in the leaves of these plants. When transcribed from the promoter CAB the data in Figure 1 shows that there was no expression of luciferase in the roots of either the light or dark plants, however expression in the leaves occurred in both the light and dark grown plants with a greater pixel intensity generated in the light grown plants. Our final promoter 35S allowed similar expression of luciferase in the light and the dark, however the pixel intensity from the leaves was slightly higher than from the roots. The second part of our investigation was to use the dynamic change in luciferase expression to study the response of the CAB promoter to light. We firstly studied the changing patterns of expression in wild type dark and light grown plants. This data is shown in Figure 2. As shown in Figure 2, the plants grown in the dark did not show any change in luciferase expression (and hence luciferase activity) when there was no light stimulus applied. In contrast the plants which had been given a light stimulus at time zero showed a sharp increase in photon emission followed by a gradual decline back to the start level after sixteen hours. Before we could screen for plants that were potentially mutant in their ability to respond to a light stimulus we had to investigate the range of responses in the wild type light plant. This data is shown in Figure 3. The data presented in Figure 3 show that all the wild type plants were able to respond to light in a similar manner i.e. they all showed an increase in luciferase activity after the light stimulus at time zero, after which time the level of luciferase activity gradually declined over a period of between five and sixteen hours to the same level of activity as at time minus six hours. There was however variation between the twenty-four plants screened for example plant C6 produced the highest response but also had the highest starting photon emission. Figures 4 and 5 show the mutagenised plants that we selected as being potentially defective in their ability to sense and respond to the light stimulus. There are three potential light mutants shown in Figure 4, plants E6, G11 and F11. In comparison to the average wild type plant, plant E6 emitted a much greater number of photons at the start of the experiment. This value then declined steeply before rising again at time zero. Between five and sixteen hours the photons emitted gradually declined, such that by the end of the experiment their levels were approximately the same as at time zero. Plant G11 showed the same pattern in its response to light as the average wild type, however there was a much shorter decline in photon emission (between about three and five hours) after which time emission gradually increased again. Finally plant F11. This plant also showed a similar pattern in response to the light as did the wild type plant however similarly to plant G11 the decline in photon emission was shorter than that of the wild type (between about one and eleven hours), and after about eleven hours emission increased once again. As shown in Figure 5, the amount of photons emitted by plant H12 was much higher than those emitted by the wild type plant. In addition whereas the emissions by the wild type plant only occurred after the light stimulus at time zero and then gradually declined back to the initial level, the amount of photons emitted by plant H12 gradually increased from the start of the investigation before declining between four and ten hours and then gradually increasing again. DiscussionAs shown in Figure 1, the expression patterns of the reporter gene luciferase varied dependent upon the promoter from which it was transcribed. The viral promoter 35S was selected to allow constitutive expression of the reporter gene in all areas of the plant. However as viruses are obligate intracellular parasites and use the host machinery for protein synthesis, transcription from this promoter would have required items such as transcription factors which may not have been ubiquitous throughout the plant and in both light and dark situations. This may therefore account for why luciferase expression was higher in the leaves than the roots. Our second promoter CAB is normally drives expression of the chlorophyll a/b binding protein, a component of photosystem II in the chloroplasts. As such, expression of this protein only occurs in the leaves of Arabidopsis i.e. where the light absorbing and photosynthetic apparatus of the plants are located and hence where this protein is needed. This is shown in our findings as when luciferase was transcribed from this promoter there was no expression at all in the roots. Our data also showed that there was more expression of luciferase in the leaves of plants grown in the light than those grown in the dark. This is because the chlorophyll binding protein is only needed when the plant is in light environments and therefore the plant has sensory mechanisms to ensure that when this is the case, greater expression of the binding protein or in our case the reporter gene luciferase occurs. There was however still a significant amount of expression from this promoter of the plants that were dark-adapted. Although our investigation didn't' allow us to determine why this occurred; however it may have been a result of the normal circadian rhythm of the plant. This process controls the plants 24 hour clock and so even when the plant was adapted to a dark environment, if the circadian clock was telling it that it was daytime and that there was the chance of a light environment developing then expression of this protein would still occur, however at reduced levels. This is shown in our findings in Figure 1. The third promoter we studied was phyA from which phytochrome A, a red light photoreceptor is transcribed in wild type plants. This protein is part of the plants machinery to detect and integrate light signals into pathways that initiate and regulate plant growth and development. As a result, phytochrome A is predominantly found in the leaves, as these are the parts of the plant which will be able to detect the light if it is present. This is shown in our results as we found that when luciferase was transcribed from the phyA promoter this expression occurred at much greater levels in the leaves than in the roots. In addition, although we did not study the expression of luciferase when transcribed from phyA light-grown plants, we would expect to find that the levels of luciferase would have been much lower than in the dark-grown plant. This would have been because once in light, the expression of the natural phytochrome A protein is down regulated at both the transcriptional and post-translational levels, as there is no longer a need to detect whether or not light is present in the growth environment. The final promoter we studied was chs. The data we obtained (as shown in Figure 1) showed that there was no expression of luciferase from chs in plants that had been grown in the dark, and in those grown in the light, there was higher expression in the leaves than in the roots. This is because the protein product normally transcribed from this promoter (chalcone synthase) is involved in the production of photoprotective pigments i.e. pigments that protect the plant from light damage. As a result, the plant only produces chalcone synthase in conditions where light damage may occur i.e. in light grown conditions and in addition modifies the expression of this enzyme such that tissues with a lesser chance of being exposed to light e.g. the roots express lower levels of chalcone synthase. As our reporter gene was being expressed in place of chalcone synthase, the data we obtained for this construct can be understood in this way. The first part of our study surrounding the dynamic changes of luciferase expression after a given light stimulus Figure 2 shows that, as there was no change in light emission in the dark plant in comparison to the light plant, the changes luciferase activity occurred as a direct consequence of the light stimulus. The data in Figure 3 allowed us to determine the range over which the wild type plants responded to the light stimulus and as a result helped us to identify four potential CAB: luc mutants as their response differed from that of the wild type. As described in the results section we identified four mutagenised plants which showed different patterns (from the wild type plants) in their response to a given light stimulus. It may be the case that these plants are mutant in their ability to respond to light and therefore they should be kept for further analysis. Plant H12 didn't show any light response and therefore may be a possible det mutant. There was also evidence that expression of luciferase from the CAB promoter in plant H12 was subject to regulation by circadian rhythms as the photons emitted declined and then increased again in the absence of a light stimulus. Plants G11 and F11 showed very similar patterns of luciferase expression, however the amount expressed from F11 was slightly higher. Similarly to plant H12, both G11 and F11 showed evidence that luciferase expression was being modulated by the plants normal circadian rhythms as although both plants responded to the light stimulus, the expression of luciferase increased again in the absence of a second light stimulus. Our final possible mutant, plant E6 showed evidence of a light response as the expression of luciferase increased after the light stimulus however it also showed evidence of circadian regulation as expression at the start of the investigation i.e. before the light stimulus was high. ConclusionReporter genes are genes with easily assayable products for example lacZ which encodes for β-galactosidase. These exogenous genes can be inserted into different genetic constructs to allow the activity of different genetic factors such as promoters, repressors etc. to be studied. The expression of the gene product is measured to give an accurate representation of the activity of the factors influencing expression. In our investigation we used three different reporter genes, green fluorescent protein (GFP), β-glucoronidase (GUS) and luciferase (luc). There are advantages and disadvantages for using each reporter gene and it is essential that each factor is taken into consideration when planning an experiment and deciding which gene to use. Firstly in comparison to many reporter genes, using GFP is relatively cheap. This is because there is no substrate needed and no specialised equipment is needed to observe the expression of the gene (only a UV lamp is needed). In addition the expression of GFP is easily observed under UV light and the visualisation of GFP is vivo allows changes in expression to be studied over time. On the other hand, as expression patterns are observed using UV light, specialist photographic equipment is needed to record these findings. In addition the red fluorescence of chlorophyll may interfere with the expression patterns of GFP. Secondly unlike GFP, when using GUS patterns of expression can be observed using the naked eye and standard photographic equipment can be used to record this finding. On the down side however, the substrate needed to observe GUS expression is very expensive, the method needed to carry out this procedure is time consuming and there is no scope to analyse expression in vivo as the plants need to be killed before expression patterns can be observed. Finally in contrast to both GUS and GFP using luciferase as a reporter gene requires using both an expensive substrate and expensive equipment. The use of this reporter gene does however have more positive aspects than the other two as it can be used to give a quantitative measure of gene expression and can also be carried out in vivo so any mutant plants can be used for further research. In addition the equipment used to measure luciferase activity is very sensitive and therefore even the expression from very weak promoters can be detected. Finally and most importantly, the enzyme luciferase is unstable and therefore it loses activity with a half-life of around thirty minutes. This therefore makes luciferase useful for studying dynamic changes in gene expression over time.",False
48,"As an introduction to the essay question posed a very brief and simplified summary of Habermas's main ideas and theories seems appropriate. Habermas argued that from the late nineteenth century science, technology and industry became interdependent forming a new kind of capitalism. Habermas viewed this interdependence as meaning capitalist growth had no limits because science and technology were the new leading productive forces. This new form of capitalism used a different kind of production and allowed the '...biblical curse of necessary labour,' to be broken 'technologically'. (Habermas, 1970; 53). Originally capitalism had legitimated itself through economic notions of just exchange in the market, however due to the changes within capitalism, the exchange process itself now operated under political control. This new capitalism also required direct legitimation, this time using culture. Habermas argued that this led to a 'depoliticisation' of society, which was legitimated 'by having technology and science also take on the role of an ideology'. (Habermas, 1972; 104). A 'Technocratic consciousness' developed which affected everyone; not simply those repressed by the dominant. Habermas argued that there was no longer a lower class because everyone was now relatively rich. He argued there was no longer an exploited class, since technology was not dependent on labour; people were no longer forced to work, nor were they forcefully repressed in to their social positions. It is this universal, cultural phenomenon which Habermas's work focuses on and which replaces the Marxist questions of class conflict based on economic relations. In this essay I will argue that Habermas retains many aspects of Marxist theory in his work, in terms of some of the concepts he uses as well as the particular area of society he chooses to study. However, his work also includes many changes to orthodox Marxian work including his view on modern capitalist development and his placing of importance on the superstructure rather than the economic base. Perhaps the most apt descriptions of Habermas's work in relation Marxian thought are as a 'positive critique' (Heller in Thompson and Held, 1982; 23) or a 'positive revision' (Layder, 1994; 187). I will argue that Habermas's ideal speech situation as a means of emancipation, though not without some merit, seems deeply flawed as the general, universal, widely used and practical system it was intended. 'While severely critical of Marxism, Habermas does not reject it all. He uses aspects of it to construct his own synthetic model, drawing on many other sources in the process'. (Layder, 1994; 187). The type of theory which Habermas chooses to study is itself of Marxian trend, 'Habermas remains 'committed to traditional... Marxist ideas...he deals in systematic theory - the grand and 'totalising' theory that was rejected by Foucault.' (Layder, 1994; 188). Though both theorists have been criticised for their 'utopian' tendencies, Habermas seems to have believed that this type of theory was necessary to 'the immensely significant general understanding of social trends and developmental processes in critical theory. '(Layder, 1994; 187). Like Marx, Habermas was looking to identify the 'big problems' in society, suggesting that it was specific ways in which society functioned which was behind its troubles. In this way both Marx and Habermas belong to that school of sociologists who try to 'form epochal diagnosis', identifying causes and characteristics of the 'current crises': exploitation, bureaucratisation, secularisation, anomie, etc. (Chernilo, Lecture, 2004). Also like Marx, Habermas maintains the notion of praxis '...that is, a blending of theory and action, or the use of theory to stimulate action and vice versa'. (J. Turner, 1991; 255). Both theorists used simple presentation to explain society and what was wrong with it, but both also'...wanted theory to expose oppression and to propose less constrictive options.' (Turner, 1991; 255). Marx believed society would be 'cured' after the revolution of the working classes, Habermas too offered diagnosis for the crisis he saw in society; first identifying the problem as technocratic consciousness, before offering the solution of ideal speech situation. Habermas's work echoes the emancipatory theme within Marxist thought; in both there exists the respective victims who need to be freed. Throughout both their theories there is the idea of repression (though in different forms) of society, which must be lifted (though by different means) in order for society to operate in equality. In both theories the definite need to change society is an inherent part of the diagnosis made for improvement. Habermas's focus on the relation of economic workings to the population, and the way this economic system is legitimated, mirrors the area of sociology which Marx chose to study. In addition, two of Marx's main concepts are maintained in Habermas; that of class conflict, although the classes Habermas defines are very different to those Marx identified; and that of ideology, although Marx's idea of the ruling class using classical economics to oppress a working class is drastically altered under Habermas. As I mentioned in my introduction, one description of Habermas's work in relation to Marx's is as a positive critique, and indeed Habermas believed that '... Marx's analysis needed drastic revision'. (Turner, 1991; 255). Habermas argued that although Marx's analysis of economics and the class structure was perfectly applicable to the era he studied, it no longer held any accuracy for a modern society. Not only did Habermas reject the idea of a working class forced in to labour by a deceitful ruling class, but he rejected the idea that change could be brought about by economics. Habermas argued that the emergent properties of capitalism were cultural not material, and therefore argued that cultural change was necessary for more general change to occur. He also argued that Marx's predictions for the advance and eventual fall of capitalism were inaccurate; instead explaining that a new and constantly developing capitalism had emerged through the interdependence of technology and science. Thus the economic base was no longer more important than the superstructure (as had been the case with market capitalism). Habermas rejected many of the more 'romantic' features of Marx; his naïve optimism about the inevitability of class revolt, and of class consciousness. (Thompson and Held, 1982; 22). The ideal speech situation was Habermas's suggestion for emancipation from the technocratic consciousness repressing society, freezing relations of domination and perpetuating and legitimating them through positivism. Technocratic consciousness was a result of the technological / scientific capitalism of modern society. It was an ideology which mirrored the values of this capitalism; positivism, empiricism etc, it had one main interest; prediction. (Outhwaite, 1994; 13).This ideology, though repressive because it ought to only be applied to cells and microbes not subjective humans, was not delusional as was the ideology of Marx's time. It kept the masses under control, whilst only treating surface issues rather than the underlying problems; i.e. the depressed housewife feels lonely and unfulfilled with only a baby for company. However instead of identifying the problem as one of her limited and prescribed position in society falling short of her capabilities and desires, she identifies it as a medical / scientific problem, and treats it with Prozac. The problem which the ideal speech situation was set out to solve was this cycle of technocratic consciousness, never getting to the root of a problem, because you can not identify the problem other than in the medical terms such an ideology allows. (Habermas, 1971; 86). The ideal speech situation devised by Habermas was intended to make people realise firstly that they were infact following an ideology, then to help them think outside it in a less positivistic way. He suggested that this could only happen in a doctor / patient type of situation, where outside influences are shut out and the patient is the ultimate arbiter of how accurate the analyst is. This situation should be transferred over in to social situations, thus providing everyday chances for everyday people to look beyond technocratic consciousness. (Outhwaite, 1994; 54). This suggestion was met with heavy criticism; it was argued that not everyone (for example our housewife with the small child) could achieve such a situation. Likewise it was argued that even if this kind of situation were to be achieved, the patient may believe to know their underlying problems, but infact be deluded. Finally, it was put forward that this theory, even if it worked, was dubious as an emancipatory salvation. (Outhwaite, 1994; 55). In reaction to such criticism Habermas revised his suggestion; arguing that if there was a group of people present they could all arbitrate each other and the eventual result would be the lifting of technocratic consciousness and the fall of this technological capitalism. However, the circular potentials of this were not lost on Habermas's critics; not only was this a harder situation to achieve, but it could simply result in deluded people deluding each other further. Only in an already emancipated society could critical reflection produce truth. (Layder, 1994; 189). Yet Habermas still maintained that the ideal speech situation was necessary to arrive at the truth. Habermas explained that this ideal speech situation would begin with intellectuals, who were emancipated through education, providing the model towards which education might lead society as a whole. Whether this situation would lead to emancipation, whether intellectuals are infact emancipated (those controlling education usually control dominant ideologies), and whether this transformation of the micro to the macro, would in reality be achievable, has been the subject of much heated debate. There are three key arguments posed by sociologists about the ideal speech situation and how convincing it is as a means of emancipation, other criticism of Habermas's theory in general (such as his theory of modern capitalism) must be left for another essay. The first, accepts that if it were possible for large numbers of people to experience such a situation then it might well prove, at least in part, to play an important emancipatory role. However, it seems doubtful that such a situation would ever be within the grasp of most ordinary people. Both the actual practicality of discussing matters in a group, and discussing them in a situation free from the culture and ideology the group has been socialised in to, seems to be questioned. (A. Heller in Thompson and Held, 1982; 32). The second key criticism is posed by Ernst Tugendhat; 'to what extent can one sustain the claim that...truth and validity can be found in language and discourse.' (Tugendhat in Rasmussen, 1991; 74). Habermas's idea that emancipation from oppression can come from education and simple discussion in the presence of others seems a little optimistic. As Tugendhat points out, there is a certain tension between Habermas's 'theory of modernity and the philosophy of language' which throws doubt over the ability of his ideal speech situation to result in liberation. (Tugendhat cited in Rasmussen, 1991; 75). The final key criticism is that Habermas's theory is simply too Utopianist in style to legitimate it as a convincing means of emancipation. Both his method of reaching freedom, and his reason for wanting it are criticised as too idealistic and too big a narrative. Sociologists such as Paul Connerton argue that although as part of a suggestion for change the ideal speech situation has some merit; as the sole 'answer' to what Habermas identifies as the 'big problem' of society it is unrealistic. (Connerton, 1970; 353). Indeed this is a criticism levelled against, many of the social theorists, for though Habermas saw general theory of society as a necessity, others argue that it is unhelpful in terms of diagnosis for problems. To conclude, I initially looked at the similarities and dissimilarities between Marx and Habermas, and although many likenesses are apparent in Habermas's work, such as his use of key concepts and emancipatory theme, I would not go as far to label him a Marxist. Instead Habermas resides firmly in the Marxist revisionist camp, 'abandoning or relegating' certain Marxist principles. (Outhwaite, 1994; 4). As a means of emancipation the ideal speech situation does appear to have some merit such as the idea of using culture to force through change. And if considered as a (very) long term solution this suggestion does seem more practical than I originally thought. However, as a realistic emancipation from the kind of all encompassing, universal ideology legitimating not only the continuation of technological capitalism but also the depoliticisation of the population, I think it falls short. As Tugendhat points out; the idea that freedom is simply a matter of ideal speech in the right situation is problematic in itself, even despite the further problems such as its circular tendencies of reproducing delusions already held, and the impracticalities of all people having access to the right kind of situation. To finally answer the question, it seems that Habermas, although still within the broad Marxian trend is more a Marxist revisionist than a strict follower of Marx. Habermas's ideal speech situation does seem plausible in some cases; it seems as though it could work to emancipate some people. However when posed as a means of freedom from the kind of ideology Habermas identifies, it seems less convincing.","By 1930 Stalin had not only managed to gain extensive personal control and eliminated all his opponents but had also managed to consolidate this power. Accountable to very few Stalin was free to pursue any policy largely unchecked. He had support from the party rank and file as well as from the central committee and the Politburo. How Stalin, the least obvious of Lenin's potential successors, came to be in this position is a much contested issue. With his death Lenin left a vacuum within the party leadership creating economic, social and political uncertainty. This in turn led to a left / right divide within the Politburo over the next steps the Bolsheviks should take, with Trotsky on the left arguing for rapid industrialisation. Stalin maintained a generally moderate position before siding with Kamenev and Zinoviev forming a powerful triumvate against the man they saw as their biggest rival. After Trotsky's eventual expulsion from the party, and then the country, came the political execution of Kamenev and Zinoviev followed by their actual execution. Stalin's other main rivals faded away either in to political destruction, suicide, execution or expulsion. A. Nove and C. Read, Stalin: Terror and Transformations, video. Nove and Read, Stalin: Terror and Transformations, video. R. Hingley, Stalin; Man and Legend, (London, 1974), ch7, p.3. I find it hard to justify putting Stalin's rise to power down to any one single factor. It does not seem possible that Stalin reached his position purely due to his own manipulation or political cunning. Nor does it seem realistic that he was put in power purely because he was the politician that most reflected the party rank and file wishes. A number of reasons seems the most probable explanation; I will argue that the most important are as follows. Stalin proved a useful asset to Lenin and as a result Lenin often supported Stalin's appointments and promotions. The positions Stalin held within the party were of vital importance in helping him gain power, his personality and his political prowess were also very important in this respect. The mistakes of Stalin's rivals during the 1920's were to prove fatal and helped him in his ascendancy to power. Many of the opportunities which led to Stalin's rise to power were actually accumulated whilst Lenin was still alive. Infact on many occasions it was Lenin that endorsed Stalin's promotion to important positions within the party. Stalin fitted the proletariat ideal lacking in so many of the profiles of the Bolshevik leaders including Lenin's own, Stalin was singled out by Lenin as having potential. His activities to raise funds for the party included bank robbing, and he was one of those 'hard core' Bolsheviks who had remained in Russia '...underground during the winter 1905 -6 repression and persecution of revolutionaries by the Tsar."" This mixture of loyalty and practicality was one Lenin could not resist, he began to rely on Stalin who proved a useful asset. 'He was an unquestioningly loyal person who actually got things done... Stalin was Lenin's political fixer...packing the congress with his supporters'. Lenin supported and approved many of the positions Stalin was appointed to within the party. Although Trotsky claimed that after Stalin's appointment as party General Secretary Lenin expressed concern; '...this cook will serve only peppery dishes', he did not seem sufficiently concerned to raise public objection until very near his death when it was too late. Indeed, prior to his appointment as General Secretary Stalin already held positions of great importance; immediately after the civil war he was Commissar of Nationalities, Commissar of the Workers and Peasants Inspectorate and a member of the Politburo. 'Several of Stalin's crucial promotions - cooption to the central committee and appointment as Pravda editor in 1912 and as General Secretary in 1922- were brought about by Lenin's influence.' L. Deutscher, Stalin; A Political Biography, (Oxford, 1949), p228. Walter Duranty, Stalin and Co. The Politburo; The Men who Ran Russia, (London, 1949), p.10. Christopher Read, The Making and Breaking of the Soviet System, (Hampshire, 2001), p.84. Read, The Making and Breaking of the Soviet System, (Hampshire, 2001), p.85. Deutscher, Stalin; A Political Biography, (Oxford, 1949), p.232. Deutscher, Stalin; A Political Biography, (Oxford, 1949), p.228. Read, The Making and Breaking of the Soviet System, (Hampshire, 2001), p.84. Stalin has been described as 'the spider in the web' in terms of his positioning within both the state and the party apparatus. He had influence within all the main offices, and the posts he held gave him direct authority or at the very least powers of persuasion over a very useful range of contacts. At the time of Lenin's death when the struggle for power was fought out amongst the Politburo members, Stalin had power within the Politburo, (the highest decision making body), the Orgburo, (the administrative bureau of the central committee), the Robkrin and the Sovarkom, as well as in the regional and local party cells and the regional and local soviets. The very nature of the centralised organisation of the party and the state worked to Stalin's advantage due to the links between all the offices. Though all his positions gave him important influence, it was his position as General Secretary which allowed him to build up the kind of power base which defeated his rivals. In this post Stalin was responsible for Politburo administration including the agenda for discussion, Stalin was the link between the Politburo and the Central Committee, he also controlled the '...appointment and promotion of individuals to key posts throughout the country."" As Edward Acton points out, this meant that 'the careers of party officials were becoming increasingly dependent upon their loyalty to the Secretariat."" This in turn gave the '...central administrative organs enormous influence over the make-up of party Congresses and thus of elections to the Central Committee and the Politburo itself'. Acton maintains that this 'movement of power from the Politburo to the Secretariat', was initially '...masked under Lenin's authority over the Secretariat."" However Deutscher claims that on Lenin's return from the recovery of his first stroke he sensed the increasing power of the bureaucracy with Stalin at its head, and immediately attempted (though unsuccessfully) to rectify the situation. The position of General Secretary had been largely overlooked as a source of power due to its administrative nature; '...the bright sparks of the Politburo felt themselves above such roles."" However Stalin was willing to take on this shunted position, and was good at it; he worked hard, he was reliable, made no fuss and needed little assistance from the other Politburo members. He converted secondary administrative power in to political power, using the powerful self perpetuating nature of the vested interests within the bureaucracy. Christopher Read, Lecture, University of Warwick, January 2004. Edward Acton, Russia, The Tsarist and Soviet Legacy, (Essex, 1995), p.197. Acton, Russia, The Tsarist and Soviet Legacy, (Essex, 1995), p.196. Acton, Russia, The Tsarist and Soviet Legacy, (Essex, 1995), p.197. Acton, Russia, The Tsarist and Soviet Legacy, (Essex, 1995), p.197. Acton, Russia, The Tsarist and Soviet Legacy, (Essex, 1995), p.198. Deutscher, Stalin; A Political Biography, (Oxford, 1949), p.247. Robert Tucker, Stalin as Revolutionary 1879 - 1929; A Study In History and Personality, (London, 1974), p.322. Deutscher, Stalin; A Political Biography, (Oxford, 1949), p.233. Idea of secondary administrative power; Read, The Making and Breaking of the Soviet System, (Hampshire, 2001), p.52. Idea of perpetuating vested interests; Hingley, Stalin; Man and Legend, (London, 1974), ch7, p5. Stalin's personality and his political manoeuvring and prowess have been interpreted in a number of ways. As evil, pre-planned, ruthless, manipulation, but also as a highly skilful reaction to economic, social and political situations. Which ever interpretation bears more truth, that Stalin's politics proved enormously valuable to him during the 1920's is undoubtable. The moderate position Stalin seemed to take during Politburo meetings both whilst Lenin was alive and after his death allowed him several advantages. It meant that he 'always seemed to follow others, never to direct them', making Trotsky, who was very active during meetings, seem the biggest threat to power. 'No one could have behaved less like a tyrant or dictator than did Stalin at Politburo meetings'. This moderate position also allowed Stalin to remain 'silent during debates...intervening only to support the majority view...giving the impression of one whose will always prevailed in the end."" Although this may be a rather cynical interpretation of Stalin's political stance during meetings, it did allow him the luxury of supporting which ever people he wished because his views were not an open matter. It also kept him from the trap he used against so many of his rivals; that of criticising the now revered Lenin. Stalin had few political inspirations of his own, but by taking the safe and unremarkable route of following Lenin he could undermine enemies. Hingley, Stalin; Man and Legend, (London, 1974), ch7, p.2. Bazhanov's memoirs cited in Hingley, Stalin; Man and Legend, (London, 1974), ch7, p.3. Hingley, Stalin; Man and Legend, (London, 1974), ch7, p.4. By placing himself in the position of Lenin's 'chief mourner', persuading the party to have Lenin's embalmed body on public show, and playing an active role at Lenin's funeral whilst Trotsky was absent, Stalin helped to create a cult around Lenin which he was at the head of. Stalin presented his views as the legacy of Lenin, following his wishes and therefore justified in whatever action he took. Stalin's work to interpret Leninism for the working man, simplifying it and explaining it, helped him present his brand of Leninism as the only one with authenticity. This initiative to enlighten the rank and file of the party was not a solitary one; Stalin seemed to pay attention to the rank and file wishes, as well as understanding how important their support could be. It has often been argued that Stalin's political actions, and some of the apparently contradictory moves he made, closely reflected the views of the rank and file members. The recognition of the importance of lower ranking Bolsheviks gave Stalin the advantage of gaining their support where other potential leaders lacked it. Acton, Russia, The Tsarist and Soviet Legacy, (Essex, 1995), p.193. Tucker, Stalin as Revolutionary 1879 - 1929, (London, 1974), p.319. Sheila Fitzpatrick, Stalinism; New Directions, (London, 2000), p.4. In terms of the political stealth Stalin demonstrated during the 1920's leadership struggle, the battle between Trotsky and Stalin is a revealing example. In particular the contrast between their political styles is interesting; Stalin a more typical politician tentatively weighing up policies and seeking out support; Trotsky a rather superior, aloof character, forthright with his often unpopular views on policy, and expecting support to find him. Stalin seems to have exploited the fears of the other Politburo members that Trotsky remained their biggest threat in terms of personal power. It seems doubtful that his motivations for the alliance with Kamenev and Zinoviev were based on anything more than a wish to remove Trotsky from the leadership race. The seeking out of this alliance created a triumvate which proved very powerful against the already unpopular Trotsky, and all under the cover of policy disagreement. With this kind of backing behind him Stalin 'carried battle in to the wider field of the central committee where he could always secure a majority."" Trotsky's potential support was either alienated by his brusque manner, or undermined by Stalin; the committee was reminded that he was a previous Menshevik, and of the many disagreements he had openly had with Lenin. Stalin often used the disagreements and arguments of the past, out of context, as a tool against his rivals. Trotsky was pitched against the Stalin, Zinoviev, and Kamenev majority as the 'other' rather than as a legitimate opponent with a valid argument. Stalin was able to use Lenin's ban on factions to make punishable Trotsky's attempts to argue his case, and at every chance undermined him; however when the committee called for action to be taken against Trotsky, Stalin defended him and bided his time. Infact throughout the battle with Trotsky, Stalin was careful to leave the heavy criticism to Zinoviev and Kamenev, again preserving the image of the moderate amongst the irrational that had proved so useful in the past. Deutscher, Stalin; A Political Biography, (Oxford, 1949), p.233. Duranty, Stalin and Co. , (London, 1949), p.9. Hingley, Stalin; Man and Legend, (London, 1974), ch7, p.5. Hingley, Stalin; Man and Legend, (London, 1974), ch7, p.6. Hingley, Stalin; Man and Legend, (London, 1974), ch7, p.5. The final factor in Stalin's rise to power involves the mistakes made by Stalin's opponents during this time. Perhaps one of the most important and most fatal mistakes was the underestimation of Stalin by his rivals; this underestimation in turn led on to other mistakes which were to seal the fates of Stalin's opposition. It was underestimation of Stalin as a worthy and threatening rival which led Kamenev and Zinoviev in to an alliance against Trotsky who they viewed as the most potent danger. 'Comrade card index' in his administrative tasks was never identified as a real problem. It was Kamenev and Zinoviev that endorsed Stalin's position as General Secretary despite knowledge of Lenin's uncertainty about the suitability of the capricious Georgian for this post. Whilst Stalin's fate hung in the balance, Zinoviev and Kamenev argued against the public reading of Lenin's testament, in effect saving Stalin from disgrace and investigation into his conduct as General Secretary. Trotsky made several key faults in his battle with Stalin, undermining himself and loosing himself support. He criticised party bureaucracy and the careerists who accompanied it; in effect criticising and threatening the positions of the very people from whom he required support. Stalin's other opponents made mistakes which Stalin with his 'peasant shrewdness' could exploit. The united opposition was such an unconvincing union of the former enemies Trotsky, Kamenev and Zinoviev that it was not only bound to fail, but bound to rouse little support from the party. These men were all politically broken from previous rounds with Stalin, and could provide little protection to any potential supporters from the increasingly ruthless tactics employed by Stalin and his followers. Likewise, the shaky union made between Zinoviev, Kamenev and Bukharin opposing Stalin's policy of rapid industrialisation and collectivisation, was almost inevitably discovered by Stalin and destroyed through utilisation of the ban on factions rule. Nove and Read, Stalin: Terror and Transformations, video. Read, The Making and Breaking of the Soviet System, (Hampshire, 2001), p.81. Deutscher, Stalin; A Political Biography, (Oxford, 1949), p.233. Read, The Making and Breaking of the Soviet System, (Hampshire, 2001), p.90. Hingley, Stalin; Man and Legend, (London, 1974), ch7, p.5. To conclude, I have identified four main factors which I have argued played the biggest part in Stalin's rise to power during the 1920's. The acceptance and support Stalin received from Lenin played an important part in allowing Stalin the possibility of playing any kind of key position within the party. The positions Stalin held permitted him great influence within a large number of party areas undetected. He was able to build up a support base and eliminate opposition; turning administrative power into political power. Stalin's personal gift at politics gave him an advantage during the uncertainty after Lenin's death. Stalin defeated those he needed to, he was a competent politician and was able to manipulate situations to his advantage in a way his rivals could not. Finally, the mistakes his opposition made, especially their failure to act upon Lenin's wishes to have Stalin removed from his post, and indeed the very uncertainty in which Lenin left the party leadership, allowed Stalin the chances which led to his rise. Read, The Making and Breaking of the Soviet System, (Hampshire, 2001), p.52. I have not argued that Stalin was a passive actor during his rise to power in the 1920's. He was an active force, using cunning, manipulation, and politics to engineer his position. However, he could not have done this without the other factors I have mentioned in this essay. All of them were important and I do not think Stalin would have emerged as leader of the party if any one was missing. The assumption that a gap existed between the theory and the practice in the social position of women between 1500 and 1700 is a contested one. Margaret Ezell argues that the expectations of female behaviour and duty in society did infact closely match the reality of women's experiences. Looking at contemporary literature and popular opinion, Ezell concludes that contrary to many historians who view the prescribed position as restrictive and misogynistic, both scripture and conduct books infact highlighted female capabilities and praised women as partners in life. 'Differences between men and women are merely superficial...women are as naturally capable as men of reason, wit and...government."" However, feminists have criticised Ezell for her limited interpretation of the literature she looks at. They argue that advice such as '...it is not that the wife has no mind of her own, but that she deliberately alters it if necessary, to conform with her husbands,' does not counter the view that women were expected to be submissive and were considered inferior to men during this period. Even Lawrence Stone, who argues that patriarchy was at its strongest and its most widely accepted at this time, acknowledges that the role set out for women often differed markedly from the actual experiences of womanhood, due mainly to the fact that the prescribed role was simply incompatible with the realities of life, especially for the poor. Indeed, Ralph Houlbrooke argues that the '...theory was simply unworkable...' with the realities faced by contemporary women. Pierre LeMoyne, 'The Gallery of Heroick Women', cited in M. Ezell, The Patriarch's Wife: Literary Evidence and the History of the Family, (New York, 1987), p.44. 'Feminist school of Phallogocentrism' who criticise Ezell, cited in Ezell, The Patriarch's Wife, (New York, 1987), p.37. Ezell, The Patriarch's Wife, (New York, 1987), p.40. L. Stone, The Family, Sex and Marriage in England 1500 - 1800, (Middlesex, 1977), p.139. Ralph Houlbrooke, The English Family 1450 - 1700, (London, 1984), p.106. In this essay I am going to argue from a standpoint assuming that a gap between the theory and the practice in the social position of women in early modern England did infact exist. I will argue that this gap existed despite both male and female attempts to keep within it, due both to its unrealistic expectations compared with the actual lives women led, and the dualities and contradictions within it. I will argue that due to three broad and overlapping areas: economic and personal factors, and a contradiction within the role which expected servility but also demanded command and power, women could not live out their lives within the boundaries of their expected social positions. The position women were to occupy in society was set out to the population through a number of mediums. Scriptural references in homilies and religious services put emphasis upon '...the procreation and religious education of children, the regulation of sexual activity, and mutual comfort and support.' A distinction was made between suitable female and male duties; 'The husband was held to be the superior partner, the wife the subordinate and inferior...her foremost duty was obedience."" In the advice guides published, women were seen to be 'material and passive...weak in body...and unfitted for work or public life.' As such they were expected to remain in the private sphere, '...unquestioningly obedient to her husband, accepting husbandly reproofs meekly...bearing her husbands faults...accommodating his various moods and on any breach being the first to seek reconciliation."" Satires and other anti-female publications played upon the popular images of women as sexually insatiable, nagging scolds, as well as playing upon the images of marriage as an end to freedom and fun, and a life of supporting a wife who would stop at nothing to get her own way. Whether popular opinion about women was more influenced by religion and advice guides or by satires is hard to tell, and indeed the opinion of the female role and marriage differed between men and women and according to place in the social order. It is probable that popular opinion took on board a mixture of both views as well as referring to personal experience of female figures and their behaviour. Finally, the expected social position of women was expressed by the authorities through the law; 'nowhere did the wife appear more completely subordinate to her husband than in the common law'. Women were never a homogeneous group; the law and indeed general opinion differed with different women; single, married, widowed and spinsters. However, in the eyes of the law, a woman belonged to the man in charge of her whether that was a father, brother or husband. Widows enjoyed a certain amount of freedom, however along with spinsters were so marginalised socially, for being 'master-less', and economically, with only a few poorly paid trades to work in, that they had very hard lives. Already, with this simple description of the expected social position of women it is possible to see some of the contradictions within it; for example, that women were sexually uncontrollable, yet the moral guardians of society. Susan Amussen argues that this contradiction is one of the main reasons why the wish for '...clear subordination of wives to husbands...was never fully realised.' Houlbrooke, The English Family, (London, 1984), p.97. Houlbrooke, The English Family, (London, 1984), p.97. Ezell, The Patriarch's Wife, (New York, 1987), p37. Martin Ingram, Church Courts, Sex and Marriage in England 1570 - 1640, (Cambridge, 1987), p.129. Houlbrooke, The English Family, (London, 1984), p.100. Susan Dwyer Amussen, An Ordered Society: Gender and Class in Early Modern England, (New York, 1988), p.115. Amussen, An Ordered Society, (New York, 1988), p.115. The gap between theory and practice in the social position of women in early modern England was certainly influenced by the economic realities they faced in their everyday lives. It was often necessary for poorer women to marry quite late; the time before marriage was spent working in service adding to the family purse. Girls had to leave home and live away from their families often giving them a taste for relative freedom, economic independence and a knowledge of their personal capabilities. Even after marriage it was often necessary for women to make economic contributions to the family; supplementing the low wages her husband was paid. Keith Wrightson argues that these female supplements to male wages were crucial to survival even if they did somewhat undermine the official female position in society. Susan Amussen points out that the gap between theory and practice actually lessened depending on the importance of the economic contributions of a wife. Some women did not need to compromise the submissive, passive role by economic contributions such as bartering at market or looking after the estate whilst a husband was absent. Although women rarely strayed in to their husbands' role once he had returned, it was essential to economic survival that they could and would perform his tasks even if this meant female and domination and authority. Men wanted their wives to be '...both subordinate and competent...and willing to accept that whatever a man did was work and whatever a woman did was her duty'. 'Women's independence and autonomy were critical to their success as wives and mothers...the contradictions between women's economic roles and their expected subordination were so severe that they posed a problem to the most carefully conforming wife.' Keith Wrightson, Earthly Necessities: Economic Lives in Early Modern Britain, 1470 - 1750, (London, 2002), p.53. Amussen, An Ordered Society, (New York, 1988), p.120. Houlbrooke, The English Family, (London, 1984), p.107. Anthony Fletcher, Gender, Sex and Subordination in England 1500 - 1800, (London, 1995), p.254. Amussen, An Ordered Society, (New York, 1988), p.121. Women were often needed in the public sphere, and even if not playing leading roles their actions still conflicted with the domesticated submission part of the role, although perhaps fulfilling the obedience part. Diane Willen argues that pauper women were actively recruited by local authorities in to the public arena. Although the work women were doing for local authorities was an extension of their traditional domestic duties; cleaning and caring, 'it questions...the existence of separate private/public spheres in the early modern period'. Both the poor women being employed and the 'local patriarchal political authorities' employing them gained economically from this arrangement. Women could earn a small wage and authorities gained a willing, cheap labour force; both parties were forced to revise women's official social position for economic gain. Widows were also women with access to the public sphere; they could by law take over some of their husband's trades or estates upon his death. Although often poorer widows were left very economically vulnerable, some were able to maintain themselves successfully; indeed advice guides warned men against marrying widows as they tended to be 'harder to tame' having already tasted and achieved economic independence. Diane Willen, 'Women in the Public Sphere in Early Modern England: The Case of the Urban Working Poor', Sixteenth Century Journal, 19, (1988), p. 559 - 575. Willen, 'Women in the Public Sphere in Early Modern England', Sixteenth Century Journal, 19, (1988), p.559. Willen, 'Women in the Public Sphere in Early Modern England', Sixteenth Century Journal, 19, (1988), p.560. Anthony Fletcher, Gender, Sex and Subordination in England 1500 - 1800, (London, 1995), p.174. The second set of factors which made the social position set out for women in early modern England impossible to fulfil even if they tried, were personal factors. By this I mean factors which come about naturally as a consequence of marriage; i.e. spending a lot of time with someone, growing to know them very well and loving them. Love and caring were seen as very important aspects of a successful marriage, in particular they were seen as part of a husband's duty. However it was well documented in contemporary literature that '...more men betrayed their command through their own fondness than ever lost it through their wives rebellion'. Though caring and affection were viewed with importance; and were often an unavoidable outcome of spending lives together, measures were taken to limit the influence affection had on the social position of women. 'Convention demanded that the wife address her husband with humility and deference...even the affectionate terms love, joy, dear and duck were not appropriate in the mouth of a subordinate."" However it must have been very difficult to maintain a purely master/subordinate relationship especially when feelings of love and affection existed in a marriage. Even the most obedient wife and most masterful husband must have come across occasions when such formal address would have been inappropriate. Thomas Fuller cited in Houlbrooke, The English Family, (London, 1984), p.101. Houlbrooke, The English Family, (London, 1984), p.101. Laura Gowing argues that the gap between theory and practice actually began during courtship, making it only natural that it continue after marriage. Gowing explains that it was sometimes necessary to deviate from the passive, submissive role of a courting woman. The double standard which existed in terms of male and female sexual morality and the importance of reputation in early modern society made courting quite a difficult process. Women had to use their intricate knowledge of the laws of courting to avoid the many traps which could leave them tied to a poor marriage match or with their reputation in tatters. Often it was impossible for a woman to remain passive during courting, especially if conjugal rights had been granted to a man after the promise of marriage, or pushy parents were disdainful of certain suitors. Often with a little bit of courage and forcefulness, men could be made to keep promises, and by scrutinising the exact wording used in the making of a marriage contract they could be escaped. 'Women could still exercise their power of evasion and refusal...language...gave women the opportunity to broaden the largely passive part allotted to them in the process of courtship.' Laura Gowing, Women, Words and Sex in Early Modern London, (Oxford, 1998), p.147. Laura Gowing, Women, Words and Sex, (Oxford, 1998), p.148. Other personal factors made the female social position hard to follow strictly; in many cases women's personal capabilities and desires surpassed the prescribed role. Often individual character and temperament sat at odds with the boundaries set out; after all, not all wives were inferior to their husbands and not all husbands' wanted/needed submissive wives. Simple practicalities such as very little age difference between partners, made the expected master/servant relationship, hard to fulfil; a woman with equal or more life experience than her husband could be hard to commandeer. Finally, women's roles and identities within society were not a constant thing, they were much less fixed than men's; as they were already malleable, it was relatively easy to simply revise these identities and social positions further. Houlbrooke, The English Family, (London, 1984), p.103. Laura Gowing, Women, Words and Sex, (Oxford, 1998), p.231. The final aspect which made it impossible for women to live wholly within their official social position, were the areas of society over which women had complete power and control; morality, reputation and of course the domestic sphere. In marriage and out of it women were to control the sexual activity of themselves and their spouses; adultery was seen as dangerous, putting the established social order in jeopardy, and even too much sex between husband and wife was viewed negatively. Women's control over reputation gave them a certain amount of power over both other women and men; as Samuel Pepys diary shows '...when his wife caught him with their maid...he at once recognised the political significance of the event'. Women could wield the reputation card to expand their limited role and position in society. Gossiping was another largely female activity allowing them to channel pressure on those they felt were morally lacking whether for bastardy, cuckolding or wife beating. Women were called upon to ensure that their men folk were satisfactorily carrying out their duties and this gave them a certain amount of power. For example, if men were neglecting their religious duties as head of the household, a wife could 'take-over' this part of his role even if he was in objection to this. This expectation of wives by religious authorities severely compromised the obedient social position they otherwise instructed her to take. Houlbrooke, The English Family, (London, 1984), p.97. B. Capp, 'The Double Standard Revisited: Plebeian Women and Male Sexual Reputation In Early Modern England', Past and Present, 162, (1999), p. 73. B. Capp, 'The Double Standard Revisited, Past and Present, 162, (1999), p. 74. Laura Gowing, Women, Words and Sex, (Oxford, 1998), p.67. Houlbrooke, The English Family, (London, 1984), p.98. The domestic sphere, often seen as the sight of women's inequality, was infact the one area of society over which women had a lot of control. Rule over servants and the running of a complex household were just some of the tasks women were expected to perform. Other tasks included care and religious education of children as well as total authority in the field of pregnancy and childbirth; 'lay ins' gave women an all female space in which to recover after childbirth. Women were also expected to give their husbands sound business advice and make decisions within the domestic sphere. The fact that these tasks could not be performed without being domineering, active and authoritarian was why a necessary gap existed between theory and practice in the social position of women. Amussen, An Ordered Society, (New York, 1988), p.107. To conclude, I have tried to show that even men and women attempting to keep within their official social positions found it an impossible task. I have looked at some of the factors that made this impossible; including the economic realities which meant women's roles had to overlap with men's and forced them beyond the passive, submissive boundaries of their role. There were also personal factors, such as the natural familiarity which grows as a result of married life which made strict adherence to the master, subservient relationship between couples hard to maintain strictly. Finally, there were areas of society in which women held most if not all power, these areas meant that women by necessity had authority which contradicted the submissive position intended for them, as well as giving women power which they could use to renegotiate their limited position. Amussen, An Ordered Society, (New York, 1988), p.109. Susan Amussen argues that the attempts of women to remain faithful to their prescribed social position, but failure due to the impractical, unrealistic nature of the position, is shown by women's emphasis on the sexual conduct part of the role; the part that was actually achievable. Faced with the impossibility of the obedience part, women placed greater importance on strict sexual conduct. Where possible women were willing to adhere to their prescribed social positions; helping to maintain the social order so important to early modern society. As Bernard Capp argues; women did not directly challenge the system, rather they worked within it, finding compromises to suit their economic, and personal realities. They also accommodated the features such as commanding, and decision making which went against their prescribed temperament but which were necessary for the domestic success also expected of them. The gap between theory and practice in the social position of women in early modern England should be explained in terms of its impractical economic nature and its contradictory and unrealistic expectations. Amussen, An Ordered Society, (New York, 1988), p.122. B. Capp, 'The Double Standard Revisited, Past and Present, 162, (1999), p.99.",True
49,"By 1930 Stalin had not only managed to gain extensive personal control and eliminated all his opponents but had also managed to consolidate this power. Accountable to very few Stalin was free to pursue any policy largely unchecked. He had support from the party rank and file as well as from the central committee and the Politburo. How Stalin, the least obvious of Lenin's potential successors, came to be in this position is a much contested issue. With his death Lenin left a vacuum within the party leadership creating economic, social and political uncertainty. This in turn led to a left / right divide within the Politburo over the next steps the Bolsheviks should take, with Trotsky on the left arguing for rapid industrialisation. Stalin maintained a generally moderate position before siding with Kamenev and Zinoviev forming a powerful triumvate against the man they saw as their biggest rival. After Trotsky's eventual expulsion from the party, and then the country, came the political execution of Kamenev and Zinoviev followed by their actual execution. Stalin's other main rivals faded away either in to political destruction, suicide, execution or expulsion. A. Nove and C. Read, Stalin: Terror and Transformations, video. Nove and Read, Stalin: Terror and Transformations, video. R. Hingley, Stalin; Man and Legend, (London, 1974), ch7, p.3. I find it hard to justify putting Stalin's rise to power down to any one single factor. It does not seem possible that Stalin reached his position purely due to his own manipulation or political cunning. Nor does it seem realistic that he was put in power purely because he was the politician that most reflected the party rank and file wishes. A number of reasons seems the most probable explanation; I will argue that the most important are as follows. Stalin proved a useful asset to Lenin and as a result Lenin often supported Stalin's appointments and promotions. The positions Stalin held within the party were of vital importance in helping him gain power, his personality and his political prowess were also very important in this respect. The mistakes of Stalin's rivals during the 1920's were to prove fatal and helped him in his ascendancy to power. Many of the opportunities which led to Stalin's rise to power were actually accumulated whilst Lenin was still alive. Infact on many occasions it was Lenin that endorsed Stalin's promotion to important positions within the party. Stalin fitted the proletariat ideal lacking in so many of the profiles of the Bolshevik leaders including Lenin's own, Stalin was singled out by Lenin as having potential. His activities to raise funds for the party included bank robbing, and he was one of those 'hard core' Bolsheviks who had remained in Russia '...underground during the winter 1905 -6 repression and persecution of revolutionaries by the Tsar."" This mixture of loyalty and practicality was one Lenin could not resist, he began to rely on Stalin who proved a useful asset. 'He was an unquestioningly loyal person who actually got things done... Stalin was Lenin's political fixer...packing the congress with his supporters'. Lenin supported and approved many of the positions Stalin was appointed to within the party. Although Trotsky claimed that after Stalin's appointment as party General Secretary Lenin expressed concern; '...this cook will serve only peppery dishes', he did not seem sufficiently concerned to raise public objection until very near his death when it was too late. Indeed, prior to his appointment as General Secretary Stalin already held positions of great importance; immediately after the civil war he was Commissar of Nationalities, Commissar of the Workers and Peasants Inspectorate and a member of the Politburo. 'Several of Stalin's crucial promotions - cooption to the central committee and appointment as Pravda editor in 1912 and as General Secretary in 1922- were brought about by Lenin's influence.' L. Deutscher, Stalin; A Political Biography, (Oxford, 1949), p228. Walter Duranty, Stalin and Co. The Politburo; The Men who Ran Russia, (London, 1949), p.10. Christopher Read, The Making and Breaking of the Soviet System, (Hampshire, 2001), p.84. Read, The Making and Breaking of the Soviet System, (Hampshire, 2001), p.85. Deutscher, Stalin; A Political Biography, (Oxford, 1949), p.232. Deutscher, Stalin; A Political Biography, (Oxford, 1949), p.228. Read, The Making and Breaking of the Soviet System, (Hampshire, 2001), p.84. Stalin has been described as 'the spider in the web' in terms of his positioning within both the state and the party apparatus. He had influence within all the main offices, and the posts he held gave him direct authority or at the very least powers of persuasion over a very useful range of contacts. At the time of Lenin's death when the struggle for power was fought out amongst the Politburo members, Stalin had power within the Politburo, (the highest decision making body), the Orgburo, (the administrative bureau of the central committee), the Robkrin and the Sovarkom, as well as in the regional and local party cells and the regional and local soviets. The very nature of the centralised organisation of the party and the state worked to Stalin's advantage due to the links between all the offices. Though all his positions gave him important influence, it was his position as General Secretary which allowed him to build up the kind of power base which defeated his rivals. In this post Stalin was responsible for Politburo administration including the agenda for discussion, Stalin was the link between the Politburo and the Central Committee, he also controlled the '...appointment and promotion of individuals to key posts throughout the country."" As Edward Acton points out, this meant that 'the careers of party officials were becoming increasingly dependent upon their loyalty to the Secretariat."" This in turn gave the '...central administrative organs enormous influence over the make-up of party Congresses and thus of elections to the Central Committee and the Politburo itself'. Acton maintains that this 'movement of power from the Politburo to the Secretariat', was initially '...masked under Lenin's authority over the Secretariat."" However Deutscher claims that on Lenin's return from the recovery of his first stroke he sensed the increasing power of the bureaucracy with Stalin at its head, and immediately attempted (though unsuccessfully) to rectify the situation. The position of General Secretary had been largely overlooked as a source of power due to its administrative nature; '...the bright sparks of the Politburo felt themselves above such roles."" However Stalin was willing to take on this shunted position, and was good at it; he worked hard, he was reliable, made no fuss and needed little assistance from the other Politburo members. He converted secondary administrative power in to political power, using the powerful self perpetuating nature of the vested interests within the bureaucracy. Christopher Read, Lecture, University of Warwick, January 2004. Edward Acton, Russia, The Tsarist and Soviet Legacy, (Essex, 1995), p.197. Acton, Russia, The Tsarist and Soviet Legacy, (Essex, 1995), p.196. Acton, Russia, The Tsarist and Soviet Legacy, (Essex, 1995), p.197. Acton, Russia, The Tsarist and Soviet Legacy, (Essex, 1995), p.197. Acton, Russia, The Tsarist and Soviet Legacy, (Essex, 1995), p.198. Deutscher, Stalin; A Political Biography, (Oxford, 1949), p.247. Robert Tucker, Stalin as Revolutionary 1879 - 1929; A Study In History and Personality, (London, 1974), p.322. Deutscher, Stalin; A Political Biography, (Oxford, 1949), p.233. Idea of secondary administrative power; Read, The Making and Breaking of the Soviet System, (Hampshire, 2001), p.52. Idea of perpetuating vested interests; Hingley, Stalin; Man and Legend, (London, 1974), ch7, p5. Stalin's personality and his political manoeuvring and prowess have been interpreted in a number of ways. As evil, pre-planned, ruthless, manipulation, but also as a highly skilful reaction to economic, social and political situations. Which ever interpretation bears more truth, that Stalin's politics proved enormously valuable to him during the 1920's is undoubtable. The moderate position Stalin seemed to take during Politburo meetings both whilst Lenin was alive and after his death allowed him several advantages. It meant that he 'always seemed to follow others, never to direct them', making Trotsky, who was very active during meetings, seem the biggest threat to power. 'No one could have behaved less like a tyrant or dictator than did Stalin at Politburo meetings'. This moderate position also allowed Stalin to remain 'silent during debates...intervening only to support the majority view...giving the impression of one whose will always prevailed in the end."" Although this may be a rather cynical interpretation of Stalin's political stance during meetings, it did allow him the luxury of supporting which ever people he wished because his views were not an open matter. It also kept him from the trap he used against so many of his rivals; that of criticising the now revered Lenin. Stalin had few political inspirations of his own, but by taking the safe and unremarkable route of following Lenin he could undermine enemies. Hingley, Stalin; Man and Legend, (London, 1974), ch7, p.2. Bazhanov's memoirs cited in Hingley, Stalin; Man and Legend, (London, 1974), ch7, p.3. Hingley, Stalin; Man and Legend, (London, 1974), ch7, p.4. By placing himself in the position of Lenin's 'chief mourner', persuading the party to have Lenin's embalmed body on public show, and playing an active role at Lenin's funeral whilst Trotsky was absent, Stalin helped to create a cult around Lenin which he was at the head of. Stalin presented his views as the legacy of Lenin, following his wishes and therefore justified in whatever action he took. Stalin's work to interpret Leninism for the working man, simplifying it and explaining it, helped him present his brand of Leninism as the only one with authenticity. This initiative to enlighten the rank and file of the party was not a solitary one; Stalin seemed to pay attention to the rank and file wishes, as well as understanding how important their support could be. It has often been argued that Stalin's political actions, and some of the apparently contradictory moves he made, closely reflected the views of the rank and file members. The recognition of the importance of lower ranking Bolsheviks gave Stalin the advantage of gaining their support where other potential leaders lacked it. Acton, Russia, The Tsarist and Soviet Legacy, (Essex, 1995), p.193. Tucker, Stalin as Revolutionary 1879 - 1929, (London, 1974), p.319. Sheila Fitzpatrick, Stalinism; New Directions, (London, 2000), p.4. In terms of the political stealth Stalin demonstrated during the 1920's leadership struggle, the battle between Trotsky and Stalin is a revealing example. In particular the contrast between their political styles is interesting; Stalin a more typical politician tentatively weighing up policies and seeking out support; Trotsky a rather superior, aloof character, forthright with his often unpopular views on policy, and expecting support to find him. Stalin seems to have exploited the fears of the other Politburo members that Trotsky remained their biggest threat in terms of personal power. It seems doubtful that his motivations for the alliance with Kamenev and Zinoviev were based on anything more than a wish to remove Trotsky from the leadership race. The seeking out of this alliance created a triumvate which proved very powerful against the already unpopular Trotsky, and all under the cover of policy disagreement. With this kind of backing behind him Stalin 'carried battle in to the wider field of the central committee where he could always secure a majority."" Trotsky's potential support was either alienated by his brusque manner, or undermined by Stalin; the committee was reminded that he was a previous Menshevik, and of the many disagreements he had openly had with Lenin. Stalin often used the disagreements and arguments of the past, out of context, as a tool against his rivals. Trotsky was pitched against the Stalin, Zinoviev, and Kamenev majority as the 'other' rather than as a legitimate opponent with a valid argument. Stalin was able to use Lenin's ban on factions to make punishable Trotsky's attempts to argue his case, and at every chance undermined him; however when the committee called for action to be taken against Trotsky, Stalin defended him and bided his time. Infact throughout the battle with Trotsky, Stalin was careful to leave the heavy criticism to Zinoviev and Kamenev, again preserving the image of the moderate amongst the irrational that had proved so useful in the past. Deutscher, Stalin; A Political Biography, (Oxford, 1949), p.233. Duranty, Stalin and Co. , (London, 1949), p.9. Hingley, Stalin; Man and Legend, (London, 1974), ch7, p.5. Hingley, Stalin; Man and Legend, (London, 1974), ch7, p.6. Hingley, Stalin; Man and Legend, (London, 1974), ch7, p.5. The final factor in Stalin's rise to power involves the mistakes made by Stalin's opponents during this time. Perhaps one of the most important and most fatal mistakes was the underestimation of Stalin by his rivals; this underestimation in turn led on to other mistakes which were to seal the fates of Stalin's opposition. It was underestimation of Stalin as a worthy and threatening rival which led Kamenev and Zinoviev in to an alliance against Trotsky who they viewed as the most potent danger. 'Comrade card index' in his administrative tasks was never identified as a real problem. It was Kamenev and Zinoviev that endorsed Stalin's position as General Secretary despite knowledge of Lenin's uncertainty about the suitability of the capricious Georgian for this post. Whilst Stalin's fate hung in the balance, Zinoviev and Kamenev argued against the public reading of Lenin's testament, in effect saving Stalin from disgrace and investigation into his conduct as General Secretary. Trotsky made several key faults in his battle with Stalin, undermining himself and loosing himself support. He criticised party bureaucracy and the careerists who accompanied it; in effect criticising and threatening the positions of the very people from whom he required support. Stalin's other opponents made mistakes which Stalin with his 'peasant shrewdness' could exploit. The united opposition was such an unconvincing union of the former enemies Trotsky, Kamenev and Zinoviev that it was not only bound to fail, but bound to rouse little support from the party. These men were all politically broken from previous rounds with Stalin, and could provide little protection to any potential supporters from the increasingly ruthless tactics employed by Stalin and his followers. Likewise, the shaky union made between Zinoviev, Kamenev and Bukharin opposing Stalin's policy of rapid industrialisation and collectivisation, was almost inevitably discovered by Stalin and destroyed through utilisation of the ban on factions rule. Nove and Read, Stalin: Terror and Transformations, video. Read, The Making and Breaking of the Soviet System, (Hampshire, 2001), p.81. Deutscher, Stalin; A Political Biography, (Oxford, 1949), p.233. Read, The Making and Breaking of the Soviet System, (Hampshire, 2001), p.90. Hingley, Stalin; Man and Legend, (London, 1974), ch7, p.5. To conclude, I have identified four main factors which I have argued played the biggest part in Stalin's rise to power during the 1920's. The acceptance and support Stalin received from Lenin played an important part in allowing Stalin the possibility of playing any kind of key position within the party. The positions Stalin held permitted him great influence within a large number of party areas undetected. He was able to build up a support base and eliminate opposition; turning administrative power into political power. Stalin's personal gift at politics gave him an advantage during the uncertainty after Lenin's death. Stalin defeated those he needed to, he was a competent politician and was able to manipulate situations to his advantage in a way his rivals could not. Finally, the mistakes his opposition made, especially their failure to act upon Lenin's wishes to have Stalin removed from his post, and indeed the very uncertainty in which Lenin left the party leadership, allowed Stalin the chances which led to his rise. Read, The Making and Breaking of the Soviet System, (Hampshire, 2001), p.52. I have not argued that Stalin was a passive actor during his rise to power in the 1920's. He was an active force, using cunning, manipulation, and politics to engineer his position. However, he could not have done this without the other factors I have mentioned in this essay. All of them were important and I do not think Stalin would have emerged as leader of the party if any one was missing. The assumption that a gap existed between the theory and the practice in the social position of women between 1500 and 1700 is a contested one. Margaret Ezell argues that the expectations of female behaviour and duty in society did infact closely match the reality of women's experiences. Looking at contemporary literature and popular opinion, Ezell concludes that contrary to many historians who view the prescribed position as restrictive and misogynistic, both scripture and conduct books infact highlighted female capabilities and praised women as partners in life. 'Differences between men and women are merely superficial...women are as naturally capable as men of reason, wit and...government."" However, feminists have criticised Ezell for her limited interpretation of the literature she looks at. They argue that advice such as '...it is not that the wife has no mind of her own, but that she deliberately alters it if necessary, to conform with her husbands,' does not counter the view that women were expected to be submissive and were considered inferior to men during this period. Even Lawrence Stone, who argues that patriarchy was at its strongest and its most widely accepted at this time, acknowledges that the role set out for women often differed markedly from the actual experiences of womanhood, due mainly to the fact that the prescribed role was simply incompatible with the realities of life, especially for the poor. Indeed, Ralph Houlbrooke argues that the '...theory was simply unworkable...' with the realities faced by contemporary women. Pierre LeMoyne, 'The Gallery of Heroick Women', cited in M. Ezell, The Patriarch's Wife: Literary Evidence and the History of the Family, (New York, 1987), p.44. 'Feminist school of Phallogocentrism' who criticise Ezell, cited in Ezell, The Patriarch's Wife, (New York, 1987), p.37. Ezell, The Patriarch's Wife, (New York, 1987), p.40. L. Stone, The Family, Sex and Marriage in England 1500 - 1800, (Middlesex, 1977), p.139. Ralph Houlbrooke, The English Family 1450 - 1700, (London, 1984), p.106. In this essay I am going to argue from a standpoint assuming that a gap between the theory and the practice in the social position of women in early modern England did infact exist. I will argue that this gap existed despite both male and female attempts to keep within it, due both to its unrealistic expectations compared with the actual lives women led, and the dualities and contradictions within it. I will argue that due to three broad and overlapping areas: economic and personal factors, and a contradiction within the role which expected servility but also demanded command and power, women could not live out their lives within the boundaries of their expected social positions. The position women were to occupy in society was set out to the population through a number of mediums. Scriptural references in homilies and religious services put emphasis upon '...the procreation and religious education of children, the regulation of sexual activity, and mutual comfort and support.' A distinction was made between suitable female and male duties; 'The husband was held to be the superior partner, the wife the subordinate and inferior...her foremost duty was obedience."" In the advice guides published, women were seen to be 'material and passive...weak in body...and unfitted for work or public life.' As such they were expected to remain in the private sphere, '...unquestioningly obedient to her husband, accepting husbandly reproofs meekly...bearing her husbands faults...accommodating his various moods and on any breach being the first to seek reconciliation."" Satires and other anti-female publications played upon the popular images of women as sexually insatiable, nagging scolds, as well as playing upon the images of marriage as an end to freedom and fun, and a life of supporting a wife who would stop at nothing to get her own way. Whether popular opinion about women was more influenced by religion and advice guides or by satires is hard to tell, and indeed the opinion of the female role and marriage differed between men and women and according to place in the social order. It is probable that popular opinion took on board a mixture of both views as well as referring to personal experience of female figures and their behaviour. Finally, the expected social position of women was expressed by the authorities through the law; 'nowhere did the wife appear more completely subordinate to her husband than in the common law'. Women were never a homogeneous group; the law and indeed general opinion differed with different women; single, married, widowed and spinsters. However, in the eyes of the law, a woman belonged to the man in charge of her whether that was a father, brother or husband. Widows enjoyed a certain amount of freedom, however along with spinsters were so marginalised socially, for being 'master-less', and economically, with only a few poorly paid trades to work in, that they had very hard lives. Already, with this simple description of the expected social position of women it is possible to see some of the contradictions within it; for example, that women were sexually uncontrollable, yet the moral guardians of society. Susan Amussen argues that this contradiction is one of the main reasons why the wish for '...clear subordination of wives to husbands...was never fully realised.' Houlbrooke, The English Family, (London, 1984), p.97. Houlbrooke, The English Family, (London, 1984), p.97. Ezell, The Patriarch's Wife, (New York, 1987), p37. Martin Ingram, Church Courts, Sex and Marriage in England 1570 - 1640, (Cambridge, 1987), p.129. Houlbrooke, The English Family, (London, 1984), p.100. Susan Dwyer Amussen, An Ordered Society: Gender and Class in Early Modern England, (New York, 1988), p.115. Amussen, An Ordered Society, (New York, 1988), p.115. The gap between theory and practice in the social position of women in early modern England was certainly influenced by the economic realities they faced in their everyday lives. It was often necessary for poorer women to marry quite late; the time before marriage was spent working in service adding to the family purse. Girls had to leave home and live away from their families often giving them a taste for relative freedom, economic independence and a knowledge of their personal capabilities. Even after marriage it was often necessary for women to make economic contributions to the family; supplementing the low wages her husband was paid. Keith Wrightson argues that these female supplements to male wages were crucial to survival even if they did somewhat undermine the official female position in society. Susan Amussen points out that the gap between theory and practice actually lessened depending on the importance of the economic contributions of a wife. Some women did not need to compromise the submissive, passive role by economic contributions such as bartering at market or looking after the estate whilst a husband was absent. Although women rarely strayed in to their husbands' role once he had returned, it was essential to economic survival that they could and would perform his tasks even if this meant female and domination and authority. Men wanted their wives to be '...both subordinate and competent...and willing to accept that whatever a man did was work and whatever a woman did was her duty'. 'Women's independence and autonomy were critical to their success as wives and mothers...the contradictions between women's economic roles and their expected subordination were so severe that they posed a problem to the most carefully conforming wife.' Keith Wrightson, Earthly Necessities: Economic Lives in Early Modern Britain, 1470 - 1750, (London, 2002), p.53. Amussen, An Ordered Society, (New York, 1988), p.120. Houlbrooke, The English Family, (London, 1984), p.107. Anthony Fletcher, Gender, Sex and Subordination in England 1500 - 1800, (London, 1995), p.254. Amussen, An Ordered Society, (New York, 1988), p.121. Women were often needed in the public sphere, and even if not playing leading roles their actions still conflicted with the domesticated submission part of the role, although perhaps fulfilling the obedience part. Diane Willen argues that pauper women were actively recruited by local authorities in to the public arena. Although the work women were doing for local authorities was an extension of their traditional domestic duties; cleaning and caring, 'it questions...the existence of separate private/public spheres in the early modern period'. Both the poor women being employed and the 'local patriarchal political authorities' employing them gained economically from this arrangement. Women could earn a small wage and authorities gained a willing, cheap labour force; both parties were forced to revise women's official social position for economic gain. Widows were also women with access to the public sphere; they could by law take over some of their husband's trades or estates upon his death. Although often poorer widows were left very economically vulnerable, some were able to maintain themselves successfully; indeed advice guides warned men against marrying widows as they tended to be 'harder to tame' having already tasted and achieved economic independence. Diane Willen, 'Women in the Public Sphere in Early Modern England: The Case of the Urban Working Poor', Sixteenth Century Journal, 19, (1988), p. 559 - 575. Willen, 'Women in the Public Sphere in Early Modern England', Sixteenth Century Journal, 19, (1988), p.559. Willen, 'Women in the Public Sphere in Early Modern England', Sixteenth Century Journal, 19, (1988), p.560. Anthony Fletcher, Gender, Sex and Subordination in England 1500 - 1800, (London, 1995), p.174. The second set of factors which made the social position set out for women in early modern England impossible to fulfil even if they tried, were personal factors. By this I mean factors which come about naturally as a consequence of marriage; i.e. spending a lot of time with someone, growing to know them very well and loving them. Love and caring were seen as very important aspects of a successful marriage, in particular they were seen as part of a husband's duty. However it was well documented in contemporary literature that '...more men betrayed their command through their own fondness than ever lost it through their wives rebellion'. Though caring and affection were viewed with importance; and were often an unavoidable outcome of spending lives together, measures were taken to limit the influence affection had on the social position of women. 'Convention demanded that the wife address her husband with humility and deference...even the affectionate terms love, joy, dear and duck were not appropriate in the mouth of a subordinate."" However it must have been very difficult to maintain a purely master/subordinate relationship especially when feelings of love and affection existed in a marriage. Even the most obedient wife and most masterful husband must have come across occasions when such formal address would have been inappropriate. Thomas Fuller cited in Houlbrooke, The English Family, (London, 1984), p.101. Houlbrooke, The English Family, (London, 1984), p.101. Laura Gowing argues that the gap between theory and practice actually began during courtship, making it only natural that it continue after marriage. Gowing explains that it was sometimes necessary to deviate from the passive, submissive role of a courting woman. The double standard which existed in terms of male and female sexual morality and the importance of reputation in early modern society made courting quite a difficult process. Women had to use their intricate knowledge of the laws of courting to avoid the many traps which could leave them tied to a poor marriage match or with their reputation in tatters. Often it was impossible for a woman to remain passive during courting, especially if conjugal rights had been granted to a man after the promise of marriage, or pushy parents were disdainful of certain suitors. Often with a little bit of courage and forcefulness, men could be made to keep promises, and by scrutinising the exact wording used in the making of a marriage contract they could be escaped. 'Women could still exercise their power of evasion and refusal...language...gave women the opportunity to broaden the largely passive part allotted to them in the process of courtship.' Laura Gowing, Women, Words and Sex in Early Modern London, (Oxford, 1998), p.147. Laura Gowing, Women, Words and Sex, (Oxford, 1998), p.148. Other personal factors made the female social position hard to follow strictly; in many cases women's personal capabilities and desires surpassed the prescribed role. Often individual character and temperament sat at odds with the boundaries set out; after all, not all wives were inferior to their husbands and not all husbands' wanted/needed submissive wives. Simple practicalities such as very little age difference between partners, made the expected master/servant relationship, hard to fulfil; a woman with equal or more life experience than her husband could be hard to commandeer. Finally, women's roles and identities within society were not a constant thing, they were much less fixed than men's; as they were already malleable, it was relatively easy to simply revise these identities and social positions further. Houlbrooke, The English Family, (London, 1984), p.103. Laura Gowing, Women, Words and Sex, (Oxford, 1998), p.231. The final aspect which made it impossible for women to live wholly within their official social position, were the areas of society over which women had complete power and control; morality, reputation and of course the domestic sphere. In marriage and out of it women were to control the sexual activity of themselves and their spouses; adultery was seen as dangerous, putting the established social order in jeopardy, and even too much sex between husband and wife was viewed negatively. Women's control over reputation gave them a certain amount of power over both other women and men; as Samuel Pepys diary shows '...when his wife caught him with their maid...he at once recognised the political significance of the event'. Women could wield the reputation card to expand their limited role and position in society. Gossiping was another largely female activity allowing them to channel pressure on those they felt were morally lacking whether for bastardy, cuckolding or wife beating. Women were called upon to ensure that their men folk were satisfactorily carrying out their duties and this gave them a certain amount of power. For example, if men were neglecting their religious duties as head of the household, a wife could 'take-over' this part of his role even if he was in objection to this. This expectation of wives by religious authorities severely compromised the obedient social position they otherwise instructed her to take. Houlbrooke, The English Family, (London, 1984), p.97. B. Capp, 'The Double Standard Revisited: Plebeian Women and Male Sexual Reputation In Early Modern England', Past and Present, 162, (1999), p. 73. B. Capp, 'The Double Standard Revisited, Past and Present, 162, (1999), p. 74. Laura Gowing, Women, Words and Sex, (Oxford, 1998), p.67. Houlbrooke, The English Family, (London, 1984), p.98. The domestic sphere, often seen as the sight of women's inequality, was infact the one area of society over which women had a lot of control. Rule over servants and the running of a complex household were just some of the tasks women were expected to perform. Other tasks included care and religious education of children as well as total authority in the field of pregnancy and childbirth; 'lay ins' gave women an all female space in which to recover after childbirth. Women were also expected to give their husbands sound business advice and make decisions within the domestic sphere. The fact that these tasks could not be performed without being domineering, active and authoritarian was why a necessary gap existed between theory and practice in the social position of women. Amussen, An Ordered Society, (New York, 1988), p.107. To conclude, I have tried to show that even men and women attempting to keep within their official social positions found it an impossible task. I have looked at some of the factors that made this impossible; including the economic realities which meant women's roles had to overlap with men's and forced them beyond the passive, submissive boundaries of their role. There were also personal factors, such as the natural familiarity which grows as a result of married life which made strict adherence to the master, subservient relationship between couples hard to maintain strictly. Finally, there were areas of society in which women held most if not all power, these areas meant that women by necessity had authority which contradicted the submissive position intended for them, as well as giving women power which they could use to renegotiate their limited position. Amussen, An Ordered Society, (New York, 1988), p.109. Susan Amussen argues that the attempts of women to remain faithful to their prescribed social position, but failure due to the impractical, unrealistic nature of the position, is shown by women's emphasis on the sexual conduct part of the role; the part that was actually achievable. Faced with the impossibility of the obedience part, women placed greater importance on strict sexual conduct. Where possible women were willing to adhere to their prescribed social positions; helping to maintain the social order so important to early modern society. As Bernard Capp argues; women did not directly challenge the system, rather they worked within it, finding compromises to suit their economic, and personal realities. They also accommodated the features such as commanding, and decision making which went against their prescribed temperament but which were necessary for the domestic success also expected of them. The gap between theory and practice in the social position of women in early modern England should be explained in terms of its impractical economic nature and its contradictory and unrealistic expectations. Amussen, An Ordered Society, (New York, 1988), p.122. B. Capp, 'The Double Standard Revisited, Past and Present, 162, (1999), p.99.","As an introduction to the essay question posed a very brief and simplified summary of Habermas's main ideas and theories seems appropriate. Habermas argued that from the late nineteenth century science, technology and industry became interdependent forming a new kind of capitalism. Habermas viewed this interdependence as meaning capitalist growth had no limits because science and technology were the new leading productive forces. This new form of capitalism used a different kind of production and allowed the '...biblical curse of necessary labour,' to be broken 'technologically'. (Habermas, 1970; 53). Originally capitalism had legitimated itself through economic notions of just exchange in the market, however due to the changes within capitalism, the exchange process itself now operated under political control. This new capitalism also required direct legitimation, this time using culture. Habermas argued that this led to a 'depoliticisation' of society, which was legitimated 'by having technology and science also take on the role of an ideology'. (Habermas, 1972; 104). A 'Technocratic consciousness' developed which affected everyone; not simply those repressed by the dominant. Habermas argued that there was no longer a lower class because everyone was now relatively rich. He argued there was no longer an exploited class, since technology was not dependent on labour; people were no longer forced to work, nor were they forcefully repressed in to their social positions. It is this universal, cultural phenomenon which Habermas's work focuses on and which replaces the Marxist questions of class conflict based on economic relations. In this essay I will argue that Habermas retains many aspects of Marxist theory in his work, in terms of some of the concepts he uses as well as the particular area of society he chooses to study. However, his work also includes many changes to orthodox Marxian work including his view on modern capitalist development and his placing of importance on the superstructure rather than the economic base. Perhaps the most apt descriptions of Habermas's work in relation Marxian thought are as a 'positive critique' (Heller in Thompson and Held, 1982; 23) or a 'positive revision' (Layder, 1994; 187). I will argue that Habermas's ideal speech situation as a means of emancipation, though not without some merit, seems deeply flawed as the general, universal, widely used and practical system it was intended. 'While severely critical of Marxism, Habermas does not reject it all. He uses aspects of it to construct his own synthetic model, drawing on many other sources in the process'. (Layder, 1994; 187). The type of theory which Habermas chooses to study is itself of Marxian trend, 'Habermas remains 'committed to traditional... Marxist ideas...he deals in systematic theory - the grand and 'totalising' theory that was rejected by Foucault.' (Layder, 1994; 188). Though both theorists have been criticised for their 'utopian' tendencies, Habermas seems to have believed that this type of theory was necessary to 'the immensely significant general understanding of social trends and developmental processes in critical theory. '(Layder, 1994; 187). Like Marx, Habermas was looking to identify the 'big problems' in society, suggesting that it was specific ways in which society functioned which was behind its troubles. In this way both Marx and Habermas belong to that school of sociologists who try to 'form epochal diagnosis', identifying causes and characteristics of the 'current crises': exploitation, bureaucratisation, secularisation, anomie, etc. (Chernilo, Lecture, 2004). Also like Marx, Habermas maintains the notion of praxis '...that is, a blending of theory and action, or the use of theory to stimulate action and vice versa'. (J. Turner, 1991; 255). Both theorists used simple presentation to explain society and what was wrong with it, but both also'...wanted theory to expose oppression and to propose less constrictive options.' (Turner, 1991; 255). Marx believed society would be 'cured' after the revolution of the working classes, Habermas too offered diagnosis for the crisis he saw in society; first identifying the problem as technocratic consciousness, before offering the solution of ideal speech situation. Habermas's work echoes the emancipatory theme within Marxist thought; in both there exists the respective victims who need to be freed. Throughout both their theories there is the idea of repression (though in different forms) of society, which must be lifted (though by different means) in order for society to operate in equality. In both theories the definite need to change society is an inherent part of the diagnosis made for improvement. Habermas's focus on the relation of economic workings to the population, and the way this economic system is legitimated, mirrors the area of sociology which Marx chose to study. In addition, two of Marx's main concepts are maintained in Habermas; that of class conflict, although the classes Habermas defines are very different to those Marx identified; and that of ideology, although Marx's idea of the ruling class using classical economics to oppress a working class is drastically altered under Habermas. As I mentioned in my introduction, one description of Habermas's work in relation to Marx's is as a positive critique, and indeed Habermas believed that '... Marx's analysis needed drastic revision'. (Turner, 1991; 255). Habermas argued that although Marx's analysis of economics and the class structure was perfectly applicable to the era he studied, it no longer held any accuracy for a modern society. Not only did Habermas reject the idea of a working class forced in to labour by a deceitful ruling class, but he rejected the idea that change could be brought about by economics. Habermas argued that the emergent properties of capitalism were cultural not material, and therefore argued that cultural change was necessary for more general change to occur. He also argued that Marx's predictions for the advance and eventual fall of capitalism were inaccurate; instead explaining that a new and constantly developing capitalism had emerged through the interdependence of technology and science. Thus the economic base was no longer more important than the superstructure (as had been the case with market capitalism). Habermas rejected many of the more 'romantic' features of Marx; his naïve optimism about the inevitability of class revolt, and of class consciousness. (Thompson and Held, 1982; 22). The ideal speech situation was Habermas's suggestion for emancipation from the technocratic consciousness repressing society, freezing relations of domination and perpetuating and legitimating them through positivism. Technocratic consciousness was a result of the technological / scientific capitalism of modern society. It was an ideology which mirrored the values of this capitalism; positivism, empiricism etc, it had one main interest; prediction. (Outhwaite, 1994; 13).This ideology, though repressive because it ought to only be applied to cells and microbes not subjective humans, was not delusional as was the ideology of Marx's time. It kept the masses under control, whilst only treating surface issues rather than the underlying problems; i.e. the depressed housewife feels lonely and unfulfilled with only a baby for company. However instead of identifying the problem as one of her limited and prescribed position in society falling short of her capabilities and desires, she identifies it as a medical / scientific problem, and treats it with Prozac. The problem which the ideal speech situation was set out to solve was this cycle of technocratic consciousness, never getting to the root of a problem, because you can not identify the problem other than in the medical terms such an ideology allows. (Habermas, 1971; 86). The ideal speech situation devised by Habermas was intended to make people realise firstly that they were infact following an ideology, then to help them think outside it in a less positivistic way. He suggested that this could only happen in a doctor / patient type of situation, where outside influences are shut out and the patient is the ultimate arbiter of how accurate the analyst is. This situation should be transferred over in to social situations, thus providing everyday chances for everyday people to look beyond technocratic consciousness. (Outhwaite, 1994; 54). This suggestion was met with heavy criticism; it was argued that not everyone (for example our housewife with the small child) could achieve such a situation. Likewise it was argued that even if this kind of situation were to be achieved, the patient may believe to know their underlying problems, but infact be deluded. Finally, it was put forward that this theory, even if it worked, was dubious as an emancipatory salvation. (Outhwaite, 1994; 55). In reaction to such criticism Habermas revised his suggestion; arguing that if there was a group of people present they could all arbitrate each other and the eventual result would be the lifting of technocratic consciousness and the fall of this technological capitalism. However, the circular potentials of this were not lost on Habermas's critics; not only was this a harder situation to achieve, but it could simply result in deluded people deluding each other further. Only in an already emancipated society could critical reflection produce truth. (Layder, 1994; 189). Yet Habermas still maintained that the ideal speech situation was necessary to arrive at the truth. Habermas explained that this ideal speech situation would begin with intellectuals, who were emancipated through education, providing the model towards which education might lead society as a whole. Whether this situation would lead to emancipation, whether intellectuals are infact emancipated (those controlling education usually control dominant ideologies), and whether this transformation of the micro to the macro, would in reality be achievable, has been the subject of much heated debate. There are three key arguments posed by sociologists about the ideal speech situation and how convincing it is as a means of emancipation, other criticism of Habermas's theory in general (such as his theory of modern capitalism) must be left for another essay. The first, accepts that if it were possible for large numbers of people to experience such a situation then it might well prove, at least in part, to play an important emancipatory role. However, it seems doubtful that such a situation would ever be within the grasp of most ordinary people. Both the actual practicality of discussing matters in a group, and discussing them in a situation free from the culture and ideology the group has been socialised in to, seems to be questioned. (A. Heller in Thompson and Held, 1982; 32). The second key criticism is posed by Ernst Tugendhat; 'to what extent can one sustain the claim that...truth and validity can be found in language and discourse.' (Tugendhat in Rasmussen, 1991; 74). Habermas's idea that emancipation from oppression can come from education and simple discussion in the presence of others seems a little optimistic. As Tugendhat points out, there is a certain tension between Habermas's 'theory of modernity and the philosophy of language' which throws doubt over the ability of his ideal speech situation to result in liberation. (Tugendhat cited in Rasmussen, 1991; 75). The final key criticism is that Habermas's theory is simply too Utopianist in style to legitimate it as a convincing means of emancipation. Both his method of reaching freedom, and his reason for wanting it are criticised as too idealistic and too big a narrative. Sociologists such as Paul Connerton argue that although as part of a suggestion for change the ideal speech situation has some merit; as the sole 'answer' to what Habermas identifies as the 'big problem' of society it is unrealistic. (Connerton, 1970; 353). Indeed this is a criticism levelled against, many of the social theorists, for though Habermas saw general theory of society as a necessity, others argue that it is unhelpful in terms of diagnosis for problems. To conclude, I initially looked at the similarities and dissimilarities between Marx and Habermas, and although many likenesses are apparent in Habermas's work, such as his use of key concepts and emancipatory theme, I would not go as far to label him a Marxist. Instead Habermas resides firmly in the Marxist revisionist camp, 'abandoning or relegating' certain Marxist principles. (Outhwaite, 1994; 4). As a means of emancipation the ideal speech situation does appear to have some merit such as the idea of using culture to force through change. And if considered as a (very) long term solution this suggestion does seem more practical than I originally thought. However, as a realistic emancipation from the kind of all encompassing, universal ideology legitimating not only the continuation of technological capitalism but also the depoliticisation of the population, I think it falls short. As Tugendhat points out; the idea that freedom is simply a matter of ideal speech in the right situation is problematic in itself, even despite the further problems such as its circular tendencies of reproducing delusions already held, and the impracticalities of all people having access to the right kind of situation. To finally answer the question, it seems that Habermas, although still within the broad Marxian trend is more a Marxist revisionist than a strict follower of Marx. Habermas's ideal speech situation does seem plausible in some cases; it seems as though it could work to emancipate some people. However when posed as a means of freedom from the kind of ideology Habermas identifies, it seems less convincing.",False
50,"By 1930 Stalin had not only managed to gain extensive personal control and eliminated all his opponents but had also managed to consolidate this power. Accountable to very few Stalin was free to pursue any policy largely unchecked. He had support from the party rank and file as well as from the central committee and the Politburo. How Stalin, the least obvious of Lenin's potential successors, came to be in this position is a much contested issue. With his death Lenin left a vacuum within the party leadership creating economic, social and political uncertainty. This in turn led to a left / right divide within the Politburo over the next steps the Bolsheviks should take, with Trotsky on the left arguing for rapid industrialisation. Stalin maintained a generally moderate position before siding with Kamenev and Zinoviev forming a powerful triumvate against the man they saw as their biggest rival. After Trotsky's eventual expulsion from the party, and then the country, came the political execution of Kamenev and Zinoviev followed by their actual execution. Stalin's other main rivals faded away either in to political destruction, suicide, execution or expulsion. A. Nove and C. Read, Stalin: Terror and Transformations, video. Nove and Read, Stalin: Terror and Transformations, video. R. Hingley, Stalin; Man and Legend, (London, 1974), ch7, p.3. I find it hard to justify putting Stalin's rise to power down to any one single factor. It does not seem possible that Stalin reached his position purely due to his own manipulation or political cunning. Nor does it seem realistic that he was put in power purely because he was the politician that most reflected the party rank and file wishes. A number of reasons seems the most probable explanation; I will argue that the most important are as follows. Stalin proved a useful asset to Lenin and as a result Lenin often supported Stalin's appointments and promotions. The positions Stalin held within the party were of vital importance in helping him gain power, his personality and his political prowess were also very important in this respect. The mistakes of Stalin's rivals during the 1920's were to prove fatal and helped him in his ascendancy to power. Many of the opportunities which led to Stalin's rise to power were actually accumulated whilst Lenin was still alive. Infact on many occasions it was Lenin that endorsed Stalin's promotion to important positions within the party. Stalin fitted the proletariat ideal lacking in so many of the profiles of the Bolshevik leaders including Lenin's own, Stalin was singled out by Lenin as having potential. His activities to raise funds for the party included bank robbing, and he was one of those 'hard core' Bolsheviks who had remained in Russia '...underground during the winter 1905 -6 repression and persecution of revolutionaries by the Tsar."" This mixture of loyalty and practicality was one Lenin could not resist, he began to rely on Stalin who proved a useful asset. 'He was an unquestioningly loyal person who actually got things done... Stalin was Lenin's political fixer...packing the congress with his supporters'. Lenin supported and approved many of the positions Stalin was appointed to within the party. Although Trotsky claimed that after Stalin's appointment as party General Secretary Lenin expressed concern; '...this cook will serve only peppery dishes', he did not seem sufficiently concerned to raise public objection until very near his death when it was too late. Indeed, prior to his appointment as General Secretary Stalin already held positions of great importance; immediately after the civil war he was Commissar of Nationalities, Commissar of the Workers and Peasants Inspectorate and a member of the Politburo. 'Several of Stalin's crucial promotions - cooption to the central committee and appointment as Pravda editor in 1912 and as General Secretary in 1922- were brought about by Lenin's influence.' L. Deutscher, Stalin; A Political Biography, (Oxford, 1949), p228. Walter Duranty, Stalin and Co. The Politburo; The Men who Ran Russia, (London, 1949), p.10. Christopher Read, The Making and Breaking of the Soviet System, (Hampshire, 2001), p.84. Read, The Making and Breaking of the Soviet System, (Hampshire, 2001), p.85. Deutscher, Stalin; A Political Biography, (Oxford, 1949), p.232. Deutscher, Stalin; A Political Biography, (Oxford, 1949), p.228. Read, The Making and Breaking of the Soviet System, (Hampshire, 2001), p.84. Stalin has been described as 'the spider in the web' in terms of his positioning within both the state and the party apparatus. He had influence within all the main offices, and the posts he held gave him direct authority or at the very least powers of persuasion over a very useful range of contacts. At the time of Lenin's death when the struggle for power was fought out amongst the Politburo members, Stalin had power within the Politburo, (the highest decision making body), the Orgburo, (the administrative bureau of the central committee), the Robkrin and the Sovarkom, as well as in the regional and local party cells and the regional and local soviets. The very nature of the centralised organisation of the party and the state worked to Stalin's advantage due to the links between all the offices. Though all his positions gave him important influence, it was his position as General Secretary which allowed him to build up the kind of power base which defeated his rivals. In this post Stalin was responsible for Politburo administration including the agenda for discussion, Stalin was the link between the Politburo and the Central Committee, he also controlled the '...appointment and promotion of individuals to key posts throughout the country."" As Edward Acton points out, this meant that 'the careers of party officials were becoming increasingly dependent upon their loyalty to the Secretariat."" This in turn gave the '...central administrative organs enormous influence over the make-up of party Congresses and thus of elections to the Central Committee and the Politburo itself'. Acton maintains that this 'movement of power from the Politburo to the Secretariat', was initially '...masked under Lenin's authority over the Secretariat."" However Deutscher claims that on Lenin's return from the recovery of his first stroke he sensed the increasing power of the bureaucracy with Stalin at its head, and immediately attempted (though unsuccessfully) to rectify the situation. The position of General Secretary had been largely overlooked as a source of power due to its administrative nature; '...the bright sparks of the Politburo felt themselves above such roles."" However Stalin was willing to take on this shunted position, and was good at it; he worked hard, he was reliable, made no fuss and needed little assistance from the other Politburo members. He converted secondary administrative power in to political power, using the powerful self perpetuating nature of the vested interests within the bureaucracy. Christopher Read, Lecture, University of Warwick, January 2004. Edward Acton, Russia, The Tsarist and Soviet Legacy, (Essex, 1995), p.197. Acton, Russia, The Tsarist and Soviet Legacy, (Essex, 1995), p.196. Acton, Russia, The Tsarist and Soviet Legacy, (Essex, 1995), p.197. Acton, Russia, The Tsarist and Soviet Legacy, (Essex, 1995), p.197. Acton, Russia, The Tsarist and Soviet Legacy, (Essex, 1995), p.198. Deutscher, Stalin; A Political Biography, (Oxford, 1949), p.247. Robert Tucker, Stalin as Revolutionary 1879 - 1929; A Study In History and Personality, (London, 1974), p.322. Deutscher, Stalin; A Political Biography, (Oxford, 1949), p.233. Idea of secondary administrative power; Read, The Making and Breaking of the Soviet System, (Hampshire, 2001), p.52. Idea of perpetuating vested interests; Hingley, Stalin; Man and Legend, (London, 1974), ch7, p5. Stalin's personality and his political manoeuvring and prowess have been interpreted in a number of ways. As evil, pre-planned, ruthless, manipulation, but also as a highly skilful reaction to economic, social and political situations. Which ever interpretation bears more truth, that Stalin's politics proved enormously valuable to him during the 1920's is undoubtable. The moderate position Stalin seemed to take during Politburo meetings both whilst Lenin was alive and after his death allowed him several advantages. It meant that he 'always seemed to follow others, never to direct them', making Trotsky, who was very active during meetings, seem the biggest threat to power. 'No one could have behaved less like a tyrant or dictator than did Stalin at Politburo meetings'. This moderate position also allowed Stalin to remain 'silent during debates...intervening only to support the majority view...giving the impression of one whose will always prevailed in the end."" Although this may be a rather cynical interpretation of Stalin's political stance during meetings, it did allow him the luxury of supporting which ever people he wished because his views were not an open matter. It also kept him from the trap he used against so many of his rivals; that of criticising the now revered Lenin. Stalin had few political inspirations of his own, but by taking the safe and unremarkable route of following Lenin he could undermine enemies. Hingley, Stalin; Man and Legend, (London, 1974), ch7, p.2. Bazhanov's memoirs cited in Hingley, Stalin; Man and Legend, (London, 1974), ch7, p.3. Hingley, Stalin; Man and Legend, (London, 1974), ch7, p.4. By placing himself in the position of Lenin's 'chief mourner', persuading the party to have Lenin's embalmed body on public show, and playing an active role at Lenin's funeral whilst Trotsky was absent, Stalin helped to create a cult around Lenin which he was at the head of. Stalin presented his views as the legacy of Lenin, following his wishes and therefore justified in whatever action he took. Stalin's work to interpret Leninism for the working man, simplifying it and explaining it, helped him present his brand of Leninism as the only one with authenticity. This initiative to enlighten the rank and file of the party was not a solitary one; Stalin seemed to pay attention to the rank and file wishes, as well as understanding how important their support could be. It has often been argued that Stalin's political actions, and some of the apparently contradictory moves he made, closely reflected the views of the rank and file members. The recognition of the importance of lower ranking Bolsheviks gave Stalin the advantage of gaining their support where other potential leaders lacked it. Acton, Russia, The Tsarist and Soviet Legacy, (Essex, 1995), p.193. Tucker, Stalin as Revolutionary 1879 - 1929, (London, 1974), p.319. Sheila Fitzpatrick, Stalinism; New Directions, (London, 2000), p.4. In terms of the political stealth Stalin demonstrated during the 1920's leadership struggle, the battle between Trotsky and Stalin is a revealing example. In particular the contrast between their political styles is interesting; Stalin a more typical politician tentatively weighing up policies and seeking out support; Trotsky a rather superior, aloof character, forthright with his often unpopular views on policy, and expecting support to find him. Stalin seems to have exploited the fears of the other Politburo members that Trotsky remained their biggest threat in terms of personal power. It seems doubtful that his motivations for the alliance with Kamenev and Zinoviev were based on anything more than a wish to remove Trotsky from the leadership race. The seeking out of this alliance created a triumvate which proved very powerful against the already unpopular Trotsky, and all under the cover of policy disagreement. With this kind of backing behind him Stalin 'carried battle in to the wider field of the central committee where he could always secure a majority."" Trotsky's potential support was either alienated by his brusque manner, or undermined by Stalin; the committee was reminded that he was a previous Menshevik, and of the many disagreements he had openly had with Lenin. Stalin often used the disagreements and arguments of the past, out of context, as a tool against his rivals. Trotsky was pitched against the Stalin, Zinoviev, and Kamenev majority as the 'other' rather than as a legitimate opponent with a valid argument. Stalin was able to use Lenin's ban on factions to make punishable Trotsky's attempts to argue his case, and at every chance undermined him; however when the committee called for action to be taken against Trotsky, Stalin defended him and bided his time. Infact throughout the battle with Trotsky, Stalin was careful to leave the heavy criticism to Zinoviev and Kamenev, again preserving the image of the moderate amongst the irrational that had proved so useful in the past. Deutscher, Stalin; A Political Biography, (Oxford, 1949), p.233. Duranty, Stalin and Co. , (London, 1949), p.9. Hingley, Stalin; Man and Legend, (London, 1974), ch7, p.5. Hingley, Stalin; Man and Legend, (London, 1974), ch7, p.6. Hingley, Stalin; Man and Legend, (London, 1974), ch7, p.5. The final factor in Stalin's rise to power involves the mistakes made by Stalin's opponents during this time. Perhaps one of the most important and most fatal mistakes was the underestimation of Stalin by his rivals; this underestimation in turn led on to other mistakes which were to seal the fates of Stalin's opposition. It was underestimation of Stalin as a worthy and threatening rival which led Kamenev and Zinoviev in to an alliance against Trotsky who they viewed as the most potent danger. 'Comrade card index' in his administrative tasks was never identified as a real problem. It was Kamenev and Zinoviev that endorsed Stalin's position as General Secretary despite knowledge of Lenin's uncertainty about the suitability of the capricious Georgian for this post. Whilst Stalin's fate hung in the balance, Zinoviev and Kamenev argued against the public reading of Lenin's testament, in effect saving Stalin from disgrace and investigation into his conduct as General Secretary. Trotsky made several key faults in his battle with Stalin, undermining himself and loosing himself support. He criticised party bureaucracy and the careerists who accompanied it; in effect criticising and threatening the positions of the very people from whom he required support. Stalin's other opponents made mistakes which Stalin with his 'peasant shrewdness' could exploit. The united opposition was such an unconvincing union of the former enemies Trotsky, Kamenev and Zinoviev that it was not only bound to fail, but bound to rouse little support from the party. These men were all politically broken from previous rounds with Stalin, and could provide little protection to any potential supporters from the increasingly ruthless tactics employed by Stalin and his followers. Likewise, the shaky union made between Zinoviev, Kamenev and Bukharin opposing Stalin's policy of rapid industrialisation and collectivisation, was almost inevitably discovered by Stalin and destroyed through utilisation of the ban on factions rule. Nove and Read, Stalin: Terror and Transformations, video. Read, The Making and Breaking of the Soviet System, (Hampshire, 2001), p.81. Deutscher, Stalin; A Political Biography, (Oxford, 1949), p.233. Read, The Making and Breaking of the Soviet System, (Hampshire, 2001), p.90. Hingley, Stalin; Man and Legend, (London, 1974), ch7, p.5. To conclude, I have identified four main factors which I have argued played the biggest part in Stalin's rise to power during the 1920's. The acceptance and support Stalin received from Lenin played an important part in allowing Stalin the possibility of playing any kind of key position within the party. The positions Stalin held permitted him great influence within a large number of party areas undetected. He was able to build up a support base and eliminate opposition; turning administrative power into political power. Stalin's personal gift at politics gave him an advantage during the uncertainty after Lenin's death. Stalin defeated those he needed to, he was a competent politician and was able to manipulate situations to his advantage in a way his rivals could not. Finally, the mistakes his opposition made, especially their failure to act upon Lenin's wishes to have Stalin removed from his post, and indeed the very uncertainty in which Lenin left the party leadership, allowed Stalin the chances which led to his rise. Read, The Making and Breaking of the Soviet System, (Hampshire, 2001), p.52. I have not argued that Stalin was a passive actor during his rise to power in the 1920's. He was an active force, using cunning, manipulation, and politics to engineer his position. However, he could not have done this without the other factors I have mentioned in this essay. All of them were important and I do not think Stalin would have emerged as leader of the party if any one was missing. The assumption that a gap existed between the theory and the practice in the social position of women between 1500 and 1700 is a contested one. Margaret Ezell argues that the expectations of female behaviour and duty in society did infact closely match the reality of women's experiences. Looking at contemporary literature and popular opinion, Ezell concludes that contrary to many historians who view the prescribed position as restrictive and misogynistic, both scripture and conduct books infact highlighted female capabilities and praised women as partners in life. 'Differences between men and women are merely superficial...women are as naturally capable as men of reason, wit and...government."" However, feminists have criticised Ezell for her limited interpretation of the literature she looks at. They argue that advice such as '...it is not that the wife has no mind of her own, but that she deliberately alters it if necessary, to conform with her husbands,' does not counter the view that women were expected to be submissive and were considered inferior to men during this period. Even Lawrence Stone, who argues that patriarchy was at its strongest and its most widely accepted at this time, acknowledges that the role set out for women often differed markedly from the actual experiences of womanhood, due mainly to the fact that the prescribed role was simply incompatible with the realities of life, especially for the poor. Indeed, Ralph Houlbrooke argues that the '...theory was simply unworkable...' with the realities faced by contemporary women. Pierre LeMoyne, 'The Gallery of Heroick Women', cited in M. Ezell, The Patriarch's Wife: Literary Evidence and the History of the Family, (New York, 1987), p.44. 'Feminist school of Phallogocentrism' who criticise Ezell, cited in Ezell, The Patriarch's Wife, (New York, 1987), p.37. Ezell, The Patriarch's Wife, (New York, 1987), p.40. L. Stone, The Family, Sex and Marriage in England 1500 - 1800, (Middlesex, 1977), p.139. Ralph Houlbrooke, The English Family 1450 - 1700, (London, 1984), p.106. In this essay I am going to argue from a standpoint assuming that a gap between the theory and the practice in the social position of women in early modern England did infact exist. I will argue that this gap existed despite both male and female attempts to keep within it, due both to its unrealistic expectations compared with the actual lives women led, and the dualities and contradictions within it. I will argue that due to three broad and overlapping areas: economic and personal factors, and a contradiction within the role which expected servility but also demanded command and power, women could not live out their lives within the boundaries of their expected social positions. The position women were to occupy in society was set out to the population through a number of mediums. Scriptural references in homilies and religious services put emphasis upon '...the procreation and religious education of children, the regulation of sexual activity, and mutual comfort and support.' A distinction was made between suitable female and male duties; 'The husband was held to be the superior partner, the wife the subordinate and inferior...her foremost duty was obedience."" In the advice guides published, women were seen to be 'material and passive...weak in body...and unfitted for work or public life.' As such they were expected to remain in the private sphere, '...unquestioningly obedient to her husband, accepting husbandly reproofs meekly...bearing her husbands faults...accommodating his various moods and on any breach being the first to seek reconciliation."" Satires and other anti-female publications played upon the popular images of women as sexually insatiable, nagging scolds, as well as playing upon the images of marriage as an end to freedom and fun, and a life of supporting a wife who would stop at nothing to get her own way. Whether popular opinion about women was more influenced by religion and advice guides or by satires is hard to tell, and indeed the opinion of the female role and marriage differed between men and women and according to place in the social order. It is probable that popular opinion took on board a mixture of both views as well as referring to personal experience of female figures and their behaviour. Finally, the expected social position of women was expressed by the authorities through the law; 'nowhere did the wife appear more completely subordinate to her husband than in the common law'. Women were never a homogeneous group; the law and indeed general opinion differed with different women; single, married, widowed and spinsters. However, in the eyes of the law, a woman belonged to the man in charge of her whether that was a father, brother or husband. Widows enjoyed a certain amount of freedom, however along with spinsters were so marginalised socially, for being 'master-less', and economically, with only a few poorly paid trades to work in, that they had very hard lives. Already, with this simple description of the expected social position of women it is possible to see some of the contradictions within it; for example, that women were sexually uncontrollable, yet the moral guardians of society. Susan Amussen argues that this contradiction is one of the main reasons why the wish for '...clear subordination of wives to husbands...was never fully realised.' Houlbrooke, The English Family, (London, 1984), p.97. Houlbrooke, The English Family, (London, 1984), p.97. Ezell, The Patriarch's Wife, (New York, 1987), p37. Martin Ingram, Church Courts, Sex and Marriage in England 1570 - 1640, (Cambridge, 1987), p.129. Houlbrooke, The English Family, (London, 1984), p.100. Susan Dwyer Amussen, An Ordered Society: Gender and Class in Early Modern England, (New York, 1988), p.115. Amussen, An Ordered Society, (New York, 1988), p.115. The gap between theory and practice in the social position of women in early modern England was certainly influenced by the economic realities they faced in their everyday lives. It was often necessary for poorer women to marry quite late; the time before marriage was spent working in service adding to the family purse. Girls had to leave home and live away from their families often giving them a taste for relative freedom, economic independence and a knowledge of their personal capabilities. Even after marriage it was often necessary for women to make economic contributions to the family; supplementing the low wages her husband was paid. Keith Wrightson argues that these female supplements to male wages were crucial to survival even if they did somewhat undermine the official female position in society. Susan Amussen points out that the gap between theory and practice actually lessened depending on the importance of the economic contributions of a wife. Some women did not need to compromise the submissive, passive role by economic contributions such as bartering at market or looking after the estate whilst a husband was absent. Although women rarely strayed in to their husbands' role once he had returned, it was essential to economic survival that they could and would perform his tasks even if this meant female and domination and authority. Men wanted their wives to be '...both subordinate and competent...and willing to accept that whatever a man did was work and whatever a woman did was her duty'. 'Women's independence and autonomy were critical to their success as wives and mothers...the contradictions between women's economic roles and their expected subordination were so severe that they posed a problem to the most carefully conforming wife.' Keith Wrightson, Earthly Necessities: Economic Lives in Early Modern Britain, 1470 - 1750, (London, 2002), p.53. Amussen, An Ordered Society, (New York, 1988), p.120. Houlbrooke, The English Family, (London, 1984), p.107. Anthony Fletcher, Gender, Sex and Subordination in England 1500 - 1800, (London, 1995), p.254. Amussen, An Ordered Society, (New York, 1988), p.121. Women were often needed in the public sphere, and even if not playing leading roles their actions still conflicted with the domesticated submission part of the role, although perhaps fulfilling the obedience part. Diane Willen argues that pauper women were actively recruited by local authorities in to the public arena. Although the work women were doing for local authorities was an extension of their traditional domestic duties; cleaning and caring, 'it questions...the existence of separate private/public spheres in the early modern period'. Both the poor women being employed and the 'local patriarchal political authorities' employing them gained economically from this arrangement. Women could earn a small wage and authorities gained a willing, cheap labour force; both parties were forced to revise women's official social position for economic gain. Widows were also women with access to the public sphere; they could by law take over some of their husband's trades or estates upon his death. Although often poorer widows were left very economically vulnerable, some were able to maintain themselves successfully; indeed advice guides warned men against marrying widows as they tended to be 'harder to tame' having already tasted and achieved economic independence. Diane Willen, 'Women in the Public Sphere in Early Modern England: The Case of the Urban Working Poor', Sixteenth Century Journal, 19, (1988), p. 559 - 575. Willen, 'Women in the Public Sphere in Early Modern England', Sixteenth Century Journal, 19, (1988), p.559. Willen, 'Women in the Public Sphere in Early Modern England', Sixteenth Century Journal, 19, (1988), p.560. Anthony Fletcher, Gender, Sex and Subordination in England 1500 - 1800, (London, 1995), p.174. The second set of factors which made the social position set out for women in early modern England impossible to fulfil even if they tried, were personal factors. By this I mean factors which come about naturally as a consequence of marriage; i.e. spending a lot of time with someone, growing to know them very well and loving them. Love and caring were seen as very important aspects of a successful marriage, in particular they were seen as part of a husband's duty. However it was well documented in contemporary literature that '...more men betrayed their command through their own fondness than ever lost it through their wives rebellion'. Though caring and affection were viewed with importance; and were often an unavoidable outcome of spending lives together, measures were taken to limit the influence affection had on the social position of women. 'Convention demanded that the wife address her husband with humility and deference...even the affectionate terms love, joy, dear and duck were not appropriate in the mouth of a subordinate."" However it must have been very difficult to maintain a purely master/subordinate relationship especially when feelings of love and affection existed in a marriage. Even the most obedient wife and most masterful husband must have come across occasions when such formal address would have been inappropriate. Thomas Fuller cited in Houlbrooke, The English Family, (London, 1984), p.101. Houlbrooke, The English Family, (London, 1984), p.101. Laura Gowing argues that the gap between theory and practice actually began during courtship, making it only natural that it continue after marriage. Gowing explains that it was sometimes necessary to deviate from the passive, submissive role of a courting woman. The double standard which existed in terms of male and female sexual morality and the importance of reputation in early modern society made courting quite a difficult process. Women had to use their intricate knowledge of the laws of courting to avoid the many traps which could leave them tied to a poor marriage match or with their reputation in tatters. Often it was impossible for a woman to remain passive during courting, especially if conjugal rights had been granted to a man after the promise of marriage, or pushy parents were disdainful of certain suitors. Often with a little bit of courage and forcefulness, men could be made to keep promises, and by scrutinising the exact wording used in the making of a marriage contract they could be escaped. 'Women could still exercise their power of evasion and refusal...language...gave women the opportunity to broaden the largely passive part allotted to them in the process of courtship.' Laura Gowing, Women, Words and Sex in Early Modern London, (Oxford, 1998), p.147. Laura Gowing, Women, Words and Sex, (Oxford, 1998), p.148. Other personal factors made the female social position hard to follow strictly; in many cases women's personal capabilities and desires surpassed the prescribed role. Often individual character and temperament sat at odds with the boundaries set out; after all, not all wives were inferior to their husbands and not all husbands' wanted/needed submissive wives. Simple practicalities such as very little age difference between partners, made the expected master/servant relationship, hard to fulfil; a woman with equal or more life experience than her husband could be hard to commandeer. Finally, women's roles and identities within society were not a constant thing, they were much less fixed than men's; as they were already malleable, it was relatively easy to simply revise these identities and social positions further. Houlbrooke, The English Family, (London, 1984), p.103. Laura Gowing, Women, Words and Sex, (Oxford, 1998), p.231. The final aspect which made it impossible for women to live wholly within their official social position, were the areas of society over which women had complete power and control; morality, reputation and of course the domestic sphere. In marriage and out of it women were to control the sexual activity of themselves and their spouses; adultery was seen as dangerous, putting the established social order in jeopardy, and even too much sex between husband and wife was viewed negatively. Women's control over reputation gave them a certain amount of power over both other women and men; as Samuel Pepys diary shows '...when his wife caught him with their maid...he at once recognised the political significance of the event'. Women could wield the reputation card to expand their limited role and position in society. Gossiping was another largely female activity allowing them to channel pressure on those they felt were morally lacking whether for bastardy, cuckolding or wife beating. Women were called upon to ensure that their men folk were satisfactorily carrying out their duties and this gave them a certain amount of power. For example, if men were neglecting their religious duties as head of the household, a wife could 'take-over' this part of his role even if he was in objection to this. This expectation of wives by religious authorities severely compromised the obedient social position they otherwise instructed her to take. Houlbrooke, The English Family, (London, 1984), p.97. B. Capp, 'The Double Standard Revisited: Plebeian Women and Male Sexual Reputation In Early Modern England', Past and Present, 162, (1999), p. 73. B. Capp, 'The Double Standard Revisited, Past and Present, 162, (1999), p. 74. Laura Gowing, Women, Words and Sex, (Oxford, 1998), p.67. Houlbrooke, The English Family, (London, 1984), p.98. The domestic sphere, often seen as the sight of women's inequality, was infact the one area of society over which women had a lot of control. Rule over servants and the running of a complex household were just some of the tasks women were expected to perform. Other tasks included care and religious education of children as well as total authority in the field of pregnancy and childbirth; 'lay ins' gave women an all female space in which to recover after childbirth. Women were also expected to give their husbands sound business advice and make decisions within the domestic sphere. The fact that these tasks could not be performed without being domineering, active and authoritarian was why a necessary gap existed between theory and practice in the social position of women. Amussen, An Ordered Society, (New York, 1988), p.107. To conclude, I have tried to show that even men and women attempting to keep within their official social positions found it an impossible task. I have looked at some of the factors that made this impossible; including the economic realities which meant women's roles had to overlap with men's and forced them beyond the passive, submissive boundaries of their role. There were also personal factors, such as the natural familiarity which grows as a result of married life which made strict adherence to the master, subservient relationship between couples hard to maintain strictly. Finally, there were areas of society in which women held most if not all power, these areas meant that women by necessity had authority which contradicted the submissive position intended for them, as well as giving women power which they could use to renegotiate their limited position. Amussen, An Ordered Society, (New York, 1988), p.109. Susan Amussen argues that the attempts of women to remain faithful to their prescribed social position, but failure due to the impractical, unrealistic nature of the position, is shown by women's emphasis on the sexual conduct part of the role; the part that was actually achievable. Faced with the impossibility of the obedience part, women placed greater importance on strict sexual conduct. Where possible women were willing to adhere to their prescribed social positions; helping to maintain the social order so important to early modern society. As Bernard Capp argues; women did not directly challenge the system, rather they worked within it, finding compromises to suit their economic, and personal realities. They also accommodated the features such as commanding, and decision making which went against their prescribed temperament but which were necessary for the domestic success also expected of them. The gap between theory and practice in the social position of women in early modern England should be explained in terms of its impractical economic nature and its contradictory and unrealistic expectations. Amussen, An Ordered Society, (New York, 1988), p.122. B. Capp, 'The Double Standard Revisited, Past and Present, 162, (1999), p.99.","The most extensive population study of the early modern period was carried out by E.A. Wrigley and R.S. Schofield, The Population History of England, 1541 - 1871: A Reconstruction, which set out calculated estimates of population trends for this period. They used the method of back-projection to estimate the population levels of the country between 1541 (the beginning of the first decade after the introduction of parish registers) and 1871 (the date of the second reliable census). This project was an immense undertaking which used four hundred and four surviving parish registers which recorded baptism, marriage and burial, to estimate demographic trends at five year intervals between these two dates. Many criticisms of both the methodology used and the results achieved have been made of this study. In very general terms these criticisms focus upon the following main areas. The sample of four hundred and four parish registers used to estimate national population figures makes up only four percent of the total number of parishes in early modern England. In addition, the sample includes too many large parishes (usually Northern areas) and too few smaller parishes (usually Eastern areas). London is also underrepresented in the study due to its lack of surviving parish registers. Even good registers however were inaccurate for certain periods (such as during the civil war) and thus only for the year 1662 were all the registers fit to use. The unreliability of back projection as a technique for working out population levels has been criticised in terms of the assumptions Wrigley and Schofield made in using this method; for example assumptions about the age structure in place throughout this period will have influenced the final results. E.A. Wrigley and R.S. Schofield, The Population History of England, 1541-1871: A Reconstruction, (Cambridge, 1981). Wrigley and Schofield, The Population History of England, p. 15. Wrigley and Schofield, The Population History of England, p. 16. Wrigley and Schofield, The Population History of England, p. 455. John Hatcher, 'Understanding the Population History of England 1450-1750', Past and Present, no.180, (August 2003), p.125. Despite such criticism the information Wrigley and Schofield provide does appear a plausible reading of the demographic trends of early modern England. After all, the authors are aware that methodological shortcomings exist; this work does not pretend to be concrete evidence but merely a likely estimation. Even taking in to consideration the limitations of the evidence and the methodology as well as the limitations of the study itself, i.e. : it operated on a very statistical level and was not placed in much of a social or cultural context, this remains a very useful work. Furthermore the huge financial costs and time commitments involved in this kind of research make it unlikely that another work of this nature will be undertaken. These are the best results we have, and particularly when used in conjunction with localised studies as well as studies of social and cultural factors this is very revealing research. In this essay I am going to argue from a standpoint assuming that the data compiled by Wrigley and Schofield is a realistic estimation of population trends. However, I will argue that an acceptation of the population trends presented by the Cambridge Group does not necessitate an acceptation of their interpretation of the causes of these population trends. I will argue that fertility and mortality were important in influencing population change. I will argue that the essential inter-linkage of these two factors makes any either/or demographical explanation implausible. Instead I will suggest that a model whereby both fertility and the two parts of mortality (disease and starvation) helped shape population and each other is a more realistic explanation to such a complex and wide-spanning phenomenon. Wrigley and Schofield, The Population History of England, p.456. The demographic trends that Wrigley and Schofield have mapped out for early modern England begin with growth between 1541 and 1661; this growth was generally steady although it slowed during the 1550s and became rapid in the 1570s and 1580s. Between 1656 and 1686 the population decreased in size before stagnating and slightly recovering between 1686 and 1750. From 1750 onwards the vigorous growth of earlier periods was resumed, although Elizabethan growth rates were not reached until the 1790s. Thus there were two periods of growth divided by one hundred years or so from the mid seventeenth century to the mid eighteenth century when population numbers stagnated. During these main periods of growth the population increased significantly almost doubling from 2.8 million to 5.3 million between 1541 and 1656, and after the years of stagnation again doubling between 1731 and 1816 and between 1861 and 1871. As I have suggested this basic information seems both plausible and realistic, however Wrigley and Schofield's interpretation of this information; an attempt to understand what caused this rapid growth, check and decline followed by renewed growth, seems less plausible. The principle question they look at is whether '...a rise in GRR (gross reproductive rate) from 2.5 to 3.0 for example is more important than a rise in eo (life expectancy from birth) from 35-40 years in effecting growth rates."" Or in other words, was it fertility or mortality that was the definitive factor in determining these population trends. Roger Schofield, 'The Impact of Scarcity and Plenty on Population Change in England 1541-1871', Journal of Interdisciplinary History, 14:2, (Autumn 1983), p.270. Schofield, 'The Impact of Scarcity and Plenty on Population Change in England', p.269. E.A. Wrigley, People, Cities and Wealth; The Transformation of Traditional Society, (Oxford, 1989), p.243. Wrigley and Schofield, The Population History of England, p.236. The Cambridge Group certainly believed that '...marriage was the hinge upon which the demographic regime turned."" They suggested that '...fertility change was more important than mortality change in altering population growth rates', that '...fertility change in turn chiefly reflected fluctuations in nuptiality' and that '...nuptiality was primarily determined by real wages'. Thus Wrigley and Schofield presented a structure where a rise in real wages would lead to a rise in the number of marriages as well as a rise in the number of early marriages and therefore a rise in the numbers of births. Wrigley and Schofield themselves note the inevitable sacrifice of comprehensiveness with the construction of simple models, however they argue that the benefits gained in clarity are essential to understanding. Therefore the picture of demography that they present for early modern England at this time is one where the preventative check system of Malthus was in operation. This system proposed that societies could control their population growth so that the need for a positive check (natural disaster, famine, disease) to regulate population size to fit food production would be unnecessary. The actual data Wrigley and Schofield present for fertility includes 'high fertility in the sixteenth century followed by slow a decline from GRR (gross reproduction rate) from 2.8 to 1.9.' This in turn was followed by 'a gently rising trend to reach 2.3 by 1756', before an acceleration of this trend 'to reach a peak value of 3.06 in 1816.' Thus they argue that 'towards and between the major turning points of about 1660 and 1815...there were very few marked departures from the trend line, but rather a steady progression.' Wrigley, People, Cities and Wealth, p.39-40. Wrigley and Schofield, The Population History of England, p.549. Wrigley and Schofield, The Population History of England, p.466. Wrigley and Schofield, The Population History of England, p.467. Hatcher, 'Understanding the Population History of England', p.124. Wrigley and Schofield, The Population History of England, p.229. There has been a lot of criticism of this model, both in terms of the methodology used to get the data on fertility and the final results presented, but also in terms of the Malthusian framework in which these results are placed. The method used to estimate age at marriage was 'family reconstitution', which traced individuals' lives through their parish. This method requires both baptism and marriage to have taken place in the same parish and assumes out-migration rates are low. In many cases however the 'sparrow phenomenon' or people flitting in and out of the records made this method impossible; in fact only twelve parish registers provided reasonable estimates for age at marriage. It has been suggested that it is probably these twelve parishes which are unrepresentative; the norm, as shown by the majority of records, was to move around the country rather than to remain in the place of birth. Thus this method uses a mere 0.12 percent of the total number of parishes to work out estimates of age at marriage on a national basis, and it is likely that this 0.12 percent is unrepresentative of the total population in terms of lifestyle habits. Marriage did not even become a regulated and controlled institution before 1753 when laws were introduced in order to make those marriages not carried out in a church illegal. Before this point marriage could be legal simply through the saying of vows in front of a witness; Laura Gowing points out that before 1753 the promise to marry followed by sex also constituted legal if irregular marriage. This ambiguity in the definition of marriage leads to further questions about the ability of Wrigley and Schofield to estimate marriage accurately because of course these non-ecclesiastical weddings would not have been recorded. John Hatcher argues that the fertility rates estimated were forced into the model of Malthusian preventative check which had already been formulated by the Cambridge Group before they even began their research. He asserts that a model where the demographic regime can be '...distilled into a counterpoint between marriage and the levels of real wages...is overly singular, simple and mechanistic...and incapable of capturing historical reality.' Wrigley and Schofield, The Population History of England, p.229. Hatcher, 'Understanding the Population History of England', p.88. Laura Gowing, Women, Words and Sex in Early Modern London, (Oxford, 1998), p.147. Hatcher, 'Understanding the Population History of England', p.91. Hatcher, 'Understanding the Population History of England', p.88-89. Even if the marriage and fertility rates, potentially unrepresentative and framed as they may be, are accepted as plausible, questions have been raised about the placement of emphasis in Wrigley and Schofield's interpretation. David Weir argued that marital behaviour altered considerably over time causing a shift in the dominant factor affecting fertility and thus population trends. 'From a period in which the incidence of marriage was dominant to one in which changes in the ages of those marrying was dominant."" Some have noted that it is difficult to assert the dominance of fertility on population trends, particularly when within the boundaries of the Malthusian model. 'Imposing' this framework on the fertility data has required Wrigley and Schofield to argue that a 'long lag' relationship of about fifty years existed between a change in real wages and its affects on fertility. 'A key puzzle of the demography is why fertility continued to rise when real wages were stagnant...the lag of course solves this problem."" J.A. Goldstone accepts that a lag is a plausible explanation; however suggests that reducing it to a twenty year difference would be more accurate. Others however are less convinced by the theory; Steve Hindle suggests that we do not know enough about marriage horizons, the preference for nuclear family households, factors influencing nuptiality and in particular the influence of institutional pressures (such as apprenticeships and the poor law) in shaping marriage, to make assertions about a long lag and the population correcting its own development in reaction to conditions of scarcity and plenty. Indeed it has been noted that as a period of low life expectancy the possibility of parents surviving to influence the marriage patterns of the next generation is most unlikely. Similarly, questions have been asked about the amount of influence a parent would have had over their children. Perhaps in cases where inheritance would depend on parental approval influence would be quite significant, but for the majority of the population inheritance would not have been left. The deterministic nature of this fertility led explanation for the population growth of 1541-1656 and 1731-1871 does not allow room for other factors, and does not take regional variations in to consideration. Although I do not wish to argue in a deterministic fashion that mortality was the most important factor in shaping the population 'replacing one bald stereotype with another', it is important to look at the quite significant part played by mortality in order to question Wrigley and Schofield's interpretations. David Weir, 'Rather Never than Late: Celibacy and Age At Marriage in English Cohort Fertility, 1541-1871', Journal of Family History, 9, (Winter, 1994). Steve Hindle, 'The Problem of Pauper Marriage in Seventeenth-Century England', Transactions of the Royal Historical Society, 6th ser. 8, (1998), p.72. Hatcher, 'Understanding the Population History of England', p.88. J.A. Goldstone, 'The Demographic Revolution in England: a Re-examination', Population Studies, 49, (1986), p.9. Goldstone, 'The Demographic Revolution in England', p.13. Hindle, 'The Problem of Pauper Marriage in Seventeenth-Century England'. Susan Amussen, An Ordered Society: Gender and Class in Early Modern England, (London, 1988), p.34. Lawrence Stone, The Family Sex and Marriage in England 1500-1800, (London, 1977), p.113. Hatcher, 'Understanding the Population History of England', p.89. The two parts of mortality; starvation and disease were both influences on the early modern population, not least because their presence pre-empted any potential rise in fertility through restricting marriage partners, increasing migration and affecting prices and wages. The influence of factors other than fertility can be seen in the rise of the population despite a fall in real wages; I am afraid that I remain unconvinced by the long lag theory and thus this slowness of response between the two seems to imply that mortality had more influence than Wrigley and Schofield suggested. John Hatcher points out that the volatility of both fertility and mortality during this time is suggestive not of Wrigley and Schofield's model where '...there were very few departures from the tend line', but of a model where many influences were working to shape the population. 'Conclusive statistical proof of the exceptional instability...high annual deviations between 1550/1 and 1574/5 of 17.7 percent in the crude death rate, and 12.2 percent and 8.0 percent respectively in the marriage and birth rates."" Hatcher also notes that assumptions made by Wrigley and Schofield to fill in gaps left by poor evidence and their small sample had led to the effects of mortality being underestimated. 'A far more severe impact is revealed by the registers not included in the study and also by the abundance of wills probated in a wide range of ecclesiastical jurisdictions."" Additionally, Wrigley and Schofield's determination to prove the dominance of fertility can be seen in their relegation of mortality to a mere 'condition' of population change whereas fertility is considered a 'cause'; mortality is presented as something which affects fertility rather than as an influence on population in its own right. As Hatcher points out 'in any analysis of causality the designation of some factors as 'conditions' or 'circumstances' and others as 'causes' is liable to exert a powerful influence over the outcome."" What Hatcher calls 'a tangle of contradictions and reservations' in fact 'lie behind the series of confident judgements...made.' Wrigley and Schofield, The Population History of England, p.229. Hatcher, 'Understanding the Population History of England', p.102. Hatcher, 'Understanding the Population History of England', p.104. Hatcher, 'Understanding the Population History of England', p.105. Hatcher, 'Understanding the Population History of England', p.106. There are a number of issues surrounding the fertility led preventative check theory and the above criticisms are useful to demonstrate a rather general lack of accuracy in Wrigley and Schofield's model; however it is also useful to look at more specific studies to identify the effects of mortality on localised areas. Peter Laslett's study on famine in early modern England concluded that the threat of starvation or 'crisis mortality' was very unlikely to occur. However, death via malnutrition and via food poisoning (through eating rotten or poor quality foods) , though not technically starvation as such, could still cause population rates to fall as well as making the population more susceptible to disease and lowering fertility. This distinction between outright starvation and malnutrition is something which Andrew Appleby also remarks upon, arguing that death due to starvation and malnutrition could be very hard to identify. Appleby suggests that the years of 'recurring population crises' between 1550 and 1640 were in fact due to mortality and further that 'the role of famine as a killer  in its own right has been underplayed' generally 'in the history of early modern England,' (my italics). Wrigley and Schofield do recognise that at particular times throughout history mortality has been more dominant than fertility in shaping demography, yet they conclude that overall fertility had the more decisive influence. Appleby's findings in Westmorland and Cumberland show that perhaps this decisive influence is misleading because the effects of mortality have been repeatedly and mistakenly underplayed. Peter Laslett, The World we have Lost: Further Explored, (Cambridge, 1965), p.151. Laslett, The World we have Lost, p.134. Andrew Appleby, 'Disease or Famine? Mortality in Cumberland and Westmorland 1580-1640', The Economic History Review, Second ser, 26, (1973), p.404. Appleby, 'Disease or Famine? ', p.403. Wrigley and Schofield, The Population History of England, p.238. Disease had specific characteristics; it was often recorded in parish registers as a cause of death and appeared in other primary evidence such as wills. Famine did not have such a 'discernable pattern to help the historian identify it' it did not have such specific characteristics, it was rarely recorded in parish registers, and those who could write (more likely the upper sorts) would have been less likely to feel the effects of famine and able to relocate in plenty of time before actual starvation set in. Appleby looks at available sources and compares them closely with the known characteristics of the prevalent diseases of the period; epidemics were rarely rural problems, they declined in Winter (the winter disease Typhus rarely killed children and is thus identifiable), disease set in very quickly and in the case of plague 'would often follow the main lines of trade between cities, fairs and seaports'. The characteristics of starvation are much harder to identify and provide proof of; parish records were often unhelpful, famine did not follow a specific development like that of disease and when it appeared in the form of malnutrition it is even harder to discern. Appleby develops a more detailed methodology for identifying famine than the three main requirements of Peter Laslett; he noticed that in cases of starvation gradual or rapid onset of deaths could occur but a dramatic increase in mortality (burials twice normal) should be expected. Several neighbouring villages would also have increased deaths, a correlation between prices and mortality should be noticeable, usually some reference to dearth would exist in one form of source or another and there should be fewer conceptions than usual. Not only does this list of the characteristics of famine offer evidence that it can be very hard to identify, but that it is very closely linked to fertility; not as a condition of the rise/fall of fertility, but as a factor in its own right. Appleby discovered that famine had played a significant part in shaping the population in Cumberland and Westmorland; at certain times it was the dominant factor causing the death toll to mount, at other times it made areas more susceptible to disease (through lessened immune systems, and through desperate importing of foods from areas with epidemics) and made women less willing and less able to conceive. Appleby, 'Disease or Famine? ', p.406. Appleby, 'Disease or Famine? ', p.404-5. Appleby, 'Disease or Famine? ', p.421. Appleby, 'Disease or Famine? ', p.422. Appleby, 'Disease or Famine? ', p.430-31. In their study of Mid-Wharfedale parishes in 1623 Moira Long and May Pickles also found that evidence of 'crisis mortality' was often a part of early modern life, if not in the form of full blown famine certainly in the form of 'crisis months'. What Long and Pickles also recognise is the importance that the threat of famine played in shaping populations, for example the fear of shortages could affect births (not necessarily through less marriages but also through stress and amenorrhea) and could persuade locals to import food raising the likelihood of an epidemic. It is the backdrop of general high mortality in England in 1623 against which Long and Pickles set their study and they found, as suggested by Appleby, that in the areas they looked at scarcity and famine were the hardest to identify. However they suggest that 'erratic' results for famine exemplified this period and that 'the balance between sufficiency and dearth in many parts...was finely poised."" Victor Skipp found considerable regional variation in occurrences of famine and shortages which might conceivably have led to their relative invisibility in the small sample of Wrigley and Schofield. These findings and Appleby's findings from specific area studies seem to reflect Hatcher's argument that the presentation of both the steadily rising influence of fertility and steadily decreasing influence of mortality is inaccurate. In the same way Wrigley and Schofield's treatment of famine as a mere condition rather a cause affecting the turning of the demographic regime is equally misleading; yes fertility was important in 'turning the regime', but it was by no means alone in its efforts. Maria Long and May Pickles, 'An Enquiry into Mortality some Mid-Wharfedale Parishes in 1623', Local Population Studies, 31, (Autumn, 1986), p.27. Long and Pickles, 'An Enquiry into Mortality some Mid-Wharfedale Parishes in 1623', p.25. Long and Pickles, 'An Enquiry into Mortality some Mid-Wharfedale Parishes in 1623', p.26. Long and Pickles, 'An Enquiry into Mortality some Mid-Wharfedale Parishes in 1623', p.34. Victor Skipp, 'Economic and Social Change in the Forest of Arden 1530-1649', Agricultural History Review, 18, (1970), p.105. Hatcher, 'Understanding the Population History of England', p.106. Wrigley, People, Cities and Wealth, p.39-40. Throughout this essay so far, I have tried to emphasise the continuous links between mortality and fertility during the early modern period; it is this essential inter-linkage which I wish to look at more explicitly now. There were interconnections between disease, migration, death, birth rate, price and wage, amongst others; in practice of course the nature of and the strengths of these links varied, nevertheless they did exist. The incidence of famine whether long or short term affected disease rates, fertility rates and migration rates as well as affecting prices and wages as the population lessens. Likewise, high fertility could force prices up, wages down, encourage migration and make disease more probable. It seems to me that the circular nature and knock on effect of all these factors combining to shape each other and the population would be better described in terms of a set of dominoes rather than as a hinge which suggests not only the domination but also the presence of only one factor. Does not the very reality of high fertility demand low levels of disease and plenty of food? And does not the very reality of famine demand high levels of population? As Laslett suggests 'each part of our subject is also our whole subject...the fund of food was obviously related to age at marriage and numbers marrying.' Laslett, The World we have Lost, p.122. Hatcher suggests that Wrigley and Schofield manipulate data to exaggerate fertility and lessen the significance of mortality; he argues that the presentation of information in The Population History of England is 'skewed' in favour of fertility. Even the date at which the study begins favours their Malthusian framework; 'the first half of the sixteenth century was a period of demographic transition, results would have been very different if the parish register series had begun a couple of decades earlier or later.' Although Hatcher accepts that this was purely coincidental, 'a historical accident', it still demonstrates that the prominence awarded to fertility by Wrigley and Schofield is not necessarily historically accurate but is based more upon interpretation, evidence and methodology used. In fact, a case for the essential interaction and inseparability of the factors influencing population can be made from within The Population History of England. From '1551 to 1566 falling fertility was the main driving force until the ""golden period of low mortality"" turned the tables; 1571 to 1611 was a period of relatively stable fertility and mortality ""fertility and mortality contributed equally to the steady fall"", fertility was dominant from 1671 to 1691 but mortality was most clearly the more important influence on growth rates from 1691 to 1751."" Hatcher sums up these characteristics and finds that 'fertility is held to dominate for around thirty-five years of the two-hundred year time span, mortality for around sixty-five years and that for a century the two variables exerted approximately equal influence."" This evidence is 'immediately contradicted' by Wrigley and Schofield through the use of 'quinquennial averages', and the discarding of 'three episodes of very high mortality before 1751'. Nonetheless it seems evident that 'fertility and mortality exchanged leadership many times within the early modern period' and that pinpointing which one was dominant at different times would prove a more useful exercise than attempting to prove which was dominant full stop. Hatcher, 'Understanding the Population History of England', p.114. Hatcher, 'Understanding the Population History of England', p.112. Hatcher, 'Understanding the Population History of England', p.107. Hatcher, 'Understanding the Population History of England', p.107. Hatcher, 'Understanding the Population History of England', p.108. Hatcher, 'Understanding the Population History of England', p.116. In conclusion, despite the problems with Wrigley and Schofield's methodology and evidence the population trends they map out are plausible and the best figures historians have to work with. I have suggested however that in many ways their interpretation of the causality of this trend is flawed. The judgement that living standards and wage rates determined nuptiality and therefore fertility, or that 'marriage is the hinge upon which the demographic regime turned' is inaccurate. Family reconstitution provided data on fertility which was unreliable and unrepresentative. However, even accepting these methodological limitations the figures then seemed to be moulded to fit a Malthusian framework of preventative check homeostasis. The 'long lag' theory used to justify the lack of direct correlation between fertility and real wages over simplifies a very complex phenomenon whilst simultaneously relegating other important factors to mere conditions or circumstances rather than causes of population change. The fact that we do not know enough about marriage patterns, non-church marriages, or the parental influences over children of every day people makes the long lag an unsustainable claim. The deterministic nature of this model undermines the significance of mortality and other factors upon population changes and upon shaping fertility. The relative instability and volatility of both fertility and mortality suggests more detail is needed in the model Wrigley and Schofield provide. Evidence such as those registers not used in The Population History sample as well as personal documents from the time show that mortality had more influence than the Cambridge Group suggest. Mortality in the form of starvation was often extremely hard to identify due to its lack of recording in parish registers. The type of scarcity which never developed in to full blown famine is even harder to spot, but was equally influential in shaping fertility and population. Studies of Westmorland and Cumberland, the Forest of Arden and of Mid-Wharfedale parishes reveal a mistaken and repeated underplaying of starvation, and its often very localised nature. In fact the presence and at times domination of mortality was important and deserves more attention in  The Population History. Similarly, although I have not focused greatly upon the influence of migration and in particular sex specific migration on population trends, through reducing the pool of those eligible for marriage, its importance should not be underestimated either. I have tried to show that the links between starvation, disease and fertility are more important and relevant than which of these major demographic variables was dominant overall. Each affected the others and each in turn was affected by many other minor demographic variables. Therefore I have suggested that a model whereby the essential interlinks of mortality, migration and fertility would prove most accurate. Hatcher claims that in total between 1551 and 1751 'fertility is dominant for 17.5 percent of the time, mortality for 32.5 percent and for 50 percent of the time the two variables exerted approximately equal influence."" Manifestly he argues that causality cannot be established until it is accepted that fertility and mortality were both important factors within the early modern period '...and a host of social, cultural and economic factors played on them to help fashion demographic behaviour."" To finally answer the question, I would suggest that the metaphor of fertility as a 'hinge upon which the demographic regime turned', be discarded and replaced with a metaphor which better encompasses the historical reality of the exertion of manifold influences, and a co-relation of factors which developed to shape what was infact less a regime, and more a volatile commune. 'Any valid depiction of reality is likely to resemble a vast irregular web'. The achievements of the Cambridge Group should not be underestimated, but equally nor should the inadequacies of their interpretative model. 'Patterns of births, marriages, deaths and migrations helped to shape society and economy and were themselves shaped by society and economy' as I have attempted to show in this essay. Goldstone, 'The Demographic Revolution in England', p.22. Hatcher, 'Understanding the Population History of England', p.107. Hatcher, 'Understanding the Population History of England', p.116. Hatcher, 'Understanding the Population History of England', p.129. Hatcher, 'Understanding the Population History of England', p.129.",True
51,"The most extensive population study of the early modern period was carried out by E.A. Wrigley and R.S. Schofield, The Population History of England, 1541 - 1871: A Reconstruction, which set out calculated estimates of population trends for this period. They used the method of back-projection to estimate the population levels of the country between 1541 (the beginning of the first decade after the introduction of parish registers) and 1871 (the date of the second reliable census). This project was an immense undertaking which used four hundred and four surviving parish registers which recorded baptism, marriage and burial, to estimate demographic trends at five year intervals between these two dates. Many criticisms of both the methodology used and the results achieved have been made of this study. In very general terms these criticisms focus upon the following main areas. The sample of four hundred and four parish registers used to estimate national population figures makes up only four percent of the total number of parishes in early modern England. In addition, the sample includes too many large parishes (usually Northern areas) and too few smaller parishes (usually Eastern areas). London is also underrepresented in the study due to its lack of surviving parish registers. Even good registers however were inaccurate for certain periods (such as during the civil war) and thus only for the year 1662 were all the registers fit to use. The unreliability of back projection as a technique for working out population levels has been criticised in terms of the assumptions Wrigley and Schofield made in using this method; for example assumptions about the age structure in place throughout this period will have influenced the final results. E.A. Wrigley and R.S. Schofield, The Population History of England, 1541-1871: A Reconstruction, (Cambridge, 1981). Wrigley and Schofield, The Population History of England, p. 15. Wrigley and Schofield, The Population History of England, p. 16. Wrigley and Schofield, The Population History of England, p. 455. John Hatcher, 'Understanding the Population History of England 1450-1750', Past and Present, no.180, (August 2003), p.125. Despite such criticism the information Wrigley and Schofield provide does appear a plausible reading of the demographic trends of early modern England. After all, the authors are aware that methodological shortcomings exist; this work does not pretend to be concrete evidence but merely a likely estimation. Even taking in to consideration the limitations of the evidence and the methodology as well as the limitations of the study itself, i.e. : it operated on a very statistical level and was not placed in much of a social or cultural context, this remains a very useful work. Furthermore the huge financial costs and time commitments involved in this kind of research make it unlikely that another work of this nature will be undertaken. These are the best results we have, and particularly when used in conjunction with localised studies as well as studies of social and cultural factors this is very revealing research. In this essay I am going to argue from a standpoint assuming that the data compiled by Wrigley and Schofield is a realistic estimation of population trends. However, I will argue that an acceptation of the population trends presented by the Cambridge Group does not necessitate an acceptation of their interpretation of the causes of these population trends. I will argue that fertility and mortality were important in influencing population change. I will argue that the essential inter-linkage of these two factors makes any either/or demographical explanation implausible. Instead I will suggest that a model whereby both fertility and the two parts of mortality (disease and starvation) helped shape population and each other is a more realistic explanation to such a complex and wide-spanning phenomenon. Wrigley and Schofield, The Population History of England, p.456. The demographic trends that Wrigley and Schofield have mapped out for early modern England begin with growth between 1541 and 1661; this growth was generally steady although it slowed during the 1550s and became rapid in the 1570s and 1580s. Between 1656 and 1686 the population decreased in size before stagnating and slightly recovering between 1686 and 1750. From 1750 onwards the vigorous growth of earlier periods was resumed, although Elizabethan growth rates were not reached until the 1790s. Thus there were two periods of growth divided by one hundred years or so from the mid seventeenth century to the mid eighteenth century when population numbers stagnated. During these main periods of growth the population increased significantly almost doubling from 2.8 million to 5.3 million between 1541 and 1656, and after the years of stagnation again doubling between 1731 and 1816 and between 1861 and 1871. As I have suggested this basic information seems both plausible and realistic, however Wrigley and Schofield's interpretation of this information; an attempt to understand what caused this rapid growth, check and decline followed by renewed growth, seems less plausible. The principle question they look at is whether '...a rise in GRR (gross reproductive rate) from 2.5 to 3.0 for example is more important than a rise in eo (life expectancy from birth) from 35-40 years in effecting growth rates."" Or in other words, was it fertility or mortality that was the definitive factor in determining these population trends. Roger Schofield, 'The Impact of Scarcity and Plenty on Population Change in England 1541-1871', Journal of Interdisciplinary History, 14:2, (Autumn 1983), p.270. Schofield, 'The Impact of Scarcity and Plenty on Population Change in England', p.269. E.A. Wrigley, People, Cities and Wealth; The Transformation of Traditional Society, (Oxford, 1989), p.243. Wrigley and Schofield, The Population History of England, p.236. The Cambridge Group certainly believed that '...marriage was the hinge upon which the demographic regime turned."" They suggested that '...fertility change was more important than mortality change in altering population growth rates', that '...fertility change in turn chiefly reflected fluctuations in nuptiality' and that '...nuptiality was primarily determined by real wages'. Thus Wrigley and Schofield presented a structure where a rise in real wages would lead to a rise in the number of marriages as well as a rise in the number of early marriages and therefore a rise in the numbers of births. Wrigley and Schofield themselves note the inevitable sacrifice of comprehensiveness with the construction of simple models, however they argue that the benefits gained in clarity are essential to understanding. Therefore the picture of demography that they present for early modern England at this time is one where the preventative check system of Malthus was in operation. This system proposed that societies could control their population growth so that the need for a positive check (natural disaster, famine, disease) to regulate population size to fit food production would be unnecessary. The actual data Wrigley and Schofield present for fertility includes 'high fertility in the sixteenth century followed by slow a decline from GRR (gross reproduction rate) from 2.8 to 1.9.' This in turn was followed by 'a gently rising trend to reach 2.3 by 1756', before an acceleration of this trend 'to reach a peak value of 3.06 in 1816.' Thus they argue that 'towards and between the major turning points of about 1660 and 1815...there were very few marked departures from the trend line, but rather a steady progression.' Wrigley, People, Cities and Wealth, p.39-40. Wrigley and Schofield, The Population History of England, p.549. Wrigley and Schofield, The Population History of England, p.466. Wrigley and Schofield, The Population History of England, p.467. Hatcher, 'Understanding the Population History of England', p.124. Wrigley and Schofield, The Population History of England, p.229. There has been a lot of criticism of this model, both in terms of the methodology used to get the data on fertility and the final results presented, but also in terms of the Malthusian framework in which these results are placed. The method used to estimate age at marriage was 'family reconstitution', which traced individuals' lives through their parish. This method requires both baptism and marriage to have taken place in the same parish and assumes out-migration rates are low. In many cases however the 'sparrow phenomenon' or people flitting in and out of the records made this method impossible; in fact only twelve parish registers provided reasonable estimates for age at marriage. It has been suggested that it is probably these twelve parishes which are unrepresentative; the norm, as shown by the majority of records, was to move around the country rather than to remain in the place of birth. Thus this method uses a mere 0.12 percent of the total number of parishes to work out estimates of age at marriage on a national basis, and it is likely that this 0.12 percent is unrepresentative of the total population in terms of lifestyle habits. Marriage did not even become a regulated and controlled institution before 1753 when laws were introduced in order to make those marriages not carried out in a church illegal. Before this point marriage could be legal simply through the saying of vows in front of a witness; Laura Gowing points out that before 1753 the promise to marry followed by sex also constituted legal if irregular marriage. This ambiguity in the definition of marriage leads to further questions about the ability of Wrigley and Schofield to estimate marriage accurately because of course these non-ecclesiastical weddings would not have been recorded. John Hatcher argues that the fertility rates estimated were forced into the model of Malthusian preventative check which had already been formulated by the Cambridge Group before they even began their research. He asserts that a model where the demographic regime can be '...distilled into a counterpoint between marriage and the levels of real wages...is overly singular, simple and mechanistic...and incapable of capturing historical reality.' Wrigley and Schofield, The Population History of England, p.229. Hatcher, 'Understanding the Population History of England', p.88. Laura Gowing, Women, Words and Sex in Early Modern London, (Oxford, 1998), p.147. Hatcher, 'Understanding the Population History of England', p.91. Hatcher, 'Understanding the Population History of England', p.88-89. Even if the marriage and fertility rates, potentially unrepresentative and framed as they may be, are accepted as plausible, questions have been raised about the placement of emphasis in Wrigley and Schofield's interpretation. David Weir argued that marital behaviour altered considerably over time causing a shift in the dominant factor affecting fertility and thus population trends. 'From a period in which the incidence of marriage was dominant to one in which changes in the ages of those marrying was dominant."" Some have noted that it is difficult to assert the dominance of fertility on population trends, particularly when within the boundaries of the Malthusian model. 'Imposing' this framework on the fertility data has required Wrigley and Schofield to argue that a 'long lag' relationship of about fifty years existed between a change in real wages and its affects on fertility. 'A key puzzle of the demography is why fertility continued to rise when real wages were stagnant...the lag of course solves this problem."" J.A. Goldstone accepts that a lag is a plausible explanation; however suggests that reducing it to a twenty year difference would be more accurate. Others however are less convinced by the theory; Steve Hindle suggests that we do not know enough about marriage horizons, the preference for nuclear family households, factors influencing nuptiality and in particular the influence of institutional pressures (such as apprenticeships and the poor law) in shaping marriage, to make assertions about a long lag and the population correcting its own development in reaction to conditions of scarcity and plenty. Indeed it has been noted that as a period of low life expectancy the possibility of parents surviving to influence the marriage patterns of the next generation is most unlikely. Similarly, questions have been asked about the amount of influence a parent would have had over their children. Perhaps in cases where inheritance would depend on parental approval influence would be quite significant, but for the majority of the population inheritance would not have been left. The deterministic nature of this fertility led explanation for the population growth of 1541-1656 and 1731-1871 does not allow room for other factors, and does not take regional variations in to consideration. Although I do not wish to argue in a deterministic fashion that mortality was the most important factor in shaping the population 'replacing one bald stereotype with another', it is important to look at the quite significant part played by mortality in order to question Wrigley and Schofield's interpretations. David Weir, 'Rather Never than Late: Celibacy and Age At Marriage in English Cohort Fertility, 1541-1871', Journal of Family History, 9, (Winter, 1994). Steve Hindle, 'The Problem of Pauper Marriage in Seventeenth-Century England', Transactions of the Royal Historical Society, 6th ser. 8, (1998), p.72. Hatcher, 'Understanding the Population History of England', p.88. J.A. Goldstone, 'The Demographic Revolution in England: a Re-examination', Population Studies, 49, (1986), p.9. Goldstone, 'The Demographic Revolution in England', p.13. Hindle, 'The Problem of Pauper Marriage in Seventeenth-Century England'. Susan Amussen, An Ordered Society: Gender and Class in Early Modern England, (London, 1988), p.34. Lawrence Stone, The Family Sex and Marriage in England 1500-1800, (London, 1977), p.113. Hatcher, 'Understanding the Population History of England', p.89. The two parts of mortality; starvation and disease were both influences on the early modern population, not least because their presence pre-empted any potential rise in fertility through restricting marriage partners, increasing migration and affecting prices and wages. The influence of factors other than fertility can be seen in the rise of the population despite a fall in real wages; I am afraid that I remain unconvinced by the long lag theory and thus this slowness of response between the two seems to imply that mortality had more influence than Wrigley and Schofield suggested. John Hatcher points out that the volatility of both fertility and mortality during this time is suggestive not of Wrigley and Schofield's model where '...there were very few departures from the tend line', but of a model where many influences were working to shape the population. 'Conclusive statistical proof of the exceptional instability...high annual deviations between 1550/1 and 1574/5 of 17.7 percent in the crude death rate, and 12.2 percent and 8.0 percent respectively in the marriage and birth rates."" Hatcher also notes that assumptions made by Wrigley and Schofield to fill in gaps left by poor evidence and their small sample had led to the effects of mortality being underestimated. 'A far more severe impact is revealed by the registers not included in the study and also by the abundance of wills probated in a wide range of ecclesiastical jurisdictions."" Additionally, Wrigley and Schofield's determination to prove the dominance of fertility can be seen in their relegation of mortality to a mere 'condition' of population change whereas fertility is considered a 'cause'; mortality is presented as something which affects fertility rather than as an influence on population in its own right. As Hatcher points out 'in any analysis of causality the designation of some factors as 'conditions' or 'circumstances' and others as 'causes' is liable to exert a powerful influence over the outcome."" What Hatcher calls 'a tangle of contradictions and reservations' in fact 'lie behind the series of confident judgements...made.' Wrigley and Schofield, The Population History of England, p.229. Hatcher, 'Understanding the Population History of England', p.102. Hatcher, 'Understanding the Population History of England', p.104. Hatcher, 'Understanding the Population History of England', p.105. Hatcher, 'Understanding the Population History of England', p.106. There are a number of issues surrounding the fertility led preventative check theory and the above criticisms are useful to demonstrate a rather general lack of accuracy in Wrigley and Schofield's model; however it is also useful to look at more specific studies to identify the effects of mortality on localised areas. Peter Laslett's study on famine in early modern England concluded that the threat of starvation or 'crisis mortality' was very unlikely to occur. However, death via malnutrition and via food poisoning (through eating rotten or poor quality foods) , though not technically starvation as such, could still cause population rates to fall as well as making the population more susceptible to disease and lowering fertility. This distinction between outright starvation and malnutrition is something which Andrew Appleby also remarks upon, arguing that death due to starvation and malnutrition could be very hard to identify. Appleby suggests that the years of 'recurring population crises' between 1550 and 1640 were in fact due to mortality and further that 'the role of famine as a killer  in its own right has been underplayed' generally 'in the history of early modern England,' (my italics). Wrigley and Schofield do recognise that at particular times throughout history mortality has been more dominant than fertility in shaping demography, yet they conclude that overall fertility had the more decisive influence. Appleby's findings in Westmorland and Cumberland show that perhaps this decisive influence is misleading because the effects of mortality have been repeatedly and mistakenly underplayed. Peter Laslett, The World we have Lost: Further Explored, (Cambridge, 1965), p.151. Laslett, The World we have Lost, p.134. Andrew Appleby, 'Disease or Famine? Mortality in Cumberland and Westmorland 1580-1640', The Economic History Review, Second ser, 26, (1973), p.404. Appleby, 'Disease or Famine? ', p.403. Wrigley and Schofield, The Population History of England, p.238. Disease had specific characteristics; it was often recorded in parish registers as a cause of death and appeared in other primary evidence such as wills. Famine did not have such a 'discernable pattern to help the historian identify it' it did not have such specific characteristics, it was rarely recorded in parish registers, and those who could write (more likely the upper sorts) would have been less likely to feel the effects of famine and able to relocate in plenty of time before actual starvation set in. Appleby looks at available sources and compares them closely with the known characteristics of the prevalent diseases of the period; epidemics were rarely rural problems, they declined in Winter (the winter disease Typhus rarely killed children and is thus identifiable), disease set in very quickly and in the case of plague 'would often follow the main lines of trade between cities, fairs and seaports'. The characteristics of starvation are much harder to identify and provide proof of; parish records were often unhelpful, famine did not follow a specific development like that of disease and when it appeared in the form of malnutrition it is even harder to discern. Appleby develops a more detailed methodology for identifying famine than the three main requirements of Peter Laslett; he noticed that in cases of starvation gradual or rapid onset of deaths could occur but a dramatic increase in mortality (burials twice normal) should be expected. Several neighbouring villages would also have increased deaths, a correlation between prices and mortality should be noticeable, usually some reference to dearth would exist in one form of source or another and there should be fewer conceptions than usual. Not only does this list of the characteristics of famine offer evidence that it can be very hard to identify, but that it is very closely linked to fertility; not as a condition of the rise/fall of fertility, but as a factor in its own right. Appleby discovered that famine had played a significant part in shaping the population in Cumberland and Westmorland; at certain times it was the dominant factor causing the death toll to mount, at other times it made areas more susceptible to disease (through lessened immune systems, and through desperate importing of foods from areas with epidemics) and made women less willing and less able to conceive. Appleby, 'Disease or Famine? ', p.406. Appleby, 'Disease or Famine? ', p.404-5. Appleby, 'Disease or Famine? ', p.421. Appleby, 'Disease or Famine? ', p.422. Appleby, 'Disease or Famine? ', p.430-31. In their study of Mid-Wharfedale parishes in 1623 Moira Long and May Pickles also found that evidence of 'crisis mortality' was often a part of early modern life, if not in the form of full blown famine certainly in the form of 'crisis months'. What Long and Pickles also recognise is the importance that the threat of famine played in shaping populations, for example the fear of shortages could affect births (not necessarily through less marriages but also through stress and amenorrhea) and could persuade locals to import food raising the likelihood of an epidemic. It is the backdrop of general high mortality in England in 1623 against which Long and Pickles set their study and they found, as suggested by Appleby, that in the areas they looked at scarcity and famine were the hardest to identify. However they suggest that 'erratic' results for famine exemplified this period and that 'the balance between sufficiency and dearth in many parts...was finely poised."" Victor Skipp found considerable regional variation in occurrences of famine and shortages which might conceivably have led to their relative invisibility in the small sample of Wrigley and Schofield. These findings and Appleby's findings from specific area studies seem to reflect Hatcher's argument that the presentation of both the steadily rising influence of fertility and steadily decreasing influence of mortality is inaccurate. In the same way Wrigley and Schofield's treatment of famine as a mere condition rather a cause affecting the turning of the demographic regime is equally misleading; yes fertility was important in 'turning the regime', but it was by no means alone in its efforts. Maria Long and May Pickles, 'An Enquiry into Mortality some Mid-Wharfedale Parishes in 1623', Local Population Studies, 31, (Autumn, 1986), p.27. Long and Pickles, 'An Enquiry into Mortality some Mid-Wharfedale Parishes in 1623', p.25. Long and Pickles, 'An Enquiry into Mortality some Mid-Wharfedale Parishes in 1623', p.26. Long and Pickles, 'An Enquiry into Mortality some Mid-Wharfedale Parishes in 1623', p.34. Victor Skipp, 'Economic and Social Change in the Forest of Arden 1530-1649', Agricultural History Review, 18, (1970), p.105. Hatcher, 'Understanding the Population History of England', p.106. Wrigley, People, Cities and Wealth, p.39-40. Throughout this essay so far, I have tried to emphasise the continuous links between mortality and fertility during the early modern period; it is this essential inter-linkage which I wish to look at more explicitly now. There were interconnections between disease, migration, death, birth rate, price and wage, amongst others; in practice of course the nature of and the strengths of these links varied, nevertheless they did exist. The incidence of famine whether long or short term affected disease rates, fertility rates and migration rates as well as affecting prices and wages as the population lessens. Likewise, high fertility could force prices up, wages down, encourage migration and make disease more probable. It seems to me that the circular nature and knock on effect of all these factors combining to shape each other and the population would be better described in terms of a set of dominoes rather than as a hinge which suggests not only the domination but also the presence of only one factor. Does not the very reality of high fertility demand low levels of disease and plenty of food? And does not the very reality of famine demand high levels of population? As Laslett suggests 'each part of our subject is also our whole subject...the fund of food was obviously related to age at marriage and numbers marrying.' Laslett, The World we have Lost, p.122. Hatcher suggests that Wrigley and Schofield manipulate data to exaggerate fertility and lessen the significance of mortality; he argues that the presentation of information in The Population History of England is 'skewed' in favour of fertility. Even the date at which the study begins favours their Malthusian framework; 'the first half of the sixteenth century was a period of demographic transition, results would have been very different if the parish register series had begun a couple of decades earlier or later.' Although Hatcher accepts that this was purely coincidental, 'a historical accident', it still demonstrates that the prominence awarded to fertility by Wrigley and Schofield is not necessarily historically accurate but is based more upon interpretation, evidence and methodology used. In fact, a case for the essential interaction and inseparability of the factors influencing population can be made from within The Population History of England. From '1551 to 1566 falling fertility was the main driving force until the ""golden period of low mortality"" turned the tables; 1571 to 1611 was a period of relatively stable fertility and mortality ""fertility and mortality contributed equally to the steady fall"", fertility was dominant from 1671 to 1691 but mortality was most clearly the more important influence on growth rates from 1691 to 1751."" Hatcher sums up these characteristics and finds that 'fertility is held to dominate for around thirty-five years of the two-hundred year time span, mortality for around sixty-five years and that for a century the two variables exerted approximately equal influence."" This evidence is 'immediately contradicted' by Wrigley and Schofield through the use of 'quinquennial averages', and the discarding of 'three episodes of very high mortality before 1751'. Nonetheless it seems evident that 'fertility and mortality exchanged leadership many times within the early modern period' and that pinpointing which one was dominant at different times would prove a more useful exercise than attempting to prove which was dominant full stop. Hatcher, 'Understanding the Population History of England', p.114. Hatcher, 'Understanding the Population History of England', p.112. Hatcher, 'Understanding the Population History of England', p.107. Hatcher, 'Understanding the Population History of England', p.107. Hatcher, 'Understanding the Population History of England', p.108. Hatcher, 'Understanding the Population History of England', p.116. In conclusion, despite the problems with Wrigley and Schofield's methodology and evidence the population trends they map out are plausible and the best figures historians have to work with. I have suggested however that in many ways their interpretation of the causality of this trend is flawed. The judgement that living standards and wage rates determined nuptiality and therefore fertility, or that 'marriage is the hinge upon which the demographic regime turned' is inaccurate. Family reconstitution provided data on fertility which was unreliable and unrepresentative. However, even accepting these methodological limitations the figures then seemed to be moulded to fit a Malthusian framework of preventative check homeostasis. The 'long lag' theory used to justify the lack of direct correlation between fertility and real wages over simplifies a very complex phenomenon whilst simultaneously relegating other important factors to mere conditions or circumstances rather than causes of population change. The fact that we do not know enough about marriage patterns, non-church marriages, or the parental influences over children of every day people makes the long lag an unsustainable claim. The deterministic nature of this model undermines the significance of mortality and other factors upon population changes and upon shaping fertility. The relative instability and volatility of both fertility and mortality suggests more detail is needed in the model Wrigley and Schofield provide. Evidence such as those registers not used in The Population History sample as well as personal documents from the time show that mortality had more influence than the Cambridge Group suggest. Mortality in the form of starvation was often extremely hard to identify due to its lack of recording in parish registers. The type of scarcity which never developed in to full blown famine is even harder to spot, but was equally influential in shaping fertility and population. Studies of Westmorland and Cumberland, the Forest of Arden and of Mid-Wharfedale parishes reveal a mistaken and repeated underplaying of starvation, and its often very localised nature. In fact the presence and at times domination of mortality was important and deserves more attention in  The Population History. Similarly, although I have not focused greatly upon the influence of migration and in particular sex specific migration on population trends, through reducing the pool of those eligible for marriage, its importance should not be underestimated either. I have tried to show that the links between starvation, disease and fertility are more important and relevant than which of these major demographic variables was dominant overall. Each affected the others and each in turn was affected by many other minor demographic variables. Therefore I have suggested that a model whereby the essential interlinks of mortality, migration and fertility would prove most accurate. Hatcher claims that in total between 1551 and 1751 'fertility is dominant for 17.5 percent of the time, mortality for 32.5 percent and for 50 percent of the time the two variables exerted approximately equal influence."" Manifestly he argues that causality cannot be established until it is accepted that fertility and mortality were both important factors within the early modern period '...and a host of social, cultural and economic factors played on them to help fashion demographic behaviour."" To finally answer the question, I would suggest that the metaphor of fertility as a 'hinge upon which the demographic regime turned', be discarded and replaced with a metaphor which better encompasses the historical reality of the exertion of manifold influences, and a co-relation of factors which developed to shape what was infact less a regime, and more a volatile commune. 'Any valid depiction of reality is likely to resemble a vast irregular web'. The achievements of the Cambridge Group should not be underestimated, but equally nor should the inadequacies of their interpretative model. 'Patterns of births, marriages, deaths and migrations helped to shape society and economy and were themselves shaped by society and economy' as I have attempted to show in this essay. Goldstone, 'The Demographic Revolution in England', p.22. Hatcher, 'Understanding the Population History of England', p.107. Hatcher, 'Understanding the Population History of England', p.116. Hatcher, 'Understanding the Population History of England', p.129. Hatcher, 'Understanding the Population History of England', p.129.","By 1930 Stalin had not only managed to gain extensive personal control and eliminated all his opponents but had also managed to consolidate this power. Accountable to very few Stalin was free to pursue any policy largely unchecked. He had support from the party rank and file as well as from the central committee and the Politburo. How Stalin, the least obvious of Lenin's potential successors, came to be in this position is a much contested issue. With his death Lenin left a vacuum within the party leadership creating economic, social and political uncertainty. This in turn led to a left / right divide within the Politburo over the next steps the Bolsheviks should take, with Trotsky on the left arguing for rapid industrialisation. Stalin maintained a generally moderate position before siding with Kamenev and Zinoviev forming a powerful triumvate against the man they saw as their biggest rival. After Trotsky's eventual expulsion from the party, and then the country, came the political execution of Kamenev and Zinoviev followed by their actual execution. Stalin's other main rivals faded away either in to political destruction, suicide, execution or expulsion. A. Nove and C. Read, Stalin: Terror and Transformations, video. Nove and Read, Stalin: Terror and Transformations, video. R. Hingley, Stalin; Man and Legend, (London, 1974), ch7, p.3. I find it hard to justify putting Stalin's rise to power down to any one single factor. It does not seem possible that Stalin reached his position purely due to his own manipulation or political cunning. Nor does it seem realistic that he was put in power purely because he was the politician that most reflected the party rank and file wishes. A number of reasons seems the most probable explanation; I will argue that the most important are as follows. Stalin proved a useful asset to Lenin and as a result Lenin often supported Stalin's appointments and promotions. The positions Stalin held within the party were of vital importance in helping him gain power, his personality and his political prowess were also very important in this respect. The mistakes of Stalin's rivals during the 1920's were to prove fatal and helped him in his ascendancy to power. Many of the opportunities which led to Stalin's rise to power were actually accumulated whilst Lenin was still alive. Infact on many occasions it was Lenin that endorsed Stalin's promotion to important positions within the party. Stalin fitted the proletariat ideal lacking in so many of the profiles of the Bolshevik leaders including Lenin's own, Stalin was singled out by Lenin as having potential. His activities to raise funds for the party included bank robbing, and he was one of those 'hard core' Bolsheviks who had remained in Russia '...underground during the winter 1905 -6 repression and persecution of revolutionaries by the Tsar."" This mixture of loyalty and practicality was one Lenin could not resist, he began to rely on Stalin who proved a useful asset. 'He was an unquestioningly loyal person who actually got things done... Stalin was Lenin's political fixer...packing the congress with his supporters'. Lenin supported and approved many of the positions Stalin was appointed to within the party. Although Trotsky claimed that after Stalin's appointment as party General Secretary Lenin expressed concern; '...this cook will serve only peppery dishes', he did not seem sufficiently concerned to raise public objection until very near his death when it was too late. Indeed, prior to his appointment as General Secretary Stalin already held positions of great importance; immediately after the civil war he was Commissar of Nationalities, Commissar of the Workers and Peasants Inspectorate and a member of the Politburo. 'Several of Stalin's crucial promotions - cooption to the central committee and appointment as Pravda editor in 1912 and as General Secretary in 1922- were brought about by Lenin's influence.' L. Deutscher, Stalin; A Political Biography, (Oxford, 1949), p228. Walter Duranty, Stalin and Co. The Politburo; The Men who Ran Russia, (London, 1949), p.10. Christopher Read, The Making and Breaking of the Soviet System, (Hampshire, 2001), p.84. Read, The Making and Breaking of the Soviet System, (Hampshire, 2001), p.85. Deutscher, Stalin; A Political Biography, (Oxford, 1949), p.232. Deutscher, Stalin; A Political Biography, (Oxford, 1949), p.228. Read, The Making and Breaking of the Soviet System, (Hampshire, 2001), p.84. Stalin has been described as 'the spider in the web' in terms of his positioning within both the state and the party apparatus. He had influence within all the main offices, and the posts he held gave him direct authority or at the very least powers of persuasion over a very useful range of contacts. At the time of Lenin's death when the struggle for power was fought out amongst the Politburo members, Stalin had power within the Politburo, (the highest decision making body), the Orgburo, (the administrative bureau of the central committee), the Robkrin and the Sovarkom, as well as in the regional and local party cells and the regional and local soviets. The very nature of the centralised organisation of the party and the state worked to Stalin's advantage due to the links between all the offices. Though all his positions gave him important influence, it was his position as General Secretary which allowed him to build up the kind of power base which defeated his rivals. In this post Stalin was responsible for Politburo administration including the agenda for discussion, Stalin was the link between the Politburo and the Central Committee, he also controlled the '...appointment and promotion of individuals to key posts throughout the country."" As Edward Acton points out, this meant that 'the careers of party officials were becoming increasingly dependent upon their loyalty to the Secretariat."" This in turn gave the '...central administrative organs enormous influence over the make-up of party Congresses and thus of elections to the Central Committee and the Politburo itself'. Acton maintains that this 'movement of power from the Politburo to the Secretariat', was initially '...masked under Lenin's authority over the Secretariat."" However Deutscher claims that on Lenin's return from the recovery of his first stroke he sensed the increasing power of the bureaucracy with Stalin at its head, and immediately attempted (though unsuccessfully) to rectify the situation. The position of General Secretary had been largely overlooked as a source of power due to its administrative nature; '...the bright sparks of the Politburo felt themselves above such roles."" However Stalin was willing to take on this shunted position, and was good at it; he worked hard, he was reliable, made no fuss and needed little assistance from the other Politburo members. He converted secondary administrative power in to political power, using the powerful self perpetuating nature of the vested interests within the bureaucracy. Christopher Read, Lecture, University of Warwick, January 2004. Edward Acton, Russia, The Tsarist and Soviet Legacy, (Essex, 1995), p.197. Acton, Russia, The Tsarist and Soviet Legacy, (Essex, 1995), p.196. Acton, Russia, The Tsarist and Soviet Legacy, (Essex, 1995), p.197. Acton, Russia, The Tsarist and Soviet Legacy, (Essex, 1995), p.197. Acton, Russia, The Tsarist and Soviet Legacy, (Essex, 1995), p.198. Deutscher, Stalin; A Political Biography, (Oxford, 1949), p.247. Robert Tucker, Stalin as Revolutionary 1879 - 1929; A Study In History and Personality, (London, 1974), p.322. Deutscher, Stalin; A Political Biography, (Oxford, 1949), p.233. Idea of secondary administrative power; Read, The Making and Breaking of the Soviet System, (Hampshire, 2001), p.52. Idea of perpetuating vested interests; Hingley, Stalin; Man and Legend, (London, 1974), ch7, p5. Stalin's personality and his political manoeuvring and prowess have been interpreted in a number of ways. As evil, pre-planned, ruthless, manipulation, but also as a highly skilful reaction to economic, social and political situations. Which ever interpretation bears more truth, that Stalin's politics proved enormously valuable to him during the 1920's is undoubtable. The moderate position Stalin seemed to take during Politburo meetings both whilst Lenin was alive and after his death allowed him several advantages. It meant that he 'always seemed to follow others, never to direct them', making Trotsky, who was very active during meetings, seem the biggest threat to power. 'No one could have behaved less like a tyrant or dictator than did Stalin at Politburo meetings'. This moderate position also allowed Stalin to remain 'silent during debates...intervening only to support the majority view...giving the impression of one whose will always prevailed in the end."" Although this may be a rather cynical interpretation of Stalin's political stance during meetings, it did allow him the luxury of supporting which ever people he wished because his views were not an open matter. It also kept him from the trap he used against so many of his rivals; that of criticising the now revered Lenin. Stalin had few political inspirations of his own, but by taking the safe and unremarkable route of following Lenin he could undermine enemies. Hingley, Stalin; Man and Legend, (London, 1974), ch7, p.2. Bazhanov's memoirs cited in Hingley, Stalin; Man and Legend, (London, 1974), ch7, p.3. Hingley, Stalin; Man and Legend, (London, 1974), ch7, p.4. By placing himself in the position of Lenin's 'chief mourner', persuading the party to have Lenin's embalmed body on public show, and playing an active role at Lenin's funeral whilst Trotsky was absent, Stalin helped to create a cult around Lenin which he was at the head of. Stalin presented his views as the legacy of Lenin, following his wishes and therefore justified in whatever action he took. Stalin's work to interpret Leninism for the working man, simplifying it and explaining it, helped him present his brand of Leninism as the only one with authenticity. This initiative to enlighten the rank and file of the party was not a solitary one; Stalin seemed to pay attention to the rank and file wishes, as well as understanding how important their support could be. It has often been argued that Stalin's political actions, and some of the apparently contradictory moves he made, closely reflected the views of the rank and file members. The recognition of the importance of lower ranking Bolsheviks gave Stalin the advantage of gaining their support where other potential leaders lacked it. Acton, Russia, The Tsarist and Soviet Legacy, (Essex, 1995), p.193. Tucker, Stalin as Revolutionary 1879 - 1929, (London, 1974), p.319. Sheila Fitzpatrick, Stalinism; New Directions, (London, 2000), p.4. In terms of the political stealth Stalin demonstrated during the 1920's leadership struggle, the battle between Trotsky and Stalin is a revealing example. In particular the contrast between their political styles is interesting; Stalin a more typical politician tentatively weighing up policies and seeking out support; Trotsky a rather superior, aloof character, forthright with his often unpopular views on policy, and expecting support to find him. Stalin seems to have exploited the fears of the other Politburo members that Trotsky remained their biggest threat in terms of personal power. It seems doubtful that his motivations for the alliance with Kamenev and Zinoviev were based on anything more than a wish to remove Trotsky from the leadership race. The seeking out of this alliance created a triumvate which proved very powerful against the already unpopular Trotsky, and all under the cover of policy disagreement. With this kind of backing behind him Stalin 'carried battle in to the wider field of the central committee where he could always secure a majority."" Trotsky's potential support was either alienated by his brusque manner, or undermined by Stalin; the committee was reminded that he was a previous Menshevik, and of the many disagreements he had openly had with Lenin. Stalin often used the disagreements and arguments of the past, out of context, as a tool against his rivals. Trotsky was pitched against the Stalin, Zinoviev, and Kamenev majority as the 'other' rather than as a legitimate opponent with a valid argument. Stalin was able to use Lenin's ban on factions to make punishable Trotsky's attempts to argue his case, and at every chance undermined him; however when the committee called for action to be taken against Trotsky, Stalin defended him and bided his time. Infact throughout the battle with Trotsky, Stalin was careful to leave the heavy criticism to Zinoviev and Kamenev, again preserving the image of the moderate amongst the irrational that had proved so useful in the past. Deutscher, Stalin; A Political Biography, (Oxford, 1949), p.233. Duranty, Stalin and Co. , (London, 1949), p.9. Hingley, Stalin; Man and Legend, (London, 1974), ch7, p.5. Hingley, Stalin; Man and Legend, (London, 1974), ch7, p.6. Hingley, Stalin; Man and Legend, (London, 1974), ch7, p.5. The final factor in Stalin's rise to power involves the mistakes made by Stalin's opponents during this time. Perhaps one of the most important and most fatal mistakes was the underestimation of Stalin by his rivals; this underestimation in turn led on to other mistakes which were to seal the fates of Stalin's opposition. It was underestimation of Stalin as a worthy and threatening rival which led Kamenev and Zinoviev in to an alliance against Trotsky who they viewed as the most potent danger. 'Comrade card index' in his administrative tasks was never identified as a real problem. It was Kamenev and Zinoviev that endorsed Stalin's position as General Secretary despite knowledge of Lenin's uncertainty about the suitability of the capricious Georgian for this post. Whilst Stalin's fate hung in the balance, Zinoviev and Kamenev argued against the public reading of Lenin's testament, in effect saving Stalin from disgrace and investigation into his conduct as General Secretary. Trotsky made several key faults in his battle with Stalin, undermining himself and loosing himself support. He criticised party bureaucracy and the careerists who accompanied it; in effect criticising and threatening the positions of the very people from whom he required support. Stalin's other opponents made mistakes which Stalin with his 'peasant shrewdness' could exploit. The united opposition was such an unconvincing union of the former enemies Trotsky, Kamenev and Zinoviev that it was not only bound to fail, but bound to rouse little support from the party. These men were all politically broken from previous rounds with Stalin, and could provide little protection to any potential supporters from the increasingly ruthless tactics employed by Stalin and his followers. Likewise, the shaky union made between Zinoviev, Kamenev and Bukharin opposing Stalin's policy of rapid industrialisation and collectivisation, was almost inevitably discovered by Stalin and destroyed through utilisation of the ban on factions rule. Nove and Read, Stalin: Terror and Transformations, video. Read, The Making and Breaking of the Soviet System, (Hampshire, 2001), p.81. Deutscher, Stalin; A Political Biography, (Oxford, 1949), p.233. Read, The Making and Breaking of the Soviet System, (Hampshire, 2001), p.90. Hingley, Stalin; Man and Legend, (London, 1974), ch7, p.5. To conclude, I have identified four main factors which I have argued played the biggest part in Stalin's rise to power during the 1920's. The acceptance and support Stalin received from Lenin played an important part in allowing Stalin the possibility of playing any kind of key position within the party. The positions Stalin held permitted him great influence within a large number of party areas undetected. He was able to build up a support base and eliminate opposition; turning administrative power into political power. Stalin's personal gift at politics gave him an advantage during the uncertainty after Lenin's death. Stalin defeated those he needed to, he was a competent politician and was able to manipulate situations to his advantage in a way his rivals could not. Finally, the mistakes his opposition made, especially their failure to act upon Lenin's wishes to have Stalin removed from his post, and indeed the very uncertainty in which Lenin left the party leadership, allowed Stalin the chances which led to his rise. Read, The Making and Breaking of the Soviet System, (Hampshire, 2001), p.52. I have not argued that Stalin was a passive actor during his rise to power in the 1920's. He was an active force, using cunning, manipulation, and politics to engineer his position. However, he could not have done this without the other factors I have mentioned in this essay. All of them were important and I do not think Stalin would have emerged as leader of the party if any one was missing. The assumption that a gap existed between the theory and the practice in the social position of women between 1500 and 1700 is a contested one. Margaret Ezell argues that the expectations of female behaviour and duty in society did infact closely match the reality of women's experiences. Looking at contemporary literature and popular opinion, Ezell concludes that contrary to many historians who view the prescribed position as restrictive and misogynistic, both scripture and conduct books infact highlighted female capabilities and praised women as partners in life. 'Differences between men and women are merely superficial...women are as naturally capable as men of reason, wit and...government."" However, feminists have criticised Ezell for her limited interpretation of the literature she looks at. They argue that advice such as '...it is not that the wife has no mind of her own, but that she deliberately alters it if necessary, to conform with her husbands,' does not counter the view that women were expected to be submissive and were considered inferior to men during this period. Even Lawrence Stone, who argues that patriarchy was at its strongest and its most widely accepted at this time, acknowledges that the role set out for women often differed markedly from the actual experiences of womanhood, due mainly to the fact that the prescribed role was simply incompatible with the realities of life, especially for the poor. Indeed, Ralph Houlbrooke argues that the '...theory was simply unworkable...' with the realities faced by contemporary women. Pierre LeMoyne, 'The Gallery of Heroick Women', cited in M. Ezell, The Patriarch's Wife: Literary Evidence and the History of the Family, (New York, 1987), p.44. 'Feminist school of Phallogocentrism' who criticise Ezell, cited in Ezell, The Patriarch's Wife, (New York, 1987), p.37. Ezell, The Patriarch's Wife, (New York, 1987), p.40. L. Stone, The Family, Sex and Marriage in England 1500 - 1800, (Middlesex, 1977), p.139. Ralph Houlbrooke, The English Family 1450 - 1700, (London, 1984), p.106. In this essay I am going to argue from a standpoint assuming that a gap between the theory and the practice in the social position of women in early modern England did infact exist. I will argue that this gap existed despite both male and female attempts to keep within it, due both to its unrealistic expectations compared with the actual lives women led, and the dualities and contradictions within it. I will argue that due to three broad and overlapping areas: economic and personal factors, and a contradiction within the role which expected servility but also demanded command and power, women could not live out their lives within the boundaries of their expected social positions. The position women were to occupy in society was set out to the population through a number of mediums. Scriptural references in homilies and religious services put emphasis upon '...the procreation and religious education of children, the regulation of sexual activity, and mutual comfort and support.' A distinction was made between suitable female and male duties; 'The husband was held to be the superior partner, the wife the subordinate and inferior...her foremost duty was obedience."" In the advice guides published, women were seen to be 'material and passive...weak in body...and unfitted for work or public life.' As such they were expected to remain in the private sphere, '...unquestioningly obedient to her husband, accepting husbandly reproofs meekly...bearing her husbands faults...accommodating his various moods and on any breach being the first to seek reconciliation."" Satires and other anti-female publications played upon the popular images of women as sexually insatiable, nagging scolds, as well as playing upon the images of marriage as an end to freedom and fun, and a life of supporting a wife who would stop at nothing to get her own way. Whether popular opinion about women was more influenced by religion and advice guides or by satires is hard to tell, and indeed the opinion of the female role and marriage differed between men and women and according to place in the social order. It is probable that popular opinion took on board a mixture of both views as well as referring to personal experience of female figures and their behaviour. Finally, the expected social position of women was expressed by the authorities through the law; 'nowhere did the wife appear more completely subordinate to her husband than in the common law'. Women were never a homogeneous group; the law and indeed general opinion differed with different women; single, married, widowed and spinsters. However, in the eyes of the law, a woman belonged to the man in charge of her whether that was a father, brother or husband. Widows enjoyed a certain amount of freedom, however along with spinsters were so marginalised socially, for being 'master-less', and economically, with only a few poorly paid trades to work in, that they had very hard lives. Already, with this simple description of the expected social position of women it is possible to see some of the contradictions within it; for example, that women were sexually uncontrollable, yet the moral guardians of society. Susan Amussen argues that this contradiction is one of the main reasons why the wish for '...clear subordination of wives to husbands...was never fully realised.' Houlbrooke, The English Family, (London, 1984), p.97. Houlbrooke, The English Family, (London, 1984), p.97. Ezell, The Patriarch's Wife, (New York, 1987), p37. Martin Ingram, Church Courts, Sex and Marriage in England 1570 - 1640, (Cambridge, 1987), p.129. Houlbrooke, The English Family, (London, 1984), p.100. Susan Dwyer Amussen, An Ordered Society: Gender and Class in Early Modern England, (New York, 1988), p.115. Amussen, An Ordered Society, (New York, 1988), p.115. The gap between theory and practice in the social position of women in early modern England was certainly influenced by the economic realities they faced in their everyday lives. It was often necessary for poorer women to marry quite late; the time before marriage was spent working in service adding to the family purse. Girls had to leave home and live away from their families often giving them a taste for relative freedom, economic independence and a knowledge of their personal capabilities. Even after marriage it was often necessary for women to make economic contributions to the family; supplementing the low wages her husband was paid. Keith Wrightson argues that these female supplements to male wages were crucial to survival even if they did somewhat undermine the official female position in society. Susan Amussen points out that the gap between theory and practice actually lessened depending on the importance of the economic contributions of a wife. Some women did not need to compromise the submissive, passive role by economic contributions such as bartering at market or looking after the estate whilst a husband was absent. Although women rarely strayed in to their husbands' role once he had returned, it was essential to economic survival that they could and would perform his tasks even if this meant female and domination and authority. Men wanted their wives to be '...both subordinate and competent...and willing to accept that whatever a man did was work and whatever a woman did was her duty'. 'Women's independence and autonomy were critical to their success as wives and mothers...the contradictions between women's economic roles and their expected subordination were so severe that they posed a problem to the most carefully conforming wife.' Keith Wrightson, Earthly Necessities: Economic Lives in Early Modern Britain, 1470 - 1750, (London, 2002), p.53. Amussen, An Ordered Society, (New York, 1988), p.120. Houlbrooke, The English Family, (London, 1984), p.107. Anthony Fletcher, Gender, Sex and Subordination in England 1500 - 1800, (London, 1995), p.254. Amussen, An Ordered Society, (New York, 1988), p.121. Women were often needed in the public sphere, and even if not playing leading roles their actions still conflicted with the domesticated submission part of the role, although perhaps fulfilling the obedience part. Diane Willen argues that pauper women were actively recruited by local authorities in to the public arena. Although the work women were doing for local authorities was an extension of their traditional domestic duties; cleaning and caring, 'it questions...the existence of separate private/public spheres in the early modern period'. Both the poor women being employed and the 'local patriarchal political authorities' employing them gained economically from this arrangement. Women could earn a small wage and authorities gained a willing, cheap labour force; both parties were forced to revise women's official social position for economic gain. Widows were also women with access to the public sphere; they could by law take over some of their husband's trades or estates upon his death. Although often poorer widows were left very economically vulnerable, some were able to maintain themselves successfully; indeed advice guides warned men against marrying widows as they tended to be 'harder to tame' having already tasted and achieved economic independence. Diane Willen, 'Women in the Public Sphere in Early Modern England: The Case of the Urban Working Poor', Sixteenth Century Journal, 19, (1988), p. 559 - 575. Willen, 'Women in the Public Sphere in Early Modern England', Sixteenth Century Journal, 19, (1988), p.559. Willen, 'Women in the Public Sphere in Early Modern England', Sixteenth Century Journal, 19, (1988), p.560. Anthony Fletcher, Gender, Sex and Subordination in England 1500 - 1800, (London, 1995), p.174. The second set of factors which made the social position set out for women in early modern England impossible to fulfil even if they tried, were personal factors. By this I mean factors which come about naturally as a consequence of marriage; i.e. spending a lot of time with someone, growing to know them very well and loving them. Love and caring were seen as very important aspects of a successful marriage, in particular they were seen as part of a husband's duty. However it was well documented in contemporary literature that '...more men betrayed their command through their own fondness than ever lost it through their wives rebellion'. Though caring and affection were viewed with importance; and were often an unavoidable outcome of spending lives together, measures were taken to limit the influence affection had on the social position of women. 'Convention demanded that the wife address her husband with humility and deference...even the affectionate terms love, joy, dear and duck were not appropriate in the mouth of a subordinate."" However it must have been very difficult to maintain a purely master/subordinate relationship especially when feelings of love and affection existed in a marriage. Even the most obedient wife and most masterful husband must have come across occasions when such formal address would have been inappropriate. Thomas Fuller cited in Houlbrooke, The English Family, (London, 1984), p.101. Houlbrooke, The English Family, (London, 1984), p.101. Laura Gowing argues that the gap between theory and practice actually began during courtship, making it only natural that it continue after marriage. Gowing explains that it was sometimes necessary to deviate from the passive, submissive role of a courting woman. The double standard which existed in terms of male and female sexual morality and the importance of reputation in early modern society made courting quite a difficult process. Women had to use their intricate knowledge of the laws of courting to avoid the many traps which could leave them tied to a poor marriage match or with their reputation in tatters. Often it was impossible for a woman to remain passive during courting, especially if conjugal rights had been granted to a man after the promise of marriage, or pushy parents were disdainful of certain suitors. Often with a little bit of courage and forcefulness, men could be made to keep promises, and by scrutinising the exact wording used in the making of a marriage contract they could be escaped. 'Women could still exercise their power of evasion and refusal...language...gave women the opportunity to broaden the largely passive part allotted to them in the process of courtship.' Laura Gowing, Women, Words and Sex in Early Modern London, (Oxford, 1998), p.147. Laura Gowing, Women, Words and Sex, (Oxford, 1998), p.148. Other personal factors made the female social position hard to follow strictly; in many cases women's personal capabilities and desires surpassed the prescribed role. Often individual character and temperament sat at odds with the boundaries set out; after all, not all wives were inferior to their husbands and not all husbands' wanted/needed submissive wives. Simple practicalities such as very little age difference between partners, made the expected master/servant relationship, hard to fulfil; a woman with equal or more life experience than her husband could be hard to commandeer. Finally, women's roles and identities within society were not a constant thing, they were much less fixed than men's; as they were already malleable, it was relatively easy to simply revise these identities and social positions further. Houlbrooke, The English Family, (London, 1984), p.103. Laura Gowing, Women, Words and Sex, (Oxford, 1998), p.231. The final aspect which made it impossible for women to live wholly within their official social position, were the areas of society over which women had complete power and control; morality, reputation and of course the domestic sphere. In marriage and out of it women were to control the sexual activity of themselves and their spouses; adultery was seen as dangerous, putting the established social order in jeopardy, and even too much sex between husband and wife was viewed negatively. Women's control over reputation gave them a certain amount of power over both other women and men; as Samuel Pepys diary shows '...when his wife caught him with their maid...he at once recognised the political significance of the event'. Women could wield the reputation card to expand their limited role and position in society. Gossiping was another largely female activity allowing them to channel pressure on those they felt were morally lacking whether for bastardy, cuckolding or wife beating. Women were called upon to ensure that their men folk were satisfactorily carrying out their duties and this gave them a certain amount of power. For example, if men were neglecting their religious duties as head of the household, a wife could 'take-over' this part of his role even if he was in objection to this. This expectation of wives by religious authorities severely compromised the obedient social position they otherwise instructed her to take. Houlbrooke, The English Family, (London, 1984), p.97. B. Capp, 'The Double Standard Revisited: Plebeian Women and Male Sexual Reputation In Early Modern England', Past and Present, 162, (1999), p. 73. B. Capp, 'The Double Standard Revisited, Past and Present, 162, (1999), p. 74. Laura Gowing, Women, Words and Sex, (Oxford, 1998), p.67. Houlbrooke, The English Family, (London, 1984), p.98. The domestic sphere, often seen as the sight of women's inequality, was infact the one area of society over which women had a lot of control. Rule over servants and the running of a complex household were just some of the tasks women were expected to perform. Other tasks included care and religious education of children as well as total authority in the field of pregnancy and childbirth; 'lay ins' gave women an all female space in which to recover after childbirth. Women were also expected to give their husbands sound business advice and make decisions within the domestic sphere. The fact that these tasks could not be performed without being domineering, active and authoritarian was why a necessary gap existed between theory and practice in the social position of women. Amussen, An Ordered Society, (New York, 1988), p.107. To conclude, I have tried to show that even men and women attempting to keep within their official social positions found it an impossible task. I have looked at some of the factors that made this impossible; including the economic realities which meant women's roles had to overlap with men's and forced them beyond the passive, submissive boundaries of their role. There were also personal factors, such as the natural familiarity which grows as a result of married life which made strict adherence to the master, subservient relationship between couples hard to maintain strictly. Finally, there were areas of society in which women held most if not all power, these areas meant that women by necessity had authority which contradicted the submissive position intended for them, as well as giving women power which they could use to renegotiate their limited position. Amussen, An Ordered Society, (New York, 1988), p.109. Susan Amussen argues that the attempts of women to remain faithful to their prescribed social position, but failure due to the impractical, unrealistic nature of the position, is shown by women's emphasis on the sexual conduct part of the role; the part that was actually achievable. Faced with the impossibility of the obedience part, women placed greater importance on strict sexual conduct. Where possible women were willing to adhere to their prescribed social positions; helping to maintain the social order so important to early modern society. As Bernard Capp argues; women did not directly challenge the system, rather they worked within it, finding compromises to suit their economic, and personal realities. They also accommodated the features such as commanding, and decision making which went against their prescribed temperament but which were necessary for the domestic success also expected of them. The gap between theory and practice in the social position of women in early modern England should be explained in terms of its impractical economic nature and its contradictory and unrealistic expectations. Amussen, An Ordered Society, (New York, 1988), p.122. B. Capp, 'The Double Standard Revisited, Past and Present, 162, (1999), p.99.",False
52,"The most extensive population study of the early modern period was carried out by E.A. Wrigley and R.S. Schofield, The Population History of England, 1541 - 1871: A Reconstruction, which set out calculated estimates of population trends for this period. They used the method of back-projection to estimate the population levels of the country between 1541 (the beginning of the first decade after the introduction of parish registers) and 1871 (the date of the second reliable census). This project was an immense undertaking which used four hundred and four surviving parish registers which recorded baptism, marriage and burial, to estimate demographic trends at five year intervals between these two dates. Many criticisms of both the methodology used and the results achieved have been made of this study. In very general terms these criticisms focus upon the following main areas. The sample of four hundred and four parish registers used to estimate national population figures makes up only four percent of the total number of parishes in early modern England. In addition, the sample includes too many large parishes (usually Northern areas) and too few smaller parishes (usually Eastern areas). London is also underrepresented in the study due to its lack of surviving parish registers. Even good registers however were inaccurate for certain periods (such as during the civil war) and thus only for the year 1662 were all the registers fit to use. The unreliability of back projection as a technique for working out population levels has been criticised in terms of the assumptions Wrigley and Schofield made in using this method; for example assumptions about the age structure in place throughout this period will have influenced the final results. E.A. Wrigley and R.S. Schofield, The Population History of England, 1541-1871: A Reconstruction, (Cambridge, 1981). Wrigley and Schofield, The Population History of England, p. 15. Wrigley and Schofield, The Population History of England, p. 16. Wrigley and Schofield, The Population History of England, p. 455. John Hatcher, 'Understanding the Population History of England 1450-1750', Past and Present, no.180, (August 2003), p.125. Despite such criticism the information Wrigley and Schofield provide does appear a plausible reading of the demographic trends of early modern England. After all, the authors are aware that methodological shortcomings exist; this work does not pretend to be concrete evidence but merely a likely estimation. Even taking in to consideration the limitations of the evidence and the methodology as well as the limitations of the study itself, i.e. : it operated on a very statistical level and was not placed in much of a social or cultural context, this remains a very useful work. Furthermore the huge financial costs and time commitments involved in this kind of research make it unlikely that another work of this nature will be undertaken. These are the best results we have, and particularly when used in conjunction with localised studies as well as studies of social and cultural factors this is very revealing research. In this essay I am going to argue from a standpoint assuming that the data compiled by Wrigley and Schofield is a realistic estimation of population trends. However, I will argue that an acceptation of the population trends presented by the Cambridge Group does not necessitate an acceptation of their interpretation of the causes of these population trends. I will argue that fertility and mortality were important in influencing population change. I will argue that the essential inter-linkage of these two factors makes any either/or demographical explanation implausible. Instead I will suggest that a model whereby both fertility and the two parts of mortality (disease and starvation) helped shape population and each other is a more realistic explanation to such a complex and wide-spanning phenomenon. Wrigley and Schofield, The Population History of England, p.456. The demographic trends that Wrigley and Schofield have mapped out for early modern England begin with growth between 1541 and 1661; this growth was generally steady although it slowed during the 1550s and became rapid in the 1570s and 1580s. Between 1656 and 1686 the population decreased in size before stagnating and slightly recovering between 1686 and 1750. From 1750 onwards the vigorous growth of earlier periods was resumed, although Elizabethan growth rates were not reached until the 1790s. Thus there were two periods of growth divided by one hundred years or so from the mid seventeenth century to the mid eighteenth century when population numbers stagnated. During these main periods of growth the population increased significantly almost doubling from 2.8 million to 5.3 million between 1541 and 1656, and after the years of stagnation again doubling between 1731 and 1816 and between 1861 and 1871. As I have suggested this basic information seems both plausible and realistic, however Wrigley and Schofield's interpretation of this information; an attempt to understand what caused this rapid growth, check and decline followed by renewed growth, seems less plausible. The principle question they look at is whether '...a rise in GRR (gross reproductive rate) from 2.5 to 3.0 for example is more important than a rise in eo (life expectancy from birth) from 35-40 years in effecting growth rates."" Or in other words, was it fertility or mortality that was the definitive factor in determining these population trends. Roger Schofield, 'The Impact of Scarcity and Plenty on Population Change in England 1541-1871', Journal of Interdisciplinary History, 14:2, (Autumn 1983), p.270. Schofield, 'The Impact of Scarcity and Plenty on Population Change in England', p.269. E.A. Wrigley, People, Cities and Wealth; The Transformation of Traditional Society, (Oxford, 1989), p.243. Wrigley and Schofield, The Population History of England, p.236. The Cambridge Group certainly believed that '...marriage was the hinge upon which the demographic regime turned."" They suggested that '...fertility change was more important than mortality change in altering population growth rates', that '...fertility change in turn chiefly reflected fluctuations in nuptiality' and that '...nuptiality was primarily determined by real wages'. Thus Wrigley and Schofield presented a structure where a rise in real wages would lead to a rise in the number of marriages as well as a rise in the number of early marriages and therefore a rise in the numbers of births. Wrigley and Schofield themselves note the inevitable sacrifice of comprehensiveness with the construction of simple models, however they argue that the benefits gained in clarity are essential to understanding. Therefore the picture of demography that they present for early modern England at this time is one where the preventative check system of Malthus was in operation. This system proposed that societies could control their population growth so that the need for a positive check (natural disaster, famine, disease) to regulate population size to fit food production would be unnecessary. The actual data Wrigley and Schofield present for fertility includes 'high fertility in the sixteenth century followed by slow a decline from GRR (gross reproduction rate) from 2.8 to 1.9.' This in turn was followed by 'a gently rising trend to reach 2.3 by 1756', before an acceleration of this trend 'to reach a peak value of 3.06 in 1816.' Thus they argue that 'towards and between the major turning points of about 1660 and 1815...there were very few marked departures from the trend line, but rather a steady progression.' Wrigley, People, Cities and Wealth, p.39-40. Wrigley and Schofield, The Population History of England, p.549. Wrigley and Schofield, The Population History of England, p.466. Wrigley and Schofield, The Population History of England, p.467. Hatcher, 'Understanding the Population History of England', p.124. Wrigley and Schofield, The Population History of England, p.229. There has been a lot of criticism of this model, both in terms of the methodology used to get the data on fertility and the final results presented, but also in terms of the Malthusian framework in which these results are placed. The method used to estimate age at marriage was 'family reconstitution', which traced individuals' lives through their parish. This method requires both baptism and marriage to have taken place in the same parish and assumes out-migration rates are low. In many cases however the 'sparrow phenomenon' or people flitting in and out of the records made this method impossible; in fact only twelve parish registers provided reasonable estimates for age at marriage. It has been suggested that it is probably these twelve parishes which are unrepresentative; the norm, as shown by the majority of records, was to move around the country rather than to remain in the place of birth. Thus this method uses a mere 0.12 percent of the total number of parishes to work out estimates of age at marriage on a national basis, and it is likely that this 0.12 percent is unrepresentative of the total population in terms of lifestyle habits. Marriage did not even become a regulated and controlled institution before 1753 when laws were introduced in order to make those marriages not carried out in a church illegal. Before this point marriage could be legal simply through the saying of vows in front of a witness; Laura Gowing points out that before 1753 the promise to marry followed by sex also constituted legal if irregular marriage. This ambiguity in the definition of marriage leads to further questions about the ability of Wrigley and Schofield to estimate marriage accurately because of course these non-ecclesiastical weddings would not have been recorded. John Hatcher argues that the fertility rates estimated were forced into the model of Malthusian preventative check which had already been formulated by the Cambridge Group before they even began their research. He asserts that a model where the demographic regime can be '...distilled into a counterpoint between marriage and the levels of real wages...is overly singular, simple and mechanistic...and incapable of capturing historical reality.' Wrigley and Schofield, The Population History of England, p.229. Hatcher, 'Understanding the Population History of England', p.88. Laura Gowing, Women, Words and Sex in Early Modern London, (Oxford, 1998), p.147. Hatcher, 'Understanding the Population History of England', p.91. Hatcher, 'Understanding the Population History of England', p.88-89. Even if the marriage and fertility rates, potentially unrepresentative and framed as they may be, are accepted as plausible, questions have been raised about the placement of emphasis in Wrigley and Schofield's interpretation. David Weir argued that marital behaviour altered considerably over time causing a shift in the dominant factor affecting fertility and thus population trends. 'From a period in which the incidence of marriage was dominant to one in which changes in the ages of those marrying was dominant."" Some have noted that it is difficult to assert the dominance of fertility on population trends, particularly when within the boundaries of the Malthusian model. 'Imposing' this framework on the fertility data has required Wrigley and Schofield to argue that a 'long lag' relationship of about fifty years existed between a change in real wages and its affects on fertility. 'A key puzzle of the demography is why fertility continued to rise when real wages were stagnant...the lag of course solves this problem."" J.A. Goldstone accepts that a lag is a plausible explanation; however suggests that reducing it to a twenty year difference would be more accurate. Others however are less convinced by the theory; Steve Hindle suggests that we do not know enough about marriage horizons, the preference for nuclear family households, factors influencing nuptiality and in particular the influence of institutional pressures (such as apprenticeships and the poor law) in shaping marriage, to make assertions about a long lag and the population correcting its own development in reaction to conditions of scarcity and plenty. Indeed it has been noted that as a period of low life expectancy the possibility of parents surviving to influence the marriage patterns of the next generation is most unlikely. Similarly, questions have been asked about the amount of influence a parent would have had over their children. Perhaps in cases where inheritance would depend on parental approval influence would be quite significant, but for the majority of the population inheritance would not have been left. The deterministic nature of this fertility led explanation for the population growth of 1541-1656 and 1731-1871 does not allow room for other factors, and does not take regional variations in to consideration. Although I do not wish to argue in a deterministic fashion that mortality was the most important factor in shaping the population 'replacing one bald stereotype with another', it is important to look at the quite significant part played by mortality in order to question Wrigley and Schofield's interpretations. David Weir, 'Rather Never than Late: Celibacy and Age At Marriage in English Cohort Fertility, 1541-1871', Journal of Family History, 9, (Winter, 1994). Steve Hindle, 'The Problem of Pauper Marriage in Seventeenth-Century England', Transactions of the Royal Historical Society, 6th ser. 8, (1998), p.72. Hatcher, 'Understanding the Population History of England', p.88. J.A. Goldstone, 'The Demographic Revolution in England: a Re-examination', Population Studies, 49, (1986), p.9. Goldstone, 'The Demographic Revolution in England', p.13. Hindle, 'The Problem of Pauper Marriage in Seventeenth-Century England'. Susan Amussen, An Ordered Society: Gender and Class in Early Modern England, (London, 1988), p.34. Lawrence Stone, The Family Sex and Marriage in England 1500-1800, (London, 1977), p.113. Hatcher, 'Understanding the Population History of England', p.89. The two parts of mortality; starvation and disease were both influences on the early modern population, not least because their presence pre-empted any potential rise in fertility through restricting marriage partners, increasing migration and affecting prices and wages. The influence of factors other than fertility can be seen in the rise of the population despite a fall in real wages; I am afraid that I remain unconvinced by the long lag theory and thus this slowness of response between the two seems to imply that mortality had more influence than Wrigley and Schofield suggested. John Hatcher points out that the volatility of both fertility and mortality during this time is suggestive not of Wrigley and Schofield's model where '...there were very few departures from the tend line', but of a model where many influences were working to shape the population. 'Conclusive statistical proof of the exceptional instability...high annual deviations between 1550/1 and 1574/5 of 17.7 percent in the crude death rate, and 12.2 percent and 8.0 percent respectively in the marriage and birth rates."" Hatcher also notes that assumptions made by Wrigley and Schofield to fill in gaps left by poor evidence and their small sample had led to the effects of mortality being underestimated. 'A far more severe impact is revealed by the registers not included in the study and also by the abundance of wills probated in a wide range of ecclesiastical jurisdictions."" Additionally, Wrigley and Schofield's determination to prove the dominance of fertility can be seen in their relegation of mortality to a mere 'condition' of population change whereas fertility is considered a 'cause'; mortality is presented as something which affects fertility rather than as an influence on population in its own right. As Hatcher points out 'in any analysis of causality the designation of some factors as 'conditions' or 'circumstances' and others as 'causes' is liable to exert a powerful influence over the outcome."" What Hatcher calls 'a tangle of contradictions and reservations' in fact 'lie behind the series of confident judgements...made.' Wrigley and Schofield, The Population History of England, p.229. Hatcher, 'Understanding the Population History of England', p.102. Hatcher, 'Understanding the Population History of England', p.104. Hatcher, 'Understanding the Population History of England', p.105. Hatcher, 'Understanding the Population History of England', p.106. There are a number of issues surrounding the fertility led preventative check theory and the above criticisms are useful to demonstrate a rather general lack of accuracy in Wrigley and Schofield's model; however it is also useful to look at more specific studies to identify the effects of mortality on localised areas. Peter Laslett's study on famine in early modern England concluded that the threat of starvation or 'crisis mortality' was very unlikely to occur. However, death via malnutrition and via food poisoning (through eating rotten or poor quality foods) , though not technically starvation as such, could still cause population rates to fall as well as making the population more susceptible to disease and lowering fertility. This distinction between outright starvation and malnutrition is something which Andrew Appleby also remarks upon, arguing that death due to starvation and malnutrition could be very hard to identify. Appleby suggests that the years of 'recurring population crises' between 1550 and 1640 were in fact due to mortality and further that 'the role of famine as a killer  in its own right has been underplayed' generally 'in the history of early modern England,' (my italics). Wrigley and Schofield do recognise that at particular times throughout history mortality has been more dominant than fertility in shaping demography, yet they conclude that overall fertility had the more decisive influence. Appleby's findings in Westmorland and Cumberland show that perhaps this decisive influence is misleading because the effects of mortality have been repeatedly and mistakenly underplayed. Peter Laslett, The World we have Lost: Further Explored, (Cambridge, 1965), p.151. Laslett, The World we have Lost, p.134. Andrew Appleby, 'Disease or Famine? Mortality in Cumberland and Westmorland 1580-1640', The Economic History Review, Second ser, 26, (1973), p.404. Appleby, 'Disease or Famine? ', p.403. Wrigley and Schofield, The Population History of England, p.238. Disease had specific characteristics; it was often recorded in parish registers as a cause of death and appeared in other primary evidence such as wills. Famine did not have such a 'discernable pattern to help the historian identify it' it did not have such specific characteristics, it was rarely recorded in parish registers, and those who could write (more likely the upper sorts) would have been less likely to feel the effects of famine and able to relocate in plenty of time before actual starvation set in. Appleby looks at available sources and compares them closely with the known characteristics of the prevalent diseases of the period; epidemics were rarely rural problems, they declined in Winter (the winter disease Typhus rarely killed children and is thus identifiable), disease set in very quickly and in the case of plague 'would often follow the main lines of trade between cities, fairs and seaports'. The characteristics of starvation are much harder to identify and provide proof of; parish records were often unhelpful, famine did not follow a specific development like that of disease and when it appeared in the form of malnutrition it is even harder to discern. Appleby develops a more detailed methodology for identifying famine than the three main requirements of Peter Laslett; he noticed that in cases of starvation gradual or rapid onset of deaths could occur but a dramatic increase in mortality (burials twice normal) should be expected. Several neighbouring villages would also have increased deaths, a correlation between prices and mortality should be noticeable, usually some reference to dearth would exist in one form of source or another and there should be fewer conceptions than usual. Not only does this list of the characteristics of famine offer evidence that it can be very hard to identify, but that it is very closely linked to fertility; not as a condition of the rise/fall of fertility, but as a factor in its own right. Appleby discovered that famine had played a significant part in shaping the population in Cumberland and Westmorland; at certain times it was the dominant factor causing the death toll to mount, at other times it made areas more susceptible to disease (through lessened immune systems, and through desperate importing of foods from areas with epidemics) and made women less willing and less able to conceive. Appleby, 'Disease or Famine? ', p.406. Appleby, 'Disease or Famine? ', p.404-5. Appleby, 'Disease or Famine? ', p.421. Appleby, 'Disease or Famine? ', p.422. Appleby, 'Disease or Famine? ', p.430-31. In their study of Mid-Wharfedale parishes in 1623 Moira Long and May Pickles also found that evidence of 'crisis mortality' was often a part of early modern life, if not in the form of full blown famine certainly in the form of 'crisis months'. What Long and Pickles also recognise is the importance that the threat of famine played in shaping populations, for example the fear of shortages could affect births (not necessarily through less marriages but also through stress and amenorrhea) and could persuade locals to import food raising the likelihood of an epidemic. It is the backdrop of general high mortality in England in 1623 against which Long and Pickles set their study and they found, as suggested by Appleby, that in the areas they looked at scarcity and famine were the hardest to identify. However they suggest that 'erratic' results for famine exemplified this period and that 'the balance between sufficiency and dearth in many parts...was finely poised."" Victor Skipp found considerable regional variation in occurrences of famine and shortages which might conceivably have led to their relative invisibility in the small sample of Wrigley and Schofield. These findings and Appleby's findings from specific area studies seem to reflect Hatcher's argument that the presentation of both the steadily rising influence of fertility and steadily decreasing influence of mortality is inaccurate. In the same way Wrigley and Schofield's treatment of famine as a mere condition rather a cause affecting the turning of the demographic regime is equally misleading; yes fertility was important in 'turning the regime', but it was by no means alone in its efforts. Maria Long and May Pickles, 'An Enquiry into Mortality some Mid-Wharfedale Parishes in 1623', Local Population Studies, 31, (Autumn, 1986), p.27. Long and Pickles, 'An Enquiry into Mortality some Mid-Wharfedale Parishes in 1623', p.25. Long and Pickles, 'An Enquiry into Mortality some Mid-Wharfedale Parishes in 1623', p.26. Long and Pickles, 'An Enquiry into Mortality some Mid-Wharfedale Parishes in 1623', p.34. Victor Skipp, 'Economic and Social Change in the Forest of Arden 1530-1649', Agricultural History Review, 18, (1970), p.105. Hatcher, 'Understanding the Population History of England', p.106. Wrigley, People, Cities and Wealth, p.39-40. Throughout this essay so far, I have tried to emphasise the continuous links between mortality and fertility during the early modern period; it is this essential inter-linkage which I wish to look at more explicitly now. There were interconnections between disease, migration, death, birth rate, price and wage, amongst others; in practice of course the nature of and the strengths of these links varied, nevertheless they did exist. The incidence of famine whether long or short term affected disease rates, fertility rates and migration rates as well as affecting prices and wages as the population lessens. Likewise, high fertility could force prices up, wages down, encourage migration and make disease more probable. It seems to me that the circular nature and knock on effect of all these factors combining to shape each other and the population would be better described in terms of a set of dominoes rather than as a hinge which suggests not only the domination but also the presence of only one factor. Does not the very reality of high fertility demand low levels of disease and plenty of food? And does not the very reality of famine demand high levels of population? As Laslett suggests 'each part of our subject is also our whole subject...the fund of food was obviously related to age at marriage and numbers marrying.' Laslett, The World we have Lost, p.122. Hatcher suggests that Wrigley and Schofield manipulate data to exaggerate fertility and lessen the significance of mortality; he argues that the presentation of information in The Population History of England is 'skewed' in favour of fertility. Even the date at which the study begins favours their Malthusian framework; 'the first half of the sixteenth century was a period of demographic transition, results would have been very different if the parish register series had begun a couple of decades earlier or later.' Although Hatcher accepts that this was purely coincidental, 'a historical accident', it still demonstrates that the prominence awarded to fertility by Wrigley and Schofield is not necessarily historically accurate but is based more upon interpretation, evidence and methodology used. In fact, a case for the essential interaction and inseparability of the factors influencing population can be made from within The Population History of England. From '1551 to 1566 falling fertility was the main driving force until the ""golden period of low mortality"" turned the tables; 1571 to 1611 was a period of relatively stable fertility and mortality ""fertility and mortality contributed equally to the steady fall"", fertility was dominant from 1671 to 1691 but mortality was most clearly the more important influence on growth rates from 1691 to 1751."" Hatcher sums up these characteristics and finds that 'fertility is held to dominate for around thirty-five years of the two-hundred year time span, mortality for around sixty-five years and that for a century the two variables exerted approximately equal influence."" This evidence is 'immediately contradicted' by Wrigley and Schofield through the use of 'quinquennial averages', and the discarding of 'three episodes of very high mortality before 1751'. Nonetheless it seems evident that 'fertility and mortality exchanged leadership many times within the early modern period' and that pinpointing which one was dominant at different times would prove a more useful exercise than attempting to prove which was dominant full stop. Hatcher, 'Understanding the Population History of England', p.114. Hatcher, 'Understanding the Population History of England', p.112. Hatcher, 'Understanding the Population History of England', p.107. Hatcher, 'Understanding the Population History of England', p.107. Hatcher, 'Understanding the Population History of England', p.108. Hatcher, 'Understanding the Population History of England', p.116. In conclusion, despite the problems with Wrigley and Schofield's methodology and evidence the population trends they map out are plausible and the best figures historians have to work with. I have suggested however that in many ways their interpretation of the causality of this trend is flawed. The judgement that living standards and wage rates determined nuptiality and therefore fertility, or that 'marriage is the hinge upon which the demographic regime turned' is inaccurate. Family reconstitution provided data on fertility which was unreliable and unrepresentative. However, even accepting these methodological limitations the figures then seemed to be moulded to fit a Malthusian framework of preventative check homeostasis. The 'long lag' theory used to justify the lack of direct correlation between fertility and real wages over simplifies a very complex phenomenon whilst simultaneously relegating other important factors to mere conditions or circumstances rather than causes of population change. The fact that we do not know enough about marriage patterns, non-church marriages, or the parental influences over children of every day people makes the long lag an unsustainable claim. The deterministic nature of this model undermines the significance of mortality and other factors upon population changes and upon shaping fertility. The relative instability and volatility of both fertility and mortality suggests more detail is needed in the model Wrigley and Schofield provide. Evidence such as those registers not used in The Population History sample as well as personal documents from the time show that mortality had more influence than the Cambridge Group suggest. Mortality in the form of starvation was often extremely hard to identify due to its lack of recording in parish registers. The type of scarcity which never developed in to full blown famine is even harder to spot, but was equally influential in shaping fertility and population. Studies of Westmorland and Cumberland, the Forest of Arden and of Mid-Wharfedale parishes reveal a mistaken and repeated underplaying of starvation, and its often very localised nature. In fact the presence and at times domination of mortality was important and deserves more attention in  The Population History. Similarly, although I have not focused greatly upon the influence of migration and in particular sex specific migration on population trends, through reducing the pool of those eligible for marriage, its importance should not be underestimated either. I have tried to show that the links between starvation, disease and fertility are more important and relevant than which of these major demographic variables was dominant overall. Each affected the others and each in turn was affected by many other minor demographic variables. Therefore I have suggested that a model whereby the essential interlinks of mortality, migration and fertility would prove most accurate. Hatcher claims that in total between 1551 and 1751 'fertility is dominant for 17.5 percent of the time, mortality for 32.5 percent and for 50 percent of the time the two variables exerted approximately equal influence."" Manifestly he argues that causality cannot be established until it is accepted that fertility and mortality were both important factors within the early modern period '...and a host of social, cultural and economic factors played on them to help fashion demographic behaviour."" To finally answer the question, I would suggest that the metaphor of fertility as a 'hinge upon which the demographic regime turned', be discarded and replaced with a metaphor which better encompasses the historical reality of the exertion of manifold influences, and a co-relation of factors which developed to shape what was infact less a regime, and more a volatile commune. 'Any valid depiction of reality is likely to resemble a vast irregular web'. The achievements of the Cambridge Group should not be underestimated, but equally nor should the inadequacies of their interpretative model. 'Patterns of births, marriages, deaths and migrations helped to shape society and economy and were themselves shaped by society and economy' as I have attempted to show in this essay. Goldstone, 'The Demographic Revolution in England', p.22. Hatcher, 'Understanding the Population History of England', p.107. Hatcher, 'Understanding the Population History of England', p.116. Hatcher, 'Understanding the Population History of England', p.129. Hatcher, 'Understanding the Population History of England', p.129.","To answer a question on Russia, I think it is first necessary to give some general details about this extraordinary and difficult to classify country. Russia stands eight thousand miles long, for much of the year it is landlocked, surrounded by foreign powers with whom it shares seas and rivers with. Russia has specific producer regions upon which some areas are dependent; Russia's variable climate makes growing hard in many areas. Russia's population in 1905 was very large and very mixed, only about fifty percent of this population was infact Russian. Nationalities including Ukrainians, Polish and Armenians made this a multi-cultural, multi-religious country which was very hard to generalise about demographically. In terms of classes, Russia had some more traditional groups such as peasants, landowners, autocracy and merchants. Whilst also having some modernising classes such as industrialists, foreigners and intelligentsia. Politically, Russia was run by a single autocratic monarch, seen as ordained by God. Democracy was seen as an evil the Russian people ought to be saved from, political parties as well as free speech were not allowed under the Tsar. Christopher Read, Lecture, University of Warwick, October 2003. Having looked briefly at some of the important features of Russia in 1905 to place it in to some kind of context, I wish firstly to outline what exactly the 1905 revolution was; looking very briefly at its causes and the events that took place. Secondly, I plan to look in some depth at the main reasons given by historians on why this revolution 'failed'. Finally I will conclude by suggesting that although the 1905 revolution did not result in a full revolution, this was not its aims, therefore simply classifying it as a failure may not be appropriate. 'Until 1905 the struggle against the autocracy was predominantly an affair of individuals and small groups...it was only in that year that the Russian masses...were stirred in to action on a larger scale."" The 1905 revolution was the response of the people (or at least some of them) to years of hardship and harsh autocratic rule. It was perhaps the taste for change they felt under Tsar Alexander II (such as peasant emancipation) that led them to question Tsar Nicholas II when he took measures to strengthen autocratic rule. The expansion of Russian industry during the 1890's '...was succeeded by a period of depression and industrial conflicts became sharper and more numerous."" Strikes, though illegal were becoming more common at this time. Propaganda spread by revolutionaries and the intelligentsia was beginning to reach and interest workers disillusioned by working conditions and unfair treatment by superiors. The peasantry still carried their resentment at the government and landowners over their emancipation which left them tied to the land by the increased power of the village commune, and forced to make redemption payments to the state. The peasants saw themselves as rightful owners of the land because they were they ones who tended it. They seem to have borne this bitterness quietly until they sensed their old enemies growing weak in 1905; and turned on the landed and wealthy. Also at this time, poverty and even starvation were not uncommon and were arguably enhanced by government policies which built up huge foreign debts. Chamberlin, The Russian Revolution, (London, 1935), p.46. Edward Acton, Russia, The Tsarist and Soviet Legacy, (Essex, 1995), p. 69. Chamberlin, The Russian Revolution, (London, 1935), p.48. E. R. Wolfe, Peasant Wars of the Twentieth Century, (London, 1971), p. 63. Wolfe, Peasant Wars of the Twentieth Century, (London, 1971), p.58. 'Student disorders, assassinations of government officials...growing demand for liberal reforms all added to the worries of the government and created a tense atmosphere."" In part to divert attention from this state of affairs, the government launched a 'little victorious war' on the Japanese. That this war proved a disaster for Russia leaving her shamefully beaten by 'the little yellow monkeys' and diverting precious resources for a lost cause did not help the political situation. Further causes of the 1905 incidence included the modernisation and democratisation of other European powers, in sharp contrast to Russia's 'backward' autocracy and economy, as well as the hesitant, weak leadership qualities of the Tsar himself. The catalyst for all these problems in Russia in 1905 proved to be what was intended as a peaceful, even slavish event. Bloody Sunday was a non-violent protest to the Tsar, by workers and their families, about poor working conditions, and poor relations with managers. The demonstration was organised by the priest, Father Gapon, a police agent put in place to help control labour organisation in the face of the increased striking. The march was preceded by a general strike, which stirred the authorities in to action, and ended up with them using the military to fire upon marchers even as they carried pictures of the Tsar and sang patriotic songs. The day of bloodshed obliterated the Tsars' image of the 'little father' of the people; many ordinary workers and their families were left shocked and horrified at the Tsars' action. Sympathy strikes and further chaos followed before the autocracy gradually regained its shaky control of Russia. I now wish to look at the reasons why this chaotic period did not result in the full revolution that occurred under similar conditions in 1917 which deposed the Tsarist regime. Chamberlin, The Russian Revolution, (London, 1935), p.47. 'The Russian Revolution of 1905',  URL , (09/11/2003). (As the Tsar 'habitually' referred to them). Chamberlin, The Russian Revolution, (London, 1935), p.47. A, Verner, The Crisis of the Russian Autocracy, (Oxford, 1990), p. 2. Acton, Russia, The Tsarist and Soviet Legacy, (Essex, 1995), p. 115. Harcave, First Blood, (London, 1965), p.89. The opposition to the government in 1905, though it did exist, was unorganised with no formal aims to work towards. The main left wing parties were still finding their feet; Lenin was still deciding which brand of socialism to support, and theorising about the application of socialism in relation to Russia's distinct features. Revolutionaries and the intelligentsia worked towards spreading anti-Tsar feeling among the urban workers and the peasantry. However their work was patchy and disorderly and the reception their work received was mixed. Students dressed up as peasants to try to spread revolutionary beliefs; however they were distrusted and consequently were relatively unsuccessful. Similarly, radicals would address gatherings of peasants, attempting to spread and develop anti-establishment ideology, and stir the peasantry in to action. In the towns, revolutionaries spread leaflets presenting their ideas to the already disgruntled work force. Though some of the ideas of revolutionaries in the towns were accepted and recognised by many, loyalty to the Tsar at this point was not faltering. Though people were unhappy with the government, they blamed ministers, believing that the Tsar was ignorant of his people's fate. Father Gapon's brand of action was much more appealing; it was more organised, with one main leader, and remained loyal to the Tsar. P. Waldron, The end of Imperial Russia, (Hampshire, 1997), p. 36. Acton, Russia, The Tsarist and Soviet Legacy, (Essex, 1995), p. 87. 'Why did the Tsar Fall? ',  URL , (09/11/2003). As I have mentioned above, the Tsar himself still had quite a lot of personal support, even if his government did not. The peasantry, even when roused in to action, attacked not the Tsar, but the landowners whom they felt were taking advantage of them, and treating them poorly. The urban factory workers were angry that they had to work such long hours in such poor conditions, the main reason that they took to striking was due to managers ignoring their wishes and maltreating them. The slowly emerging middle and upper classes remained largely loyal to the Tsar until the bloody Sunday disaster, which created more uncontrollable disorder. Landowners, capitalists and the urban rich supported the autocracy whilst it was able to control the masses and protect property. With bloody Sunday and the subsequent rural attacks on property, strikes and general disarray, the Tsars ability to maintain order was brought in to question. However, in 1905 it seems that the concessions the Tsar was forced to make such as the October Manifesto reassured the upper and middle classes and restored their loyalty to at least a 'safe' level. Finally, the loyalty of the armed forces was still in place in 1905, though strained, the forces did not disobey the government at this time, as they were later to do in 1917. The fact that before bloody Sunday the Tsar still remained the champion of the urban and rural people, and still had loyalty from the middle/upper classes, Capitalists, the Church, and the army; perhaps saved him from possible downfall after bloody Sunday. He was able to use the army to stifle protest, and no organised activity against him had been fore-planned because previously he had maintained support. Therefore he could still make concessions to the country satisfying the upper, middle and lower classes before action against the dynasty could be taken. Ascher, The Revolution of 1905 (2 vols), (Stanford, California, 1988). p, 16. Waldron, The end of Imperial Russia, (Hampshire, 1997), p.1. The government reaction to the 1905 uprising was an important factor in why the Tsar was not overthrown. As Christopher Read points out, it was not necessarily the 'implications and nuances of the changes taking place' in 1905 which led to the unrest, it was more the fact that these changes '...were being contained within a stubbornly unchanging framework..."" By easing this unchanging framework with the concessions made in reaction to the demonstrations, the government was able to maintain their position of power. Faced with sympathy strikes, general discontent from people of all classes, the dismay of other European countries, as well as a fear that the army's loyalty was lessening, the government was forced in to conceding to certain public demands. The October Manifesto was able to quell discontent to a large extent; the upper classes felt that they now had some control, whilst the lower classes felt that they had won a small victory on which they could build upon. The Manifesto, though using very vague terminology, and lacking the Tsars sincere approval, promised Russia a more democratic society; that this manifesto infact changed little, perhaps helped lead to a different outcome for the autocracy in 1917. However, the Manifesto's mixture of reassurance of the Tsars' care for his people, sorrow at their current situation, and assurance of future amendments proved enough to ease the political tension; at least for long enough to ensure the Tsar had ensured the safety of his position. 'The welfare of the Russian sovereign is inseparable from the welfare of the people. The turmoil and unrest...fill our heart with great and heavy sorrow... We grant the population unshakable foundations of civil liberties.' Christopher Read, The Making and Breaking of the Soviet System, (Hampshire, 2001), p, 6. Harcave, First Blood, (London, 1965), p.105. Cited in Bernard Pares, The Manifesto of October 17, 1905, The Fall of the Russian Monarchy, (New York, 1939), p, 503 - 04. Differences between the situation in 1905 and 1917 help highlight its 'failure'; for example, although it could be argued that almost all sections of the Russian population were against the Tsar in 1905, the situation in 1917 was more acute. The opposition was more ready for action, the concessions made in 1905 had been proved insincere, scapegoats for the people's continuing misery such as Russian minorities, had grown angry and organised, and the army defied orders. Moreover, central to the 'failure' of the 1905 revolt, was the reality that its aims were initially not political, but were in fact economical. The strikers wanted better working conditions, and the peasants' land reform, though some sections of the opposition such as the intelligentsia were looking towards a more democratic society, many wanted to accommodate these changes within the existing Tsarist framework. The aims of 1905 were not revolution for the majority of people. Instead their aims seem to have been in search of better life conditions and above all to be listened to by their managers, and the authorities; and at least to some extent these aims were realised in 1905. Through striking, managers were forced to pay attention to their subordinates, and some changes such as a shorter working day were implemented. Likewise, although the sincerity of the October Manifesto, as well as the Tsars' support of it can be questioned, it was still a concession the autocracy were forced to make. Though the Duma proved to have little autonomy or power, 1905 was a building block towards further changes. I would argue that the 1917 revolution would not have occurred without it; therefore to suggest it was simply a failure seems too deterministic an explanation. To conclude, I approached this question by setting the context by briefly mentioning some of the key characteristics of Russia. Secondly I looked at some of the reasons why the 1905 revolution transpired. These are important to understand why 1905, which could have ended in the overthrow of the Tsar, did not. For example, the fact that the peasants held grievances with the landed elite rather than the Tsar himself meant that when they got the chance to act on their anger the Tsar was safe. Similarly the fact that the workers wanted reform rather than revolution is also significant in understanding the nature of 1905. The causes of the 1905 revolution did not spontaneously appear in that year, however a mixture of factors meant that peoples long felt disillusionment with the government and their situation, could finally be expressed. The reason why the culmination of all these grievances as well as the poor initial reaction to protest of the government, did not end with full revolution can not be put down to one single cause. That the aims of the broad cross section of society involved in the uprising were not revolution, is I think, very important. That so many sections of the vast population were involved, from the rich, to the ethnic minorities, peasantry to the intelligentsia, white collar workers to foreigners, factory workers to capitalists makes it surprising perhaps that revolution did not occur. However, revolutionary activity was unorganised partly because previously there had been no perceived need for revolution. Other important factors include that this was the first time that confidence in and loyalty to the Tsar was severely challenged. Also, the fact that at this point the Tsar still had scapegoats available to him, in the form of his ministers as well as vulnerable groups in society meant he was still trusted when the October Manifesto was announced. By 1917 the Tsar himself had become the scapegoat, saving the ruling class from blame, he had allowed himself to be put in a position where he could not escape blame; conducting the war. Finally, I would argue that although the 1905 revolution did not bring about an overthrow of the existing order, to class it as a failure is too simplistic a categorisation. To a large extent the aims of many of those involved were achieved, at least in principle if not in reality. Furthermore 1905 was an essential factor in the eventual revolution of 1917, allowing people to see that they did have some power to change the system, alerting them to the shortcomings of the government.",True
53,"To answer a question on Russia, I think it is first necessary to give some general details about this extraordinary and difficult to classify country. Russia stands eight thousand miles long, for much of the year it is landlocked, surrounded by foreign powers with whom it shares seas and rivers with. Russia has specific producer regions upon which some areas are dependent; Russia's variable climate makes growing hard in many areas. Russia's population in 1905 was very large and very mixed, only about fifty percent of this population was infact Russian. Nationalities including Ukrainians, Polish and Armenians made this a multi-cultural, multi-religious country which was very hard to generalise about demographically. In terms of classes, Russia had some more traditional groups such as peasants, landowners, autocracy and merchants. Whilst also having some modernising classes such as industrialists, foreigners and intelligentsia. Politically, Russia was run by a single autocratic monarch, seen as ordained by God. Democracy was seen as an evil the Russian people ought to be saved from, political parties as well as free speech were not allowed under the Tsar. Christopher Read, Lecture, University of Warwick, October 2003. Having looked briefly at some of the important features of Russia in 1905 to place it in to some kind of context, I wish firstly to outline what exactly the 1905 revolution was; looking very briefly at its causes and the events that took place. Secondly, I plan to look in some depth at the main reasons given by historians on why this revolution 'failed'. Finally I will conclude by suggesting that although the 1905 revolution did not result in a full revolution, this was not its aims, therefore simply classifying it as a failure may not be appropriate. 'Until 1905 the struggle against the autocracy was predominantly an affair of individuals and small groups...it was only in that year that the Russian masses...were stirred in to action on a larger scale."" The 1905 revolution was the response of the people (or at least some of them) to years of hardship and harsh autocratic rule. It was perhaps the taste for change they felt under Tsar Alexander II (such as peasant emancipation) that led them to question Tsar Nicholas II when he took measures to strengthen autocratic rule. The expansion of Russian industry during the 1890's '...was succeeded by a period of depression and industrial conflicts became sharper and more numerous."" Strikes, though illegal were becoming more common at this time. Propaganda spread by revolutionaries and the intelligentsia was beginning to reach and interest workers disillusioned by working conditions and unfair treatment by superiors. The peasantry still carried their resentment at the government and landowners over their emancipation which left them tied to the land by the increased power of the village commune, and forced to make redemption payments to the state. The peasants saw themselves as rightful owners of the land because they were they ones who tended it. They seem to have borne this bitterness quietly until they sensed their old enemies growing weak in 1905; and turned on the landed and wealthy. Also at this time, poverty and even starvation were not uncommon and were arguably enhanced by government policies which built up huge foreign debts. Chamberlin, The Russian Revolution, (London, 1935), p.46. Edward Acton, Russia, The Tsarist and Soviet Legacy, (Essex, 1995), p. 69. Chamberlin, The Russian Revolution, (London, 1935), p.48. E. R. Wolfe, Peasant Wars of the Twentieth Century, (London, 1971), p. 63. Wolfe, Peasant Wars of the Twentieth Century, (London, 1971), p.58. 'Student disorders, assassinations of government officials...growing demand for liberal reforms all added to the worries of the government and created a tense atmosphere."" In part to divert attention from this state of affairs, the government launched a 'little victorious war' on the Japanese. That this war proved a disaster for Russia leaving her shamefully beaten by 'the little yellow monkeys' and diverting precious resources for a lost cause did not help the political situation. Further causes of the 1905 incidence included the modernisation and democratisation of other European powers, in sharp contrast to Russia's 'backward' autocracy and economy, as well as the hesitant, weak leadership qualities of the Tsar himself. The catalyst for all these problems in Russia in 1905 proved to be what was intended as a peaceful, even slavish event. Bloody Sunday was a non-violent protest to the Tsar, by workers and their families, about poor working conditions, and poor relations with managers. The demonstration was organised by the priest, Father Gapon, a police agent put in place to help control labour organisation in the face of the increased striking. The march was preceded by a general strike, which stirred the authorities in to action, and ended up with them using the military to fire upon marchers even as they carried pictures of the Tsar and sang patriotic songs. The day of bloodshed obliterated the Tsars' image of the 'little father' of the people; many ordinary workers and their families were left shocked and horrified at the Tsars' action. Sympathy strikes and further chaos followed before the autocracy gradually regained its shaky control of Russia. I now wish to look at the reasons why this chaotic period did not result in the full revolution that occurred under similar conditions in 1917 which deposed the Tsarist regime. Chamberlin, The Russian Revolution, (London, 1935), p.47. 'The Russian Revolution of 1905',  URL , (09/11/2003). (As the Tsar 'habitually' referred to them). Chamberlin, The Russian Revolution, (London, 1935), p.47. A, Verner, The Crisis of the Russian Autocracy, (Oxford, 1990), p. 2. Acton, Russia, The Tsarist and Soviet Legacy, (Essex, 1995), p. 115. Harcave, First Blood, (London, 1965), p.89. The opposition to the government in 1905, though it did exist, was unorganised with no formal aims to work towards. The main left wing parties were still finding their feet; Lenin was still deciding which brand of socialism to support, and theorising about the application of socialism in relation to Russia's distinct features. Revolutionaries and the intelligentsia worked towards spreading anti-Tsar feeling among the urban workers and the peasantry. However their work was patchy and disorderly and the reception their work received was mixed. Students dressed up as peasants to try to spread revolutionary beliefs; however they were distrusted and consequently were relatively unsuccessful. Similarly, radicals would address gatherings of peasants, attempting to spread and develop anti-establishment ideology, and stir the peasantry in to action. In the towns, revolutionaries spread leaflets presenting their ideas to the already disgruntled work force. Though some of the ideas of revolutionaries in the towns were accepted and recognised by many, loyalty to the Tsar at this point was not faltering. Though people were unhappy with the government, they blamed ministers, believing that the Tsar was ignorant of his people's fate. Father Gapon's brand of action was much more appealing; it was more organised, with one main leader, and remained loyal to the Tsar. P. Waldron, The end of Imperial Russia, (Hampshire, 1997), p. 36. Acton, Russia, The Tsarist and Soviet Legacy, (Essex, 1995), p. 87. 'Why did the Tsar Fall? ',  URL , (09/11/2003). As I have mentioned above, the Tsar himself still had quite a lot of personal support, even if his government did not. The peasantry, even when roused in to action, attacked not the Tsar, but the landowners whom they felt were taking advantage of them, and treating them poorly. The urban factory workers were angry that they had to work such long hours in such poor conditions, the main reason that they took to striking was due to managers ignoring their wishes and maltreating them. The slowly emerging middle and upper classes remained largely loyal to the Tsar until the bloody Sunday disaster, which created more uncontrollable disorder. Landowners, capitalists and the urban rich supported the autocracy whilst it was able to control the masses and protect property. With bloody Sunday and the subsequent rural attacks on property, strikes and general disarray, the Tsars ability to maintain order was brought in to question. However, in 1905 it seems that the concessions the Tsar was forced to make such as the October Manifesto reassured the upper and middle classes and restored their loyalty to at least a 'safe' level. Finally, the loyalty of the armed forces was still in place in 1905, though strained, the forces did not disobey the government at this time, as they were later to do in 1917. The fact that before bloody Sunday the Tsar still remained the champion of the urban and rural people, and still had loyalty from the middle/upper classes, Capitalists, the Church, and the army; perhaps saved him from possible downfall after bloody Sunday. He was able to use the army to stifle protest, and no organised activity against him had been fore-planned because previously he had maintained support. Therefore he could still make concessions to the country satisfying the upper, middle and lower classes before action against the dynasty could be taken. Ascher, The Revolution of 1905 (2 vols), (Stanford, California, 1988). p, 16. Waldron, The end of Imperial Russia, (Hampshire, 1997), p.1. The government reaction to the 1905 uprising was an important factor in why the Tsar was not overthrown. As Christopher Read points out, it was not necessarily the 'implications and nuances of the changes taking place' in 1905 which led to the unrest, it was more the fact that these changes '...were being contained within a stubbornly unchanging framework..."" By easing this unchanging framework with the concessions made in reaction to the demonstrations, the government was able to maintain their position of power. Faced with sympathy strikes, general discontent from people of all classes, the dismay of other European countries, as well as a fear that the army's loyalty was lessening, the government was forced in to conceding to certain public demands. The October Manifesto was able to quell discontent to a large extent; the upper classes felt that they now had some control, whilst the lower classes felt that they had won a small victory on which they could build upon. The Manifesto, though using very vague terminology, and lacking the Tsars sincere approval, promised Russia a more democratic society; that this manifesto infact changed little, perhaps helped lead to a different outcome for the autocracy in 1917. However, the Manifesto's mixture of reassurance of the Tsars' care for his people, sorrow at their current situation, and assurance of future amendments proved enough to ease the political tension; at least for long enough to ensure the Tsar had ensured the safety of his position. 'The welfare of the Russian sovereign is inseparable from the welfare of the people. The turmoil and unrest...fill our heart with great and heavy sorrow... We grant the population unshakable foundations of civil liberties.' Christopher Read, The Making and Breaking of the Soviet System, (Hampshire, 2001), p, 6. Harcave, First Blood, (London, 1965), p.105. Cited in Bernard Pares, The Manifesto of October 17, 1905, The Fall of the Russian Monarchy, (New York, 1939), p, 503 - 04. Differences between the situation in 1905 and 1917 help highlight its 'failure'; for example, although it could be argued that almost all sections of the Russian population were against the Tsar in 1905, the situation in 1917 was more acute. The opposition was more ready for action, the concessions made in 1905 had been proved insincere, scapegoats for the people's continuing misery such as Russian minorities, had grown angry and organised, and the army defied orders. Moreover, central to the 'failure' of the 1905 revolt, was the reality that its aims were initially not political, but were in fact economical. The strikers wanted better working conditions, and the peasants' land reform, though some sections of the opposition such as the intelligentsia were looking towards a more democratic society, many wanted to accommodate these changes within the existing Tsarist framework. The aims of 1905 were not revolution for the majority of people. Instead their aims seem to have been in search of better life conditions and above all to be listened to by their managers, and the authorities; and at least to some extent these aims were realised in 1905. Through striking, managers were forced to pay attention to their subordinates, and some changes such as a shorter working day were implemented. Likewise, although the sincerity of the October Manifesto, as well as the Tsars' support of it can be questioned, it was still a concession the autocracy were forced to make. Though the Duma proved to have little autonomy or power, 1905 was a building block towards further changes. I would argue that the 1917 revolution would not have occurred without it; therefore to suggest it was simply a failure seems too deterministic an explanation. To conclude, I approached this question by setting the context by briefly mentioning some of the key characteristics of Russia. Secondly I looked at some of the reasons why the 1905 revolution transpired. These are important to understand why 1905, which could have ended in the overthrow of the Tsar, did not. For example, the fact that the peasants held grievances with the landed elite rather than the Tsar himself meant that when they got the chance to act on their anger the Tsar was safe. Similarly the fact that the workers wanted reform rather than revolution is also significant in understanding the nature of 1905. The causes of the 1905 revolution did not spontaneously appear in that year, however a mixture of factors meant that peoples long felt disillusionment with the government and their situation, could finally be expressed. The reason why the culmination of all these grievances as well as the poor initial reaction to protest of the government, did not end with full revolution can not be put down to one single cause. That the aims of the broad cross section of society involved in the uprising were not revolution, is I think, very important. That so many sections of the vast population were involved, from the rich, to the ethnic minorities, peasantry to the intelligentsia, white collar workers to foreigners, factory workers to capitalists makes it surprising perhaps that revolution did not occur. However, revolutionary activity was unorganised partly because previously there had been no perceived need for revolution. Other important factors include that this was the first time that confidence in and loyalty to the Tsar was severely challenged. Also, the fact that at this point the Tsar still had scapegoats available to him, in the form of his ministers as well as vulnerable groups in society meant he was still trusted when the October Manifesto was announced. By 1917 the Tsar himself had become the scapegoat, saving the ruling class from blame, he had allowed himself to be put in a position where he could not escape blame; conducting the war. Finally, I would argue that although the 1905 revolution did not bring about an overthrow of the existing order, to class it as a failure is too simplistic a categorisation. To a large extent the aims of many of those involved were achieved, at least in principle if not in reality. Furthermore 1905 was an essential factor in the eventual revolution of 1917, allowing people to see that they did have some power to change the system, alerting them to the shortcomings of the government.","The most extensive population study of the early modern period was carried out by E.A. Wrigley and R.S. Schofield, The Population History of England, 1541 - 1871: A Reconstruction, which set out calculated estimates of population trends for this period. They used the method of back-projection to estimate the population levels of the country between 1541 (the beginning of the first decade after the introduction of parish registers) and 1871 (the date of the second reliable census). This project was an immense undertaking which used four hundred and four surviving parish registers which recorded baptism, marriage and burial, to estimate demographic trends at five year intervals between these two dates. Many criticisms of both the methodology used and the results achieved have been made of this study. In very general terms these criticisms focus upon the following main areas. The sample of four hundred and four parish registers used to estimate national population figures makes up only four percent of the total number of parishes in early modern England. In addition, the sample includes too many large parishes (usually Northern areas) and too few smaller parishes (usually Eastern areas). London is also underrepresented in the study due to its lack of surviving parish registers. Even good registers however were inaccurate for certain periods (such as during the civil war) and thus only for the year 1662 were all the registers fit to use. The unreliability of back projection as a technique for working out population levels has been criticised in terms of the assumptions Wrigley and Schofield made in using this method; for example assumptions about the age structure in place throughout this period will have influenced the final results. E.A. Wrigley and R.S. Schofield, The Population History of England, 1541-1871: A Reconstruction, (Cambridge, 1981). Wrigley and Schofield, The Population History of England, p. 15. Wrigley and Schofield, The Population History of England, p. 16. Wrigley and Schofield, The Population History of England, p. 455. John Hatcher, 'Understanding the Population History of England 1450-1750', Past and Present, no.180, (August 2003), p.125. Despite such criticism the information Wrigley and Schofield provide does appear a plausible reading of the demographic trends of early modern England. After all, the authors are aware that methodological shortcomings exist; this work does not pretend to be concrete evidence but merely a likely estimation. Even taking in to consideration the limitations of the evidence and the methodology as well as the limitations of the study itself, i.e. : it operated on a very statistical level and was not placed in much of a social or cultural context, this remains a very useful work. Furthermore the huge financial costs and time commitments involved in this kind of research make it unlikely that another work of this nature will be undertaken. These are the best results we have, and particularly when used in conjunction with localised studies as well as studies of social and cultural factors this is very revealing research. In this essay I am going to argue from a standpoint assuming that the data compiled by Wrigley and Schofield is a realistic estimation of population trends. However, I will argue that an acceptation of the population trends presented by the Cambridge Group does not necessitate an acceptation of their interpretation of the causes of these population trends. I will argue that fertility and mortality were important in influencing population change. I will argue that the essential inter-linkage of these two factors makes any either/or demographical explanation implausible. Instead I will suggest that a model whereby both fertility and the two parts of mortality (disease and starvation) helped shape population and each other is a more realistic explanation to such a complex and wide-spanning phenomenon. Wrigley and Schofield, The Population History of England, p.456. The demographic trends that Wrigley and Schofield have mapped out for early modern England begin with growth between 1541 and 1661; this growth was generally steady although it slowed during the 1550s and became rapid in the 1570s and 1580s. Between 1656 and 1686 the population decreased in size before stagnating and slightly recovering between 1686 and 1750. From 1750 onwards the vigorous growth of earlier periods was resumed, although Elizabethan growth rates were not reached until the 1790s. Thus there were two periods of growth divided by one hundred years or so from the mid seventeenth century to the mid eighteenth century when population numbers stagnated. During these main periods of growth the population increased significantly almost doubling from 2.8 million to 5.3 million between 1541 and 1656, and after the years of stagnation again doubling between 1731 and 1816 and between 1861 and 1871. As I have suggested this basic information seems both plausible and realistic, however Wrigley and Schofield's interpretation of this information; an attempt to understand what caused this rapid growth, check and decline followed by renewed growth, seems less plausible. The principle question they look at is whether '...a rise in GRR (gross reproductive rate) from 2.5 to 3.0 for example is more important than a rise in eo (life expectancy from birth) from 35-40 years in effecting growth rates."" Or in other words, was it fertility or mortality that was the definitive factor in determining these population trends. Roger Schofield, 'The Impact of Scarcity and Plenty on Population Change in England 1541-1871', Journal of Interdisciplinary History, 14:2, (Autumn 1983), p.270. Schofield, 'The Impact of Scarcity and Plenty on Population Change in England', p.269. E.A. Wrigley, People, Cities and Wealth; The Transformation of Traditional Society, (Oxford, 1989), p.243. Wrigley and Schofield, The Population History of England, p.236. The Cambridge Group certainly believed that '...marriage was the hinge upon which the demographic regime turned."" They suggested that '...fertility change was more important than mortality change in altering population growth rates', that '...fertility change in turn chiefly reflected fluctuations in nuptiality' and that '...nuptiality was primarily determined by real wages'. Thus Wrigley and Schofield presented a structure where a rise in real wages would lead to a rise in the number of marriages as well as a rise in the number of early marriages and therefore a rise in the numbers of births. Wrigley and Schofield themselves note the inevitable sacrifice of comprehensiveness with the construction of simple models, however they argue that the benefits gained in clarity are essential to understanding. Therefore the picture of demography that they present for early modern England at this time is one where the preventative check system of Malthus was in operation. This system proposed that societies could control their population growth so that the need for a positive check (natural disaster, famine, disease) to regulate population size to fit food production would be unnecessary. The actual data Wrigley and Schofield present for fertility includes 'high fertility in the sixteenth century followed by slow a decline from GRR (gross reproduction rate) from 2.8 to 1.9.' This in turn was followed by 'a gently rising trend to reach 2.3 by 1756', before an acceleration of this trend 'to reach a peak value of 3.06 in 1816.' Thus they argue that 'towards and between the major turning points of about 1660 and 1815...there were very few marked departures from the trend line, but rather a steady progression.' Wrigley, People, Cities and Wealth, p.39-40. Wrigley and Schofield, The Population History of England, p.549. Wrigley and Schofield, The Population History of England, p.466. Wrigley and Schofield, The Population History of England, p.467. Hatcher, 'Understanding the Population History of England', p.124. Wrigley and Schofield, The Population History of England, p.229. There has been a lot of criticism of this model, both in terms of the methodology used to get the data on fertility and the final results presented, but also in terms of the Malthusian framework in which these results are placed. The method used to estimate age at marriage was 'family reconstitution', which traced individuals' lives through their parish. This method requires both baptism and marriage to have taken place in the same parish and assumes out-migration rates are low. In many cases however the 'sparrow phenomenon' or people flitting in and out of the records made this method impossible; in fact only twelve parish registers provided reasonable estimates for age at marriage. It has been suggested that it is probably these twelve parishes which are unrepresentative; the norm, as shown by the majority of records, was to move around the country rather than to remain in the place of birth. Thus this method uses a mere 0.12 percent of the total number of parishes to work out estimates of age at marriage on a national basis, and it is likely that this 0.12 percent is unrepresentative of the total population in terms of lifestyle habits. Marriage did not even become a regulated and controlled institution before 1753 when laws were introduced in order to make those marriages not carried out in a church illegal. Before this point marriage could be legal simply through the saying of vows in front of a witness; Laura Gowing points out that before 1753 the promise to marry followed by sex also constituted legal if irregular marriage. This ambiguity in the definition of marriage leads to further questions about the ability of Wrigley and Schofield to estimate marriage accurately because of course these non-ecclesiastical weddings would not have been recorded. John Hatcher argues that the fertility rates estimated were forced into the model of Malthusian preventative check which had already been formulated by the Cambridge Group before they even began their research. He asserts that a model where the demographic regime can be '...distilled into a counterpoint between marriage and the levels of real wages...is overly singular, simple and mechanistic...and incapable of capturing historical reality.' Wrigley and Schofield, The Population History of England, p.229. Hatcher, 'Understanding the Population History of England', p.88. Laura Gowing, Women, Words and Sex in Early Modern London, (Oxford, 1998), p.147. Hatcher, 'Understanding the Population History of England', p.91. Hatcher, 'Understanding the Population History of England', p.88-89. Even if the marriage and fertility rates, potentially unrepresentative and framed as they may be, are accepted as plausible, questions have been raised about the placement of emphasis in Wrigley and Schofield's interpretation. David Weir argued that marital behaviour altered considerably over time causing a shift in the dominant factor affecting fertility and thus population trends. 'From a period in which the incidence of marriage was dominant to one in which changes in the ages of those marrying was dominant."" Some have noted that it is difficult to assert the dominance of fertility on population trends, particularly when within the boundaries of the Malthusian model. 'Imposing' this framework on the fertility data has required Wrigley and Schofield to argue that a 'long lag' relationship of about fifty years existed between a change in real wages and its affects on fertility. 'A key puzzle of the demography is why fertility continued to rise when real wages were stagnant...the lag of course solves this problem."" J.A. Goldstone accepts that a lag is a plausible explanation; however suggests that reducing it to a twenty year difference would be more accurate. Others however are less convinced by the theory; Steve Hindle suggests that we do not know enough about marriage horizons, the preference for nuclear family households, factors influencing nuptiality and in particular the influence of institutional pressures (such as apprenticeships and the poor law) in shaping marriage, to make assertions about a long lag and the population correcting its own development in reaction to conditions of scarcity and plenty. Indeed it has been noted that as a period of low life expectancy the possibility of parents surviving to influence the marriage patterns of the next generation is most unlikely. Similarly, questions have been asked about the amount of influence a parent would have had over their children. Perhaps in cases where inheritance would depend on parental approval influence would be quite significant, but for the majority of the population inheritance would not have been left. The deterministic nature of this fertility led explanation for the population growth of 1541-1656 and 1731-1871 does not allow room for other factors, and does not take regional variations in to consideration. Although I do not wish to argue in a deterministic fashion that mortality was the most important factor in shaping the population 'replacing one bald stereotype with another', it is important to look at the quite significant part played by mortality in order to question Wrigley and Schofield's interpretations. David Weir, 'Rather Never than Late: Celibacy and Age At Marriage in English Cohort Fertility, 1541-1871', Journal of Family History, 9, (Winter, 1994). Steve Hindle, 'The Problem of Pauper Marriage in Seventeenth-Century England', Transactions of the Royal Historical Society, 6th ser. 8, (1998), p.72. Hatcher, 'Understanding the Population History of England', p.88. J.A. Goldstone, 'The Demographic Revolution in England: a Re-examination', Population Studies, 49, (1986), p.9. Goldstone, 'The Demographic Revolution in England', p.13. Hindle, 'The Problem of Pauper Marriage in Seventeenth-Century England'. Susan Amussen, An Ordered Society: Gender and Class in Early Modern England, (London, 1988), p.34. Lawrence Stone, The Family Sex and Marriage in England 1500-1800, (London, 1977), p.113. Hatcher, 'Understanding the Population History of England', p.89. The two parts of mortality; starvation and disease were both influences on the early modern population, not least because their presence pre-empted any potential rise in fertility through restricting marriage partners, increasing migration and affecting prices and wages. The influence of factors other than fertility can be seen in the rise of the population despite a fall in real wages; I am afraid that I remain unconvinced by the long lag theory and thus this slowness of response between the two seems to imply that mortality had more influence than Wrigley and Schofield suggested. John Hatcher points out that the volatility of both fertility and mortality during this time is suggestive not of Wrigley and Schofield's model where '...there were very few departures from the tend line', but of a model where many influences were working to shape the population. 'Conclusive statistical proof of the exceptional instability...high annual deviations between 1550/1 and 1574/5 of 17.7 percent in the crude death rate, and 12.2 percent and 8.0 percent respectively in the marriage and birth rates."" Hatcher also notes that assumptions made by Wrigley and Schofield to fill in gaps left by poor evidence and their small sample had led to the effects of mortality being underestimated. 'A far more severe impact is revealed by the registers not included in the study and also by the abundance of wills probated in a wide range of ecclesiastical jurisdictions."" Additionally, Wrigley and Schofield's determination to prove the dominance of fertility can be seen in their relegation of mortality to a mere 'condition' of population change whereas fertility is considered a 'cause'; mortality is presented as something which affects fertility rather than as an influence on population in its own right. As Hatcher points out 'in any analysis of causality the designation of some factors as 'conditions' or 'circumstances' and others as 'causes' is liable to exert a powerful influence over the outcome."" What Hatcher calls 'a tangle of contradictions and reservations' in fact 'lie behind the series of confident judgements...made.' Wrigley and Schofield, The Population History of England, p.229. Hatcher, 'Understanding the Population History of England', p.102. Hatcher, 'Understanding the Population History of England', p.104. Hatcher, 'Understanding the Population History of England', p.105. Hatcher, 'Understanding the Population History of England', p.106. There are a number of issues surrounding the fertility led preventative check theory and the above criticisms are useful to demonstrate a rather general lack of accuracy in Wrigley and Schofield's model; however it is also useful to look at more specific studies to identify the effects of mortality on localised areas. Peter Laslett's study on famine in early modern England concluded that the threat of starvation or 'crisis mortality' was very unlikely to occur. However, death via malnutrition and via food poisoning (through eating rotten or poor quality foods) , though not technically starvation as such, could still cause population rates to fall as well as making the population more susceptible to disease and lowering fertility. This distinction between outright starvation and malnutrition is something which Andrew Appleby also remarks upon, arguing that death due to starvation and malnutrition could be very hard to identify. Appleby suggests that the years of 'recurring population crises' between 1550 and 1640 were in fact due to mortality and further that 'the role of famine as a killer  in its own right has been underplayed' generally 'in the history of early modern England,' (my italics). Wrigley and Schofield do recognise that at particular times throughout history mortality has been more dominant than fertility in shaping demography, yet they conclude that overall fertility had the more decisive influence. Appleby's findings in Westmorland and Cumberland show that perhaps this decisive influence is misleading because the effects of mortality have been repeatedly and mistakenly underplayed. Peter Laslett, The World we have Lost: Further Explored, (Cambridge, 1965), p.151. Laslett, The World we have Lost, p.134. Andrew Appleby, 'Disease or Famine? Mortality in Cumberland and Westmorland 1580-1640', The Economic History Review, Second ser, 26, (1973), p.404. Appleby, 'Disease or Famine? ', p.403. Wrigley and Schofield, The Population History of England, p.238. Disease had specific characteristics; it was often recorded in parish registers as a cause of death and appeared in other primary evidence such as wills. Famine did not have such a 'discernable pattern to help the historian identify it' it did not have such specific characteristics, it was rarely recorded in parish registers, and those who could write (more likely the upper sorts) would have been less likely to feel the effects of famine and able to relocate in plenty of time before actual starvation set in. Appleby looks at available sources and compares them closely with the known characteristics of the prevalent diseases of the period; epidemics were rarely rural problems, they declined in Winter (the winter disease Typhus rarely killed children and is thus identifiable), disease set in very quickly and in the case of plague 'would often follow the main lines of trade between cities, fairs and seaports'. The characteristics of starvation are much harder to identify and provide proof of; parish records were often unhelpful, famine did not follow a specific development like that of disease and when it appeared in the form of malnutrition it is even harder to discern. Appleby develops a more detailed methodology for identifying famine than the three main requirements of Peter Laslett; he noticed that in cases of starvation gradual or rapid onset of deaths could occur but a dramatic increase in mortality (burials twice normal) should be expected. Several neighbouring villages would also have increased deaths, a correlation between prices and mortality should be noticeable, usually some reference to dearth would exist in one form of source or another and there should be fewer conceptions than usual. Not only does this list of the characteristics of famine offer evidence that it can be very hard to identify, but that it is very closely linked to fertility; not as a condition of the rise/fall of fertility, but as a factor in its own right. Appleby discovered that famine had played a significant part in shaping the population in Cumberland and Westmorland; at certain times it was the dominant factor causing the death toll to mount, at other times it made areas more susceptible to disease (through lessened immune systems, and through desperate importing of foods from areas with epidemics) and made women less willing and less able to conceive. Appleby, 'Disease or Famine? ', p.406. Appleby, 'Disease or Famine? ', p.404-5. Appleby, 'Disease or Famine? ', p.421. Appleby, 'Disease or Famine? ', p.422. Appleby, 'Disease or Famine? ', p.430-31. In their study of Mid-Wharfedale parishes in 1623 Moira Long and May Pickles also found that evidence of 'crisis mortality' was often a part of early modern life, if not in the form of full blown famine certainly in the form of 'crisis months'. What Long and Pickles also recognise is the importance that the threat of famine played in shaping populations, for example the fear of shortages could affect births (not necessarily through less marriages but also through stress and amenorrhea) and could persuade locals to import food raising the likelihood of an epidemic. It is the backdrop of general high mortality in England in 1623 against which Long and Pickles set their study and they found, as suggested by Appleby, that in the areas they looked at scarcity and famine were the hardest to identify. However they suggest that 'erratic' results for famine exemplified this period and that 'the balance between sufficiency and dearth in many parts...was finely poised."" Victor Skipp found considerable regional variation in occurrences of famine and shortages which might conceivably have led to their relative invisibility in the small sample of Wrigley and Schofield. These findings and Appleby's findings from specific area studies seem to reflect Hatcher's argument that the presentation of both the steadily rising influence of fertility and steadily decreasing influence of mortality is inaccurate. In the same way Wrigley and Schofield's treatment of famine as a mere condition rather a cause affecting the turning of the demographic regime is equally misleading; yes fertility was important in 'turning the regime', but it was by no means alone in its efforts. Maria Long and May Pickles, 'An Enquiry into Mortality some Mid-Wharfedale Parishes in 1623', Local Population Studies, 31, (Autumn, 1986), p.27. Long and Pickles, 'An Enquiry into Mortality some Mid-Wharfedale Parishes in 1623', p.25. Long and Pickles, 'An Enquiry into Mortality some Mid-Wharfedale Parishes in 1623', p.26. Long and Pickles, 'An Enquiry into Mortality some Mid-Wharfedale Parishes in 1623', p.34. Victor Skipp, 'Economic and Social Change in the Forest of Arden 1530-1649', Agricultural History Review, 18, (1970), p.105. Hatcher, 'Understanding the Population History of England', p.106. Wrigley, People, Cities and Wealth, p.39-40. Throughout this essay so far, I have tried to emphasise the continuous links between mortality and fertility during the early modern period; it is this essential inter-linkage which I wish to look at more explicitly now. There were interconnections between disease, migration, death, birth rate, price and wage, amongst others; in practice of course the nature of and the strengths of these links varied, nevertheless they did exist. The incidence of famine whether long or short term affected disease rates, fertility rates and migration rates as well as affecting prices and wages as the population lessens. Likewise, high fertility could force prices up, wages down, encourage migration and make disease more probable. It seems to me that the circular nature and knock on effect of all these factors combining to shape each other and the population would be better described in terms of a set of dominoes rather than as a hinge which suggests not only the domination but also the presence of only one factor. Does not the very reality of high fertility demand low levels of disease and plenty of food? And does not the very reality of famine demand high levels of population? As Laslett suggests 'each part of our subject is also our whole subject...the fund of food was obviously related to age at marriage and numbers marrying.' Laslett, The World we have Lost, p.122. Hatcher suggests that Wrigley and Schofield manipulate data to exaggerate fertility and lessen the significance of mortality; he argues that the presentation of information in The Population History of England is 'skewed' in favour of fertility. Even the date at which the study begins favours their Malthusian framework; 'the first half of the sixteenth century was a period of demographic transition, results would have been very different if the parish register series had begun a couple of decades earlier or later.' Although Hatcher accepts that this was purely coincidental, 'a historical accident', it still demonstrates that the prominence awarded to fertility by Wrigley and Schofield is not necessarily historically accurate but is based more upon interpretation, evidence and methodology used. In fact, a case for the essential interaction and inseparability of the factors influencing population can be made from within The Population History of England. From '1551 to 1566 falling fertility was the main driving force until the ""golden period of low mortality"" turned the tables; 1571 to 1611 was a period of relatively stable fertility and mortality ""fertility and mortality contributed equally to the steady fall"", fertility was dominant from 1671 to 1691 but mortality was most clearly the more important influence on growth rates from 1691 to 1751."" Hatcher sums up these characteristics and finds that 'fertility is held to dominate for around thirty-five years of the two-hundred year time span, mortality for around sixty-five years and that for a century the two variables exerted approximately equal influence."" This evidence is 'immediately contradicted' by Wrigley and Schofield through the use of 'quinquennial averages', and the discarding of 'three episodes of very high mortality before 1751'. Nonetheless it seems evident that 'fertility and mortality exchanged leadership many times within the early modern period' and that pinpointing which one was dominant at different times would prove a more useful exercise than attempting to prove which was dominant full stop. Hatcher, 'Understanding the Population History of England', p.114. Hatcher, 'Understanding the Population History of England', p.112. Hatcher, 'Understanding the Population History of England', p.107. Hatcher, 'Understanding the Population History of England', p.107. Hatcher, 'Understanding the Population History of England', p.108. Hatcher, 'Understanding the Population History of England', p.116. In conclusion, despite the problems with Wrigley and Schofield's methodology and evidence the population trends they map out are plausible and the best figures historians have to work with. I have suggested however that in many ways their interpretation of the causality of this trend is flawed. The judgement that living standards and wage rates determined nuptiality and therefore fertility, or that 'marriage is the hinge upon which the demographic regime turned' is inaccurate. Family reconstitution provided data on fertility which was unreliable and unrepresentative. However, even accepting these methodological limitations the figures then seemed to be moulded to fit a Malthusian framework of preventative check homeostasis. The 'long lag' theory used to justify the lack of direct correlation between fertility and real wages over simplifies a very complex phenomenon whilst simultaneously relegating other important factors to mere conditions or circumstances rather than causes of population change. The fact that we do not know enough about marriage patterns, non-church marriages, or the parental influences over children of every day people makes the long lag an unsustainable claim. The deterministic nature of this model undermines the significance of mortality and other factors upon population changes and upon shaping fertility. The relative instability and volatility of both fertility and mortality suggests more detail is needed in the model Wrigley and Schofield provide. Evidence such as those registers not used in The Population History sample as well as personal documents from the time show that mortality had more influence than the Cambridge Group suggest. Mortality in the form of starvation was often extremely hard to identify due to its lack of recording in parish registers. The type of scarcity which never developed in to full blown famine is even harder to spot, but was equally influential in shaping fertility and population. Studies of Westmorland and Cumberland, the Forest of Arden and of Mid-Wharfedale parishes reveal a mistaken and repeated underplaying of starvation, and its often very localised nature. In fact the presence and at times domination of mortality was important and deserves more attention in  The Population History. Similarly, although I have not focused greatly upon the influence of migration and in particular sex specific migration on population trends, through reducing the pool of those eligible for marriage, its importance should not be underestimated either. I have tried to show that the links between starvation, disease and fertility are more important and relevant than which of these major demographic variables was dominant overall. Each affected the others and each in turn was affected by many other minor demographic variables. Therefore I have suggested that a model whereby the essential interlinks of mortality, migration and fertility would prove most accurate. Hatcher claims that in total between 1551 and 1751 'fertility is dominant for 17.5 percent of the time, mortality for 32.5 percent and for 50 percent of the time the two variables exerted approximately equal influence."" Manifestly he argues that causality cannot be established until it is accepted that fertility and mortality were both important factors within the early modern period '...and a host of social, cultural and economic factors played on them to help fashion demographic behaviour."" To finally answer the question, I would suggest that the metaphor of fertility as a 'hinge upon which the demographic regime turned', be discarded and replaced with a metaphor which better encompasses the historical reality of the exertion of manifold influences, and a co-relation of factors which developed to shape what was infact less a regime, and more a volatile commune. 'Any valid depiction of reality is likely to resemble a vast irregular web'. The achievements of the Cambridge Group should not be underestimated, but equally nor should the inadequacies of their interpretative model. 'Patterns of births, marriages, deaths and migrations helped to shape society and economy and were themselves shaped by society and economy' as I have attempted to show in this essay. Goldstone, 'The Demographic Revolution in England', p.22. Hatcher, 'Understanding the Population History of England', p.107. Hatcher, 'Understanding the Population History of England', p.116. Hatcher, 'Understanding the Population History of England', p.129. Hatcher, 'Understanding the Population History of England', p.129.",False
54,"Dividing development into distinct stages has occurred in many areas of psychology, but most frequently in cognitive development of children and adolescents. There has always been disagreement as to whether development should be categorised in stages or whether it is a constantly moving process which never stops. Flavell (1977) suggested that stages in development must show distinct, qualitative changes rather than differences in quantitative ability. He also noted that transition between stages must incur several simultaneous changes in many areas of cognition, and that these transitions should be rapid rather than gradual. Some psychologists have tried to suggest that development is an accumulation of small qualitative changes over time, causing smooth, continuous improvement, called linear dynamics, (Harris & Butterworth, 2002), however most parents and teachers will agree that children's development does feature many sudden periods of growth. There have been a number of stage models of cognitive development in recent psychology, which tend to agree on many points, such as the number of stages and the types of changes, suggesting that each has some merit. Freud famously described stages of development in children, including oral, anal, phallic, oedipal, latency and genital activity, (Harris & Butterworth, 2002), although these were never given much credit as Freud was seen to overemphasise sexual thoughts in young children. Vygotsky focused more on learning as a result of social interaction, but stated that cognitive development was limited to a certain range at any given time. His ordered stages of development included affiliation, play, learning, peer and work and have had some impact on the teaching of infant children, where the 'play' stage has encouraged more play centred learning in the classroom, (Donaldson, 1987). Other similar stage theories , such as that of Jerome Bruner, (Flavell, 1977), differ from these as they are not age dependent or invariant, and include fewer stages, in the case of Bruner, the enactive stage where knowledge is primarily in the form of motor responses, the iconic stage where knowledge is stored as visual images, and the symbolic stage where knowledge is stored as words or symbols. Bruner's stages have had less impact in the classroom, but his iconic stage has encouraged the use of pictures and diagrams in learning. A more recent stage theory, that of William Perry (1970), concentrates on the development of college students (17-21) and has been used throughout the USA in teaching many subjects. It includes nine points of development, from basic duality (one), through relativism (point five), towards commitment (nine). Without doubt the most famous of the stage models is Jean Piaget's model of cognitive development, where childhood is divided into four stages based on age, which reflect the differing cognitive abilities of children at different ages. Whilst he recognised that transition could be at different times for each child, he outlined the nature of learning in each stage, (Boden, 1994). Firstly the sensori-motor stage, from birth to two years, shows cognition restricted to simple reflexes but quickly increases as learning through play occurs, (Sutherland, 1992). His ideas of imitation and learning through experience have influenced the way parents interact with their infants and can be seen in most playgroups and homes. However Piaget has been criticised for underestimating the cognitive ability of babies, which is understandable given the lack of imaging techniques that were available to him. Secondly, the pre-operational stage (two - seven) shows developments such as language and object permanence. During the concrete operations stage (seven - eleven), children show increased abilities in logical concepts such as conservation and reversibility, and during the final formal operations stage (eleven onwards), which some adults never reach, using many different operations simultaneously can be seen. Children think logically and mathematically and work systematically through solutions, (Papert, 1999). In many ways Piaget's four stages have dramatically changed the form of education today, from school structure to curriculum. His ideas of the ability of children has influenced what is taught to children, for example division would not be taught to a five year old as Piaget says they lack the concept of reversibility, (Dricoll, 2000). In the pre-conceptual stage children cannot focus on more than one task at once, therefore a teacher will usually move children explicitly from one task to another, whereas a seven year old might be expected to listen and work at the same time. The general idea of stages can be seen used in a great many areas in life. Firstly in terminology; children are often categorised according to age, e.g. toddlers, infants, teenagers and adolescents. Secondly stages are used in commercial products including medicines, clothing and toys, where a toy might be labelled for seven to eleven year olds. Most importantly these stages are used in the schooling system, where children of the same age are taught the same material and classes are formed according to age, then grouped into infants, juniors or seniors. This system has worked well for many years and children can be seen to progress much as Piaget implied, however a fault of stage theories can be that children should be grouped according to ability not age, as some children are being taught at an inappropriate level and many are being slowed in their learning by one or two members of the class who think or behave at a lower level. Criticisms of stage models of cognitive development are frequent and difficult to overcome in general. Clearly there must always be some oversimplification of structurally distinct stages, even if fifty stages were used. The nature of development is extremely complex and difficult to record, as a child's mind is so malleable. Children are unique in every way, and for every pair that develops at a similar rate there will be another who doesn't 'fit' the trend. Children or their parents should not be made to feel abnormal if cognitive development doesn't follow an exact formula. An equally important disadvantage is that stage models are often inaccurate or simply wrong. A huge amount of research since Piaget's theory has found that most children can perform functions such as object-permanence well ahead of the prescribed timescale and others find that pre-operational children are less egocentric than Piaget believed, (Donaldson, 1987). Furthermore the extent of individual differences means that children of similar ages may vary widely across the stages. Flavell (1977) said that children should notice several changes simultaneously, but in reality this rarely happens. A child can acquire skills at any time and in varying order. It is inaccurate to say that at age seven a child will develop conservation, as many have demonstrated conservation of number years before conservation of weight, (MacConnell & Daehler, 2004). Even the foundations of stage models can be criticised to the same extent. Piaget was said to have little or no evidence for much of his theory, as his tasks set for children were often too difficult for the age and very confusing to perform. He used limited numbers, often theorising from just his own children, and showed an absence of control groups or statistical analysis. It is possible that today Piaget would find very different conclusions and formulate different stages, and in this way stage models can be described as almost entirely subjective. Even if all stage models correlated and described the same skill levels at the same ages, which does not happen, they would still constitute a narrowed form of cognitive enquiry. Most cognitive models completely ignore the social, cultural, educational and motivational background of the child, which all have a profound effect on cognitive development. Lawrence Kohlberg (1963) formulated an entire stage theory based solely on moral development of children, and the same could be done for family background, quality of education or place of birth, as cognitive development cannot rely simply on biological development, as Piaget thought. Despite the great number of disadvantages to stage models which have reduced their favour over the last twenty years, it cannot be denied that they have influenced our understanding of cognitive development hugely. Many studies of a cross section of western cultures have found that an average rate of development does fit roughly into Piaget's four stages, although with a number of sub-stages, (Bowden, 1994). The idea of stages of development is held by many and still used in the education system as a simple way to fairly divide children. Piaget's and other stage models can be used as a marker for children's development, to suggest where a child of a specific age should be cognitively, and to highlight any possible learning difficulties at an early age, allowing appropriate teaching. Whilst other theories, such as those of connectivity or information processing, might account for the high level of complexity in cognition, they are often thought to be too complicated for use by parents and teachers and less implementable. In the field of development theories a conflict will always arise between parsimony (looking for one general model rather than many different ones) and accuracy (the general model may fail to do justice to the complexity of cognitive development. It must be said that the stage models given so far do oversimplify development and ignore other influences, but perhaps in the future a practical stage theory may be developed which takes into account all we know about physical and neural development in the brain and tries to incorporate the observable changes in skills, education, culture and environment. The idea of behaviourism has been around for a long time but was made famous by the work of Ivan Pavlov. He conditioned the behaviour of dogs to salivate at the sound of a bell by repeatedly associating the ring of a bell with presentation of food. B. F. Skinner extended this idea whilst working with pigeons to show that animals could be trained to perform various acts in order to get food, as long as the association was made between food and the desired response. Skinner always recommended positive reinforcement such as rewards over negative reinforcement or punishment, (Eysenck, 2002). Such ideas have gradually leaked into the classroom, to increase desired behaviour from children, and even into the National Health Service, to attempt to 'cure' psychological disorders. One major criticism of the behaviouristic approach addresses the claim that behaviour can be explained without reference to mental activity. Human beings have a sense of feeling and of free will; behaviourism disregards this and sees the personality as a set of outward reactions rather than a distinct entity. This means that psychological disorders are not examined for their cause or brain dysfunction but simply observed. As a treatment in the practice of mental health this is often dismissed by cognitive scientists developing intricate internal information processing models and by medical experts who view the brain in terms of chemical reactions, (Comer, 1992). Despite this opposition though, behavioural therapy is currently used for the treatment of many psychological disorders and can have excellent results. It is perhaps fair to say that the success of such therapy will depend mostly on the type of disorder and its origin, if known, as there are a number of disorders that are considered too inbuilt to be changed merely through training. Certain kinds of psychological disorder can be reasonably explained by the behaviourist model and these tend to be phobias and anxiety disorders rather than personality disorders. Phobias are often explained as the result of classical conditioning, much like Pavlov's dogs. A person learns to associate a traumatic reaction (e.g. fear, shock) with a particular occurrence even though the level of fear is not in proportion with the possible danger. For example contact with a very aggressive dog as a child may cause someone to avoid situations with dogs in future so the fear is not stopped. However this can be effectively treated with behavioural therapy. One form is systematic desensitisation where a patient learns to associate a relaxed, happy state with the idea of the phobic situation and learns to gradually accept it through the opposite of the original conditioning. This treatment is a highly appropriate one, which can be explained due to the cause of the phobia being a behavioural one, therefore the treatment works like the cause in reverse. Wolpe's original method of constructing a hierarchy of fears, training in relaxation then graded exposure (1969) was updated by Shapiro (1989) who replaced muscle relaxation with induced eye movements. This proved very popular in the treatment of traumatic memories with over 50% moving from clinical to normal levels on the Impact of Events scale, (Greenwald, 1996). Other methods include flooding, where a patient is placed directly in the feared situation for some time to allow reality testing. This may be uncomfortable at first but results are quicker. Despite the success rate though, further research has shown that neither relaxation nor construction of a hierarchy of fears is necessary, and that exposure to real feared stimulus is better than imaginary ones. Whilst treatments boast that 70 % show some improvement in symptoms, few patients are ever completely free of anxiety and the systematic desensitisation methods used are slow to show results, (Seligman, 1995). Not all behavioural therapies use positive reinforcement as Skinner suggested though, and negative, punishing therapies have been used for many years. Aversion therapy is a form of behaviour modification that employs an unpleasant or even painful stimulus to prevent certain unwanted behaviour in patients. The first recorded use of this was in 1930 for the treatment of alcoholism, an area where it is still used today, but in the 1950s aversion therapy was widely accepted as a 'treatment' for sexual deviation such as homosexuality. Men could be shown images of attractive men whilst being administered an electric shock, and then shown pictures of women afterwards. In many cases this didn't work at all but simply gave its patients nightmares. Today aversion therapy is occasionally used on alcoholics, where chemicals such as disulfiram or Antabuse can be given, which react with alcohol to cause nausea and vomiting. The idea is to allow the patient to associate alcohol with the nausea and reduce or prevent excessive drinking, (Eysenck, 2002). Although this can have immediate beneficial effects, it is unethical to cause any patient such discomfort and often does not work in a number of cases where alcoholism has become a way of life and the cause is rooted in deeper psychological problems. Negative reinforcement is not such an effective way of altering behaviour as rewards. It is also criticised for being a controlling approach and it is sometimes asked in who's interest is the treatment - the individual or society? In cases such as alcoholism, paedophilia and antisocial personality disorder the treatment may work in the treatment situation but not in the unpredictable situations of real life, and they often target only the problem, not the cause of behaviour. In many other disorders behaviourism is used but in conjunction with counselling drugs or cognitive therapy, and has a good success rate. Cognitive behaviour therapy is promoted in the Department of Health guidelines as the first line treatment for depression, eating disorders, panic disorder, anxiety disorders and deliberate self-harm. It is efficient, relatively easy to learn and easy to deliver, (Department of Health, 2000). In the treatment of anorexia nervosa patients are encouraged to understand their thought and behaviour patterns and try to change any behaviours that are harmful or not helpful. Behaviour therapy can be effective in improving positive behaviours but concentrates on current actions, never looking into the past. This often means that disorders improve for only a short period of time then reoccur, as the cause is not addressed. In the safety of a therapist's office behaviour therapy can seem to work well but often does not continue in the normal environment of the patient. One such idea that has received this criticism is the token economies that are used in mental institutions. Although not strictly a form of treatment this system can be used to generally encourage desired behaviour and eliminate abnormal behaviour. Allyon and Azrin (1968) used a token economy with patients, giving them tokens for acts such as making their bed or brushing their hair, which could later be exchanged for pleasant activities. It was very successful and is implemented in many hospitals and even schools for children with behavioural disorders, however it can be seen that this is not indicative of real life and cannot continue indefinitely. Ideally people should learn to behave appropriately by themselves, not through bribery, but this is sometimes the only way of creating desired responses. Token economies are the most useful with groups who are particularly hard to treat, such as the chronically disturbed, schizophrenics, children and those with learning difficulties. With patients who display deep personality disorders though, treatment can be very difficult as behaviour is hard to predict and rewards of this kind give little motivation. One particular example is narcissistic personality disorder, which is an all-pervasive disorder similar to alcoholism but worse. It consumes the whole personality and therefore cannot be changed at will with therapy or training. Narcissists display many reckless behaviours, some incontrollable, and so rational learning therapies rarely have any effect. It is these severe psychological disorders which cannot be treated by a strictly behavioural approach - although behaviour modification can help with some effects, there is no effective therapy for narcissism itself, (Young, 1990). It appears then that behaviour therapy, in its many forms, can be a useful and well-used form of treatment for some psychological disorders but is rarely recommended as the sole treatment for any mental health problems. In general terms, behaviourism is an effective approach only when used along side other therapies and treatments, including cognitive therapy, counselling or even pharmacological prescriptions. The strictly behavioural approach is only successful in more minor cases of disorders such as posttraumatic stress, anxiety disorders or phobias. Any long term, deeply rooted disorders require careful examination of causes and the mental processes which accompany observable behaviour. It might be said that taken alone behaviour therapy is as effective as other popular therapies alone (Smithal. 1980), but the best approach is a more diverse mixture of psychoanalytic theory, medical treatment and cognitive behaviour therapy. Abstract A survey was carried out to investigate the causes and occurrences of the emotions shame, guilt and embarrassment. Ten participants were asked to cite three examples of occasions when they would feel these emotions. Responses were then looked at for general trends and categorised as to the type of situation or response given. It was generally noted that embarrassment was a more public emotion than shame or guilt, and that shame and guilt were brought about by similar situations. These results are discussed in terms of the meanings of these three emotions and the nature of social consciousness. IntroductionGuilt, shame and embarrassment are emotions of similar origin and type, in that they are moments of unpleasant self-consciousness that we all experience. It is sometimes believed that they are basically the same. However Tangney et al., (1996) described these emotions as distinctly separate experiences. A study was conducted amongst undergraduates to determine what sort of experiences lead to feelings of guilt, shame and embarrassment, and how different the three emotions were considered to be. It seemed that shame and guilt were similar emotions, with shame a more public experience, and that embarrassment was the most public of all, and milder than shame or guilt, (Tangney et al., 1996). The present study therefore aimed to differentiate between the emotions, if possible, to assess what kind of situations bring about these feelings and if the circumstances are similar in each case. It was expected that guilt and shame would be closely linked in terms of how they are felt and when, but that embarrassment would be a different emotion and caused by more public or social situations, rather than self-induced. MethodDesignThe experiment used qualitative methods to give a true result of what causes guilt, shame and embarrassment. This was a non-experimental study which simply asked participants to give examples of situations. The survey answers could then be looked at objectively. ParticipantsTen participants were asked to volunteer on a random basis from the local area, with varying ages and occupations. Each gave three examples of guilt, three of shame and three of embarrassment, to give thirty responses for each emotion. The ages ranged from 19 - 56 years, with a mean age of 29.5 years. Five of the participants were male, and five female. None of the participants had any experience of studying Psychology or of taking part in similar experiments. MaterialsThe questionnaire given to each subject was an A4 sheet divided into three sections. At the top of the page participants were asked to circle male or female, but not to give a name. The first question asked: ""Please write two or three lines giving, if possible, three examples of occasions when you have felt or would feel embarrassment;"". Space was then left to give three answers. The second and third questions were laid out in the same way but with the words ""shame"", then ""guilt"". ProcedureParticipants were given the questionnaire and asked to fill in the sheet. All were told that participation was entirely voluntary and that they could withdraw at any time. Up to 24 hours were given to complete the sheet, if needed. The answers were then collected and analysed. Responses were put into the following groups: firstly, active causes or passive causes, then public/group situations or private/one on one situations. Then emotions felt during the event or after it, actual occurrences or hypothetical responses, specific, detailed answers or general situations, and lastly, long answers (over ten words) or short answers (under ten words). Once a tally had been made for the number of answers that fell into each category, general conclusions from the responses were also made. ResultsThe participants' answers were looked at for general trends that might identify the differences in these emotions. It was immediately noticed that nearly every answer was given in the passive or infinitive tenses, and not referring to an actual incident that has happened. It was also noticed that many examples were cited by more than one person, for example, three people quoted 'tripping over in public' as an embarrassing event, four people said for guilt 'relaxing instead of working' or something similar, and seven answers for shame involved lying. Using these common themes the answers were classified according to the groups mentioned in the method. Some of these groupings showed no differing results so they will be omitted, as the aim is to assess the differences between these emotions. One trend emerged immediately that was expected; results for shame and guilt were very similar for most categories, but different to results for embarrassment. Firstly a tally was made for answers that involved active events that the person had chosen to do, rather than passive events which the person had no control over. For guilt, 19 responses were active (63%) and 11 were passive (37%), similarly for shame 20 answers were active (67%) and 10 were passive (33%), but for embarrassment nearly all were passive events (87%) with only 4 active events (13%). This would show that embarrassment tends to be out of our control, whereas shame and guilt are results of regretting our own actions. Another interesting result was for one-on-one/private situations against occurrences in public or in front of a group of people. For guilt, 87% of answers and for shame 93% involved the self or just one other person, whereas for embarrassment only 2 answers were personal. A huge 93% of embarrassment responses occurred in front of many people. Answers typically included social situations, such as parties, clubs or restaurants, where perhaps people feel they are on show in front of strangers and must conform to certain behaviour patterns. Related to this category were the groupings for mention of a personal relationship, versus strangers or public. Personal relationships included partner, friend, children or family, whilst public included the generic term 'work', or general public. For guilt, 30% of responses included a personal relation, for shame, a considerable 57%, but for embarrassment no answers referred to a personal relationship. This implies that embarrassment is caused by people unknown to the subject, rarely by close friends or family. Also 100% of embarrassment examples showed the feeling occurring during the event, whilst the majority of guilt and shame emotions (73% and 97%) came after the event. Almost all the responses were short answers (10 words or under) with only five examples in total over ten words. However, surprisingly, 80% of embarrassment responses did describe detail of a specific event rather than general situations, compared to just 27% for both shame and guilt. A pattern emerged, then, that shame and guilt responses were in general very similar and written in similar ways, with embarrassment being a seemingly different emotion. The only instances where shame and guilt emotions differed were firstly, that eight guilt responses were felt during the event with only one shame response described during the event, and secondly that 17 shame answers included reference to a personal relationship, compared to only 9 guilt answers. This might be pure chance, but it does seem to suggest that shame is a more intense feeling, often coming from disgust at letting a loved one or friend down, whereas guilt is a milder version of this, more often relating to people in general, for example many answers involved guilt at not helping someone in general. DiscussionIt emerged that shame and guilt had more in common, for example being emotions relating to personal failure or letting close friends and relations down. Embarrassment tended to focus on more trivial and light-hearted mistakes which were beyond the participant's control. Finally embarrassment occurred mainly in front of a group of people who were not known well, e.g. at a restaurant or party, whereas shame and guilt related to more personal situations involving just the self or one other person. This was generally as expected and followed the pattern of results given by Tangney (1996). The methods used in this study were qualitative methods, which allowed free, unstructured answers that were likely to be honest and reliable, but at the same time did not manipulate any variables (situations) and used no control groups. This means that it cannot be concluded that any situations specifically cause the emotions shame, guilt or embarrassment, only that participants claimed to feel guilt etc. in these types of situations. It might be necessary to conduct further studies under more controlled conditions, for example by asking participants to perform specific events, then asking what emotions were experienced and to what degree. If the experiment were to be repeated using qualitative methods it would be advisable to gain responses from a far larger sample. The survey could also include some kind of grading system, e.g. how guilty did this make you feel, from 1-5? Responses could also be correlated against males and females separately, to establish differences in males and females. In conclusion, shame, guilt and embarrassment are not words for the same emotion, but contrasting feelings of self-consciousness, which arise from different situations and events. Shame and guilt emotions were considered similar, arising from personal failure or failing what is expected of a close relationship. Embarrassment arose from uncomfortable, unfamiliar public situations, which could not be controlled. Embarrassment tended to be a more trivial and transient emotion, but it also reflected a lapse in what is expected in social environments. These findings tell us something of the nature of social interaction; self-conscious emotions are brought about from a failure to comply with what is socially expected of a person, either in personal relationships or social situations. It would appear that these emotions are closely linked to the idea of conformity; people judge themselves based on what they assume others would think yet tend to be their own worst critic. AbstractAn experiment was carried out to investigate the effect of background noise that is either semantically related or unrelated to printed word lists, which must be learned in one minute then immediately recalled, in relation to Baddeley's phonological similarity effect study, (1966). Correctly recalled words from visual lists were counted after participants experienced either related spoken words over headphones, random, unrelated words or no noise at all during the study phase. It was found that participants showed significantly poorer short term memory after experiencing background noise that was related to the study lists than unrelated, and that both noise conditions resulted in significantly fewer words recalled than the control condition, showing that background speech impairs the learning process and that semantically related speech impairs learning and memory more than random speech. The results are discussed in terms of their implications in learning environments and in everyday conversation. Introduction. The study of human memory is of great importance in learning and education, as well as for disorders such as amnesia, and to generally understand the functions and processes of memory. For psychologists, the classification of memory into three categories: episodic (memory for events), semantic (factual memory) and procedural (memory for skills), is useful in distinguishing between different functions of memory. A huge amount of literature exists on short-term memory, possibly because it is easy to test, and often focuses on the performance of memory when subjected to background noise or distraction. Often noise such as traffic, types of music or volume is used, although there are a few studies which look at the effect of speech - both meaningful and irrelevant - as a distractor. This is valuable research when considering the relevance of noisy situations in real life, for example in the classroom or when trying to focus on one conversation. Hygge, Boman and Enmarker, (2001), compared the effects of road traffic noise and irrelevant speech, played through loudspeakers at 66dB, then tested recall of word lists and a comprehension task. Based on previous research on irrelevant speech in short term memory (LeCompte, 1994; Martin et al., 1998; Tremblay et al., 2000), the prediction was that speech would disrupt recall from episodic memory more than traffic noise, as irrelevant speech should tax limited resources for parallel semantic processing. They found a significant effect of noise overall on the recall task but no significant difference between traffic noise and random speech. Many studies have tested the effect of different types of speech on recall, but most focus on the effect of phonologically similar or dissimilar words on the number of items recalled, (Boyle, 1996). Results suggest that interference from background speech which is similar sounding disrupts working memory more than dissimilar sounding words. However very little research has been published testing the effects of semantically similar (related meaning) words as a distractor to memory. Baddeley (1966), found that only phonological difference in word pronunciation increased recall, whether or not word meanings were similar. However, Geraci and Franklin (2004), published a study entitled 'The influence of linguistic labels on source monitoring decisions', which showed that semantically related words, e.g. rabbit and bunny, lead to more confusion and errors in recall than phonetically related words or homographs. This experiment aims to test the effects of semantically similar words against random, unrelated words, when given as background noise, on the recall of printed word lists. It is predicted that some spoken words that are semantically similar to the printed words will be encoded along with the printed words and may displace the learned words from short-term memory store, therefore few printed words will be correctly remembered. When the spoken words are semantically unrelated it is expected that the noise will be a distractor, causing some memory impairment, but that words will not be displaced, therefore most words will be correctly recalled. In the control condition where no background noise is present memory should not be impaired and it is expected that participants will remember up to nine items, according to the seven plus or minus two rule, (Miller, 1956). Method. DesignThe experiment used a one-way within-subjects design. The independent variable was the type of background noise during the study phase and there were three levels: semantically related speech, semantically unrelated speech and no noise. The dependent variable was the number of correctly recalled words during the test phase. ParticipantsThe participants were 30 students from the University of , both male and female and between the ages of 18 and 40. Participants were asked to volunteer their time. MaterialsThe background noise used was a set of three MP3 files, each lasting one minute. The first and second contained lists of ten words (adjectives) repeated once, read by a male voice at a rate of one word per five seconds. The third was a recording made in a silent room. The tracks were played on a Sony digital MP3 player, through a set of headband lightweight headphones with the following specifications: 27mm diameter ferrite speakers, 20-20,000 Hz frequency and 32 ohms +/- 10% impedance. Volume was set at 60 dB. The printed word lists consisted of ten semantically similar adjectives of three to seven letters in each word list (A, B and C). An example sheet is given in appendix (i). Subjects were given plain A4 paper to write responses on in the test phase. A stopwatch was also used for timing the one minute study phase and the five minute test phase. ProcedureSubjects were asked, one at a time, to sit alone at a desk in a silent room and put on the headphones. Subjects were given the printed sheet containing all three word lists and asked to start learning the first list only, trying to ignore the sound from the headphones, while the appropriate sound file was played at the same time. After one minute, timed on a stopwatch, participants were asked to stop learning and turn the sheet over. The headphones were also removed. Subjects were then given a sheet of plain A4 paper and a pen and asked to immediately recall as many words from the printed list only as possible, in any order. After exactly five minutes, timed again, the paper was removed and the headphones put on again. The second file was then played at the same time as the subject turned over the sheet and started to learn the second word list. After one minute of the study phase the subject was tested as before. The third file was then played as the subject learned the third word list and was tested again. The subject was debriefed and thanked for their time before leaving. The design was counterbalanced so that each of the thirty participants looked at the word lists and corresponding files in a different order, for example some learned B then C then A, some C then A then B and so on. Results. The participants' scores were then used to calculate mean scores for the three conditions as displayed in table 1 and figure 1. These data were subjected to a repeated measures analysis of variance test which revealed that the means differed significantly, F(2,58) = 73.95, pDiscussion. An experiment was carried out to find out whether speech as background noise disrupts verbal processing more when it is semantically related to the task than when it is completely unrelated, as predicted by Geraci and Franklin (2004). This was done by measuring ability to learn words using short term memory whilst hearing either related words, unrelated words or no noise through headphones. Results show that noise served as a general distractor during the test phase and impaired memory significantly, however semantically related words over headphones also impaired the learning process significantly more than unrelated words. Table 1 shows that the mean score for the related condition was 1.4 words fewer than for unrelated words, whilst standard error for unrelated words was far lower than for related words. These results tell us that hearing background noise that is related to the task can interfere with the verbal processing which occurs when trying to learn a list of words. It is clear that the differences were not produced by interference during recall or storage, as no noise was experienced in the test phase, but that the speech impaired the encoding process of short term memory. When trying to explain these results it is necessary to look at the raw data (appendix iii) and the responses given. In terms of errors made the original responses showed that participants during the no noise condition were able to remember a mean number of 8.7 words, as predicted by the seven items plus or minus two rule, (Miller, 1956), and this can be seen from the raw data, as most participants recalled between seven and nine words. It was also noted during the study that most subjects during this condition finished recalling words before the five minutes allowed, that is they were able to access their short term memory quickly and easily. Interestingly, in the unrelated condition the same was noticed; most subjects recalled as many as they could before the five minute boundary and appeared to be able to access the words quickly. In the related condition, however, participants often used the entire five minutes to recall words slowly. Many of these subjects recalled up to fifteen words, but many of these were words from the audio list (over headphones). In the other conditions hardly any words were remembered from the auditory source. These findings suggest that the semantically related words confused the subject and made it harder to concentrate on the written list only. The recollection of non-printed words suggests that many audio words were encoded at the same time, displacing the written words from short term storage. The results can be explained using Baddeley's model of working memory, (1974), in particular the phonological articulatory loop. It is likely that participants used silent verbal rehearsal as a technique for learning the printed words, and that words heard over headphones that were related were confused with those being rehearsed silently and wrongly encoded. However at the same time the results contradict Baddeley's findings from a study of memory for word sequences as a function of acoustic, semantic and formal similarity, (1966), which found that semantic similarity did not impair memory. It can be said that this experiment is more specifically focused on semantic relationships than Baddeley's, ignoring phonological similarity, and therefore the results are more pronounced. It could also be seen, though, that the results are the product of an engineered situation and do not mirror other environments. It is possible that subjects were more confused by auditory information than usual because it was presented over headphones which block out other noise. In other situations continuing background noise is blocked out, but this may be more difficult when using headphones. Reassurance can be taken from the fact that the unrelated condition gave higher mean scores, therefore subjects were able to block out the noise to a greater extent. Other potential problems might be seen in the experimental design, for example in that only ten words were given to be learned in the study phase. It is possible that this is too few, meaning subjects learn words then are distracted by audio sounds. It would be beneficial to change one or more of the fixed variables to see whether the same results are gained. Overall the results proved significant and can be explained by current memory research, including theories of verbal rehearsal and short term memory displacement. The findings mean that the actual conditions during the encoding stage of memory are just as important if not more so than conditions during retrieval. This information has many implications in the fields of learning and memory. Obviously in the classroom adjustments can be made to optimise learning, for instance if a teacher talks about a topic whilst her pupils are trying to read or write about the same topic there may be confusion: the best conditions for learning are without background noise, so it is advisable for the teacher to talk first and allow the class to listen, then to let the class learn in silence or with unrelated background noise such as music or quiet conversation. In informative presentations and training the same can be said; if slides or handouts are being read whilst the speaker gives related information verbally, memory can be impaired. When trying to learn information it is better to focus on one source at a time than to try to absorb facts from a number of sources simultaneously. This can of course be tested using varied conditions and other senses. Replications of this study would be useful in order to clarify and extend these findings. Different sources of information such as visual images or numbers may give rise to different results; it cannot be said whether or not these findings are limited to words only. Additionally it would be more relevant to everyday situations to test memory whilst actual conversations or television recordings are played as background noise, or to test long term memory by delayed recall. Conditions could also be extended to include another language, white noise, music or timed beeps. This would clarify exactly what kind of noise is distracting to memory; all noise or just speech, all speech or just speech of a viable language. In conclusion, it has been found that semantically related speech presented as an auditory source can impair short term memory of word lists presented visually. These results support the predicted outcome and can be used as evidence for the working memory model and the phonological loop, (Baddeley, 1992), suggesting that working memory has limited resources for semantic processing. The findings may also have relevance within areas of learning and memory.","IntroductionResearchers have argued that road crossing skills in children depend on attentional skills and ability to concentrate on a task during a distracting event. Dunbar, Lewis and Hill (2001) found that children showing greater attention switching skills were more aware of traffic when observed crossing roads and that maintaining attention throughout distractions might indicate a greater likelihood of crossing roads in a safe manner. Previous studies had used attention switching tasks such as reporting letters and numbers heard in either of two auditory streams, i.e. attending to the left or right ear (Pearson & Lane, 1991), to assess differences in time taken to move attention between the streams. It was found that younger children took longer and showed higher error rates when switching attention than older children, suggesting that attention skills improve with age. This study aims to test the idea that attentional skills improve with age, using the test of everyday attention for children, whereby children must inhibit their natural responses while a distracting event occurs. The study also aims to test earlier findings that primary school age boys have more road accidents than girls of the same age, by comparing attentional performance between the two gender groups. There are, therefore, three main hypotheses; firstly that responses in the task will be slower where a distracting event occurs than where there is no distraction, across all groups, secondly, that the response speed will improve with age in both distracting and quiet conditions, and thirdly, that boys will perform more slowly than girls during the distracting event. MethodParticipants96 children participated in total; 32 from each of three age groups: 4-5 year olds, 6-7 year olds and 8-9 year olds. Half of the children in each age group were boys, half were girls. MaterialsChildren were shown lists of numbers, either 1 or 2, in random order in a path. A television showing a cartoon was also used in the distracting condition. ProcedureEach child was taken into a quiet room and shown an array containing the numbers 1 and 2 in a random sequence. They were initially asked to read out the numbers as they appeared (i.e. 1 is 1, 2 is 2) for the baseline assessment. In the inhibition task each child was asked to read out a sequence, but saying ""2"" when they saw 1 and saying ""1"" when they saw 2. In the quiet condition the room was kept quiet throughout, but in the distracting condition the child was told they could watch a cartoon on the television when they had finished the task, but the cartoon was remotely switched on before the child finished, half way through the task. All children completed each of the three tasks, and their responses were timed in each condition. ResultsTwo difference scores were calculated for each child by subtracting i) the baseline response from the quiet condition response, then ii) the baseline response from the distraction condition response. The figures indicated the time taken to inhibit the salient response for each condition so could be compared with each other directly to give a measure of the difference in inhibition times. Poor performance, indicated by a larger inhibition time, would suggest poorer attentional skills. A clustered box plot revealed three outliers which could not be explained, two in the 4-5 years group (both girls) and one in the 6-7 years group (a boy), however, reanalysis with these excluded yielded the same pattern of results so the outliers were included in all analysis. Scatter plots of the mean difference scores against their standard deviations showed no obvious relationships, suggesting the homogeneity of variance assumption was not violated, therefore the data was not transformed but used in its raw form. The results were then analysed in a three way within groups analysis of variance (ANOVA), of sex (2) x age (3) x score (2), with difference score as the dependent variable. ANOVA outputs showed a significant main effect of distraction condition on inhibition times, F(1,90) = 83.465, p0.05, no significant interaction between age and sex, F(2,90) = 0.235, p>0.05 and no significant interaction between age and distraction condition, F(2,90) = 1.261, p>0.05. The interaction between distraction condition and sex was significant at the 5% level but not at the 1% level, therefore further research would be desirable in order to make firmer conclusions, F(1,90) = 4.562, pDiscussionResults show that all three hypotheses were supported by the data gathered. Analysis firstly showed a significant main effect of distraction condition, meaning that the inhibition times of children were significantly higher when a distracting cartoon was played than when the room was quiet throughout, suggesting that the distracting event did impair children's attention performance. More specifically, analysis confirmed that inhibition times of boys was significantly higher than that of girls in both distraction conditions, supporting the theory that boys were more distracted than girls by the cartoon and earlier findings that boys have poorer attention skills than girls of a similar age. Whilst initial ANOVA results gave no indication of age effects, a planned linear contrast over age was significant, suggesting that as age increases, attentional performance will increase so inhibition times decreased overall, this was particularly significant in the distraction condition (p=0.000 compared to p=0.017)) suggesting that age affects inhibition times more when a distracting event occurs, however there were no age-sex interactions found. Whilst an interaction effect between sex and distraction condition was observed, the significance level was not low enough to be completely confident (p=0.035) therefore further or repeated research would be advantageous to rule out the possibility of a type I error. Again, post hoc age group comparisons revealed a slightly significant difference between the 4-5 and the 8-9 age groups, but given that the probability level was 0.046 and the large number of comparisons made, a chance level of probability cannot be ruled out, therefore the result should be ignored until proven in further research. In general, mean scores were much as expected, with distraction scores higher than no distraction scores, and scores tending to decrease (improve) with age, however the mean distraction inhibition score for 8-9 year old boys was higher than expected (15.50) and was, in fact, the largest inhibition time found. Since there were no outliers in this group it is difficult to conclude why the times were this high, and a design problem can never be ruled out. Repeat experiments would be useful in determining any cause for this result. As expected though, the lowest inhibition times were generally found in the oldest girls tested, in both conditions. Possible problems in the design of the study could account for odd results, although no firm conclusions can be made. Although the within subjects design removes between subjects variance, here the within subjects design may have affected the recorded response times. Practice of the opposite worlds test will clearly increase ability, so that even with counter balancing, the more practised second condition may lead to outliers in some cases. Other problems include the type of distraction - in this case a children's cartoon. It might be that cartoons appeal more to boys than girls, leading to greater distraction, and that another sort of distraction, such as a friend entering the room or a conversation starting, may distract girls more. The choice may also be criticised for its irrelevance to the road-crossing scenario, where a friendly conversation might be more appropriate. It should be noted, however, that the test of everyday attention, which was used as the stimulus, has been investigated by psychology professionals and is thought to be both reliable and valid, with high test-retest reliability and significant correlation with other measures of attention, suggesting that results can be generalised to various measures of attention performance (Robertson, Ward, Ridgeway & Nimmo-Smith, 1996). As the findings here are in line with the hypotheses made and in accordance with previous research and theory in the field, it can be concluded that results of this study are both reliable and valid , with generalisability of findings should further research be conducted along these lines. It is strongly suggested that research in the area be continued, as there are many areas of attention and cognitive skills yet to be looked at in conjunction with road traffic awareness. The links are important to future training in road safety with children, and a thorough understanding of the cognitive and motor skills involved in road crossing will improve our ability to teach children affective road skills. Whilst this study does not in any way prove links between attention and traffic skills, it is still useful in addressing possible areas for further research and understanding children's attention levels. Results from this experiment could lead to better training for children and the possible implementation of video or computer games involving abstract tasks to train attention skills. These results also bring to light the vulnerability of younger children and of boys compared to girls. For both parents and those working with children, boys should be made especially aware of the importance of concentration at the roadside and of possible dangers. For younger primary age children, supervision should be stressed when near roads, and roads surrounding schools should be adapted to younger pedestrians, perhaps with warning signs, speed limits and pavement barriers.",True
55,"IntroductionResearchers have argued that road crossing skills in children depend on attentional skills and ability to concentrate on a task during a distracting event. Dunbar, Lewis and Hill (2001) found that children showing greater attention switching skills were more aware of traffic when observed crossing roads and that maintaining attention throughout distractions might indicate a greater likelihood of crossing roads in a safe manner. Previous studies had used attention switching tasks such as reporting letters and numbers heard in either of two auditory streams, i.e. attending to the left or right ear (Pearson & Lane, 1991), to assess differences in time taken to move attention between the streams. It was found that younger children took longer and showed higher error rates when switching attention than older children, suggesting that attention skills improve with age. This study aims to test the idea that attentional skills improve with age, using the test of everyday attention for children, whereby children must inhibit their natural responses while a distracting event occurs. The study also aims to test earlier findings that primary school age boys have more road accidents than girls of the same age, by comparing attentional performance between the two gender groups. There are, therefore, three main hypotheses; firstly that responses in the task will be slower where a distracting event occurs than where there is no distraction, across all groups, secondly, that the response speed will improve with age in both distracting and quiet conditions, and thirdly, that boys will perform more slowly than girls during the distracting event. MethodParticipants96 children participated in total; 32 from each of three age groups: 4-5 year olds, 6-7 year olds and 8-9 year olds. Half of the children in each age group were boys, half were girls. MaterialsChildren were shown lists of numbers, either 1 or 2, in random order in a path. A television showing a cartoon was also used in the distracting condition. ProcedureEach child was taken into a quiet room and shown an array containing the numbers 1 and 2 in a random sequence. They were initially asked to read out the numbers as they appeared (i.e. 1 is 1, 2 is 2) for the baseline assessment. In the inhibition task each child was asked to read out a sequence, but saying ""2"" when they saw 1 and saying ""1"" when they saw 2. In the quiet condition the room was kept quiet throughout, but in the distracting condition the child was told they could watch a cartoon on the television when they had finished the task, but the cartoon was remotely switched on before the child finished, half way through the task. All children completed each of the three tasks, and their responses were timed in each condition. ResultsTwo difference scores were calculated for each child by subtracting i) the baseline response from the quiet condition response, then ii) the baseline response from the distraction condition response. The figures indicated the time taken to inhibit the salient response for each condition so could be compared with each other directly to give a measure of the difference in inhibition times. Poor performance, indicated by a larger inhibition time, would suggest poorer attentional skills. A clustered box plot revealed three outliers which could not be explained, two in the 4-5 years group (both girls) and one in the 6-7 years group (a boy), however, reanalysis with these excluded yielded the same pattern of results so the outliers were included in all analysis. Scatter plots of the mean difference scores against their standard deviations showed no obvious relationships, suggesting the homogeneity of variance assumption was not violated, therefore the data was not transformed but used in its raw form. The results were then analysed in a three way within groups analysis of variance (ANOVA), of sex (2) x age (3) x score (2), with difference score as the dependent variable. ANOVA outputs showed a significant main effect of distraction condition on inhibition times, F(1,90) = 83.465, p0.05, no significant interaction between age and sex, F(2,90) = 0.235, p>0.05 and no significant interaction between age and distraction condition, F(2,90) = 1.261, p>0.05. The interaction between distraction condition and sex was significant at the 5% level but not at the 1% level, therefore further research would be desirable in order to make firmer conclusions, F(1,90) = 4.562, pDiscussionResults show that all three hypotheses were supported by the data gathered. Analysis firstly showed a significant main effect of distraction condition, meaning that the inhibition times of children were significantly higher when a distracting cartoon was played than when the room was quiet throughout, suggesting that the distracting event did impair children's attention performance. More specifically, analysis confirmed that inhibition times of boys was significantly higher than that of girls in both distraction conditions, supporting the theory that boys were more distracted than girls by the cartoon and earlier findings that boys have poorer attention skills than girls of a similar age. Whilst initial ANOVA results gave no indication of age effects, a planned linear contrast over age was significant, suggesting that as age increases, attentional performance will increase so inhibition times decreased overall, this was particularly significant in the distraction condition (p=0.000 compared to p=0.017)) suggesting that age affects inhibition times more when a distracting event occurs, however there were no age-sex interactions found. Whilst an interaction effect between sex and distraction condition was observed, the significance level was not low enough to be completely confident (p=0.035) therefore further or repeated research would be advantageous to rule out the possibility of a type I error. Again, post hoc age group comparisons revealed a slightly significant difference between the 4-5 and the 8-9 age groups, but given that the probability level was 0.046 and the large number of comparisons made, a chance level of probability cannot be ruled out, therefore the result should be ignored until proven in further research. In general, mean scores were much as expected, with distraction scores higher than no distraction scores, and scores tending to decrease (improve) with age, however the mean distraction inhibition score for 8-9 year old boys was higher than expected (15.50) and was, in fact, the largest inhibition time found. Since there were no outliers in this group it is difficult to conclude why the times were this high, and a design problem can never be ruled out. Repeat experiments would be useful in determining any cause for this result. As expected though, the lowest inhibition times were generally found in the oldest girls tested, in both conditions. Possible problems in the design of the study could account for odd results, although no firm conclusions can be made. Although the within subjects design removes between subjects variance, here the within subjects design may have affected the recorded response times. Practice of the opposite worlds test will clearly increase ability, so that even with counter balancing, the more practised second condition may lead to outliers in some cases. Other problems include the type of distraction - in this case a children's cartoon. It might be that cartoons appeal more to boys than girls, leading to greater distraction, and that another sort of distraction, such as a friend entering the room or a conversation starting, may distract girls more. The choice may also be criticised for its irrelevance to the road-crossing scenario, where a friendly conversation might be more appropriate. It should be noted, however, that the test of everyday attention, which was used as the stimulus, has been investigated by psychology professionals and is thought to be both reliable and valid, with high test-retest reliability and significant correlation with other measures of attention, suggesting that results can be generalised to various measures of attention performance (Robertson, Ward, Ridgeway & Nimmo-Smith, 1996). As the findings here are in line with the hypotheses made and in accordance with previous research and theory in the field, it can be concluded that results of this study are both reliable and valid , with generalisability of findings should further research be conducted along these lines. It is strongly suggested that research in the area be continued, as there are many areas of attention and cognitive skills yet to be looked at in conjunction with road traffic awareness. The links are important to future training in road safety with children, and a thorough understanding of the cognitive and motor skills involved in road crossing will improve our ability to teach children affective road skills. Whilst this study does not in any way prove links between attention and traffic skills, it is still useful in addressing possible areas for further research and understanding children's attention levels. Results from this experiment could lead to better training for children and the possible implementation of video or computer games involving abstract tasks to train attention skills. These results also bring to light the vulnerability of younger children and of boys compared to girls. For both parents and those working with children, boys should be made especially aware of the importance of concentration at the roadside and of possible dangers. For younger primary age children, supervision should be stressed when near roads, and roads surrounding schools should be adapted to younger pedestrians, perhaps with warning signs, speed limits and pavement barriers.","Dividing development into distinct stages has occurred in many areas of psychology, but most frequently in cognitive development of children and adolescents. There has always been disagreement as to whether development should be categorised in stages or whether it is a constantly moving process which never stops. Flavell (1977) suggested that stages in development must show distinct, qualitative changes rather than differences in quantitative ability. He also noted that transition between stages must incur several simultaneous changes in many areas of cognition, and that these transitions should be rapid rather than gradual. Some psychologists have tried to suggest that development is an accumulation of small qualitative changes over time, causing smooth, continuous improvement, called linear dynamics, (Harris & Butterworth, 2002), however most parents and teachers will agree that children's development does feature many sudden periods of growth. There have been a number of stage models of cognitive development in recent psychology, which tend to agree on many points, such as the number of stages and the types of changes, suggesting that each has some merit. Freud famously described stages of development in children, including oral, anal, phallic, oedipal, latency and genital activity, (Harris & Butterworth, 2002), although these were never given much credit as Freud was seen to overemphasise sexual thoughts in young children. Vygotsky focused more on learning as a result of social interaction, but stated that cognitive development was limited to a certain range at any given time. His ordered stages of development included affiliation, play, learning, peer and work and have had some impact on the teaching of infant children, where the 'play' stage has encouraged more play centred learning in the classroom, (Donaldson, 1987). Other similar stage theories , such as that of Jerome Bruner, (Flavell, 1977), differ from these as they are not age dependent or invariant, and include fewer stages, in the case of Bruner, the enactive stage where knowledge is primarily in the form of motor responses, the iconic stage where knowledge is stored as visual images, and the symbolic stage where knowledge is stored as words or symbols. Bruner's stages have had less impact in the classroom, but his iconic stage has encouraged the use of pictures and diagrams in learning. A more recent stage theory, that of William Perry (1970), concentrates on the development of college students (17-21) and has been used throughout the USA in teaching many subjects. It includes nine points of development, from basic duality (one), through relativism (point five), towards commitment (nine). Without doubt the most famous of the stage models is Jean Piaget's model of cognitive development, where childhood is divided into four stages based on age, which reflect the differing cognitive abilities of children at different ages. Whilst he recognised that transition could be at different times for each child, he outlined the nature of learning in each stage, (Boden, 1994). Firstly the sensori-motor stage, from birth to two years, shows cognition restricted to simple reflexes but quickly increases as learning through play occurs, (Sutherland, 1992). His ideas of imitation and learning through experience have influenced the way parents interact with their infants and can be seen in most playgroups and homes. However Piaget has been criticised for underestimating the cognitive ability of babies, which is understandable given the lack of imaging techniques that were available to him. Secondly, the pre-operational stage (two - seven) shows developments such as language and object permanence. During the concrete operations stage (seven - eleven), children show increased abilities in logical concepts such as conservation and reversibility, and during the final formal operations stage (eleven onwards), which some adults never reach, using many different operations simultaneously can be seen. Children think logically and mathematically and work systematically through solutions, (Papert, 1999). In many ways Piaget's four stages have dramatically changed the form of education today, from school structure to curriculum. His ideas of the ability of children has influenced what is taught to children, for example division would not be taught to a five year old as Piaget says they lack the concept of reversibility, (Dricoll, 2000). In the pre-conceptual stage children cannot focus on more than one task at once, therefore a teacher will usually move children explicitly from one task to another, whereas a seven year old might be expected to listen and work at the same time. The general idea of stages can be seen used in a great many areas in life. Firstly in terminology; children are often categorised according to age, e.g. toddlers, infants, teenagers and adolescents. Secondly stages are used in commercial products including medicines, clothing and toys, where a toy might be labelled for seven to eleven year olds. Most importantly these stages are used in the schooling system, where children of the same age are taught the same material and classes are formed according to age, then grouped into infants, juniors or seniors. This system has worked well for many years and children can be seen to progress much as Piaget implied, however a fault of stage theories can be that children should be grouped according to ability not age, as some children are being taught at an inappropriate level and many are being slowed in their learning by one or two members of the class who think or behave at a lower level. Criticisms of stage models of cognitive development are frequent and difficult to overcome in general. Clearly there must always be some oversimplification of structurally distinct stages, even if fifty stages were used. The nature of development is extremely complex and difficult to record, as a child's mind is so malleable. Children are unique in every way, and for every pair that develops at a similar rate there will be another who doesn't 'fit' the trend. Children or their parents should not be made to feel abnormal if cognitive development doesn't follow an exact formula. An equally important disadvantage is that stage models are often inaccurate or simply wrong. A huge amount of research since Piaget's theory has found that most children can perform functions such as object-permanence well ahead of the prescribed timescale and others find that pre-operational children are less egocentric than Piaget believed, (Donaldson, 1987). Furthermore the extent of individual differences means that children of similar ages may vary widely across the stages. Flavell (1977) said that children should notice several changes simultaneously, but in reality this rarely happens. A child can acquire skills at any time and in varying order. It is inaccurate to say that at age seven a child will develop conservation, as many have demonstrated conservation of number years before conservation of weight, (MacConnell & Daehler, 2004). Even the foundations of stage models can be criticised to the same extent. Piaget was said to have little or no evidence for much of his theory, as his tasks set for children were often too difficult for the age and very confusing to perform. He used limited numbers, often theorising from just his own children, and showed an absence of control groups or statistical analysis. It is possible that today Piaget would find very different conclusions and formulate different stages, and in this way stage models can be described as almost entirely subjective. Even if all stage models correlated and described the same skill levels at the same ages, which does not happen, they would still constitute a narrowed form of cognitive enquiry. Most cognitive models completely ignore the social, cultural, educational and motivational background of the child, which all have a profound effect on cognitive development. Lawrence Kohlberg (1963) formulated an entire stage theory based solely on moral development of children, and the same could be done for family background, quality of education or place of birth, as cognitive development cannot rely simply on biological development, as Piaget thought. Despite the great number of disadvantages to stage models which have reduced their favour over the last twenty years, it cannot be denied that they have influenced our understanding of cognitive development hugely. Many studies of a cross section of western cultures have found that an average rate of development does fit roughly into Piaget's four stages, although with a number of sub-stages, (Bowden, 1994). The idea of stages of development is held by many and still used in the education system as a simple way to fairly divide children. Piaget's and other stage models can be used as a marker for children's development, to suggest where a child of a specific age should be cognitively, and to highlight any possible learning difficulties at an early age, allowing appropriate teaching. Whilst other theories, such as those of connectivity or information processing, might account for the high level of complexity in cognition, they are often thought to be too complicated for use by parents and teachers and less implementable. In the field of development theories a conflict will always arise between parsimony (looking for one general model rather than many different ones) and accuracy (the general model may fail to do justice to the complexity of cognitive development. It must be said that the stage models given so far do oversimplify development and ignore other influences, but perhaps in the future a practical stage theory may be developed which takes into account all we know about physical and neural development in the brain and tries to incorporate the observable changes in skills, education, culture and environment. The idea of behaviourism has been around for a long time but was made famous by the work of Ivan Pavlov. He conditioned the behaviour of dogs to salivate at the sound of a bell by repeatedly associating the ring of a bell with presentation of food. B. F. Skinner extended this idea whilst working with pigeons to show that animals could be trained to perform various acts in order to get food, as long as the association was made between food and the desired response. Skinner always recommended positive reinforcement such as rewards over negative reinforcement or punishment, (Eysenck, 2002). Such ideas have gradually leaked into the classroom, to increase desired behaviour from children, and even into the National Health Service, to attempt to 'cure' psychological disorders. One major criticism of the behaviouristic approach addresses the claim that behaviour can be explained without reference to mental activity. Human beings have a sense of feeling and of free will; behaviourism disregards this and sees the personality as a set of outward reactions rather than a distinct entity. This means that psychological disorders are not examined for their cause or brain dysfunction but simply observed. As a treatment in the practice of mental health this is often dismissed by cognitive scientists developing intricate internal information processing models and by medical experts who view the brain in terms of chemical reactions, (Comer, 1992). Despite this opposition though, behavioural therapy is currently used for the treatment of many psychological disorders and can have excellent results. It is perhaps fair to say that the success of such therapy will depend mostly on the type of disorder and its origin, if known, as there are a number of disorders that are considered too inbuilt to be changed merely through training. Certain kinds of psychological disorder can be reasonably explained by the behaviourist model and these tend to be phobias and anxiety disorders rather than personality disorders. Phobias are often explained as the result of classical conditioning, much like Pavlov's dogs. A person learns to associate a traumatic reaction (e.g. fear, shock) with a particular occurrence even though the level of fear is not in proportion with the possible danger. For example contact with a very aggressive dog as a child may cause someone to avoid situations with dogs in future so the fear is not stopped. However this can be effectively treated with behavioural therapy. One form is systematic desensitisation where a patient learns to associate a relaxed, happy state with the idea of the phobic situation and learns to gradually accept it through the opposite of the original conditioning. This treatment is a highly appropriate one, which can be explained due to the cause of the phobia being a behavioural one, therefore the treatment works like the cause in reverse. Wolpe's original method of constructing a hierarchy of fears, training in relaxation then graded exposure (1969) was updated by Shapiro (1989) who replaced muscle relaxation with induced eye movements. This proved very popular in the treatment of traumatic memories with over 50% moving from clinical to normal levels on the Impact of Events scale, (Greenwald, 1996). Other methods include flooding, where a patient is placed directly in the feared situation for some time to allow reality testing. This may be uncomfortable at first but results are quicker. Despite the success rate though, further research has shown that neither relaxation nor construction of a hierarchy of fears is necessary, and that exposure to real feared stimulus is better than imaginary ones. Whilst treatments boast that 70 % show some improvement in symptoms, few patients are ever completely free of anxiety and the systematic desensitisation methods used are slow to show results, (Seligman, 1995). Not all behavioural therapies use positive reinforcement as Skinner suggested though, and negative, punishing therapies have been used for many years. Aversion therapy is a form of behaviour modification that employs an unpleasant or even painful stimulus to prevent certain unwanted behaviour in patients. The first recorded use of this was in 1930 for the treatment of alcoholism, an area where it is still used today, but in the 1950s aversion therapy was widely accepted as a 'treatment' for sexual deviation such as homosexuality. Men could be shown images of attractive men whilst being administered an electric shock, and then shown pictures of women afterwards. In many cases this didn't work at all but simply gave its patients nightmares. Today aversion therapy is occasionally used on alcoholics, where chemicals such as disulfiram or Antabuse can be given, which react with alcohol to cause nausea and vomiting. The idea is to allow the patient to associate alcohol with the nausea and reduce or prevent excessive drinking, (Eysenck, 2002). Although this can have immediate beneficial effects, it is unethical to cause any patient such discomfort and often does not work in a number of cases where alcoholism has become a way of life and the cause is rooted in deeper psychological problems. Negative reinforcement is not such an effective way of altering behaviour as rewards. It is also criticised for being a controlling approach and it is sometimes asked in who's interest is the treatment - the individual or society? In cases such as alcoholism, paedophilia and antisocial personality disorder the treatment may work in the treatment situation but not in the unpredictable situations of real life, and they often target only the problem, not the cause of behaviour. In many other disorders behaviourism is used but in conjunction with counselling drugs or cognitive therapy, and has a good success rate. Cognitive behaviour therapy is promoted in the Department of Health guidelines as the first line treatment for depression, eating disorders, panic disorder, anxiety disorders and deliberate self-harm. It is efficient, relatively easy to learn and easy to deliver, (Department of Health, 2000). In the treatment of anorexia nervosa patients are encouraged to understand their thought and behaviour patterns and try to change any behaviours that are harmful or not helpful. Behaviour therapy can be effective in improving positive behaviours but concentrates on current actions, never looking into the past. This often means that disorders improve for only a short period of time then reoccur, as the cause is not addressed. In the safety of a therapist's office behaviour therapy can seem to work well but often does not continue in the normal environment of the patient. One such idea that has received this criticism is the token economies that are used in mental institutions. Although not strictly a form of treatment this system can be used to generally encourage desired behaviour and eliminate abnormal behaviour. Allyon and Azrin (1968) used a token economy with patients, giving them tokens for acts such as making their bed or brushing their hair, which could later be exchanged for pleasant activities. It was very successful and is implemented in many hospitals and even schools for children with behavioural disorders, however it can be seen that this is not indicative of real life and cannot continue indefinitely. Ideally people should learn to behave appropriately by themselves, not through bribery, but this is sometimes the only way of creating desired responses. Token economies are the most useful with groups who are particularly hard to treat, such as the chronically disturbed, schizophrenics, children and those with learning difficulties. With patients who display deep personality disorders though, treatment can be very difficult as behaviour is hard to predict and rewards of this kind give little motivation. One particular example is narcissistic personality disorder, which is an all-pervasive disorder similar to alcoholism but worse. It consumes the whole personality and therefore cannot be changed at will with therapy or training. Narcissists display many reckless behaviours, some incontrollable, and so rational learning therapies rarely have any effect. It is these severe psychological disorders which cannot be treated by a strictly behavioural approach - although behaviour modification can help with some effects, there is no effective therapy for narcissism itself, (Young, 1990). It appears then that behaviour therapy, in its many forms, can be a useful and well-used form of treatment for some psychological disorders but is rarely recommended as the sole treatment for any mental health problems. In general terms, behaviourism is an effective approach only when used along side other therapies and treatments, including cognitive therapy, counselling or even pharmacological prescriptions. The strictly behavioural approach is only successful in more minor cases of disorders such as posttraumatic stress, anxiety disorders or phobias. Any long term, deeply rooted disorders require careful examination of causes and the mental processes which accompany observable behaviour. It might be said that taken alone behaviour therapy is as effective as other popular therapies alone (Smithal. 1980), but the best approach is a more diverse mixture of psychoanalytic theory, medical treatment and cognitive behaviour therapy. Abstract A survey was carried out to investigate the causes and occurrences of the emotions shame, guilt and embarrassment. Ten participants were asked to cite three examples of occasions when they would feel these emotions. Responses were then looked at for general trends and categorised as to the type of situation or response given. It was generally noted that embarrassment was a more public emotion than shame or guilt, and that shame and guilt were brought about by similar situations. These results are discussed in terms of the meanings of these three emotions and the nature of social consciousness. IntroductionGuilt, shame and embarrassment are emotions of similar origin and type, in that they are moments of unpleasant self-consciousness that we all experience. It is sometimes believed that they are basically the same. However Tangney et al., (1996) described these emotions as distinctly separate experiences. A study was conducted amongst undergraduates to determine what sort of experiences lead to feelings of guilt, shame and embarrassment, and how different the three emotions were considered to be. It seemed that shame and guilt were similar emotions, with shame a more public experience, and that embarrassment was the most public of all, and milder than shame or guilt, (Tangney et al., 1996). The present study therefore aimed to differentiate between the emotions, if possible, to assess what kind of situations bring about these feelings and if the circumstances are similar in each case. It was expected that guilt and shame would be closely linked in terms of how they are felt and when, but that embarrassment would be a different emotion and caused by more public or social situations, rather than self-induced. MethodDesignThe experiment used qualitative methods to give a true result of what causes guilt, shame and embarrassment. This was a non-experimental study which simply asked participants to give examples of situations. The survey answers could then be looked at objectively. ParticipantsTen participants were asked to volunteer on a random basis from the local area, with varying ages and occupations. Each gave three examples of guilt, three of shame and three of embarrassment, to give thirty responses for each emotion. The ages ranged from 19 - 56 years, with a mean age of 29.5 years. Five of the participants were male, and five female. None of the participants had any experience of studying Psychology or of taking part in similar experiments. MaterialsThe questionnaire given to each subject was an A4 sheet divided into three sections. At the top of the page participants were asked to circle male or female, but not to give a name. The first question asked: ""Please write two or three lines giving, if possible, three examples of occasions when you have felt or would feel embarrassment;"". Space was then left to give three answers. The second and third questions were laid out in the same way but with the words ""shame"", then ""guilt"". ProcedureParticipants were given the questionnaire and asked to fill in the sheet. All were told that participation was entirely voluntary and that they could withdraw at any time. Up to 24 hours were given to complete the sheet, if needed. The answers were then collected and analysed. Responses were put into the following groups: firstly, active causes or passive causes, then public/group situations or private/one on one situations. Then emotions felt during the event or after it, actual occurrences or hypothetical responses, specific, detailed answers or general situations, and lastly, long answers (over ten words) or short answers (under ten words). Once a tally had been made for the number of answers that fell into each category, general conclusions from the responses were also made. ResultsThe participants' answers were looked at for general trends that might identify the differences in these emotions. It was immediately noticed that nearly every answer was given in the passive or infinitive tenses, and not referring to an actual incident that has happened. It was also noticed that many examples were cited by more than one person, for example, three people quoted 'tripping over in public' as an embarrassing event, four people said for guilt 'relaxing instead of working' or something similar, and seven answers for shame involved lying. Using these common themes the answers were classified according to the groups mentioned in the method. Some of these groupings showed no differing results so they will be omitted, as the aim is to assess the differences between these emotions. One trend emerged immediately that was expected; results for shame and guilt were very similar for most categories, but different to results for embarrassment. Firstly a tally was made for answers that involved active events that the person had chosen to do, rather than passive events which the person had no control over. For guilt, 19 responses were active (63%) and 11 were passive (37%), similarly for shame 20 answers were active (67%) and 10 were passive (33%), but for embarrassment nearly all were passive events (87%) with only 4 active events (13%). This would show that embarrassment tends to be out of our control, whereas shame and guilt are results of regretting our own actions. Another interesting result was for one-on-one/private situations against occurrences in public or in front of a group of people. For guilt, 87% of answers and for shame 93% involved the self or just one other person, whereas for embarrassment only 2 answers were personal. A huge 93% of embarrassment responses occurred in front of many people. Answers typically included social situations, such as parties, clubs or restaurants, where perhaps people feel they are on show in front of strangers and must conform to certain behaviour patterns. Related to this category were the groupings for mention of a personal relationship, versus strangers or public. Personal relationships included partner, friend, children or family, whilst public included the generic term 'work', or general public. For guilt, 30% of responses included a personal relation, for shame, a considerable 57%, but for embarrassment no answers referred to a personal relationship. This implies that embarrassment is caused by people unknown to the subject, rarely by close friends or family. Also 100% of embarrassment examples showed the feeling occurring during the event, whilst the majority of guilt and shame emotions (73% and 97%) came after the event. Almost all the responses were short answers (10 words or under) with only five examples in total over ten words. However, surprisingly, 80% of embarrassment responses did describe detail of a specific event rather than general situations, compared to just 27% for both shame and guilt. A pattern emerged, then, that shame and guilt responses were in general very similar and written in similar ways, with embarrassment being a seemingly different emotion. The only instances where shame and guilt emotions differed were firstly, that eight guilt responses were felt during the event with only one shame response described during the event, and secondly that 17 shame answers included reference to a personal relationship, compared to only 9 guilt answers. This might be pure chance, but it does seem to suggest that shame is a more intense feeling, often coming from disgust at letting a loved one or friend down, whereas guilt is a milder version of this, more often relating to people in general, for example many answers involved guilt at not helping someone in general. DiscussionIt emerged that shame and guilt had more in common, for example being emotions relating to personal failure or letting close friends and relations down. Embarrassment tended to focus on more trivial and light-hearted mistakes which were beyond the participant's control. Finally embarrassment occurred mainly in front of a group of people who were not known well, e.g. at a restaurant or party, whereas shame and guilt related to more personal situations involving just the self or one other person. This was generally as expected and followed the pattern of results given by Tangney (1996). The methods used in this study were qualitative methods, which allowed free, unstructured answers that were likely to be honest and reliable, but at the same time did not manipulate any variables (situations) and used no control groups. This means that it cannot be concluded that any situations specifically cause the emotions shame, guilt or embarrassment, only that participants claimed to feel guilt etc. in these types of situations. It might be necessary to conduct further studies under more controlled conditions, for example by asking participants to perform specific events, then asking what emotions were experienced and to what degree. If the experiment were to be repeated using qualitative methods it would be advisable to gain responses from a far larger sample. The survey could also include some kind of grading system, e.g. how guilty did this make you feel, from 1-5? Responses could also be correlated against males and females separately, to establish differences in males and females. In conclusion, shame, guilt and embarrassment are not words for the same emotion, but contrasting feelings of self-consciousness, which arise from different situations and events. Shame and guilt emotions were considered similar, arising from personal failure or failing what is expected of a close relationship. Embarrassment arose from uncomfortable, unfamiliar public situations, which could not be controlled. Embarrassment tended to be a more trivial and transient emotion, but it also reflected a lapse in what is expected in social environments. These findings tell us something of the nature of social interaction; self-conscious emotions are brought about from a failure to comply with what is socially expected of a person, either in personal relationships or social situations. It would appear that these emotions are closely linked to the idea of conformity; people judge themselves based on what they assume others would think yet tend to be their own worst critic. AbstractAn experiment was carried out to investigate the effect of background noise that is either semantically related or unrelated to printed word lists, which must be learned in one minute then immediately recalled, in relation to Baddeley's phonological similarity effect study, (1966). Correctly recalled words from visual lists were counted after participants experienced either related spoken words over headphones, random, unrelated words or no noise at all during the study phase. It was found that participants showed significantly poorer short term memory after experiencing background noise that was related to the study lists than unrelated, and that both noise conditions resulted in significantly fewer words recalled than the control condition, showing that background speech impairs the learning process and that semantically related speech impairs learning and memory more than random speech. The results are discussed in terms of their implications in learning environments and in everyday conversation. Introduction. The study of human memory is of great importance in learning and education, as well as for disorders such as amnesia, and to generally understand the functions and processes of memory. For psychologists, the classification of memory into three categories: episodic (memory for events), semantic (factual memory) and procedural (memory for skills), is useful in distinguishing between different functions of memory. A huge amount of literature exists on short-term memory, possibly because it is easy to test, and often focuses on the performance of memory when subjected to background noise or distraction. Often noise such as traffic, types of music or volume is used, although there are a few studies which look at the effect of speech - both meaningful and irrelevant - as a distractor. This is valuable research when considering the relevance of noisy situations in real life, for example in the classroom or when trying to focus on one conversation. Hygge, Boman and Enmarker, (2001), compared the effects of road traffic noise and irrelevant speech, played through loudspeakers at 66dB, then tested recall of word lists and a comprehension task. Based on previous research on irrelevant speech in short term memory (LeCompte, 1994; Martin et al., 1998; Tremblay et al., 2000), the prediction was that speech would disrupt recall from episodic memory more than traffic noise, as irrelevant speech should tax limited resources for parallel semantic processing. They found a significant effect of noise overall on the recall task but no significant difference between traffic noise and random speech. Many studies have tested the effect of different types of speech on recall, but most focus on the effect of phonologically similar or dissimilar words on the number of items recalled, (Boyle, 1996). Results suggest that interference from background speech which is similar sounding disrupts working memory more than dissimilar sounding words. However very little research has been published testing the effects of semantically similar (related meaning) words as a distractor to memory. Baddeley (1966), found that only phonological difference in word pronunciation increased recall, whether or not word meanings were similar. However, Geraci and Franklin (2004), published a study entitled 'The influence of linguistic labels on source monitoring decisions', which showed that semantically related words, e.g. rabbit and bunny, lead to more confusion and errors in recall than phonetically related words or homographs. This experiment aims to test the effects of semantically similar words against random, unrelated words, when given as background noise, on the recall of printed word lists. It is predicted that some spoken words that are semantically similar to the printed words will be encoded along with the printed words and may displace the learned words from short-term memory store, therefore few printed words will be correctly remembered. When the spoken words are semantically unrelated it is expected that the noise will be a distractor, causing some memory impairment, but that words will not be displaced, therefore most words will be correctly recalled. In the control condition where no background noise is present memory should not be impaired and it is expected that participants will remember up to nine items, according to the seven plus or minus two rule, (Miller, 1956). Method. DesignThe experiment used a one-way within-subjects design. The independent variable was the type of background noise during the study phase and there were three levels: semantically related speech, semantically unrelated speech and no noise. The dependent variable was the number of correctly recalled words during the test phase. ParticipantsThe participants were 30 students from the University of , both male and female and between the ages of 18 and 40. Participants were asked to volunteer their time. MaterialsThe background noise used was a set of three MP3 files, each lasting one minute. The first and second contained lists of ten words (adjectives) repeated once, read by a male voice at a rate of one word per five seconds. The third was a recording made in a silent room. The tracks were played on a Sony digital MP3 player, through a set of headband lightweight headphones with the following specifications: 27mm diameter ferrite speakers, 20-20,000 Hz frequency and 32 ohms +/- 10% impedance. Volume was set at 60 dB. The printed word lists consisted of ten semantically similar adjectives of three to seven letters in each word list (A, B and C). An example sheet is given in appendix (i). Subjects were given plain A4 paper to write responses on in the test phase. A stopwatch was also used for timing the one minute study phase and the five minute test phase. ProcedureSubjects were asked, one at a time, to sit alone at a desk in a silent room and put on the headphones. Subjects were given the printed sheet containing all three word lists and asked to start learning the first list only, trying to ignore the sound from the headphones, while the appropriate sound file was played at the same time. After one minute, timed on a stopwatch, participants were asked to stop learning and turn the sheet over. The headphones were also removed. Subjects were then given a sheet of plain A4 paper and a pen and asked to immediately recall as many words from the printed list only as possible, in any order. After exactly five minutes, timed again, the paper was removed and the headphones put on again. The second file was then played at the same time as the subject turned over the sheet and started to learn the second word list. After one minute of the study phase the subject was tested as before. The third file was then played as the subject learned the third word list and was tested again. The subject was debriefed and thanked for their time before leaving. The design was counterbalanced so that each of the thirty participants looked at the word lists and corresponding files in a different order, for example some learned B then C then A, some C then A then B and so on. Results. The participants' scores were then used to calculate mean scores for the three conditions as displayed in table 1 and figure 1. These data were subjected to a repeated measures analysis of variance test which revealed that the means differed significantly, F(2,58) = 73.95, pDiscussion. An experiment was carried out to find out whether speech as background noise disrupts verbal processing more when it is semantically related to the task than when it is completely unrelated, as predicted by Geraci and Franklin (2004). This was done by measuring ability to learn words using short term memory whilst hearing either related words, unrelated words or no noise through headphones. Results show that noise served as a general distractor during the test phase and impaired memory significantly, however semantically related words over headphones also impaired the learning process significantly more than unrelated words. Table 1 shows that the mean score for the related condition was 1.4 words fewer than for unrelated words, whilst standard error for unrelated words was far lower than for related words. These results tell us that hearing background noise that is related to the task can interfere with the verbal processing which occurs when trying to learn a list of words. It is clear that the differences were not produced by interference during recall or storage, as no noise was experienced in the test phase, but that the speech impaired the encoding process of short term memory. When trying to explain these results it is necessary to look at the raw data (appendix iii) and the responses given. In terms of errors made the original responses showed that participants during the no noise condition were able to remember a mean number of 8.7 words, as predicted by the seven items plus or minus two rule, (Miller, 1956), and this can be seen from the raw data, as most participants recalled between seven and nine words. It was also noted during the study that most subjects during this condition finished recalling words before the five minutes allowed, that is they were able to access their short term memory quickly and easily. Interestingly, in the unrelated condition the same was noticed; most subjects recalled as many as they could before the five minute boundary and appeared to be able to access the words quickly. In the related condition, however, participants often used the entire five minutes to recall words slowly. Many of these subjects recalled up to fifteen words, but many of these were words from the audio list (over headphones). In the other conditions hardly any words were remembered from the auditory source. These findings suggest that the semantically related words confused the subject and made it harder to concentrate on the written list only. The recollection of non-printed words suggests that many audio words were encoded at the same time, displacing the written words from short term storage. The results can be explained using Baddeley's model of working memory, (1974), in particular the phonological articulatory loop. It is likely that participants used silent verbal rehearsal as a technique for learning the printed words, and that words heard over headphones that were related were confused with those being rehearsed silently and wrongly encoded. However at the same time the results contradict Baddeley's findings from a study of memory for word sequences as a function of acoustic, semantic and formal similarity, (1966), which found that semantic similarity did not impair memory. It can be said that this experiment is more specifically focused on semantic relationships than Baddeley's, ignoring phonological similarity, and therefore the results are more pronounced. It could also be seen, though, that the results are the product of an engineered situation and do not mirror other environments. It is possible that subjects were more confused by auditory information than usual because it was presented over headphones which block out other noise. In other situations continuing background noise is blocked out, but this may be more difficult when using headphones. Reassurance can be taken from the fact that the unrelated condition gave higher mean scores, therefore subjects were able to block out the noise to a greater extent. Other potential problems might be seen in the experimental design, for example in that only ten words were given to be learned in the study phase. It is possible that this is too few, meaning subjects learn words then are distracted by audio sounds. It would be beneficial to change one or more of the fixed variables to see whether the same results are gained. Overall the results proved significant and can be explained by current memory research, including theories of verbal rehearsal and short term memory displacement. The findings mean that the actual conditions during the encoding stage of memory are just as important if not more so than conditions during retrieval. This information has many implications in the fields of learning and memory. Obviously in the classroom adjustments can be made to optimise learning, for instance if a teacher talks about a topic whilst her pupils are trying to read or write about the same topic there may be confusion: the best conditions for learning are without background noise, so it is advisable for the teacher to talk first and allow the class to listen, then to let the class learn in silence or with unrelated background noise such as music or quiet conversation. In informative presentations and training the same can be said; if slides or handouts are being read whilst the speaker gives related information verbally, memory can be impaired. When trying to learn information it is better to focus on one source at a time than to try to absorb facts from a number of sources simultaneously. This can of course be tested using varied conditions and other senses. Replications of this study would be useful in order to clarify and extend these findings. Different sources of information such as visual images or numbers may give rise to different results; it cannot be said whether or not these findings are limited to words only. Additionally it would be more relevant to everyday situations to test memory whilst actual conversations or television recordings are played as background noise, or to test long term memory by delayed recall. Conditions could also be extended to include another language, white noise, music or timed beeps. This would clarify exactly what kind of noise is distracting to memory; all noise or just speech, all speech or just speech of a viable language. In conclusion, it has been found that semantically related speech presented as an auditory source can impair short term memory of word lists presented visually. These results support the predicted outcome and can be used as evidence for the working memory model and the phonological loop, (Baddeley, 1992), suggesting that working memory has limited resources for semantic processing. The findings may also have relevance within areas of learning and memory.",False
56,"IntroductionResearchers have argued that road crossing skills in children depend on attentional skills and ability to concentrate on a task during a distracting event. Dunbar, Lewis and Hill (2001) found that children showing greater attention switching skills were more aware of traffic when observed crossing roads and that maintaining attention throughout distractions might indicate a greater likelihood of crossing roads in a safe manner. Previous studies had used attention switching tasks such as reporting letters and numbers heard in either of two auditory streams, i.e. attending to the left or right ear (Pearson & Lane, 1991), to assess differences in time taken to move attention between the streams. It was found that younger children took longer and showed higher error rates when switching attention than older children, suggesting that attention skills improve with age. This study aims to test the idea that attentional skills improve with age, using the test of everyday attention for children, whereby children must inhibit their natural responses while a distracting event occurs. The study also aims to test earlier findings that primary school age boys have more road accidents than girls of the same age, by comparing attentional performance between the two gender groups. There are, therefore, three main hypotheses; firstly that responses in the task will be slower where a distracting event occurs than where there is no distraction, across all groups, secondly, that the response speed will improve with age in both distracting and quiet conditions, and thirdly, that boys will perform more slowly than girls during the distracting event. MethodParticipants96 children participated in total; 32 from each of three age groups: 4-5 year olds, 6-7 year olds and 8-9 year olds. Half of the children in each age group were boys, half were girls. MaterialsChildren were shown lists of numbers, either 1 or 2, in random order in a path. A television showing a cartoon was also used in the distracting condition. ProcedureEach child was taken into a quiet room and shown an array containing the numbers 1 and 2 in a random sequence. They were initially asked to read out the numbers as they appeared (i.e. 1 is 1, 2 is 2) for the baseline assessment. In the inhibition task each child was asked to read out a sequence, but saying ""2"" when they saw 1 and saying ""1"" when they saw 2. In the quiet condition the room was kept quiet throughout, but in the distracting condition the child was told they could watch a cartoon on the television when they had finished the task, but the cartoon was remotely switched on before the child finished, half way through the task. All children completed each of the three tasks, and their responses were timed in each condition. ResultsTwo difference scores were calculated for each child by subtracting i) the baseline response from the quiet condition response, then ii) the baseline response from the distraction condition response. The figures indicated the time taken to inhibit the salient response for each condition so could be compared with each other directly to give a measure of the difference in inhibition times. Poor performance, indicated by a larger inhibition time, would suggest poorer attentional skills. A clustered box plot revealed three outliers which could not be explained, two in the 4-5 years group (both girls) and one in the 6-7 years group (a boy), however, reanalysis with these excluded yielded the same pattern of results so the outliers were included in all analysis. Scatter plots of the mean difference scores against their standard deviations showed no obvious relationships, suggesting the homogeneity of variance assumption was not violated, therefore the data was not transformed but used in its raw form. The results were then analysed in a three way within groups analysis of variance (ANOVA), of sex (2) x age (3) x score (2), with difference score as the dependent variable. ANOVA outputs showed a significant main effect of distraction condition on inhibition times, F(1,90) = 83.465, p0.05, no significant interaction between age and sex, F(2,90) = 0.235, p>0.05 and no significant interaction between age and distraction condition, F(2,90) = 1.261, p>0.05. The interaction between distraction condition and sex was significant at the 5% level but not at the 1% level, therefore further research would be desirable in order to make firmer conclusions, F(1,90) = 4.562, pDiscussionResults show that all three hypotheses were supported by the data gathered. Analysis firstly showed a significant main effect of distraction condition, meaning that the inhibition times of children were significantly higher when a distracting cartoon was played than when the room was quiet throughout, suggesting that the distracting event did impair children's attention performance. More specifically, analysis confirmed that inhibition times of boys was significantly higher than that of girls in both distraction conditions, supporting the theory that boys were more distracted than girls by the cartoon and earlier findings that boys have poorer attention skills than girls of a similar age. Whilst initial ANOVA results gave no indication of age effects, a planned linear contrast over age was significant, suggesting that as age increases, attentional performance will increase so inhibition times decreased overall, this was particularly significant in the distraction condition (p=0.000 compared to p=0.017)) suggesting that age affects inhibition times more when a distracting event occurs, however there were no age-sex interactions found. Whilst an interaction effect between sex and distraction condition was observed, the significance level was not low enough to be completely confident (p=0.035) therefore further or repeated research would be advantageous to rule out the possibility of a type I error. Again, post hoc age group comparisons revealed a slightly significant difference between the 4-5 and the 8-9 age groups, but given that the probability level was 0.046 and the large number of comparisons made, a chance level of probability cannot be ruled out, therefore the result should be ignored until proven in further research. In general, mean scores were much as expected, with distraction scores higher than no distraction scores, and scores tending to decrease (improve) with age, however the mean distraction inhibition score for 8-9 year old boys was higher than expected (15.50) and was, in fact, the largest inhibition time found. Since there were no outliers in this group it is difficult to conclude why the times were this high, and a design problem can never be ruled out. Repeat experiments would be useful in determining any cause for this result. As expected though, the lowest inhibition times were generally found in the oldest girls tested, in both conditions. Possible problems in the design of the study could account for odd results, although no firm conclusions can be made. Although the within subjects design removes between subjects variance, here the within subjects design may have affected the recorded response times. Practice of the opposite worlds test will clearly increase ability, so that even with counter balancing, the more practised second condition may lead to outliers in some cases. Other problems include the type of distraction - in this case a children's cartoon. It might be that cartoons appeal more to boys than girls, leading to greater distraction, and that another sort of distraction, such as a friend entering the room or a conversation starting, may distract girls more. The choice may also be criticised for its irrelevance to the road-crossing scenario, where a friendly conversation might be more appropriate. It should be noted, however, that the test of everyday attention, which was used as the stimulus, has been investigated by psychology professionals and is thought to be both reliable and valid, with high test-retest reliability and significant correlation with other measures of attention, suggesting that results can be generalised to various measures of attention performance (Robertson, Ward, Ridgeway & Nimmo-Smith, 1996). As the findings here are in line with the hypotheses made and in accordance with previous research and theory in the field, it can be concluded that results of this study are both reliable and valid , with generalisability of findings should further research be conducted along these lines. It is strongly suggested that research in the area be continued, as there are many areas of attention and cognitive skills yet to be looked at in conjunction with road traffic awareness. The links are important to future training in road safety with children, and a thorough understanding of the cognitive and motor skills involved in road crossing will improve our ability to teach children affective road skills. Whilst this study does not in any way prove links between attention and traffic skills, it is still useful in addressing possible areas for further research and understanding children's attention levels. Results from this experiment could lead to better training for children and the possible implementation of video or computer games involving abstract tasks to train attention skills. These results also bring to light the vulnerability of younger children and of boys compared to girls. For both parents and those working with children, boys should be made especially aware of the importance of concentration at the roadside and of possible dangers. For younger primary age children, supervision should be stressed when near roads, and roads surrounding schools should be adapted to younger pedestrians, perhaps with warning signs, speed limits and pavement barriers.","Qualitative research is theoretically research that leads to dichotomous results, i.e. data values of zero or one; however in psychology qualitative findings are usually those not arrived at by statistical methods, (Grohol, 2005). Methods are usually naturalistic and involve observing behaviour without manipulating variables. Qualitative methods are often thus considered less scientific than quantitative methods, yet they play a vital role in understanding behaviour in its natural environment and are frequently used to develop hypotheses and theories, giving exploratory analysis which can aid quantitative experimental design. Due to the inductive nature of qualitative research, however, it can only be a useful medium when explored in sufficient detail; therefore validity and reliability are critical to its utility. Since qualitative research differs so greatly from quantitative study, researchers have developed non-quantitative criteria for assessing reliability and validity. For instance, Guba and Lincoln, (1994), proposed four criteria for judging the soundness of qualitative research, e.g. as repeated observations of the same events are impossible (reliability), qualitative methods must show dependability and the research must account for how changes in context produced changes in observation. Instead of internal validity, qualitative methods need credibility; to establish that results are credible from the participants' perspectives. In place of external validity the research should show transferability, i.e. that results can be generalised to other settings, and where objectivity cannot be proven, the results must show confirmability so that other researchers could corroborate results. While some argue that the criteria are still subjective rather than realist in nature, at least these alternatives try to establish some soundness to research and remind us of the importance of quality in all methods. In situations of naturalistic observations Neuman and Wiegand, (2000) suggested some quality assurance measures when taking field notes. Firstly to take notes immediately after any observations, without analysing, and to write quickly but in great detail, avoiding evaluative judgements or summaries. To ensure results are objective, counts should be made of key words, phrases or behaviour, records should be made of order or sequence of events and where possible maps or diagrams could be used to note specific locations or movements. This advice, however, is very limited in context and does not give general ideas of validity or reliability as required in other areas of research. As Silverman, (1997), suggested, reliability and validity take on different forms in different methods, but some general necessities apply to most studies. Validity must be ensured by allowing other researchers and the general public access to field notes, tape recordings, transcripts and materials relevant to the method. In general, Silverman argues that accurate tape or video recordings can be more reliable and valid than field notes, as the content can be repeated and analysed fully, detailed content is assured and importantly, the recordings can be reviewed by other researchers to ensure validity. There can be a number of problems with recordings though, where tapes can lose aspects of social interaction such as gestures. Even video can miss movements and temporal processes therefore should be set up very carefully and trialled to ensure quality. Silverman suggests longitudinal studies to gain perspective on behaviour patterns over time, ensuring technical quality of data and adequacy of transcripts, for example in detail and length. An example of a study using videotape analysis is Aldert Vrij's study on detecting deceit via analyses of verbal and nonverbal behaviour, (2004). Here subjects were videotaped and their behaviour and speech analysed from the tape. To increase validity of any conclusions made, 196 participants were used to take a 'mean' response. Lengths of responses were given and content was coded and analysed according to a set of pre-established criteria (CBCA scoring), for example specific words and phrases were looked for. In this way the results are free of bias and personal evaluations and the study could, in essence, be repeated. In terms of reliability of such studies, traditional tests can be used in some cases, for example test-retest reliability can be taken from the same questionnaire given to large samples on different occasions. A correlation coefficient can be calculated between the two sets of data and the results classed as reliable if giving over 0.8 correlation. Also increasing the sample size and variability of a study will increase reliability as mean values obtained are more accurate, (Hayes, 1997). Once results have led to conclusions it I important to validate these in any discussion, to improve the quality of the research. One concept is that of reflexivity, where the researcher reflects on and evaluates the method design and even personal perspective, (Merrick, 1993). This has been used in a study by Vigil, Hodges and Klee (2005) about parental language input. In the introduction and discussion the researchers examine their own theories and expectations, gender, age, culture, political beliefs and therefore possible bias. They then consider ways to counteract this and evaluate the possible input of such bias, an idea known as triangulation, (Merrick, 1993). It can be seen that with detailed and careful analysis of results and evaluation of possible weaknesses, quality research is possible and is not only useful nut necessary in gaining a deeper understanding of behaviour in its natural environment. Qualitative research can be influential in developing psychological theory but must be looked at with caution. When conducting qualitative research, as many measures as possible must be employed to ensure that conclusions are more than just inferences, and have a sound methodology behind them.",True
57,"Qualitative research is theoretically research that leads to dichotomous results, i.e. data values of zero or one; however in psychology qualitative findings are usually those not arrived at by statistical methods, (Grohol, 2005). Methods are usually naturalistic and involve observing behaviour without manipulating variables. Qualitative methods are often thus considered less scientific than quantitative methods, yet they play a vital role in understanding behaviour in its natural environment and are frequently used to develop hypotheses and theories, giving exploratory analysis which can aid quantitative experimental design. Due to the inductive nature of qualitative research, however, it can only be a useful medium when explored in sufficient detail; therefore validity and reliability are critical to its utility. Since qualitative research differs so greatly from quantitative study, researchers have developed non-quantitative criteria for assessing reliability and validity. For instance, Guba and Lincoln, (1994), proposed four criteria for judging the soundness of qualitative research, e.g. as repeated observations of the same events are impossible (reliability), qualitative methods must show dependability and the research must account for how changes in context produced changes in observation. Instead of internal validity, qualitative methods need credibility; to establish that results are credible from the participants' perspectives. In place of external validity the research should show transferability, i.e. that results can be generalised to other settings, and where objectivity cannot be proven, the results must show confirmability so that other researchers could corroborate results. While some argue that the criteria are still subjective rather than realist in nature, at least these alternatives try to establish some soundness to research and remind us of the importance of quality in all methods. In situations of naturalistic observations Neuman and Wiegand, (2000) suggested some quality assurance measures when taking field notes. Firstly to take notes immediately after any observations, without analysing, and to write quickly but in great detail, avoiding evaluative judgements or summaries. To ensure results are objective, counts should be made of key words, phrases or behaviour, records should be made of order or sequence of events and where possible maps or diagrams could be used to note specific locations or movements. This advice, however, is very limited in context and does not give general ideas of validity or reliability as required in other areas of research. As Silverman, (1997), suggested, reliability and validity take on different forms in different methods, but some general necessities apply to most studies. Validity must be ensured by allowing other researchers and the general public access to field notes, tape recordings, transcripts and materials relevant to the method. In general, Silverman argues that accurate tape or video recordings can be more reliable and valid than field notes, as the content can be repeated and analysed fully, detailed content is assured and importantly, the recordings can be reviewed by other researchers to ensure validity. There can be a number of problems with recordings though, where tapes can lose aspects of social interaction such as gestures. Even video can miss movements and temporal processes therefore should be set up very carefully and trialled to ensure quality. Silverman suggests longitudinal studies to gain perspective on behaviour patterns over time, ensuring technical quality of data and adequacy of transcripts, for example in detail and length. An example of a study using videotape analysis is Aldert Vrij's study on detecting deceit via analyses of verbal and nonverbal behaviour, (2004). Here subjects were videotaped and their behaviour and speech analysed from the tape. To increase validity of any conclusions made, 196 participants were used to take a 'mean' response. Lengths of responses were given and content was coded and analysed according to a set of pre-established criteria (CBCA scoring), for example specific words and phrases were looked for. In this way the results are free of bias and personal evaluations and the study could, in essence, be repeated. In terms of reliability of such studies, traditional tests can be used in some cases, for example test-retest reliability can be taken from the same questionnaire given to large samples on different occasions. A correlation coefficient can be calculated between the two sets of data and the results classed as reliable if giving over 0.8 correlation. Also increasing the sample size and variability of a study will increase reliability as mean values obtained are more accurate, (Hayes, 1997). Once results have led to conclusions it I important to validate these in any discussion, to improve the quality of the research. One concept is that of reflexivity, where the researcher reflects on and evaluates the method design and even personal perspective, (Merrick, 1993). This has been used in a study by Vigil, Hodges and Klee (2005) about parental language input. In the introduction and discussion the researchers examine their own theories and expectations, gender, age, culture, political beliefs and therefore possible bias. They then consider ways to counteract this and evaluate the possible input of such bias, an idea known as triangulation, (Merrick, 1993). It can be seen that with detailed and careful analysis of results and evaluation of possible weaknesses, quality research is possible and is not only useful nut necessary in gaining a deeper understanding of behaviour in its natural environment. Qualitative research can be influential in developing psychological theory but must be looked at with caution. When conducting qualitative research, as many measures as possible must be employed to ensure that conclusions are more than just inferences, and have a sound methodology behind them.","IntroductionResearchers have argued that road crossing skills in children depend on attentional skills and ability to concentrate on a task during a distracting event. Dunbar, Lewis and Hill (2001) found that children showing greater attention switching skills were more aware of traffic when observed crossing roads and that maintaining attention throughout distractions might indicate a greater likelihood of crossing roads in a safe manner. Previous studies had used attention switching tasks such as reporting letters and numbers heard in either of two auditory streams, i.e. attending to the left or right ear (Pearson & Lane, 1991), to assess differences in time taken to move attention between the streams. It was found that younger children took longer and showed higher error rates when switching attention than older children, suggesting that attention skills improve with age. This study aims to test the idea that attentional skills improve with age, using the test of everyday attention for children, whereby children must inhibit their natural responses while a distracting event occurs. The study also aims to test earlier findings that primary school age boys have more road accidents than girls of the same age, by comparing attentional performance between the two gender groups. There are, therefore, three main hypotheses; firstly that responses in the task will be slower where a distracting event occurs than where there is no distraction, across all groups, secondly, that the response speed will improve with age in both distracting and quiet conditions, and thirdly, that boys will perform more slowly than girls during the distracting event. MethodParticipants96 children participated in total; 32 from each of three age groups: 4-5 year olds, 6-7 year olds and 8-9 year olds. Half of the children in each age group were boys, half were girls. MaterialsChildren were shown lists of numbers, either 1 or 2, in random order in a path. A television showing a cartoon was also used in the distracting condition. ProcedureEach child was taken into a quiet room and shown an array containing the numbers 1 and 2 in a random sequence. They were initially asked to read out the numbers as they appeared (i.e. 1 is 1, 2 is 2) for the baseline assessment. In the inhibition task each child was asked to read out a sequence, but saying ""2"" when they saw 1 and saying ""1"" when they saw 2. In the quiet condition the room was kept quiet throughout, but in the distracting condition the child was told they could watch a cartoon on the television when they had finished the task, but the cartoon was remotely switched on before the child finished, half way through the task. All children completed each of the three tasks, and their responses were timed in each condition. ResultsTwo difference scores were calculated for each child by subtracting i) the baseline response from the quiet condition response, then ii) the baseline response from the distraction condition response. The figures indicated the time taken to inhibit the salient response for each condition so could be compared with each other directly to give a measure of the difference in inhibition times. Poor performance, indicated by a larger inhibition time, would suggest poorer attentional skills. A clustered box plot revealed three outliers which could not be explained, two in the 4-5 years group (both girls) and one in the 6-7 years group (a boy), however, reanalysis with these excluded yielded the same pattern of results so the outliers were included in all analysis. Scatter plots of the mean difference scores against their standard deviations showed no obvious relationships, suggesting the homogeneity of variance assumption was not violated, therefore the data was not transformed but used in its raw form. The results were then analysed in a three way within groups analysis of variance (ANOVA), of sex (2) x age (3) x score (2), with difference score as the dependent variable. ANOVA outputs showed a significant main effect of distraction condition on inhibition times, F(1,90) = 83.465, p0.05, no significant interaction between age and sex, F(2,90) = 0.235, p>0.05 and no significant interaction between age and distraction condition, F(2,90) = 1.261, p>0.05. The interaction between distraction condition and sex was significant at the 5% level but not at the 1% level, therefore further research would be desirable in order to make firmer conclusions, F(1,90) = 4.562, pDiscussionResults show that all three hypotheses were supported by the data gathered. Analysis firstly showed a significant main effect of distraction condition, meaning that the inhibition times of children were significantly higher when a distracting cartoon was played than when the room was quiet throughout, suggesting that the distracting event did impair children's attention performance. More specifically, analysis confirmed that inhibition times of boys was significantly higher than that of girls in both distraction conditions, supporting the theory that boys were more distracted than girls by the cartoon and earlier findings that boys have poorer attention skills than girls of a similar age. Whilst initial ANOVA results gave no indication of age effects, a planned linear contrast over age was significant, suggesting that as age increases, attentional performance will increase so inhibition times decreased overall, this was particularly significant in the distraction condition (p=0.000 compared to p=0.017)) suggesting that age affects inhibition times more when a distracting event occurs, however there were no age-sex interactions found. Whilst an interaction effect between sex and distraction condition was observed, the significance level was not low enough to be completely confident (p=0.035) therefore further or repeated research would be advantageous to rule out the possibility of a type I error. Again, post hoc age group comparisons revealed a slightly significant difference between the 4-5 and the 8-9 age groups, but given that the probability level was 0.046 and the large number of comparisons made, a chance level of probability cannot be ruled out, therefore the result should be ignored until proven in further research. In general, mean scores were much as expected, with distraction scores higher than no distraction scores, and scores tending to decrease (improve) with age, however the mean distraction inhibition score for 8-9 year old boys was higher than expected (15.50) and was, in fact, the largest inhibition time found. Since there were no outliers in this group it is difficult to conclude why the times were this high, and a design problem can never be ruled out. Repeat experiments would be useful in determining any cause for this result. As expected though, the lowest inhibition times were generally found in the oldest girls tested, in both conditions. Possible problems in the design of the study could account for odd results, although no firm conclusions can be made. Although the within subjects design removes between subjects variance, here the within subjects design may have affected the recorded response times. Practice of the opposite worlds test will clearly increase ability, so that even with counter balancing, the more practised second condition may lead to outliers in some cases. Other problems include the type of distraction - in this case a children's cartoon. It might be that cartoons appeal more to boys than girls, leading to greater distraction, and that another sort of distraction, such as a friend entering the room or a conversation starting, may distract girls more. The choice may also be criticised for its irrelevance to the road-crossing scenario, where a friendly conversation might be more appropriate. It should be noted, however, that the test of everyday attention, which was used as the stimulus, has been investigated by psychology professionals and is thought to be both reliable and valid, with high test-retest reliability and significant correlation with other measures of attention, suggesting that results can be generalised to various measures of attention performance (Robertson, Ward, Ridgeway & Nimmo-Smith, 1996). As the findings here are in line with the hypotheses made and in accordance with previous research and theory in the field, it can be concluded that results of this study are both reliable and valid , with generalisability of findings should further research be conducted along these lines. It is strongly suggested that research in the area be continued, as there are many areas of attention and cognitive skills yet to be looked at in conjunction with road traffic awareness. The links are important to future training in road safety with children, and a thorough understanding of the cognitive and motor skills involved in road crossing will improve our ability to teach children affective road skills. Whilst this study does not in any way prove links between attention and traffic skills, it is still useful in addressing possible areas for further research and understanding children's attention levels. Results from this experiment could lead to better training for children and the possible implementation of video or computer games involving abstract tasks to train attention skills. These results also bring to light the vulnerability of younger children and of boys compared to girls. For both parents and those working with children, boys should be made especially aware of the importance of concentration at the roadside and of possible dangers. For younger primary age children, supervision should be stressed when near roads, and roads surrounding schools should be adapted to younger pedestrians, perhaps with warning signs, speed limits and pavement barriers.",False
58,"Steven Katz claims in his essay on the historical perspective of the Holocaust that 'the intentional murder of European Jewry during World War II, is historically and phenomenologically unique', and it is possible to forget that despite the notoriety of the Jewish Question there have been other massacres during the twentieth century. Annihilations have occurred in many countries throughout the century including Turkey, Rwanda, China, Bosnia and Cambodia, and whilst there are significant differences between each one- such as method of slaughter, death toll and time span of persecution- there are some factors common to them all. In fact, the Holocaust was not even the first great massacre of the twentieth century as the Armenian genocide occurred during the First World War, and Melson argues that it is 'a more accurate archetype than is the Holocaust for current mass murder in the postcolonial third world and in the contemporary post-communist world'. However this does not exclude the Holocaust from sharing common characteristics with other massacres (as Katz disputes) for, on a fundamental level, patterns can be drawn between them. For instance they all arise from revolutionary or wartime climates, and also the motivation for their persecution was due to threat felt by the perpetrating parties. Therefore the Holocaust is not entirely unique, and neither are the other massacres of the twentieth century despite their geographical and statistical differences. Rosenbaum, Alan (ed), Is the Holocaust Unique? Perspectives on comparative Genocide, (Oxford, 1996), p 19 Ibid, p 97 First of all, persecution of minority groups is a trait of more than one massacre. For instance in Turkey during World War One genocide occurred against the non-Muslim (Armenian) communities living within the Ottoman Empire, the Holocaust was the persecution of Jews within Christian Europe, and the Bosnian massacre of the 1990s was carried out against all those upon Bosnian territory who could be labelled as 'Turks'. The Cambodian massacre of the 1970s also involved the persecution of minority religious groups such as Buddhists, and national communities such as the Thai and Chinese, however this differs slightly as it was less focused upon one group, and more centred upon merely eradicating those not suited to the oppressive restrictions of the Pol Pot regime. Even within this minority group persecution there are further similarities. As Melson notes, the Armenian genocide and the Holocaust are alike in that the 'Armenians and Jews were ethno-religious minorities of inferior status that had experienced rapid social progress and mobilisation in the nineteenth century' and such progress had intimidated the ruling governments sufficiently to lead to them ordering their destruction. He continues to note that 'the Armenians challenged the traditional hierarchy of Ottoman society as they became better educated, wealthier and more urban' and the Nazis also feared the flourishing success of Jewish communities during the 1930s, however the Holocaust added another dimension to minority persecution as a 'broader, biologically-driven aim of extermination' existed due to Nazi views that Jews were genetically inferior. The concept of social mobility of minority groups raises the question of threat, a common motivator of the perpetrating governments towards massacre. Just as the Nazis were threatened by the Jewish success, and the Ottomans by the Armenians, the Bosnians were threatened by the rise of Serbian nationalism that resulted in the fall of Yugoslavia, and the subsequent threat posed by the Serbs and Croats demanding land. As Melson notes, this threat of attack led the Bosnian Serbs- under their leader Radovan Karadzic- to practice 'massacre, ethnic cleansing, and cultural destruction against those they called the ""Turks""'. In all three cases, even if there were other motivating factors, the perpetrators acted partly from fear. Rosenbaum, Is the Holocaust Unique?, p 90 Burleigh, Michael (ed), Confronting the Nazi Past: New Debates on Modern German History, (London, 1996), p 77 Rosenbaum, Is the Holocaust Unique? , p 95 The case of Cambodian genocide raises the example of massacre on a much broader, more national scale, and this form of massacre can be found elsewhere in the twentieth century- most notably in Nanking, China, during World War Two. As Callum Macdonald notes, in December 1937 'Japanese troops embarked on an orgy of murder, rape, arson and looting which lasted over six weeks' and rather than being directed at one particular ethnic or religious group it was an attack upon the Chinese nation. Similarly, in Cambodia whereas Pol Pot appeared to be attacking minorities, the massacre as a whole was a general ambush upon those that did not conform to the ideals of the communist party. This involved the slaughter of nearly 3000 Buddhists, 215,000 Chinese, 12000 Thai, and 'the Vietnamese community...was entirely eradicated'. The Muslim Chams were also targeted for, like the cases of Armenia and the Holocaust, 'their distinct religion, language and culture, large villages and autonomous networks threatened the atomized, closely supervised society that the Pol Pot leadership planned'. However this threat extended to all the aforementioned groups within Cambodia and became more of a national assault like the Japanese rape of Nanking. The only significant point that establishes Nanking as different in motive, though, is that it was an inter-national hostility that led to massacre (like in Bosnia). In Cambodia, Armenia and the Holocaust, massacre was derived from the respective government initiatives to impress their regime and ideals upon their country. Levene, Mark and Roberts, Penny (eds), The Massacre in History, (Oxford, 1999), p 223 Andreopoulos, Genocide: Conceptual and Historical Dimensions, (Pennsylvania, 1994), p 198 Ibid, p 199 Another common factor between many massacres of the twentieth century concerns the conditions under which it was possible for massacre to occur. In Armenia, the Holocaust, Cambodia and Bosnia, annihilations took place as a result of the instability caused by revolution. The revolutions of the Ottoman Empire led to the rise of Pan-Turkism, where the Turkish ethnic group was identified 'as the authentic political community on which the Turkish state could and should rely' and this led to the exclusion of Armenians from Turkey. In Germany, the collapse of the Weimar Republic 'allowed the Nazis to recast German identity and ideology' and their subsequent control of government apparatus meant they could alter social and racial policy to infiltrate anti-Semitic ideas. In Cambodia, the rise of communism resulting from political civil war between the first independence leader Sihanouk and his successor Lon Nol, and also from American intervention from the Vietnam war, allowed Pol Pot to seize control and cut Cambodia off from the rest of the world as well as killing 1.5 million of its eight million population. Similarly, in Bosnia massacre occurred due to the chaos of Serbian nationalism causing the collapse of Yugoslavia and birth of Croatia, Serbia and Bosnia as independent countries and their subsequent territorial disagreements. In all four cases revolution created an instability that allowed for the rise of new ideas and thus new prejudices. Rosenbaum, Is the Holocaust Unique? , p 90 Ibid. As well as revolution, War also plays a big responsibility in the viability of many twentieth century massacres. As Dadrian notes 'it is no coincidence that the two major genocides of this century (Armenian and Holocaust) were consummated in the vortex of two global wars' as there is a somewhat opportunistic element to the conditions under which massacre occurred. The onslaught of war allowed 'emergency powers' to be put in place- by Nazis and Turk Ittihadists- to increase security forces, and also the greater role of the militia in both cases allowed the mobilisation and mass murder of the persecuted. Both the Jews and the Armenians were 'considered as legitimate targets for destruction' by being levelled as internal enemies, thus creating some form of rationale within the country and also perpetrating massacre as an effective extension of the war already occurring. The presence of war helped to cover both massacres from other nations and therefore remove any opposition to the annihilations. As Wyman notes, in the case of the Holocaust 'seventeen months of systematic, cold-blooded murder ran their course between the time the Einsatzgruppen were turned loose on the Russian front, in June 1941, and the day in late November 1942 when the extermination plan was confirmed to the world' and it was only because of world warfare that this was possible. Perhaps the most prevalent example of this is the Japanese massacre in Nanking, for it was only because war erupted that it occurred- an opportunistic attack upon another nation rather than a chance to fulfil ethno-religious prejudice under the guise of warfare. The Japanese invaded China in the summer of 1937, and it was only after the fall of Shanghai in August that 'the mood of imperial troops...turned ugly, and many, it was said, lusted for revenge as they marched toward Nanking'. Chang accuses warfare as motivating civilians to rationalise massacre as acceptable, saying 'the rape of Nanking should be perceived as a cautionary tale- an illustration of how easily human beings can be encourages to allow their teenagers to be moulded into efficient killing machines able to suppress their better natures'. War was also used as a direct excuse for the massacre of Cambodia, for the Pol Pot regime gained favour as a result of American bombing of rural areas between 1970-73. The US army had intervened in the Vietnam war and 'soon encroached upon Cambodia's territory and integrity' and in response the Khmer Rouge, Pol Pot's forces, 'used the bombings devastation and massacre of civilians as recruitment propaganda' for their party. Similarly, in Bosnia ethnic massacre occurred under the post-independence territory wars, where Serbia desired greater Croatian territory and Bosnian Serbs wished to prevent Serb and Croat nationalists from encroaching on Bosnian territory. In all these instances, war provided both an opportunity to rationalise massacre and a veil to conceal it being perpetrated. Rosenbaum, Is the Holocaust Unique? , p 118 Ibid, p 120 Wyman, David, Abandonment of the Jews: America and the Holocaust, (New York, 1984), p 58 Chang, Iris, The Rape of Nanking: The forgotten Holocaust of World War II, (London, 1997), p 34 Ibid, p 220 Andreopoulos, Genocide, p 194 Ibid. Within each massacre there are smaller parallels between other annihilations of the twentieth century. For instance both the Armenian and Bosnian genocides involved making claims upon land, and the minority groups becoming deposed in order for the ruling parties to ''cleanse' the area of their presence, and destroy their culture'. The Armenian genocide also shares a similar approach of justification with the Nanking massacre in that both parties felt the 'need to subhumanize the intended victim'. However the Turks achieved this through stressing their actions as increasing their own mobility rather than oppressing the Armenians, whereas in Nanking it was left to the soldiers' imaginations. Chang notes how their 'automatic impulse was to dehumanise the prisoners by comparing them to insects and animals'. The need to surrealise such atrocious actions is common to all massacres as it takes subhuman mental strength to kill another person, but in the cases of Turkey and Nanking this need seems more overtly prevalent than in some other cases. Rosenbaum, Is the Holocaust Unique? , p 96 Andreopoulos, Genocide, p 10 Chang, The Rape of Nanking, p 44 Therefore, overall, whereas each twentieth century massacre occurred in very different manifestations, at different times, and for varying periods of time with differing scales of death, they do share some fundamental characteristics. In the cases of the Armenians, the Holocaust, Bosnia and Cambodia, all massacres involve the persecution of minority groups threatening the government, and the annihilations themselves were carried out in an unstable post-revolutionary climate. In all five examples warfare made massacre possible, and this is a crucial common factor between the countries as the ability to perpetrate such large-scale murders was only possible under the veil of warfare. Also, many countries felt the need to dehumanize victims in order to commit massacres and therefore not only were there similarities in the characteristics of their causation, but their methods of coping with atrocity. Melson's argument of the Armenian genocide as 'a more accurate archetype' than the Holocaust appears true as such persecution provides a base example for comparison with subsequent massacre, however the Holocaust- and the other examples mentioned- whilst seeming unique also form a pattern of traits applicable to massacre throughout the twentieth century. Rosenbaum, Is the Holocaust Unique? , p 88 Lombroso believed that degenerates, more specifically criminals, possessed 'numerous anomalies in the face, skeleton and various psychic and sensitive functions' that were indicative of their immoral behaviour. These physical characteristics made them identifiable as prone to criminality and, on a more general level, subordinate to the aesthetically 'normal' members of society. Moreover, such characteristics could be inherited and therefore people could be born as criminals. In 'The Portrait of Dorian Gray', Wilde uses Lombroso's concept of degeneration and physicality to establish both character description and the demise of Dorian Gray through his painting. Lombroso, Cesare, Criminal Man according to the classification of Cesare Lombroso, (London, 1911), p5 Henry Wooton comments in the novel that 'crime belongs exclusively to the lower orders' and Wilde uses Lombroso's theory to illustrate this within his description of the lower classes that Dorian encounters through both the theatre and the opium den. In particular the theatre owner is described as 'a hideous jew' with 'greasy ringlets' and 'beaming from ear to ear with an oily, tremulous smile'. Similarly, the other characters involved in the play are described in an equally derogatory manner- the orchestral conductor as 'a young hebrew', Romeo as 'a stout elderly gentleman, with corked eyebrows...and a figure like a beer barrell' and Romeo and Mercutio together as 'both as grotesque as the scenery'. Dorian Gray in fact comments on the theatregoers as being 'common, rough people with their coarse faces'. The concept of a 'coarse face' is particularly relevant to Lombroso as he believed that degenerates had identifiable physical features due to the atavistic nature of their faces and skeletons. He notes criminals in particular as exhibiting a prominent jaw, long arms and large ears- all somewhat animalistic and Neanderthal qualities. Wilde uses the idea of biologically backward appearance to enhance his depiction of the lower classes. Wilde, Oscar, The Portrait of Dorian Gray, (London, 2003), p266 Ibid, p66 Ibid. Ibid, p105 Ibid, p68 Wilde, Portrait of Dorian Gray, p68 In terms of Dorian Gray himself, Wilde uses Lombroso's theory as a vehicle to display his deterioration through the portrait. Although this does not concur with Lombroso's belief that criminality was hereditary, it does exploit the concept of immorality having physical effects- even if it is in retrospect and evident only on the portrait (as Gray has bound himself to a lifetime of youthfulness). For instance, Gray is described as observing 'its beautiful marred face' and that 'he would examine with minute care...the hideous lines that seared the wrinkling forehead, or crawled around around the heavy sensual mouth, wondering sometimes which were the more horrible, the signs of sin or the signs of age'. Gray also comments that 'if a wretched man has a vice, it shows itself in the lines of his mouth, the droop of his eyelids'- Lombroso's theory exactly. Therefore, Wilde uses Lombrosian theory as a basis for the physical description of many characters, but in Dorian Gray himself manipulates the concept to turn it into a literary vehicle with which to sustain the reader's attention. Lombroso believed indicative physical features were inherited and present from birth; Wilde depicted them as appearing and mutating as the soul of Dorian Gray began to degenerate. Ibid, p118 Ibid, p162 Ibid, p279 There are other features of Lombroso's theory on criminality that Wilde has manipulated in The Portrait of Dorian Gray. For instance, Lombroso commented on the often cold nature of criminals towards their kin as 'their moral sense is sterile because it is suffocated by passions and the deadening force of habit'. Whilst Dorian Gray's family are only revealed through portraits that he studies, he seems to blame them for his condition- not only displaying some hostility towards them but making an acknowledgement of hereditary degeneration. For example, he says of Philip Herbert's portrait: 'Had some strange poisonous germ crept from body to body till it had reached his own? ', and also that 'he knew what he had got' from the 'wine-dashed' lips of his mother. 'Wine-dashed' also implies that his mother was an alcoholic, another disease associated with the concept of degeneration. All his observations imply a sense of pre-disposition akin to the thoughts of Lombroso, and resentment towards such inheritance concurring with his beliefs as well. In terms of moral sterility Dorian Gray shows no remorse for his role in Sybil Vane's suicide, to which Basil comments 'I don't know what has come over you. You talk as if you had no heart, no pity in you'. Also, not only does he murder Basil but deals with his death in a methodical manner- summoning Alan Campbell to dispose of the body, and systematically burning all remaining evidence- and describes after the event that 'he felt strangely calm'. Equally when Campbell subsequently commits suicide Gray accepts no responsibility for causing another death. It is only when Gray eventually admits to himself the devastating effects he appears to have on all that he meets that he begins to be troubled by his actions, breaking with Lombrosian theory. Lombroso, Criminal Man, p24 Wilde, Portrait of Dorian Gray, p175 Ibid, p176 Ibid, p138 Ibid, p200 Lombroso also believed that besides hostility towards kin, criminals also were especially friendly towards strangers and felt an affinity with animals. Throughout the novel Dorian Gray proves popular with the other characters, courting the affection of various females (most prominently Sybil Vane and the Duchess, but Gray comments on numerous occasions that he has experienced much female attention throughout his life) and Basil, and also in striking an immediate friendship with Henry Wooton. When Dorian Gray goes hunting with Sir Geoffrey Clouston, he begs him not to shoot a rabbit because 'there was something in the animal's grace of movement that strangely charmed Dorian Gray'. Therefore Wilde even weaves some of the less prominent Lombrosian characteristics of the born criminal into his characterisation of Dorian Gray. Wilde , Portrait of Dorian Gray, p252 However, when considered on the whole, Lombroso only appears to influence Wilde on a foundational level as Dorian Gray himself seems to be equally swayed by external influences. His whole demise is derived though a desire to stay young, mostly courted by the compliments he receives regarding his beauty and comments such as Henry Wooton's 'beauty is a form of genius- is higher, indeed, than genius, as it needs no explanation'. Wooton also influences Gray by lending him a book. This idea of environmentally influenced degeneration is closer to Nordauism than Lombrosian theory. Although Wilde published Portrait of Dorian Gray in1890, two years before Nordau wrote 'Degeneration', his theories were very much in development and the concept of degeneration was a highly discussed contemporary topic. Wilde also uses external influences for a literary purpose- to heighten the demise of Dorian Gray, and also to create a sense of empathy with the character. With Lombroso's theory there is little sympathy for the criminal as they are depicted as some form of physical monstrosity, inherent on committing illegal and immoral acts due to their heredity, but in Gray's final act of slashing the portrait and being discovered as a 'withered, wrinkled and loathsome' looking corpse the reader is able to understand the futility of Dorian Gray's situation and blame his acquaintances and society for his demise just as much (perhaps) as the actions of Gray himself. Ibid, p34 Wilde, Portrait of Dorian Gray, p279 Therefore, 'The Picture of Dorian Gray' is influenced fundamentally by Lombroso. Wilde uses the concept of physicality in his descriptions, and also forms Gray's personality according to many of the characteristics of the born criminal. However, he also draws upon contemporary developing theories of environmentally induced degeneration to seal the fate of the protagonist. Overall, abiding only by the principles of Lombrosian theory it would be difficult to create a character with both any sense of remorse and any empathy within the reader, and both of these are necessary to maintain interest in and affinity with the novel. It would also prevent any final sense of demise, climax or conclusion to the novel. Wilde, subsequently, uses Lombroso's beliefs about criminality and degeneration as a basic literary vehicle from which to develop his novel- after all, his aim was not to create a scientific document but an enthralling tale of the deterioration of a man as a result of his whimsical desire for eternal youthfulness.","The title of David Landes' seminal 1998 work 'The Wealth and Poverty of Nations: Why some are so rich and some so poor' encapsulates the prevalent question among modern society of why there is such a great disparity of wealth between countries. With the benefit of hindsight, events such as the Industrial Revolution have contributed to a Eurocentric opinion of western superiority, at least until the development of modern North America. However, in the year 1300 this was not the case as in actual fact East Asia possessed a more advanced society than anywhere in the West. During the early modern period, Asian technological and agricultural innovations were envied by Europe, for at this point Europe was no more likely than East Asia to experience an industrialisation process. The economic divergence between East and West has been greatly researched, with historians providing explanations ranging from inherent climatic differences and oppressive state politics to trade and the advantages of European territorial expansion in order to explain East Asia's lack of economic progress compared to Europe. However, many of these claims are rather over-enthusiastic in their emphasis. Whilst European climate was more suitable to agricultural productivity, Asia's technological advantage counteracted this through the creation of irrigation systems. In fact, the West adopted several of their inventions. The first two sections of this essay will focus upon demonstrating that these two factors can be encompassed- like many other explanations- within the greatest reason for slow Asian economic progress: attitude. The comparative superiority of East Asia in 1300 meant that competitive spirit was not fostered in the same manner as Europe, and that attempts to regulate their admirable systems enforced an introverted society based on centralised state control, Sino centric governmental policies and hierarchy. The final section portrays how the effects of this attitude accumulated across most facets of East Asian society creating a rather stagnant economy, not conducive to the burgeoning trade and emerging global networks that Europe was able to exploit. For the purposes of this essay, at times East Asia shall be abbreviated to 'East', and Europe shall be substituted for 'West'. i- Environmental DifferencesDavid Landes claims that Europe's emerging economic superiority was due to its climate: the temperate weather, the even supply of rainfall all year round, and freedom from extreme natural phenomena such as flooding made the continent conducive to agricultural and commercial success. He notes that 'tropical areas generally average enough rainfall, but the timing is often irregular and unpredictable, the downpours anything but gentle' and therefore in terms of producing food for personal consumption and profit, Europe was in a greater position. Whilst this is a sensible observation, Landes extends his discourse to claim that the European climate was not merely the finest in practical terms, but that it was the most invigorating and therefore productive environment in the world, quoting the comments of a Bangladeshi diplomat: David Landes, The Wealth and Poverty of Nations: Why some are so rich and some so poor, (London, 1998), p. 13 'In countries like India, Pakistan, Indonesia, Nigeria and Ghana I have always felt enervated by the slightest physical or mental exertion, whereas in the UK, France, Germany or the US I have felt reinforced and stimulated by the temperate climate'David Landes, The Wealth and Poverty of Nations, p. 15 The moderate environment of Europe may have been beneficial in terms of agricultural production, but it is somewhat over-zealous to claim that the inherent climatic differences between the East and West were responsible for their divergence. Steady annual rainfall aided European agriculture, but the rice-oriented Asian societies developed canals and irrigation systems to counteract their natural disadvantage, creating a more technologically advanced society (which will be discussed in greater detail later). The Chinese invented the chain pump in the first century AD, which allowed water to be transferred from lower to higher ground, permitting the irrigation of crops, as picture one shows, and increasing their yield. These devices meant that, as E.L Jones notes, 'by the thirteenth century China...(had) probably the most sophisticated agriculture in the world'. This superiority was necessary though as unlike Europe, which had a variety of staple dietary products (predominantly wheat, but maize and potatoes from the New World after 1492), Asia depended on rice as its major source of nutrients. Landes claims that this was another hindrance, as rice has a lower nutritional content than other staples. The main disadvantages concerning rice were, though, its land-intensive and labour-intensive cultivation requirements. As C.A. Bayly notes, China and South East Asia 'required large efforts by local communities to maintain the irrigation systems which watered the crops'. Irrigation problems were two-fold; firstly, the wet harvesting of rice subjected workers to parasites such as schitosomes, which cause snail fever. This problem was especially apparent in the stagnant water of canals, such as the Chinese Grand Canal built in 605 AD. Secondly, there were political ramifications. In South East Asia Muslim Emperors were responsible for the maintenance of canal systems, and in China the Emperors managed the irrigation system of the Yellow River and controlled the Grand Canal. State control of the water systems caused (as Karl Wittfogel terms it) a ""hydraulic society"". The necessary waterways were managed by the government, which led to a centralisation of power. Ruler control of canals in Asia granted the state ultimate authority regarding rice cultivation and distribution and therefore workers were restricted in the amount of enterprise they could exercise. By 1800 the Grand Canal and Yellow River dyke systems were all in a state of decay, and there was little that farmers could do to improve the situation; the state considered irrigation a necessity in terms of subsistence, not a vehicle for entrepreneurship and commercialisation. E.L.Jones as cited in Landes, The Wealth and Poverty of Nations, p. 26 C.A. Bayly, The Birth of the Modern World 1780-1914: Global Connections and Comparisons, (Oxford, 2004), p. 28 State attitude towards irrigation was also due to ecological constraints in East Asia. Demographic changes were of much greater concern in the East than Europe, as it was experiencing a period of rapid population growth- in 1750 Japan had a population of approximately twenty eight million, and the Chinese population grew from approximately 100-150 million in 1650, to 200-250 million by 1750. This increase created a rapidly escalating need for food, therefore the states' primary concern was provision rather than profit. Demographic growth was also somewhat self-perpetuating; increased demand for food necessitated extra manpower for cultivation. This created a culture in which early marriage and many children were encouraged, which led to yet more food and therefore manpower being needed. As P.H.H. Vries notes 'China's internal transport with its densely knit system of waterways probably was the most efficient in the world', but this was because it had to be in order to provide subsistence for a large country. Efficiency was essential, and therefore demography hampered the East Asia's chances of large-scale commercialisation and industrialisation more than the environment itself. Landes emphasises climatic differences too fervently, for it was resulting ecological constraints the state's response and that hindered economic progress. As the next section explains, East Asia was able to overcome natural disadvantage through technology; however, their success was brief, for technical superiority highlighted problems similar to those posed by state control over water. Landes, The Wealth and Poverty of Nations, p. 23 P.H.H. Vries, 'Are Coal and Colonies Really Crucial? Kenneth Pomeranz and the Great Divergence', Journal of World History, 12 (2001), p. 415 ii- TechnologyTechnology is a representative case of how the potential for development in East Asia was not wholly fulfilled, whereas Europe learned from Asian inventions and progressed ahead of them. The East experienced many technological developments, such as the aforementioned irrigation system, prior to Europe and by the early modern period they possessed a level of superiority. For instance, China explored rudimentary time measurement with water clocks during the Song period (960-1279). Also, agriculture was advanced not only through hydraulic engineering such as ditches and dykes, but through the development of an iron plough during the sixth century, which could adjust the depth of the furrow. Asia, in fact, was responsible for some of the most fundamental inventions in early history: the compass (960 AD), gunpowder (tenth century), paper (100 AD) and printing. As Joel Mokyr notes 'by the end of the middle ages, it appears that China was about ready to undergo a process eerily similar to the great British Industrial Revolution'. However, this prophetic stage of development was not fulfilled in the manner that European technological development succeeded. Joel Mokyr, The Lever of Riches: Technological creativity and Economic Progress, (Oxford, 1990), pp. 212-213 The main problems with Eastern technological development were that either significant inventions were forgotten, or other advancements were not enhanced to their full potential. In the first instance, time measurement is an excellent example. Achievements such as Su Sung's forty-foot high clock, produced in 1086, had faded by the sixteenth century. Nothing like the European weight-driven clocks were developed, and in the seventeenth century the Jesuits were able to use the novelty of western timepieces as an attraction towards their appeal in the East. Also, in agriculture, many of the superior Asian techniques were forgotten even though they had been recorded. For instance, Wang Chen's work 'Treatise on Agriculture', published in 1331, had only one copy left by 1530. As Mokyr concludes 'a gradual deceleration took place that by the nineteenth century resulted in unmistakable backwardness compared to Europe', as Asia failed to exploit some of its early seeds of innovation to their full potential. The second factor is more important in explaining divergence as many of East Asia's inventions were adapted by the west, which they developed further in order to enhance industrial progress. For example, although the Chinese invented printing, the process that they created was xylography- a wooden block with the text carved on it on reverse, which was then imprinted onto paper. Europeans adopted this technique throughout the fifteenth and sixteenth centuries, however they progressed to using moveable letters when printing documents, which allowed for articles to be printed and subsequently circulated with higher frequency. Although Korea used metal moveable type from 1240 onwards, Asia as a whole failed to develop a similar system in parallel to the West. Another example of Eastern loss of technological superiority is warfare. The Chinese invented gunpowder, but it was used predominantly in rockets and bombs. Overall their weaponry consisted of more antiquated equipment such as bows and arrows. Europe, however, brought gunpowder back to the west and developed cannons. The East then had to learn how to use them from their European counterparts. Even then there were differences in reception throughout East Asia; Japan embraced western inventions, whereas throughout the sixteenth century Chinese officials had to repeatedly ask for the Chinese army to adopt the cannon before it was introduced. Mokyr, The Lever of Riches, p. 219 Overall, in explaining East Asia's dwindling economic progress E.L. Jones' claim that 'the technological category of explanation is probably a diversion' is true in that inventions themselves did not cause a divergence, but the East's decline in technological innovation reveals a far more important cause of economic developmental discrepancy: attitude. Europe's success lay in its interest in Eastern inventions and their subsequent desire to improve upon them. East Asia's attitude, on the other hand, seemed to be the antithesis of Europe, for they had little interest in what the West had to offer unless it became a vital acquisition (such as the cannon). Creativity was not encouraged within the East, for in China even the most prosperous emperors such as K'ang Chi (1622-1722) and Ch'ien Lung (1736-1795), as Mokyr notes, 'discouraged the kind of dynamism that was throbbing throughout Europe at that time'. As the next section of this essay will demonstrate, technology is an illustrative example of an Eastern ethos that restricted growth in many aspects of economy and society, an ethos which can be labelled as the most prominent cause for the slowing of East Asian economic progress. E.L. Jones, The Record of Global Economic Development, (Cheltenham, 2002),p 26 Ibid, p. 237 iii- AttitudesOne of the greatest areas where attitude constricted eastern development was trade. Interest in long-distance trade had prevailed during the late twelfth and early thirteenth centuries, but this was not a lasting feature of Asian history- after 1431-33 there were no more economic or diplomatic voyages made by China during the early modern period. The East's loss of interest in the West was the opposite of European attitude; in 1498 Vasco de Gama discovered a sea trade route between Portugal and India, the first Portuguese ships reached China by 1514, and the Spanish had occupied the Philippines by 1565. Whilst the East had retreated, the West had attempted to encroach upon their land and exploit newfound areas for luxury goods. Asia's loss of interest in foreign voyages was symptomatic of their attitude towards burgeoning foreign trade, for they lacked demand for foreign goods. Europe, on the other hand, sought many products, importing tea, porcelain, lacquerware, spices and sugar from the East. Between the beginning of the eighteenth century and 1800 Britain's import of tea increased from 400,000 lb per annum to 23,000,000 lb per annum, and import duties on Chinese tea provided the British government with a tenth of their total revenue. As Western products did not hold the same appeal in Asia, the main attraction for international trade was the acquisition of silver, particularly in China. China, though, did not necessarily have to engage in European trade as there was a great demand for silks in Japan, which had a large supply of precious metals. It is worth noting, however, that in some instances European merchants acted as a middleman between China and Japan. Also, the Chinese accumulation of silver could have been utilised more effectively, for it was traded as bullion rather than being minted, which meant that the unit of account varied between trades and regions. The great interest in Chinese products led to the commodity economy became monetised, but the government failed to fully exploit the potential for economic growth. Whilst competition between Spain, Portugal, England and the Netherlands drove economic and territorial expansion in the West, East Asian policies began to restrict the dividends of European interest. In China, as Fairbank continues, 'mercantile interests were felt to be inherently in conflict with those of society and the state and had to be curbed as far as possible'. Sea voyages were restricted to small vessels and the Ming dynasty (1368-1644) banned private trade abroad. Similarly, Japan only operated with go-shuinsen ships, official trading vessels, and merchants required the approval of the Shogun. After 1639 they extended this attitude to a Jakoku or 'closed country' policy; The Japanese could not leave the country, no catholic country could enter, and foreign trade had to be conducted via Nagasaki. However, C.A. Bayly notes that the early modern period 'saw the rise of the European chartered companies...in the Asian world', demonstrating how the west was gaining both territorial and economic power. China and Japan may have proved difficult to infiltrate, but the lack of competition that they provided allowed Europe to maintain acquisitions like the Spanish occupation of the Philippines, and capitalise upon available resources such as control of the Indian textiles market. The Western exploitative attitude towards trade allowed them to gain products that improved their wealth and economic progress, whereas East Asia had little to gain in return except silver, which it did not mint to create commercial unity or ease of transaction. John King Fairbank, China: A New History, (1992, London), p. 135 Christopher Bayly, The Birth of the Modern World, 1780-1914: Global Connections and Comparisons, (Oxford, 2004), p. 44 As well as expanding eastwards, Europe also gained an extremely valuable advantage above Asia through Columbus' discovery of the Americas in 1492. Acquisition of the New World supplied the West with new food products, a territorial outlet for an expanding population, and resources such as coal. In terms of food, the Americas provided protein-rich fish from the Caribbean and alternative staples to wheat such as maize and potatoes. The Spanish introduced cattle to Latin America, which meant that Europe was able to import beef as well. Asia did adopt maize, potatoes, sweet potatoes and peanuts from the New World into their diet, which increased the land under cultivation by thirty five percent between 1660 and 1760, as these new products could be grown in drier conditions than rice. However, these lands were a lower yield than the paddy fields, and deforestation to plant the new crops caused soil erosion and flooding into the low lying, fertile areas. As Bayly concludes, the New World was of most benefit to the west because it provided 'new, nutritious varieties of food...(which) spread across the Old World in the wake of the Spanish and Portuguese ""discoveries"", improving fertility and resistance to disease'. Better diet enabled a stronger workforce, conducive to increasing production. Bayly, The Birth of the Modern World, p. 49 The New World was also an important asset in terms of population; migration relieved the pressure of demographic growth, and colonisation allowed the exploitation of indigenous people and imported African slaves in order to provide cheap labour. This was particularly useful for the extraction of one of America's most useful resources to the west: coal. Kenneth Pomeranz, in particular, stresses the importance of coal to Europe, as the transition from a timber to coal fuelled society 'relaxed the land constraint in a more fundamental way than any other innovation before turn-of-the-century chemicals and electricity'. Although Pomeranz disputes that divergence occurred before 1800, Europe definitely exploited American resources from the moment of discovery. East Asia had coalmines but they were not as accessible as those in the New World; China's best coal deposits were in Shaanxi, 700 miles from the country's main area of production the Yangzi Delta. Parthasarathi concludes that 'the massive use of coal certainly made possible an energy economy that far surpassed one based on timber and made possible huge iron output, the steam engine and eventually railways', thus providing Europe with the physical resources it needed to power the inventions of the Industrial Revolution. The Americas gave the West nutrients, fuel and territory that it could exploit and command, altogether creating the potential for an energised society capable of increased economic production and therefore greater prosperity. Whilst Pomeranz's conclusion stressing the cruciality of coal and colonies is both overzealous and too late in history to explain economic divergence by 1800, the New World provides a good example of how Western attitude to trade and exploitation helped them to progress ahead of the East. Kenneth Pomeranz, 'Political Economy and Ecology on the Eve of Industrialization: Europe, China and the Global Conjuncture', American Historical Review, 107 (2002), p. 441 Prasannan Parthasarathi, 'Review Article: The Great Divergence', Past and Present, 176 (2002), p. 282 The backward attitude of the East extended further than their approach to trade; Asian government and state policies were centred on hierarchy and repressive central control. In China, as Fairbank notes, the government: 'operated vertically, from the state upon the individual, more than it did horizontally, to resolve conflicts between one individual and another'. This made deviation very difficult, especially since the law was viewed as ultimate in sanctioning behaviour, with legislation predominantly concerning public activities. As Stephen Haw notes, 'the courts were intended to be terrifying places, which good citizens would avoid at almost all costs' and therefore society felt compelled to side with the interests of the state. The later Ming dynasty advocated repressive rule as they sought a controllable environment, and the successive Qing rule extended Ming foundations by ceasing all public services. Without a formal police force, commercial law or road maintenance, China was denied some essential elements for economic development. This was further exacerbated by the fact that the local governments under central Qing rule were somewhat ineffective, for the population of around two hundred million was divided into only eighteen provinces. An imperial bureaucracy based upon centralised control left little capacity for the accumulation of private profit. Similarly, Japanese politics was based on a philosophy of absolute government. Mason and Caiger observe that the Shogun held 'a right to command which could extend, if need be, to each man, woman or child in the country', and the Tokugawa administration (1600-1868) was underpinned with regulations and restrictions for important social groups; for instance, after 1615 nobles were not allowed to leave their ancestral cities. This detailed level of control fostered an environment in which obedience was key. Across East Asia the state 'oversaw, regulated and repressed' and whilst this did not necessarily mean the absence of autonomy, it did entail the desires of the state being valued above personal ambition. Fairbank, China, p. 184 Stephen Haw, China: A Cultural History, (London, 1990), p. 123 R.H.P. Mason and J.G. Caiger, A History of Japan, (Tokyo, 1973), p. 158 Landes, The Wealth and Poverty of Nations, p. 35 Europe, on the other hand, possessed an advantage in its state control: it was politically fragmented. Differences in governments created competition between countries, which led to the desire for economic superiority and the emergence of city-states. As Van Zanden observes, only in Europe did city-states 'manage to dominate the socio-political and economic life of large regions', and this meant that mercantile interests were of greater concern to European governments than they were in Asia. Many merchants believed in the phrase stadtloft macht frei (city air makes one free), and governments benefited as urban migration created a new workforce to fuel the growing manufacturing industries. England and the Netherlands- perhaps the most advanced European states during this period- established an equilibrium between central authority and commercial interest groups, providing them with the ability 'to mobilise resources on an unprecedented scale'. The mutual benefits of compromise between government and mercantile interests influenced European legislation, for both English common law and European Roman law protected property rights. This removed some of the vulnerability of entrepreneurial nous and thus encouraged economic development. Such an attitude was enhanced by financial changes; In East Asian government monetary matters were controlled by individual rulers, whereas European financial institutions were independent both of government whims and the fortunes of individual merchants. The Bank of England provided independent checks on the state of the economy and concept of partnership came into fruition with the Dutch pioneering of a joint-stock company. As Van Zanden concludes 'social and political relationships in Western Europe appear to have been less hierarchical than in large parts of Asia' and this enabled mercantile interests to prosper. Repressive government in an increasingly global economy was not conducive to economic progress. Jan Luiten Van Zanden, 'The Great Convergence from a West-European Perspective, Some Thoughts and Hypotheses', Itinerario, xxiv (2000), p. 15  Ibid, p. 18 Van Zanden, 'The Great Convergence from a West-European Perspective', Itinerario, xxiv, p. 21 Attitude affected the economic progress of East and West on more levels than political policies, for Europe and the Orient possessed very different mentalities on the whole. Whilst the West was dominated by Christianity, which provided moral reason and fuelled war, competition and change, the East was more secular, being dictated by Confucianism and natural philosophy. These differing mentalities contributed to their economic divergence. Religious prevalence, in particular, demonstrates the importance of attitude; in Europe, the philosophical separation of Church and State gave scope for free economic thought. Although faith played a great part in the monarchy, it supported rather than oppressed mercantilism. Max Weber claimed that Protestant ethic encouraged men to apply themselves rationally to their work, thus promoting economic progress. Also, Europe experienced religious wars during the early modern period, such as disputes between Catholics and Protestants in France from 1562-98. Conflicts of faith and the emergence of groups such as Lutheranism encouraged individual thought within religion, an ideology that could be extended to all aspects of life. In Asia, however, there was no established faith. As Bayly notes 'in Muslim and Asian societies a broad recognition of the supremacy of the emperor's cult, not uniformity of belief, was what was required' and therefore there was no separation of the Church; the state was all-powerful. Both the Chinese and Japanese states were highly influenced by the ideology of Confucianism, though this was never established as a religion. Mason and Caiger agree that Tokugawa policies were also Confucian in nature. Confucianism stressed love within the family (Hsiao) and loyalty to the state (Chung), which meant that businesses were kept within the family unit rather than sharing mercantile interests like Europe, and government attitude to politics and trade was respected rather than disputed. As Landes claims, the hierarchy that such an ideology impressed upon East Asia meant that Confucianism produced 'a culturally and intellectually homeostatic society'. This stagnation produced a desire for the east to maintain the status quo they had achieved, so major changes such as foreign inventions and trade were viewed with fear rather than interest. As Fairbank notes, China tried to extend economic possibilities with expansion of old systems rather than creating new ones: 'the tradition in China had been not to build a better mousetrap but to get the official mouse monopoly'. Bayly, The Birth of the Modern World, p. 32 Landes, The Wealth and Poverty of Nations, p. 38 Fairbank, China, p. 181 The East was hindered further by other cultural influences such as the role of women in society. Early modern Europe experienced the growth of cottage industry through utilising the skills of women. They were able to spin and weave from their homes and the sale of their products supplemented family income, allowing an improved living standard. Asia, as aforementioned, encouraged early marriage followed by many children; Confucianism not only restricted growth in terms of foreign interaction, it restricted their use of human resources where Europe employed initiative to further their progress. Also, China never developed a system of formal logic. This has obvious organisational repercussions, but it is also symptomatic of East Asian culture as a whole. The West was cultivating an atmosphere of change, innovation and progress, whilst the East strived to reinforce structure, control, and remain somewhat introverted. As Landes concludes, 'if we learn anything from the history of economic development, it is that culture makes all the difference' and the contradictory mentalities of the two continents goes a long way in explaining the divergence that occurred between them during the early modern period. Landes, The Wealth and Poverty of Nations, p. 516 ConclusionWith the benefit of hindsight it could be easy to view the slow progress of the East Asian economy as inevitable; the early modern period was an era in which trade networks were cemented across the world, where new continents were discovered and where new resources and techniques created the potential for both industrialisation and the expansion of consumerism. European religious and political beliefs formed an ethos of utilisation, competition and conscientiousness, all favourable for embracing these changes and thus encouraging economic growth. The West changed its diet to incorporate food from the New World, modified decorum to embrace Chinese porcelain, and sought fresh energy resources such as coal in order to increase their prosperity. On the other hand, East Asia had the advantage at the beginning of the period: It had overcome environmental problems such as water distribution with irrigation systems, and experimented with technological developments to create an accomplished agricultural system and efficient methods of chinaware production. However, this supremacy led to an environment in which foreign interaction was viewed with hostility. Instead of exploiting the full potential of European trade, they resisted it; instead of adopting western technology, they ignored it; instead of allowing the flourish of mercantile interests, they repressed them. In the move towards modernisation, the West cultivated the seeds of capitalism whereas the East retained Confucianism, and in an increasingly material society, Europe fared better. It would be erroneous to suggest that the East Asian economy remained stagnant by 1800, for it did change and develop, but the inherent mentality of state control imposed limitations upon the extent to which new thought and direction could emerge. As Bayly concludes 'Europe's...""exceptionalism"" was to be found not in one fact, but in an...accumulation of many characteristics seen separately in other parts of the world', characteristics that the West learned from their encounters and promoted within Europe. Equally, East Asia's slow progress can be seen as an accumulation of characteristics- although these were predominantly from within their society- that led to a situation in which the East became economically inferior to Europe. Bayly, The Birth of the Modern World, p. 71",True
59,"The title of David Landes' seminal 1998 work 'The Wealth and Poverty of Nations: Why some are so rich and some so poor' encapsulates the prevalent question among modern society of why there is such a great disparity of wealth between countries. With the benefit of hindsight, events such as the Industrial Revolution have contributed to a Eurocentric opinion of western superiority, at least until the development of modern North America. However, in the year 1300 this was not the case as in actual fact East Asia possessed a more advanced society than anywhere in the West. During the early modern period, Asian technological and agricultural innovations were envied by Europe, for at this point Europe was no more likely than East Asia to experience an industrialisation process. The economic divergence between East and West has been greatly researched, with historians providing explanations ranging from inherent climatic differences and oppressive state politics to trade and the advantages of European territorial expansion in order to explain East Asia's lack of economic progress compared to Europe. However, many of these claims are rather over-enthusiastic in their emphasis. Whilst European climate was more suitable to agricultural productivity, Asia's technological advantage counteracted this through the creation of irrigation systems. In fact, the West adopted several of their inventions. The first two sections of this essay will focus upon demonstrating that these two factors can be encompassed- like many other explanations- within the greatest reason for slow Asian economic progress: attitude. The comparative superiority of East Asia in 1300 meant that competitive spirit was not fostered in the same manner as Europe, and that attempts to regulate their admirable systems enforced an introverted society based on centralised state control, Sino centric governmental policies and hierarchy. The final section portrays how the effects of this attitude accumulated across most facets of East Asian society creating a rather stagnant economy, not conducive to the burgeoning trade and emerging global networks that Europe was able to exploit. For the purposes of this essay, at times East Asia shall be abbreviated to 'East', and Europe shall be substituted for 'West'. i- Environmental DifferencesDavid Landes claims that Europe's emerging economic superiority was due to its climate: the temperate weather, the even supply of rainfall all year round, and freedom from extreme natural phenomena such as flooding made the continent conducive to agricultural and commercial success. He notes that 'tropical areas generally average enough rainfall, but the timing is often irregular and unpredictable, the downpours anything but gentle' and therefore in terms of producing food for personal consumption and profit, Europe was in a greater position. Whilst this is a sensible observation, Landes extends his discourse to claim that the European climate was not merely the finest in practical terms, but that it was the most invigorating and therefore productive environment in the world, quoting the comments of a Bangladeshi diplomat: David Landes, The Wealth and Poverty of Nations: Why some are so rich and some so poor, (London, 1998), p. 13 'In countries like India, Pakistan, Indonesia, Nigeria and Ghana I have always felt enervated by the slightest physical or mental exertion, whereas in the UK, France, Germany or the US I have felt reinforced and stimulated by the temperate climate'David Landes, The Wealth and Poverty of Nations, p. 15 The moderate environment of Europe may have been beneficial in terms of agricultural production, but it is somewhat over-zealous to claim that the inherent climatic differences between the East and West were responsible for their divergence. Steady annual rainfall aided European agriculture, but the rice-oriented Asian societies developed canals and irrigation systems to counteract their natural disadvantage, creating a more technologically advanced society (which will be discussed in greater detail later). The Chinese invented the chain pump in the first century AD, which allowed water to be transferred from lower to higher ground, permitting the irrigation of crops, as picture one shows, and increasing their yield. These devices meant that, as E.L Jones notes, 'by the thirteenth century China...(had) probably the most sophisticated agriculture in the world'. This superiority was necessary though as unlike Europe, which had a variety of staple dietary products (predominantly wheat, but maize and potatoes from the New World after 1492), Asia depended on rice as its major source of nutrients. Landes claims that this was another hindrance, as rice has a lower nutritional content than other staples. The main disadvantages concerning rice were, though, its land-intensive and labour-intensive cultivation requirements. As C.A. Bayly notes, China and South East Asia 'required large efforts by local communities to maintain the irrigation systems which watered the crops'. Irrigation problems were two-fold; firstly, the wet harvesting of rice subjected workers to parasites such as schitosomes, which cause snail fever. This problem was especially apparent in the stagnant water of canals, such as the Chinese Grand Canal built in 605 AD. Secondly, there were political ramifications. In South East Asia Muslim Emperors were responsible for the maintenance of canal systems, and in China the Emperors managed the irrigation system of the Yellow River and controlled the Grand Canal. State control of the water systems caused (as Karl Wittfogel terms it) a ""hydraulic society"". The necessary waterways were managed by the government, which led to a centralisation of power. Ruler control of canals in Asia granted the state ultimate authority regarding rice cultivation and distribution and therefore workers were restricted in the amount of enterprise they could exercise. By 1800 the Grand Canal and Yellow River dyke systems were all in a state of decay, and there was little that farmers could do to improve the situation; the state considered irrigation a necessity in terms of subsistence, not a vehicle for entrepreneurship and commercialisation. E.L.Jones as cited in Landes, The Wealth and Poverty of Nations, p. 26 C.A. Bayly, The Birth of the Modern World 1780-1914: Global Connections and Comparisons, (Oxford, 2004), p. 28 State attitude towards irrigation was also due to ecological constraints in East Asia. Demographic changes were of much greater concern in the East than Europe, as it was experiencing a period of rapid population growth- in 1750 Japan had a population of approximately twenty eight million, and the Chinese population grew from approximately 100-150 million in 1650, to 200-250 million by 1750. This increase created a rapidly escalating need for food, therefore the states' primary concern was provision rather than profit. Demographic growth was also somewhat self-perpetuating; increased demand for food necessitated extra manpower for cultivation. This created a culture in which early marriage and many children were encouraged, which led to yet more food and therefore manpower being needed. As P.H.H. Vries notes 'China's internal transport with its densely knit system of waterways probably was the most efficient in the world', but this was because it had to be in order to provide subsistence for a large country. Efficiency was essential, and therefore demography hampered the East Asia's chances of large-scale commercialisation and industrialisation more than the environment itself. Landes emphasises climatic differences too fervently, for it was resulting ecological constraints the state's response and that hindered economic progress. As the next section explains, East Asia was able to overcome natural disadvantage through technology; however, their success was brief, for technical superiority highlighted problems similar to those posed by state control over water. Landes, The Wealth and Poverty of Nations, p. 23 P.H.H. Vries, 'Are Coal and Colonies Really Crucial? Kenneth Pomeranz and the Great Divergence', Journal of World History, 12 (2001), p. 415 ii- TechnologyTechnology is a representative case of how the potential for development in East Asia was not wholly fulfilled, whereas Europe learned from Asian inventions and progressed ahead of them. The East experienced many technological developments, such as the aforementioned irrigation system, prior to Europe and by the early modern period they possessed a level of superiority. For instance, China explored rudimentary time measurement with water clocks during the Song period (960-1279). Also, agriculture was advanced not only through hydraulic engineering such as ditches and dykes, but through the development of an iron plough during the sixth century, which could adjust the depth of the furrow. Asia, in fact, was responsible for some of the most fundamental inventions in early history: the compass (960 AD), gunpowder (tenth century), paper (100 AD) and printing. As Joel Mokyr notes 'by the end of the middle ages, it appears that China was about ready to undergo a process eerily similar to the great British Industrial Revolution'. However, this prophetic stage of development was not fulfilled in the manner that European technological development succeeded. Joel Mokyr, The Lever of Riches: Technological creativity and Economic Progress, (Oxford, 1990), pp. 212-213 The main problems with Eastern technological development were that either significant inventions were forgotten, or other advancements were not enhanced to their full potential. In the first instance, time measurement is an excellent example. Achievements such as Su Sung's forty-foot high clock, produced in 1086, had faded by the sixteenth century. Nothing like the European weight-driven clocks were developed, and in the seventeenth century the Jesuits were able to use the novelty of western timepieces as an attraction towards their appeal in the East. Also, in agriculture, many of the superior Asian techniques were forgotten even though they had been recorded. For instance, Wang Chen's work 'Treatise on Agriculture', published in 1331, had only one copy left by 1530. As Mokyr concludes 'a gradual deceleration took place that by the nineteenth century resulted in unmistakable backwardness compared to Europe', as Asia failed to exploit some of its early seeds of innovation to their full potential. The second factor is more important in explaining divergence as many of East Asia's inventions were adapted by the west, which they developed further in order to enhance industrial progress. For example, although the Chinese invented printing, the process that they created was xylography- a wooden block with the text carved on it on reverse, which was then imprinted onto paper. Europeans adopted this technique throughout the fifteenth and sixteenth centuries, however they progressed to using moveable letters when printing documents, which allowed for articles to be printed and subsequently circulated with higher frequency. Although Korea used metal moveable type from 1240 onwards, Asia as a whole failed to develop a similar system in parallel to the West. Another example of Eastern loss of technological superiority is warfare. The Chinese invented gunpowder, but it was used predominantly in rockets and bombs. Overall their weaponry consisted of more antiquated equipment such as bows and arrows. Europe, however, brought gunpowder back to the west and developed cannons. The East then had to learn how to use them from their European counterparts. Even then there were differences in reception throughout East Asia; Japan embraced western inventions, whereas throughout the sixteenth century Chinese officials had to repeatedly ask for the Chinese army to adopt the cannon before it was introduced. Mokyr, The Lever of Riches, p. 219 Overall, in explaining East Asia's dwindling economic progress E.L. Jones' claim that 'the technological category of explanation is probably a diversion' is true in that inventions themselves did not cause a divergence, but the East's decline in technological innovation reveals a far more important cause of economic developmental discrepancy: attitude. Europe's success lay in its interest in Eastern inventions and their subsequent desire to improve upon them. East Asia's attitude, on the other hand, seemed to be the antithesis of Europe, for they had little interest in what the West had to offer unless it became a vital acquisition (such as the cannon). Creativity was not encouraged within the East, for in China even the most prosperous emperors such as K'ang Chi (1622-1722) and Ch'ien Lung (1736-1795), as Mokyr notes, 'discouraged the kind of dynamism that was throbbing throughout Europe at that time'. As the next section of this essay will demonstrate, technology is an illustrative example of an Eastern ethos that restricted growth in many aspects of economy and society, an ethos which can be labelled as the most prominent cause for the slowing of East Asian economic progress. E.L. Jones, The Record of Global Economic Development, (Cheltenham, 2002),p 26 Ibid, p. 237 iii- AttitudesOne of the greatest areas where attitude constricted eastern development was trade. Interest in long-distance trade had prevailed during the late twelfth and early thirteenth centuries, but this was not a lasting feature of Asian history- after 1431-33 there were no more economic or diplomatic voyages made by China during the early modern period. The East's loss of interest in the West was the opposite of European attitude; in 1498 Vasco de Gama discovered a sea trade route between Portugal and India, the first Portuguese ships reached China by 1514, and the Spanish had occupied the Philippines by 1565. Whilst the East had retreated, the West had attempted to encroach upon their land and exploit newfound areas for luxury goods. Asia's loss of interest in foreign voyages was symptomatic of their attitude towards burgeoning foreign trade, for they lacked demand for foreign goods. Europe, on the other hand, sought many products, importing tea, porcelain, lacquerware, spices and sugar from the East. Between the beginning of the eighteenth century and 1800 Britain's import of tea increased from 400,000 lb per annum to 23,000,000 lb per annum, and import duties on Chinese tea provided the British government with a tenth of their total revenue. As Western products did not hold the same appeal in Asia, the main attraction for international trade was the acquisition of silver, particularly in China. China, though, did not necessarily have to engage in European trade as there was a great demand for silks in Japan, which had a large supply of precious metals. It is worth noting, however, that in some instances European merchants acted as a middleman between China and Japan. Also, the Chinese accumulation of silver could have been utilised more effectively, for it was traded as bullion rather than being minted, which meant that the unit of account varied between trades and regions. The great interest in Chinese products led to the commodity economy became monetised, but the government failed to fully exploit the potential for economic growth. Whilst competition between Spain, Portugal, England and the Netherlands drove economic and territorial expansion in the West, East Asian policies began to restrict the dividends of European interest. In China, as Fairbank continues, 'mercantile interests were felt to be inherently in conflict with those of society and the state and had to be curbed as far as possible'. Sea voyages were restricted to small vessels and the Ming dynasty (1368-1644) banned private trade abroad. Similarly, Japan only operated with go-shuinsen ships, official trading vessels, and merchants required the approval of the Shogun. After 1639 they extended this attitude to a Jakoku or 'closed country' policy; The Japanese could not leave the country, no catholic country could enter, and foreign trade had to be conducted via Nagasaki. However, C.A. Bayly notes that the early modern period 'saw the rise of the European chartered companies...in the Asian world', demonstrating how the west was gaining both territorial and economic power. China and Japan may have proved difficult to infiltrate, but the lack of competition that they provided allowed Europe to maintain acquisitions like the Spanish occupation of the Philippines, and capitalise upon available resources such as control of the Indian textiles market. The Western exploitative attitude towards trade allowed them to gain products that improved their wealth and economic progress, whereas East Asia had little to gain in return except silver, which it did not mint to create commercial unity or ease of transaction. John King Fairbank, China: A New History, (1992, London), p. 135 Christopher Bayly, The Birth of the Modern World, 1780-1914: Global Connections and Comparisons, (Oxford, 2004), p. 44 As well as expanding eastwards, Europe also gained an extremely valuable advantage above Asia through Columbus' discovery of the Americas in 1492. Acquisition of the New World supplied the West with new food products, a territorial outlet for an expanding population, and resources such as coal. In terms of food, the Americas provided protein-rich fish from the Caribbean and alternative staples to wheat such as maize and potatoes. The Spanish introduced cattle to Latin America, which meant that Europe was able to import beef as well. Asia did adopt maize, potatoes, sweet potatoes and peanuts from the New World into their diet, which increased the land under cultivation by thirty five percent between 1660 and 1760, as these new products could be grown in drier conditions than rice. However, these lands were a lower yield than the paddy fields, and deforestation to plant the new crops caused soil erosion and flooding into the low lying, fertile areas. As Bayly concludes, the New World was of most benefit to the west because it provided 'new, nutritious varieties of food...(which) spread across the Old World in the wake of the Spanish and Portuguese ""discoveries"", improving fertility and resistance to disease'. Better diet enabled a stronger workforce, conducive to increasing production. Bayly, The Birth of the Modern World, p. 49 The New World was also an important asset in terms of population; migration relieved the pressure of demographic growth, and colonisation allowed the exploitation of indigenous people and imported African slaves in order to provide cheap labour. This was particularly useful for the extraction of one of America's most useful resources to the west: coal. Kenneth Pomeranz, in particular, stresses the importance of coal to Europe, as the transition from a timber to coal fuelled society 'relaxed the land constraint in a more fundamental way than any other innovation before turn-of-the-century chemicals and electricity'. Although Pomeranz disputes that divergence occurred before 1800, Europe definitely exploited American resources from the moment of discovery. East Asia had coalmines but they were not as accessible as those in the New World; China's best coal deposits were in Shaanxi, 700 miles from the country's main area of production the Yangzi Delta. Parthasarathi concludes that 'the massive use of coal certainly made possible an energy economy that far surpassed one based on timber and made possible huge iron output, the steam engine and eventually railways', thus providing Europe with the physical resources it needed to power the inventions of the Industrial Revolution. The Americas gave the West nutrients, fuel and territory that it could exploit and command, altogether creating the potential for an energised society capable of increased economic production and therefore greater prosperity. Whilst Pomeranz's conclusion stressing the cruciality of coal and colonies is both overzealous and too late in history to explain economic divergence by 1800, the New World provides a good example of how Western attitude to trade and exploitation helped them to progress ahead of the East. Kenneth Pomeranz, 'Political Economy and Ecology on the Eve of Industrialization: Europe, China and the Global Conjuncture', American Historical Review, 107 (2002), p. 441 Prasannan Parthasarathi, 'Review Article: The Great Divergence', Past and Present, 176 (2002), p. 282 The backward attitude of the East extended further than their approach to trade; Asian government and state policies were centred on hierarchy and repressive central control. In China, as Fairbank notes, the government: 'operated vertically, from the state upon the individual, more than it did horizontally, to resolve conflicts between one individual and another'. This made deviation very difficult, especially since the law was viewed as ultimate in sanctioning behaviour, with legislation predominantly concerning public activities. As Stephen Haw notes, 'the courts were intended to be terrifying places, which good citizens would avoid at almost all costs' and therefore society felt compelled to side with the interests of the state. The later Ming dynasty advocated repressive rule as they sought a controllable environment, and the successive Qing rule extended Ming foundations by ceasing all public services. Without a formal police force, commercial law or road maintenance, China was denied some essential elements for economic development. This was further exacerbated by the fact that the local governments under central Qing rule were somewhat ineffective, for the population of around two hundred million was divided into only eighteen provinces. An imperial bureaucracy based upon centralised control left little capacity for the accumulation of private profit. Similarly, Japanese politics was based on a philosophy of absolute government. Mason and Caiger observe that the Shogun held 'a right to command which could extend, if need be, to each man, woman or child in the country', and the Tokugawa administration (1600-1868) was underpinned with regulations and restrictions for important social groups; for instance, after 1615 nobles were not allowed to leave their ancestral cities. This detailed level of control fostered an environment in which obedience was key. Across East Asia the state 'oversaw, regulated and repressed' and whilst this did not necessarily mean the absence of autonomy, it did entail the desires of the state being valued above personal ambition. Fairbank, China, p. 184 Stephen Haw, China: A Cultural History, (London, 1990), p. 123 R.H.P. Mason and J.G. Caiger, A History of Japan, (Tokyo, 1973), p. 158 Landes, The Wealth and Poverty of Nations, p. 35 Europe, on the other hand, possessed an advantage in its state control: it was politically fragmented. Differences in governments created competition between countries, which led to the desire for economic superiority and the emergence of city-states. As Van Zanden observes, only in Europe did city-states 'manage to dominate the socio-political and economic life of large regions', and this meant that mercantile interests were of greater concern to European governments than they were in Asia. Many merchants believed in the phrase stadtloft macht frei (city air makes one free), and governments benefited as urban migration created a new workforce to fuel the growing manufacturing industries. England and the Netherlands- perhaps the most advanced European states during this period- established an equilibrium between central authority and commercial interest groups, providing them with the ability 'to mobilise resources on an unprecedented scale'. The mutual benefits of compromise between government and mercantile interests influenced European legislation, for both English common law and European Roman law protected property rights. This removed some of the vulnerability of entrepreneurial nous and thus encouraged economic development. Such an attitude was enhanced by financial changes; In East Asian government monetary matters were controlled by individual rulers, whereas European financial institutions were independent both of government whims and the fortunes of individual merchants. The Bank of England provided independent checks on the state of the economy and concept of partnership came into fruition with the Dutch pioneering of a joint-stock company. As Van Zanden concludes 'social and political relationships in Western Europe appear to have been less hierarchical than in large parts of Asia' and this enabled mercantile interests to prosper. Repressive government in an increasingly global economy was not conducive to economic progress. Jan Luiten Van Zanden, 'The Great Convergence from a West-European Perspective, Some Thoughts and Hypotheses', Itinerario, xxiv (2000), p. 15  Ibid, p. 18 Van Zanden, 'The Great Convergence from a West-European Perspective', Itinerario, xxiv, p. 21 Attitude affected the economic progress of East and West on more levels than political policies, for Europe and the Orient possessed very different mentalities on the whole. Whilst the West was dominated by Christianity, which provided moral reason and fuelled war, competition and change, the East was more secular, being dictated by Confucianism and natural philosophy. These differing mentalities contributed to their economic divergence. Religious prevalence, in particular, demonstrates the importance of attitude; in Europe, the philosophical separation of Church and State gave scope for free economic thought. Although faith played a great part in the monarchy, it supported rather than oppressed mercantilism. Max Weber claimed that Protestant ethic encouraged men to apply themselves rationally to their work, thus promoting economic progress. Also, Europe experienced religious wars during the early modern period, such as disputes between Catholics and Protestants in France from 1562-98. Conflicts of faith and the emergence of groups such as Lutheranism encouraged individual thought within religion, an ideology that could be extended to all aspects of life. In Asia, however, there was no established faith. As Bayly notes 'in Muslim and Asian societies a broad recognition of the supremacy of the emperor's cult, not uniformity of belief, was what was required' and therefore there was no separation of the Church; the state was all-powerful. Both the Chinese and Japanese states were highly influenced by the ideology of Confucianism, though this was never established as a religion. Mason and Caiger agree that Tokugawa policies were also Confucian in nature. Confucianism stressed love within the family (Hsiao) and loyalty to the state (Chung), which meant that businesses were kept within the family unit rather than sharing mercantile interests like Europe, and government attitude to politics and trade was respected rather than disputed. As Landes claims, the hierarchy that such an ideology impressed upon East Asia meant that Confucianism produced 'a culturally and intellectually homeostatic society'. This stagnation produced a desire for the east to maintain the status quo they had achieved, so major changes such as foreign inventions and trade were viewed with fear rather than interest. As Fairbank notes, China tried to extend economic possibilities with expansion of old systems rather than creating new ones: 'the tradition in China had been not to build a better mousetrap but to get the official mouse monopoly'. Bayly, The Birth of the Modern World, p. 32 Landes, The Wealth and Poverty of Nations, p. 38 Fairbank, China, p. 181 The East was hindered further by other cultural influences such as the role of women in society. Early modern Europe experienced the growth of cottage industry through utilising the skills of women. They were able to spin and weave from their homes and the sale of their products supplemented family income, allowing an improved living standard. Asia, as aforementioned, encouraged early marriage followed by many children; Confucianism not only restricted growth in terms of foreign interaction, it restricted their use of human resources where Europe employed initiative to further their progress. Also, China never developed a system of formal logic. This has obvious organisational repercussions, but it is also symptomatic of East Asian culture as a whole. The West was cultivating an atmosphere of change, innovation and progress, whilst the East strived to reinforce structure, control, and remain somewhat introverted. As Landes concludes, 'if we learn anything from the history of economic development, it is that culture makes all the difference' and the contradictory mentalities of the two continents goes a long way in explaining the divergence that occurred between them during the early modern period. Landes, The Wealth and Poverty of Nations, p. 516 ConclusionWith the benefit of hindsight it could be easy to view the slow progress of the East Asian economy as inevitable; the early modern period was an era in which trade networks were cemented across the world, where new continents were discovered and where new resources and techniques created the potential for both industrialisation and the expansion of consumerism. European religious and political beliefs formed an ethos of utilisation, competition and conscientiousness, all favourable for embracing these changes and thus encouraging economic growth. The West changed its diet to incorporate food from the New World, modified decorum to embrace Chinese porcelain, and sought fresh energy resources such as coal in order to increase their prosperity. On the other hand, East Asia had the advantage at the beginning of the period: It had overcome environmental problems such as water distribution with irrigation systems, and experimented with technological developments to create an accomplished agricultural system and efficient methods of chinaware production. However, this supremacy led to an environment in which foreign interaction was viewed with hostility. Instead of exploiting the full potential of European trade, they resisted it; instead of adopting western technology, they ignored it; instead of allowing the flourish of mercantile interests, they repressed them. In the move towards modernisation, the West cultivated the seeds of capitalism whereas the East retained Confucianism, and in an increasingly material society, Europe fared better. It would be erroneous to suggest that the East Asian economy remained stagnant by 1800, for it did change and develop, but the inherent mentality of state control imposed limitations upon the extent to which new thought and direction could emerge. As Bayly concludes 'Europe's...""exceptionalism"" was to be found not in one fact, but in an...accumulation of many characteristics seen separately in other parts of the world', characteristics that the West learned from their encounters and promoted within Europe. Equally, East Asia's slow progress can be seen as an accumulation of characteristics- although these were predominantly from within their society- that led to a situation in which the East became economically inferior to Europe. Bayly, The Birth of the Modern World, p. 71","Steven Katz claims in his essay on the historical perspective of the Holocaust that 'the intentional murder of European Jewry during World War II, is historically and phenomenologically unique', and it is possible to forget that despite the notoriety of the Jewish Question there have been other massacres during the twentieth century. Annihilations have occurred in many countries throughout the century including Turkey, Rwanda, China, Bosnia and Cambodia, and whilst there are significant differences between each one- such as method of slaughter, death toll and time span of persecution- there are some factors common to them all. In fact, the Holocaust was not even the first great massacre of the twentieth century as the Armenian genocide occurred during the First World War, and Melson argues that it is 'a more accurate archetype than is the Holocaust for current mass murder in the postcolonial third world and in the contemporary post-communist world'. However this does not exclude the Holocaust from sharing common characteristics with other massacres (as Katz disputes) for, on a fundamental level, patterns can be drawn between them. For instance they all arise from revolutionary or wartime climates, and also the motivation for their persecution was due to threat felt by the perpetrating parties. Therefore the Holocaust is not entirely unique, and neither are the other massacres of the twentieth century despite their geographical and statistical differences. Rosenbaum, Alan (ed), Is the Holocaust Unique? Perspectives on comparative Genocide, (Oxford, 1996), p 19 Ibid, p 97 First of all, persecution of minority groups is a trait of more than one massacre. For instance in Turkey during World War One genocide occurred against the non-Muslim (Armenian) communities living within the Ottoman Empire, the Holocaust was the persecution of Jews within Christian Europe, and the Bosnian massacre of the 1990s was carried out against all those upon Bosnian territory who could be labelled as 'Turks'. The Cambodian massacre of the 1970s also involved the persecution of minority religious groups such as Buddhists, and national communities such as the Thai and Chinese, however this differs slightly as it was less focused upon one group, and more centred upon merely eradicating those not suited to the oppressive restrictions of the Pol Pot regime. Even within this minority group persecution there are further similarities. As Melson notes, the Armenian genocide and the Holocaust are alike in that the 'Armenians and Jews were ethno-religious minorities of inferior status that had experienced rapid social progress and mobilisation in the nineteenth century' and such progress had intimidated the ruling governments sufficiently to lead to them ordering their destruction. He continues to note that 'the Armenians challenged the traditional hierarchy of Ottoman society as they became better educated, wealthier and more urban' and the Nazis also feared the flourishing success of Jewish communities during the 1930s, however the Holocaust added another dimension to minority persecution as a 'broader, biologically-driven aim of extermination' existed due to Nazi views that Jews were genetically inferior. The concept of social mobility of minority groups raises the question of threat, a common motivator of the perpetrating governments towards massacre. Just as the Nazis were threatened by the Jewish success, and the Ottomans by the Armenians, the Bosnians were threatened by the rise of Serbian nationalism that resulted in the fall of Yugoslavia, and the subsequent threat posed by the Serbs and Croats demanding land. As Melson notes, this threat of attack led the Bosnian Serbs- under their leader Radovan Karadzic- to practice 'massacre, ethnic cleansing, and cultural destruction against those they called the ""Turks""'. In all three cases, even if there were other motivating factors, the perpetrators acted partly from fear. Rosenbaum, Is the Holocaust Unique?, p 90 Burleigh, Michael (ed), Confronting the Nazi Past: New Debates on Modern German History, (London, 1996), p 77 Rosenbaum, Is the Holocaust Unique? , p 95 The case of Cambodian genocide raises the example of massacre on a much broader, more national scale, and this form of massacre can be found elsewhere in the twentieth century- most notably in Nanking, China, during World War Two. As Callum Macdonald notes, in December 1937 'Japanese troops embarked on an orgy of murder, rape, arson and looting which lasted over six weeks' and rather than being directed at one particular ethnic or religious group it was an attack upon the Chinese nation. Similarly, in Cambodia whereas Pol Pot appeared to be attacking minorities, the massacre as a whole was a general ambush upon those that did not conform to the ideals of the communist party. This involved the slaughter of nearly 3000 Buddhists, 215,000 Chinese, 12000 Thai, and 'the Vietnamese community...was entirely eradicated'. The Muslim Chams were also targeted for, like the cases of Armenia and the Holocaust, 'their distinct religion, language and culture, large villages and autonomous networks threatened the atomized, closely supervised society that the Pol Pot leadership planned'. However this threat extended to all the aforementioned groups within Cambodia and became more of a national assault like the Japanese rape of Nanking. The only significant point that establishes Nanking as different in motive, though, is that it was an inter-national hostility that led to massacre (like in Bosnia). In Cambodia, Armenia and the Holocaust, massacre was derived from the respective government initiatives to impress their regime and ideals upon their country. Levene, Mark and Roberts, Penny (eds), The Massacre in History, (Oxford, 1999), p 223 Andreopoulos, Genocide: Conceptual and Historical Dimensions, (Pennsylvania, 1994), p 198 Ibid, p 199 Another common factor between many massacres of the twentieth century concerns the conditions under which it was possible for massacre to occur. In Armenia, the Holocaust, Cambodia and Bosnia, annihilations took place as a result of the instability caused by revolution. The revolutions of the Ottoman Empire led to the rise of Pan-Turkism, where the Turkish ethnic group was identified 'as the authentic political community on which the Turkish state could and should rely' and this led to the exclusion of Armenians from Turkey. In Germany, the collapse of the Weimar Republic 'allowed the Nazis to recast German identity and ideology' and their subsequent control of government apparatus meant they could alter social and racial policy to infiltrate anti-Semitic ideas. In Cambodia, the rise of communism resulting from political civil war between the first independence leader Sihanouk and his successor Lon Nol, and also from American intervention from the Vietnam war, allowed Pol Pot to seize control and cut Cambodia off from the rest of the world as well as killing 1.5 million of its eight million population. Similarly, in Bosnia massacre occurred due to the chaos of Serbian nationalism causing the collapse of Yugoslavia and birth of Croatia, Serbia and Bosnia as independent countries and their subsequent territorial disagreements. In all four cases revolution created an instability that allowed for the rise of new ideas and thus new prejudices. Rosenbaum, Is the Holocaust Unique? , p 90 Ibid. As well as revolution, War also plays a big responsibility in the viability of many twentieth century massacres. As Dadrian notes 'it is no coincidence that the two major genocides of this century (Armenian and Holocaust) were consummated in the vortex of two global wars' as there is a somewhat opportunistic element to the conditions under which massacre occurred. The onslaught of war allowed 'emergency powers' to be put in place- by Nazis and Turk Ittihadists- to increase security forces, and also the greater role of the militia in both cases allowed the mobilisation and mass murder of the persecuted. Both the Jews and the Armenians were 'considered as legitimate targets for destruction' by being levelled as internal enemies, thus creating some form of rationale within the country and also perpetrating massacre as an effective extension of the war already occurring. The presence of war helped to cover both massacres from other nations and therefore remove any opposition to the annihilations. As Wyman notes, in the case of the Holocaust 'seventeen months of systematic, cold-blooded murder ran their course between the time the Einsatzgruppen were turned loose on the Russian front, in June 1941, and the day in late November 1942 when the extermination plan was confirmed to the world' and it was only because of world warfare that this was possible. Perhaps the most prevalent example of this is the Japanese massacre in Nanking, for it was only because war erupted that it occurred- an opportunistic attack upon another nation rather than a chance to fulfil ethno-religious prejudice under the guise of warfare. The Japanese invaded China in the summer of 1937, and it was only after the fall of Shanghai in August that 'the mood of imperial troops...turned ugly, and many, it was said, lusted for revenge as they marched toward Nanking'. Chang accuses warfare as motivating civilians to rationalise massacre as acceptable, saying 'the rape of Nanking should be perceived as a cautionary tale- an illustration of how easily human beings can be encourages to allow their teenagers to be moulded into efficient killing machines able to suppress their better natures'. War was also used as a direct excuse for the massacre of Cambodia, for the Pol Pot regime gained favour as a result of American bombing of rural areas between 1970-73. The US army had intervened in the Vietnam war and 'soon encroached upon Cambodia's territory and integrity' and in response the Khmer Rouge, Pol Pot's forces, 'used the bombings devastation and massacre of civilians as recruitment propaganda' for their party. Similarly, in Bosnia ethnic massacre occurred under the post-independence territory wars, where Serbia desired greater Croatian territory and Bosnian Serbs wished to prevent Serb and Croat nationalists from encroaching on Bosnian territory. In all these instances, war provided both an opportunity to rationalise massacre and a veil to conceal it being perpetrated. Rosenbaum, Is the Holocaust Unique? , p 118 Ibid, p 120 Wyman, David, Abandonment of the Jews: America and the Holocaust, (New York, 1984), p 58 Chang, Iris, The Rape of Nanking: The forgotten Holocaust of World War II, (London, 1997), p 34 Ibid, p 220 Andreopoulos, Genocide, p 194 Ibid. Within each massacre there are smaller parallels between other annihilations of the twentieth century. For instance both the Armenian and Bosnian genocides involved making claims upon land, and the minority groups becoming deposed in order for the ruling parties to ''cleanse' the area of their presence, and destroy their culture'. The Armenian genocide also shares a similar approach of justification with the Nanking massacre in that both parties felt the 'need to subhumanize the intended victim'. However the Turks achieved this through stressing their actions as increasing their own mobility rather than oppressing the Armenians, whereas in Nanking it was left to the soldiers' imaginations. Chang notes how their 'automatic impulse was to dehumanise the prisoners by comparing them to insects and animals'. The need to surrealise such atrocious actions is common to all massacres as it takes subhuman mental strength to kill another person, but in the cases of Turkey and Nanking this need seems more overtly prevalent than in some other cases. Rosenbaum, Is the Holocaust Unique? , p 96 Andreopoulos, Genocide, p 10 Chang, The Rape of Nanking, p 44 Therefore, overall, whereas each twentieth century massacre occurred in very different manifestations, at different times, and for varying periods of time with differing scales of death, they do share some fundamental characteristics. In the cases of the Armenians, the Holocaust, Bosnia and Cambodia, all massacres involve the persecution of minority groups threatening the government, and the annihilations themselves were carried out in an unstable post-revolutionary climate. In all five examples warfare made massacre possible, and this is a crucial common factor between the countries as the ability to perpetrate such large-scale murders was only possible under the veil of warfare. Also, many countries felt the need to dehumanize victims in order to commit massacres and therefore not only were there similarities in the characteristics of their causation, but their methods of coping with atrocity. Melson's argument of the Armenian genocide as 'a more accurate archetype' than the Holocaust appears true as such persecution provides a base example for comparison with subsequent massacre, however the Holocaust- and the other examples mentioned- whilst seeming unique also form a pattern of traits applicable to massacre throughout the twentieth century. Rosenbaum, Is the Holocaust Unique? , p 88 Lombroso believed that degenerates, more specifically criminals, possessed 'numerous anomalies in the face, skeleton and various psychic and sensitive functions' that were indicative of their immoral behaviour. These physical characteristics made them identifiable as prone to criminality and, on a more general level, subordinate to the aesthetically 'normal' members of society. Moreover, such characteristics could be inherited and therefore people could be born as criminals. In 'The Portrait of Dorian Gray', Wilde uses Lombroso's concept of degeneration and physicality to establish both character description and the demise of Dorian Gray through his painting. Lombroso, Cesare, Criminal Man according to the classification of Cesare Lombroso, (London, 1911), p5 Henry Wooton comments in the novel that 'crime belongs exclusively to the lower orders' and Wilde uses Lombroso's theory to illustrate this within his description of the lower classes that Dorian encounters through both the theatre and the opium den. In particular the theatre owner is described as 'a hideous jew' with 'greasy ringlets' and 'beaming from ear to ear with an oily, tremulous smile'. Similarly, the other characters involved in the play are described in an equally derogatory manner- the orchestral conductor as 'a young hebrew', Romeo as 'a stout elderly gentleman, with corked eyebrows...and a figure like a beer barrell' and Romeo and Mercutio together as 'both as grotesque as the scenery'. Dorian Gray in fact comments on the theatregoers as being 'common, rough people with their coarse faces'. The concept of a 'coarse face' is particularly relevant to Lombroso as he believed that degenerates had identifiable physical features due to the atavistic nature of their faces and skeletons. He notes criminals in particular as exhibiting a prominent jaw, long arms and large ears- all somewhat animalistic and Neanderthal qualities. Wilde uses the idea of biologically backward appearance to enhance his depiction of the lower classes. Wilde, Oscar, The Portrait of Dorian Gray, (London, 2003), p266 Ibid, p66 Ibid. Ibid, p105 Ibid, p68 Wilde, Portrait of Dorian Gray, p68 In terms of Dorian Gray himself, Wilde uses Lombroso's theory as a vehicle to display his deterioration through the portrait. Although this does not concur with Lombroso's belief that criminality was hereditary, it does exploit the concept of immorality having physical effects- even if it is in retrospect and evident only on the portrait (as Gray has bound himself to a lifetime of youthfulness). For instance, Gray is described as observing 'its beautiful marred face' and that 'he would examine with minute care...the hideous lines that seared the wrinkling forehead, or crawled around around the heavy sensual mouth, wondering sometimes which were the more horrible, the signs of sin or the signs of age'. Gray also comments that 'if a wretched man has a vice, it shows itself in the lines of his mouth, the droop of his eyelids'- Lombroso's theory exactly. Therefore, Wilde uses Lombrosian theory as a basis for the physical description of many characters, but in Dorian Gray himself manipulates the concept to turn it into a literary vehicle with which to sustain the reader's attention. Lombroso believed indicative physical features were inherited and present from birth; Wilde depicted them as appearing and mutating as the soul of Dorian Gray began to degenerate. Ibid, p118 Ibid, p162 Ibid, p279 There are other features of Lombroso's theory on criminality that Wilde has manipulated in The Portrait of Dorian Gray. For instance, Lombroso commented on the often cold nature of criminals towards their kin as 'their moral sense is sterile because it is suffocated by passions and the deadening force of habit'. Whilst Dorian Gray's family are only revealed through portraits that he studies, he seems to blame them for his condition- not only displaying some hostility towards them but making an acknowledgement of hereditary degeneration. For example, he says of Philip Herbert's portrait: 'Had some strange poisonous germ crept from body to body till it had reached his own? ', and also that 'he knew what he had got' from the 'wine-dashed' lips of his mother. 'Wine-dashed' also implies that his mother was an alcoholic, another disease associated with the concept of degeneration. All his observations imply a sense of pre-disposition akin to the thoughts of Lombroso, and resentment towards such inheritance concurring with his beliefs as well. In terms of moral sterility Dorian Gray shows no remorse for his role in Sybil Vane's suicide, to which Basil comments 'I don't know what has come over you. You talk as if you had no heart, no pity in you'. Also, not only does he murder Basil but deals with his death in a methodical manner- summoning Alan Campbell to dispose of the body, and systematically burning all remaining evidence- and describes after the event that 'he felt strangely calm'. Equally when Campbell subsequently commits suicide Gray accepts no responsibility for causing another death. It is only when Gray eventually admits to himself the devastating effects he appears to have on all that he meets that he begins to be troubled by his actions, breaking with Lombrosian theory. Lombroso, Criminal Man, p24 Wilde, Portrait of Dorian Gray, p175 Ibid, p176 Ibid, p138 Ibid, p200 Lombroso also believed that besides hostility towards kin, criminals also were especially friendly towards strangers and felt an affinity with animals. Throughout the novel Dorian Gray proves popular with the other characters, courting the affection of various females (most prominently Sybil Vane and the Duchess, but Gray comments on numerous occasions that he has experienced much female attention throughout his life) and Basil, and also in striking an immediate friendship with Henry Wooton. When Dorian Gray goes hunting with Sir Geoffrey Clouston, he begs him not to shoot a rabbit because 'there was something in the animal's grace of movement that strangely charmed Dorian Gray'. Therefore Wilde even weaves some of the less prominent Lombrosian characteristics of the born criminal into his characterisation of Dorian Gray. Wilde , Portrait of Dorian Gray, p252 However, when considered on the whole, Lombroso only appears to influence Wilde on a foundational level as Dorian Gray himself seems to be equally swayed by external influences. His whole demise is derived though a desire to stay young, mostly courted by the compliments he receives regarding his beauty and comments such as Henry Wooton's 'beauty is a form of genius- is higher, indeed, than genius, as it needs no explanation'. Wooton also influences Gray by lending him a book. This idea of environmentally influenced degeneration is closer to Nordauism than Lombrosian theory. Although Wilde published Portrait of Dorian Gray in1890, two years before Nordau wrote 'Degeneration', his theories were very much in development and the concept of degeneration was a highly discussed contemporary topic. Wilde also uses external influences for a literary purpose- to heighten the demise of Dorian Gray, and also to create a sense of empathy with the character. With Lombroso's theory there is little sympathy for the criminal as they are depicted as some form of physical monstrosity, inherent on committing illegal and immoral acts due to their heredity, but in Gray's final act of slashing the portrait and being discovered as a 'withered, wrinkled and loathsome' looking corpse the reader is able to understand the futility of Dorian Gray's situation and blame his acquaintances and society for his demise just as much (perhaps) as the actions of Gray himself. Ibid, p34 Wilde, Portrait of Dorian Gray, p279 Therefore, 'The Picture of Dorian Gray' is influenced fundamentally by Lombroso. Wilde uses the concept of physicality in his descriptions, and also forms Gray's personality according to many of the characteristics of the born criminal. However, he also draws upon contemporary developing theories of environmentally induced degeneration to seal the fate of the protagonist. Overall, abiding only by the principles of Lombrosian theory it would be difficult to create a character with both any sense of remorse and any empathy within the reader, and both of these are necessary to maintain interest in and affinity with the novel. It would also prevent any final sense of demise, climax or conclusion to the novel. Wilde, subsequently, uses Lombroso's beliefs about criminality and degeneration as a basic literary vehicle from which to develop his novel- after all, his aim was not to create a scientific document but an enthralling tale of the deterioration of a man as a result of his whimsical desire for eternal youthfulness.",False
60,"The title of David Landes' seminal 1998 work 'The Wealth and Poverty of Nations: Why some are so rich and some so poor' encapsulates the prevalent question among modern society of why there is such a great disparity of wealth between countries. With the benefit of hindsight, events such as the Industrial Revolution have contributed to a Eurocentric opinion of western superiority, at least until the development of modern North America. However, in the year 1300 this was not the case as in actual fact East Asia possessed a more advanced society than anywhere in the West. During the early modern period, Asian technological and agricultural innovations were envied by Europe, for at this point Europe was no more likely than East Asia to experience an industrialisation process. The economic divergence between East and West has been greatly researched, with historians providing explanations ranging from inherent climatic differences and oppressive state politics to trade and the advantages of European territorial expansion in order to explain East Asia's lack of economic progress compared to Europe. However, many of these claims are rather over-enthusiastic in their emphasis. Whilst European climate was more suitable to agricultural productivity, Asia's technological advantage counteracted this through the creation of irrigation systems. In fact, the West adopted several of their inventions. The first two sections of this essay will focus upon demonstrating that these two factors can be encompassed- like many other explanations- within the greatest reason for slow Asian economic progress: attitude. The comparative superiority of East Asia in 1300 meant that competitive spirit was not fostered in the same manner as Europe, and that attempts to regulate their admirable systems enforced an introverted society based on centralised state control, Sino centric governmental policies and hierarchy. The final section portrays how the effects of this attitude accumulated across most facets of East Asian society creating a rather stagnant economy, not conducive to the burgeoning trade and emerging global networks that Europe was able to exploit. For the purposes of this essay, at times East Asia shall be abbreviated to 'East', and Europe shall be substituted for 'West'. i- Environmental DifferencesDavid Landes claims that Europe's emerging economic superiority was due to its climate: the temperate weather, the even supply of rainfall all year round, and freedom from extreme natural phenomena such as flooding made the continent conducive to agricultural and commercial success. He notes that 'tropical areas generally average enough rainfall, but the timing is often irregular and unpredictable, the downpours anything but gentle' and therefore in terms of producing food for personal consumption and profit, Europe was in a greater position. Whilst this is a sensible observation, Landes extends his discourse to claim that the European climate was not merely the finest in practical terms, but that it was the most invigorating and therefore productive environment in the world, quoting the comments of a Bangladeshi diplomat: David Landes, The Wealth and Poverty of Nations: Why some are so rich and some so poor, (London, 1998), p. 13 'In countries like India, Pakistan, Indonesia, Nigeria and Ghana I have always felt enervated by the slightest physical or mental exertion, whereas in the UK, France, Germany or the US I have felt reinforced and stimulated by the temperate climate'David Landes, The Wealth and Poverty of Nations, p. 15 The moderate environment of Europe may have been beneficial in terms of agricultural production, but it is somewhat over-zealous to claim that the inherent climatic differences between the East and West were responsible for their divergence. Steady annual rainfall aided European agriculture, but the rice-oriented Asian societies developed canals and irrigation systems to counteract their natural disadvantage, creating a more technologically advanced society (which will be discussed in greater detail later). The Chinese invented the chain pump in the first century AD, which allowed water to be transferred from lower to higher ground, permitting the irrigation of crops, as picture one shows, and increasing their yield. These devices meant that, as E.L Jones notes, 'by the thirteenth century China...(had) probably the most sophisticated agriculture in the world'. This superiority was necessary though as unlike Europe, which had a variety of staple dietary products (predominantly wheat, but maize and potatoes from the New World after 1492), Asia depended on rice as its major source of nutrients. Landes claims that this was another hindrance, as rice has a lower nutritional content than other staples. The main disadvantages concerning rice were, though, its land-intensive and labour-intensive cultivation requirements. As C.A. Bayly notes, China and South East Asia 'required large efforts by local communities to maintain the irrigation systems which watered the crops'. Irrigation problems were two-fold; firstly, the wet harvesting of rice subjected workers to parasites such as schitosomes, which cause snail fever. This problem was especially apparent in the stagnant water of canals, such as the Chinese Grand Canal built in 605 AD. Secondly, there were political ramifications. In South East Asia Muslim Emperors were responsible for the maintenance of canal systems, and in China the Emperors managed the irrigation system of the Yellow River and controlled the Grand Canal. State control of the water systems caused (as Karl Wittfogel terms it) a ""hydraulic society"". The necessary waterways were managed by the government, which led to a centralisation of power. Ruler control of canals in Asia granted the state ultimate authority regarding rice cultivation and distribution and therefore workers were restricted in the amount of enterprise they could exercise. By 1800 the Grand Canal and Yellow River dyke systems were all in a state of decay, and there was little that farmers could do to improve the situation; the state considered irrigation a necessity in terms of subsistence, not a vehicle for entrepreneurship and commercialisation. E.L.Jones as cited in Landes, The Wealth and Poverty of Nations, p. 26 C.A. Bayly, The Birth of the Modern World 1780-1914: Global Connections and Comparisons, (Oxford, 2004), p. 28 State attitude towards irrigation was also due to ecological constraints in East Asia. Demographic changes were of much greater concern in the East than Europe, as it was experiencing a period of rapid population growth- in 1750 Japan had a population of approximately twenty eight million, and the Chinese population grew from approximately 100-150 million in 1650, to 200-250 million by 1750. This increase created a rapidly escalating need for food, therefore the states' primary concern was provision rather than profit. Demographic growth was also somewhat self-perpetuating; increased demand for food necessitated extra manpower for cultivation. This created a culture in which early marriage and many children were encouraged, which led to yet more food and therefore manpower being needed. As P.H.H. Vries notes 'China's internal transport with its densely knit system of waterways probably was the most efficient in the world', but this was because it had to be in order to provide subsistence for a large country. Efficiency was essential, and therefore demography hampered the East Asia's chances of large-scale commercialisation and industrialisation more than the environment itself. Landes emphasises climatic differences too fervently, for it was resulting ecological constraints the state's response and that hindered economic progress. As the next section explains, East Asia was able to overcome natural disadvantage through technology; however, their success was brief, for technical superiority highlighted problems similar to those posed by state control over water. Landes, The Wealth and Poverty of Nations, p. 23 P.H.H. Vries, 'Are Coal and Colonies Really Crucial? Kenneth Pomeranz and the Great Divergence', Journal of World History, 12 (2001), p. 415 ii- TechnologyTechnology is a representative case of how the potential for development in East Asia was not wholly fulfilled, whereas Europe learned from Asian inventions and progressed ahead of them. The East experienced many technological developments, such as the aforementioned irrigation system, prior to Europe and by the early modern period they possessed a level of superiority. For instance, China explored rudimentary time measurement with water clocks during the Song period (960-1279). Also, agriculture was advanced not only through hydraulic engineering such as ditches and dykes, but through the development of an iron plough during the sixth century, which could adjust the depth of the furrow. Asia, in fact, was responsible for some of the most fundamental inventions in early history: the compass (960 AD), gunpowder (tenth century), paper (100 AD) and printing. As Joel Mokyr notes 'by the end of the middle ages, it appears that China was about ready to undergo a process eerily similar to the great British Industrial Revolution'. However, this prophetic stage of development was not fulfilled in the manner that European technological development succeeded. Joel Mokyr, The Lever of Riches: Technological creativity and Economic Progress, (Oxford, 1990), pp. 212-213 The main problems with Eastern technological development were that either significant inventions were forgotten, or other advancements were not enhanced to their full potential. In the first instance, time measurement is an excellent example. Achievements such as Su Sung's forty-foot high clock, produced in 1086, had faded by the sixteenth century. Nothing like the European weight-driven clocks were developed, and in the seventeenth century the Jesuits were able to use the novelty of western timepieces as an attraction towards their appeal in the East. Also, in agriculture, many of the superior Asian techniques were forgotten even though they had been recorded. For instance, Wang Chen's work 'Treatise on Agriculture', published in 1331, had only one copy left by 1530. As Mokyr concludes 'a gradual deceleration took place that by the nineteenth century resulted in unmistakable backwardness compared to Europe', as Asia failed to exploit some of its early seeds of innovation to their full potential. The second factor is more important in explaining divergence as many of East Asia's inventions were adapted by the west, which they developed further in order to enhance industrial progress. For example, although the Chinese invented printing, the process that they created was xylography- a wooden block with the text carved on it on reverse, which was then imprinted onto paper. Europeans adopted this technique throughout the fifteenth and sixteenth centuries, however they progressed to using moveable letters when printing documents, which allowed for articles to be printed and subsequently circulated with higher frequency. Although Korea used metal moveable type from 1240 onwards, Asia as a whole failed to develop a similar system in parallel to the West. Another example of Eastern loss of technological superiority is warfare. The Chinese invented gunpowder, but it was used predominantly in rockets and bombs. Overall their weaponry consisted of more antiquated equipment such as bows and arrows. Europe, however, brought gunpowder back to the west and developed cannons. The East then had to learn how to use them from their European counterparts. Even then there were differences in reception throughout East Asia; Japan embraced western inventions, whereas throughout the sixteenth century Chinese officials had to repeatedly ask for the Chinese army to adopt the cannon before it was introduced. Mokyr, The Lever of Riches, p. 219 Overall, in explaining East Asia's dwindling economic progress E.L. Jones' claim that 'the technological category of explanation is probably a diversion' is true in that inventions themselves did not cause a divergence, but the East's decline in technological innovation reveals a far more important cause of economic developmental discrepancy: attitude. Europe's success lay in its interest in Eastern inventions and their subsequent desire to improve upon them. East Asia's attitude, on the other hand, seemed to be the antithesis of Europe, for they had little interest in what the West had to offer unless it became a vital acquisition (such as the cannon). Creativity was not encouraged within the East, for in China even the most prosperous emperors such as K'ang Chi (1622-1722) and Ch'ien Lung (1736-1795), as Mokyr notes, 'discouraged the kind of dynamism that was throbbing throughout Europe at that time'. As the next section of this essay will demonstrate, technology is an illustrative example of an Eastern ethos that restricted growth in many aspects of economy and society, an ethos which can be labelled as the most prominent cause for the slowing of East Asian economic progress. E.L. Jones, The Record of Global Economic Development, (Cheltenham, 2002),p 26 Ibid, p. 237 iii- AttitudesOne of the greatest areas where attitude constricted eastern development was trade. Interest in long-distance trade had prevailed during the late twelfth and early thirteenth centuries, but this was not a lasting feature of Asian history- after 1431-33 there were no more economic or diplomatic voyages made by China during the early modern period. The East's loss of interest in the West was the opposite of European attitude; in 1498 Vasco de Gama discovered a sea trade route between Portugal and India, the first Portuguese ships reached China by 1514, and the Spanish had occupied the Philippines by 1565. Whilst the East had retreated, the West had attempted to encroach upon their land and exploit newfound areas for luxury goods. Asia's loss of interest in foreign voyages was symptomatic of their attitude towards burgeoning foreign trade, for they lacked demand for foreign goods. Europe, on the other hand, sought many products, importing tea, porcelain, lacquerware, spices and sugar from the East. Between the beginning of the eighteenth century and 1800 Britain's import of tea increased from 400,000 lb per annum to 23,000,000 lb per annum, and import duties on Chinese tea provided the British government with a tenth of their total revenue. As Western products did not hold the same appeal in Asia, the main attraction for international trade was the acquisition of silver, particularly in China. China, though, did not necessarily have to engage in European trade as there was a great demand for silks in Japan, which had a large supply of precious metals. It is worth noting, however, that in some instances European merchants acted as a middleman between China and Japan. Also, the Chinese accumulation of silver could have been utilised more effectively, for it was traded as bullion rather than being minted, which meant that the unit of account varied between trades and regions. The great interest in Chinese products led to the commodity economy became monetised, but the government failed to fully exploit the potential for economic growth. Whilst competition between Spain, Portugal, England and the Netherlands drove economic and territorial expansion in the West, East Asian policies began to restrict the dividends of European interest. In China, as Fairbank continues, 'mercantile interests were felt to be inherently in conflict with those of society and the state and had to be curbed as far as possible'. Sea voyages were restricted to small vessels and the Ming dynasty (1368-1644) banned private trade abroad. Similarly, Japan only operated with go-shuinsen ships, official trading vessels, and merchants required the approval of the Shogun. After 1639 they extended this attitude to a Jakoku or 'closed country' policy; The Japanese could not leave the country, no catholic country could enter, and foreign trade had to be conducted via Nagasaki. However, C.A. Bayly notes that the early modern period 'saw the rise of the European chartered companies...in the Asian world', demonstrating how the west was gaining both territorial and economic power. China and Japan may have proved difficult to infiltrate, but the lack of competition that they provided allowed Europe to maintain acquisitions like the Spanish occupation of the Philippines, and capitalise upon available resources such as control of the Indian textiles market. The Western exploitative attitude towards trade allowed them to gain products that improved their wealth and economic progress, whereas East Asia had little to gain in return except silver, which it did not mint to create commercial unity or ease of transaction. John King Fairbank, China: A New History, (1992, London), p. 135 Christopher Bayly, The Birth of the Modern World, 1780-1914: Global Connections and Comparisons, (Oxford, 2004), p. 44 As well as expanding eastwards, Europe also gained an extremely valuable advantage above Asia through Columbus' discovery of the Americas in 1492. Acquisition of the New World supplied the West with new food products, a territorial outlet for an expanding population, and resources such as coal. In terms of food, the Americas provided protein-rich fish from the Caribbean and alternative staples to wheat such as maize and potatoes. The Spanish introduced cattle to Latin America, which meant that Europe was able to import beef as well. Asia did adopt maize, potatoes, sweet potatoes and peanuts from the New World into their diet, which increased the land under cultivation by thirty five percent between 1660 and 1760, as these new products could be grown in drier conditions than rice. However, these lands were a lower yield than the paddy fields, and deforestation to plant the new crops caused soil erosion and flooding into the low lying, fertile areas. As Bayly concludes, the New World was of most benefit to the west because it provided 'new, nutritious varieties of food...(which) spread across the Old World in the wake of the Spanish and Portuguese ""discoveries"", improving fertility and resistance to disease'. Better diet enabled a stronger workforce, conducive to increasing production. Bayly, The Birth of the Modern World, p. 49 The New World was also an important asset in terms of population; migration relieved the pressure of demographic growth, and colonisation allowed the exploitation of indigenous people and imported African slaves in order to provide cheap labour. This was particularly useful for the extraction of one of America's most useful resources to the west: coal. Kenneth Pomeranz, in particular, stresses the importance of coal to Europe, as the transition from a timber to coal fuelled society 'relaxed the land constraint in a more fundamental way than any other innovation before turn-of-the-century chemicals and electricity'. Although Pomeranz disputes that divergence occurred before 1800, Europe definitely exploited American resources from the moment of discovery. East Asia had coalmines but they were not as accessible as those in the New World; China's best coal deposits were in Shaanxi, 700 miles from the country's main area of production the Yangzi Delta. Parthasarathi concludes that 'the massive use of coal certainly made possible an energy economy that far surpassed one based on timber and made possible huge iron output, the steam engine and eventually railways', thus providing Europe with the physical resources it needed to power the inventions of the Industrial Revolution. The Americas gave the West nutrients, fuel and territory that it could exploit and command, altogether creating the potential for an energised society capable of increased economic production and therefore greater prosperity. Whilst Pomeranz's conclusion stressing the cruciality of coal and colonies is both overzealous and too late in history to explain economic divergence by 1800, the New World provides a good example of how Western attitude to trade and exploitation helped them to progress ahead of the East. Kenneth Pomeranz, 'Political Economy and Ecology on the Eve of Industrialization: Europe, China and the Global Conjuncture', American Historical Review, 107 (2002), p. 441 Prasannan Parthasarathi, 'Review Article: The Great Divergence', Past and Present, 176 (2002), p. 282 The backward attitude of the East extended further than their approach to trade; Asian government and state policies were centred on hierarchy and repressive central control. In China, as Fairbank notes, the government: 'operated vertically, from the state upon the individual, more than it did horizontally, to resolve conflicts between one individual and another'. This made deviation very difficult, especially since the law was viewed as ultimate in sanctioning behaviour, with legislation predominantly concerning public activities. As Stephen Haw notes, 'the courts were intended to be terrifying places, which good citizens would avoid at almost all costs' and therefore society felt compelled to side with the interests of the state. The later Ming dynasty advocated repressive rule as they sought a controllable environment, and the successive Qing rule extended Ming foundations by ceasing all public services. Without a formal police force, commercial law or road maintenance, China was denied some essential elements for economic development. This was further exacerbated by the fact that the local governments under central Qing rule were somewhat ineffective, for the population of around two hundred million was divided into only eighteen provinces. An imperial bureaucracy based upon centralised control left little capacity for the accumulation of private profit. Similarly, Japanese politics was based on a philosophy of absolute government. Mason and Caiger observe that the Shogun held 'a right to command which could extend, if need be, to each man, woman or child in the country', and the Tokugawa administration (1600-1868) was underpinned with regulations and restrictions for important social groups; for instance, after 1615 nobles were not allowed to leave their ancestral cities. This detailed level of control fostered an environment in which obedience was key. Across East Asia the state 'oversaw, regulated and repressed' and whilst this did not necessarily mean the absence of autonomy, it did entail the desires of the state being valued above personal ambition. Fairbank, China, p. 184 Stephen Haw, China: A Cultural History, (London, 1990), p. 123 R.H.P. Mason and J.G. Caiger, A History of Japan, (Tokyo, 1973), p. 158 Landes, The Wealth and Poverty of Nations, p. 35 Europe, on the other hand, possessed an advantage in its state control: it was politically fragmented. Differences in governments created competition between countries, which led to the desire for economic superiority and the emergence of city-states. As Van Zanden observes, only in Europe did city-states 'manage to dominate the socio-political and economic life of large regions', and this meant that mercantile interests were of greater concern to European governments than they were in Asia. Many merchants believed in the phrase stadtloft macht frei (city air makes one free), and governments benefited as urban migration created a new workforce to fuel the growing manufacturing industries. England and the Netherlands- perhaps the most advanced European states during this period- established an equilibrium between central authority and commercial interest groups, providing them with the ability 'to mobilise resources on an unprecedented scale'. The mutual benefits of compromise between government and mercantile interests influenced European legislation, for both English common law and European Roman law protected property rights. This removed some of the vulnerability of entrepreneurial nous and thus encouraged economic development. Such an attitude was enhanced by financial changes; In East Asian government monetary matters were controlled by individual rulers, whereas European financial institutions were independent both of government whims and the fortunes of individual merchants. The Bank of England provided independent checks on the state of the economy and concept of partnership came into fruition with the Dutch pioneering of a joint-stock company. As Van Zanden concludes 'social and political relationships in Western Europe appear to have been less hierarchical than in large parts of Asia' and this enabled mercantile interests to prosper. Repressive government in an increasingly global economy was not conducive to economic progress. Jan Luiten Van Zanden, 'The Great Convergence from a West-European Perspective, Some Thoughts and Hypotheses', Itinerario, xxiv (2000), p. 15  Ibid, p. 18 Van Zanden, 'The Great Convergence from a West-European Perspective', Itinerario, xxiv, p. 21 Attitude affected the economic progress of East and West on more levels than political policies, for Europe and the Orient possessed very different mentalities on the whole. Whilst the West was dominated by Christianity, which provided moral reason and fuelled war, competition and change, the East was more secular, being dictated by Confucianism and natural philosophy. These differing mentalities contributed to their economic divergence. Religious prevalence, in particular, demonstrates the importance of attitude; in Europe, the philosophical separation of Church and State gave scope for free economic thought. Although faith played a great part in the monarchy, it supported rather than oppressed mercantilism. Max Weber claimed that Protestant ethic encouraged men to apply themselves rationally to their work, thus promoting economic progress. Also, Europe experienced religious wars during the early modern period, such as disputes between Catholics and Protestants in France from 1562-98. Conflicts of faith and the emergence of groups such as Lutheranism encouraged individual thought within religion, an ideology that could be extended to all aspects of life. In Asia, however, there was no established faith. As Bayly notes 'in Muslim and Asian societies a broad recognition of the supremacy of the emperor's cult, not uniformity of belief, was what was required' and therefore there was no separation of the Church; the state was all-powerful. Both the Chinese and Japanese states were highly influenced by the ideology of Confucianism, though this was never established as a religion. Mason and Caiger agree that Tokugawa policies were also Confucian in nature. Confucianism stressed love within the family (Hsiao) and loyalty to the state (Chung), which meant that businesses were kept within the family unit rather than sharing mercantile interests like Europe, and government attitude to politics and trade was respected rather than disputed. As Landes claims, the hierarchy that such an ideology impressed upon East Asia meant that Confucianism produced 'a culturally and intellectually homeostatic society'. This stagnation produced a desire for the east to maintain the status quo they had achieved, so major changes such as foreign inventions and trade were viewed with fear rather than interest. As Fairbank notes, China tried to extend economic possibilities with expansion of old systems rather than creating new ones: 'the tradition in China had been not to build a better mousetrap but to get the official mouse monopoly'. Bayly, The Birth of the Modern World, p. 32 Landes, The Wealth and Poverty of Nations, p. 38 Fairbank, China, p. 181 The East was hindered further by other cultural influences such as the role of women in society. Early modern Europe experienced the growth of cottage industry through utilising the skills of women. They were able to spin and weave from their homes and the sale of their products supplemented family income, allowing an improved living standard. Asia, as aforementioned, encouraged early marriage followed by many children; Confucianism not only restricted growth in terms of foreign interaction, it restricted their use of human resources where Europe employed initiative to further their progress. Also, China never developed a system of formal logic. This has obvious organisational repercussions, but it is also symptomatic of East Asian culture as a whole. The West was cultivating an atmosphere of change, innovation and progress, whilst the East strived to reinforce structure, control, and remain somewhat introverted. As Landes concludes, 'if we learn anything from the history of economic development, it is that culture makes all the difference' and the contradictory mentalities of the two continents goes a long way in explaining the divergence that occurred between them during the early modern period. Landes, The Wealth and Poverty of Nations, p. 516 ConclusionWith the benefit of hindsight it could be easy to view the slow progress of the East Asian economy as inevitable; the early modern period was an era in which trade networks were cemented across the world, where new continents were discovered and where new resources and techniques created the potential for both industrialisation and the expansion of consumerism. European religious and political beliefs formed an ethos of utilisation, competition and conscientiousness, all favourable for embracing these changes and thus encouraging economic growth. The West changed its diet to incorporate food from the New World, modified decorum to embrace Chinese porcelain, and sought fresh energy resources such as coal in order to increase their prosperity. On the other hand, East Asia had the advantage at the beginning of the period: It had overcome environmental problems such as water distribution with irrigation systems, and experimented with technological developments to create an accomplished agricultural system and efficient methods of chinaware production. However, this supremacy led to an environment in which foreign interaction was viewed with hostility. Instead of exploiting the full potential of European trade, they resisted it; instead of adopting western technology, they ignored it; instead of allowing the flourish of mercantile interests, they repressed them. In the move towards modernisation, the West cultivated the seeds of capitalism whereas the East retained Confucianism, and in an increasingly material society, Europe fared better. It would be erroneous to suggest that the East Asian economy remained stagnant by 1800, for it did change and develop, but the inherent mentality of state control imposed limitations upon the extent to which new thought and direction could emerge. As Bayly concludes 'Europe's...""exceptionalism"" was to be found not in one fact, but in an...accumulation of many characteristics seen separately in other parts of the world', characteristics that the West learned from their encounters and promoted within Europe. Equally, East Asia's slow progress can be seen as an accumulation of characteristics- although these were predominantly from within their society- that led to a situation in which the East became economically inferior to Europe. Bayly, The Birth of the Modern World, p. 71","As Alfred Crosby notes 'it is impossible to doubt that the transfer of Old World foods and livestock to the Americas had an immense impact on the Indian'. From Columbus' discovery of the New World in 1492, European travellers brought a host of animals, plants and seeds across the Atlantic and many of these were assimilated into the Indian diet. However, the speed and success of such reception varies among the different foodstuffs that colonisers attempted to introduce; whilst meat and vegetables complemented the indigenous diet, attempts to present wheat as a superior staple to maize were far less successful. Many of the products cultivated in the Americas were primarily for the consumption of colonial settlers, or indeed exportation back to Europe, and therefore although there was some acceptance of Old World products into Indian diets, on the whole it was not until the eighteenth century onwards that a genuine fusion of cuisines began to take place. Alfred Crosby, The Columbian Exchange: Biological and Cultural Consequences of 1492, (Wessport, 2003), pp 97-98 In terms of the staple diet, as Crosby notes 'in many of the most elemental ways, the Indian remained Indian' as wheat did no replace manioc, potatoes and- most importantly- maize as the basis of the indigenous diet. John Super concurs that 'all three of the New World staples outstripped the major European competitors of wheat, barley and rice in productivity per unit of land' as climactically grains had difficulty adapting to the greater heat and humidity of Latin America. Existing staples, however, were not only already successfully cultivated but were produced in many facets as food and drink. However, more importantly, European staples lacked the social value that maize held within indigenous society. As well as forming the basis of the Indian diet, maize possessed a great religious significance; the Mayan Bible, the Popol Vuh, recounts how man was formed from maize and subsequently one of the Gods worshipped in New World religious practice was the God of Maize. As Sophie Coe observes 'even the Europeans instantly identified maize as the equivalent to their own principal carbohydrate staple, wheat, and classified it as pan or bread, with all the religious and social connotations that the word implied'. Tortillas were the dietary basis of the Mayan people, and the principal Mayan maize product- tamales- was both a commodity and a ceremonial food. Maize was an inherent part of indigenous identity both nutritionally and metaphorically, and although colonialism inflicted religious conversion upon local people Europeans were unable to overcome the great reliance and significance that was placed upon New World Staples. As Jeffery Pilcher concludes 'their (the indigenous peoples) lives centred around maize; it provided the essence of their identity'. Rather than becoming assimilated within indigenous diet, attempts to introduce wheat served to highlight the difference between nutritional regimes and symbolise the values of two different cultures. Ibid, p 74 John Super and Thomas Wright (eds), Food, Politics and Society in Latin America, (London, 1985), p 5 Sophie Coe, America's First Cuisines, (Austin, 1994), p 9 Jeffery Pilcher, Que Vivan Los Tamales! Food and the Making of Mexican Identity, (Albuquerque, 1998), p 33 Whilst the Columbian Exchange failed to reform the staple diet of the New World, other products such as meat, fruit and vegetables- which complimented the existing foundation of maize, manioc and potatoes- were successfully incorporated into the indigenous nutritional regime. Seventeenth century naturalist Bernabe Coco claimed that 'we Spaniards found so poor and destitute of the animals most necessary to nourish and give service to mankind' and thus colonisers were quick to introduce the foundations of their nutritional regime to the New World. Columbus' voyages to the Americas marked the immediate transference of such items as the ships contained food representative of a microcosm of the Spanish diet, including livestock. On his second voyage Columbus brought seeds and cuttings for melons, onions, radishes, salad greens, grape vines, sugar canes and fruit stones for orchards. Bananas were also brought over from the Canary Islands in 1516 by European settlers. As Super notes 'the combination of New and Old World vegetables, full of minerals and vitamins, self-propagating or easily grown, provided a rich, abundant element in the emerging nutritional regimes'. The indigenous population may not have been receptive to foodstuffs that fundamentally undermined the basis of their existing diet, but they did accept items that could enrich it. The proliferation of crops and items was also beneficial in that the increased variety of products reduced the chance of crop failures and starvation. As Gary Paul Nabhan observes, diversity aided Latin Americans as 'different crops respond in different ways to drought, early freezes, insects, and diseases' and therefore improved their basic supply of nutrients as well as broadening their diet. However, it is debatable to what extent these products were grown for dietary reasons, for in many cases they were cultivated for commercial distribution. As Crosby notes 'the economic underpinnings of most of the important European settlements in the tropical and semitropical zones of America historically have been the raising of a certain few crops on large plantations for export to Europe'. This is typified in the introduction of sugar, for by the 1530s there were thirty-four sugar mills on the island of Espanola, and by 1610 fifty seven thousand tonnes of sugar were being produced annually in Brazil. In Brazil production was carried out through the use of slave labour, demonstrating the greater effect of food introduction for European commercial gain rather than local diet reformation; workers nutrition was subordinate to quest for profit. Crosby, The Columbian Exchange, pp 64-65 Super and Wright, Food, Politics and Society in Latin America, p 3 Nelson Foster and Linda Cordell, Chilies to Chocolate: Foods America Gave the World, (London, 1992), pp 147-148 Crosby, The Columbian Exchange, p 68 Perhaps the greatest change in the New World diet came in the introduction of Old World meats to Latin America. Indigenous nutrition consisted of a predominantly vegetarian diet, although Crosby notes that 'the turkey and muscovy duck were certainly present in pre-Columbian America, and some think that a type of chicken was also'. Livestock did not encounter the same degree of difficulty that occurred in introducing plants, as they were far more adaptable to the climactic conditions; in fact, their ability to settle and reproduce was so great that their expanding numbers quickly became a problem for natives, who found cattle inhabiting their land and destroying their crops. As Pilcher notes 'herds literally overran the countryside, driving Indians from their fields'. However, this practical nuisance did not deter indigenous consumption. Initially, pork emerged as the first meat to be consumed in any quantity, but was soon superseded by beef. Cattle proved the most adaptable livestock to the different climactic conditions and successful breeding led to a dramatic drop in the price of meat- between 1532 and 1538 the price of beef in Latin America fell by seventy five percent. In fact, Pilcher claims that 'urban meat prices fell so low in the sixteenth century that ranchers slaughtered the animals for their hides and often left the carcasses to rot'. As well as the nutritional value of meat itself, the increased availability of other animal products such as milk, cheese and eggs supplemented the level of protein in Indian diets. In fact, meat was incorporated so successfully into the American diet that in 1589 when the Viceroy of Peru advised a healthy diet in order to prevent disease, mutton, fowl and goat were three of the foods that he mentioned specifically. As Super concludes 'meat, a scarce commodity before the conquest, became commonplace in the nutritional regimes of Spaniards and Indians by the middle of the sixteenth century' and subsequently livestock have become an inherent part of the Latin American diet. Ibid, p 95 Pilcher, Que Vivan Los Tamales!, p 30 Pilcher, Que Vivan Los Tamales!, p 30 Super and Wright, Food, Politics and Society in Latin America, p 6 As well as introducing new foodstuffs into the indigenous diet, the Columbian Exchange also reformed the role of existing items- most notably alcohol. In Pre-Colonial Latin America the main alcoholic beverage consumed was Pulque, and consumption was closely linked to religious ritual. Drunkenness as a whole was frowned upon, but inebriation during times of celebration such as harvest, births, marriages and other religious ceremonies was considered a display of devotion; as Taylor notes 'alcohol, especially pulque, was associated with periodic, peaceful rituals that expressed village solidarity'. However, colonisation introduced grape vines in a wider abundance, which increased the volume of wine available. Also, the Spanish introduced the technique of distillation and thus Indians had access to much stronger alcoholic beverages than they had experienced through the fermentation process. This wider range of alcoholic beverages not only increased access to alcohol, but also contributed to the secularisation of drunkenness, as the Spanish drunk on a daily basis in moderation and the new drinks were not closely linked to ritual as pulque was. The result of this was the prevalence of drunkenness on a more regular basis among Indians, and an increased involvement of alcohol into the regular indigenous diet as a whole. Colonialism led to the development of alcohol from a product consumed infrequently as part of ritual to a drink featured in the everyday lives of Latin Americans. William Taylor, Drinking, Homicide and Rebellion in Colonial Mexican Villages, (Stanford, 1979), p 62 On the whole it is easier to note the effect of New World products on the rest of the world, but the influence of European foodstuffs on Latin America cannot be forgotten. Whilst the Columbian Exchange did not alter the staple diet of natives, auxiliary items were more readily adapted to greatly reform the meals that Indians ate; the tortilla endured, but was consumed with meat, fruits and vegetables introduced by the Old World. New items not only meant changes in variety of consumption, but the increased products reduced the chances of starvation from crop failure, and also led to better sources of nutrients- most notably protein. As Super concludes 'nutritional regimes were an expression of the changing historical reality of Latin America' and whilst at an initial glance it may appear that Indian diet was only modified on a tertiary level, Old World produce was significant on a level other than mere reformation of diet. As the New World was embracing the influx of foreign foodstuffs, they were also enduring the commercialisation and slavery implemented by colonisers in their attempt to exploit these newfound lands for European benefit. The Columbian Exchange not only reformed the diet of Latin America, it altered the manner in which foodstuffs were produced and in some ways led to the subordination of natives under the colonial government. Super and Wright, Food, Politics and Society in Latin America, p 17",True
61,"As Alfred Crosby notes 'it is impossible to doubt that the transfer of Old World foods and livestock to the Americas had an immense impact on the Indian'. From Columbus' discovery of the New World in 1492, European travellers brought a host of animals, plants and seeds across the Atlantic and many of these were assimilated into the Indian diet. However, the speed and success of such reception varies among the different foodstuffs that colonisers attempted to introduce; whilst meat and vegetables complemented the indigenous diet, attempts to present wheat as a superior staple to maize were far less successful. Many of the products cultivated in the Americas were primarily for the consumption of colonial settlers, or indeed exportation back to Europe, and therefore although there was some acceptance of Old World products into Indian diets, on the whole it was not until the eighteenth century onwards that a genuine fusion of cuisines began to take place. Alfred Crosby, The Columbian Exchange: Biological and Cultural Consequences of 1492, (Wessport, 2003), pp 97-98 In terms of the staple diet, as Crosby notes 'in many of the most elemental ways, the Indian remained Indian' as wheat did no replace manioc, potatoes and- most importantly- maize as the basis of the indigenous diet. John Super concurs that 'all three of the New World staples outstripped the major European competitors of wheat, barley and rice in productivity per unit of land' as climactically grains had difficulty adapting to the greater heat and humidity of Latin America. Existing staples, however, were not only already successfully cultivated but were produced in many facets as food and drink. However, more importantly, European staples lacked the social value that maize held within indigenous society. As well as forming the basis of the Indian diet, maize possessed a great religious significance; the Mayan Bible, the Popol Vuh, recounts how man was formed from maize and subsequently one of the Gods worshipped in New World religious practice was the God of Maize. As Sophie Coe observes 'even the Europeans instantly identified maize as the equivalent to their own principal carbohydrate staple, wheat, and classified it as pan or bread, with all the religious and social connotations that the word implied'. Tortillas were the dietary basis of the Mayan people, and the principal Mayan maize product- tamales- was both a commodity and a ceremonial food. Maize was an inherent part of indigenous identity both nutritionally and metaphorically, and although colonialism inflicted religious conversion upon local people Europeans were unable to overcome the great reliance and significance that was placed upon New World Staples. As Jeffery Pilcher concludes 'their (the indigenous peoples) lives centred around maize; it provided the essence of their identity'. Rather than becoming assimilated within indigenous diet, attempts to introduce wheat served to highlight the difference between nutritional regimes and symbolise the values of two different cultures. Ibid, p 74 John Super and Thomas Wright (eds), Food, Politics and Society in Latin America, (London, 1985), p 5 Sophie Coe, America's First Cuisines, (Austin, 1994), p 9 Jeffery Pilcher, Que Vivan Los Tamales! Food and the Making of Mexican Identity, (Albuquerque, 1998), p 33 Whilst the Columbian Exchange failed to reform the staple diet of the New World, other products such as meat, fruit and vegetables- which complimented the existing foundation of maize, manioc and potatoes- were successfully incorporated into the indigenous nutritional regime. Seventeenth century naturalist Bernabe Coco claimed that 'we Spaniards found so poor and destitute of the animals most necessary to nourish and give service to mankind' and thus colonisers were quick to introduce the foundations of their nutritional regime to the New World. Columbus' voyages to the Americas marked the immediate transference of such items as the ships contained food representative of a microcosm of the Spanish diet, including livestock. On his second voyage Columbus brought seeds and cuttings for melons, onions, radishes, salad greens, grape vines, sugar canes and fruit stones for orchards. Bananas were also brought over from the Canary Islands in 1516 by European settlers. As Super notes 'the combination of New and Old World vegetables, full of minerals and vitamins, self-propagating or easily grown, provided a rich, abundant element in the emerging nutritional regimes'. The indigenous population may not have been receptive to foodstuffs that fundamentally undermined the basis of their existing diet, but they did accept items that could enrich it. The proliferation of crops and items was also beneficial in that the increased variety of products reduced the chance of crop failures and starvation. As Gary Paul Nabhan observes, diversity aided Latin Americans as 'different crops respond in different ways to drought, early freezes, insects, and diseases' and therefore improved their basic supply of nutrients as well as broadening their diet. However, it is debatable to what extent these products were grown for dietary reasons, for in many cases they were cultivated for commercial distribution. As Crosby notes 'the economic underpinnings of most of the important European settlements in the tropical and semitropical zones of America historically have been the raising of a certain few crops on large plantations for export to Europe'. This is typified in the introduction of sugar, for by the 1530s there were thirty-four sugar mills on the island of Espanola, and by 1610 fifty seven thousand tonnes of sugar were being produced annually in Brazil. In Brazil production was carried out through the use of slave labour, demonstrating the greater effect of food introduction for European commercial gain rather than local diet reformation; workers nutrition was subordinate to quest for profit. Crosby, The Columbian Exchange, pp 64-65 Super and Wright, Food, Politics and Society in Latin America, p 3 Nelson Foster and Linda Cordell, Chilies to Chocolate: Foods America Gave the World, (London, 1992), pp 147-148 Crosby, The Columbian Exchange, p 68 Perhaps the greatest change in the New World diet came in the introduction of Old World meats to Latin America. Indigenous nutrition consisted of a predominantly vegetarian diet, although Crosby notes that 'the turkey and muscovy duck were certainly present in pre-Columbian America, and some think that a type of chicken was also'. Livestock did not encounter the same degree of difficulty that occurred in introducing plants, as they were far more adaptable to the climactic conditions; in fact, their ability to settle and reproduce was so great that their expanding numbers quickly became a problem for natives, who found cattle inhabiting their land and destroying their crops. As Pilcher notes 'herds literally overran the countryside, driving Indians from their fields'. However, this practical nuisance did not deter indigenous consumption. Initially, pork emerged as the first meat to be consumed in any quantity, but was soon superseded by beef. Cattle proved the most adaptable livestock to the different climactic conditions and successful breeding led to a dramatic drop in the price of meat- between 1532 and 1538 the price of beef in Latin America fell by seventy five percent. In fact, Pilcher claims that 'urban meat prices fell so low in the sixteenth century that ranchers slaughtered the animals for their hides and often left the carcasses to rot'. As well as the nutritional value of meat itself, the increased availability of other animal products such as milk, cheese and eggs supplemented the level of protein in Indian diets. In fact, meat was incorporated so successfully into the American diet that in 1589 when the Viceroy of Peru advised a healthy diet in order to prevent disease, mutton, fowl and goat were three of the foods that he mentioned specifically. As Super concludes 'meat, a scarce commodity before the conquest, became commonplace in the nutritional regimes of Spaniards and Indians by the middle of the sixteenth century' and subsequently livestock have become an inherent part of the Latin American diet. Ibid, p 95 Pilcher, Que Vivan Los Tamales!, p 30 Pilcher, Que Vivan Los Tamales!, p 30 Super and Wright, Food, Politics and Society in Latin America, p 6 As well as introducing new foodstuffs into the indigenous diet, the Columbian Exchange also reformed the role of existing items- most notably alcohol. In Pre-Colonial Latin America the main alcoholic beverage consumed was Pulque, and consumption was closely linked to religious ritual. Drunkenness as a whole was frowned upon, but inebriation during times of celebration such as harvest, births, marriages and other religious ceremonies was considered a display of devotion; as Taylor notes 'alcohol, especially pulque, was associated with periodic, peaceful rituals that expressed village solidarity'. However, colonisation introduced grape vines in a wider abundance, which increased the volume of wine available. Also, the Spanish introduced the technique of distillation and thus Indians had access to much stronger alcoholic beverages than they had experienced through the fermentation process. This wider range of alcoholic beverages not only increased access to alcohol, but also contributed to the secularisation of drunkenness, as the Spanish drunk on a daily basis in moderation and the new drinks were not closely linked to ritual as pulque was. The result of this was the prevalence of drunkenness on a more regular basis among Indians, and an increased involvement of alcohol into the regular indigenous diet as a whole. Colonialism led to the development of alcohol from a product consumed infrequently as part of ritual to a drink featured in the everyday lives of Latin Americans. William Taylor, Drinking, Homicide and Rebellion in Colonial Mexican Villages, (Stanford, 1979), p 62 On the whole it is easier to note the effect of New World products on the rest of the world, but the influence of European foodstuffs on Latin America cannot be forgotten. Whilst the Columbian Exchange did not alter the staple diet of natives, auxiliary items were more readily adapted to greatly reform the meals that Indians ate; the tortilla endured, but was consumed with meat, fruits and vegetables introduced by the Old World. New items not only meant changes in variety of consumption, but the increased products reduced the chances of starvation from crop failure, and also led to better sources of nutrients- most notably protein. As Super concludes 'nutritional regimes were an expression of the changing historical reality of Latin America' and whilst at an initial glance it may appear that Indian diet was only modified on a tertiary level, Old World produce was significant on a level other than mere reformation of diet. As the New World was embracing the influx of foreign foodstuffs, they were also enduring the commercialisation and slavery implemented by colonisers in their attempt to exploit these newfound lands for European benefit. The Columbian Exchange not only reformed the diet of Latin America, it altered the manner in which foodstuffs were produced and in some ways led to the subordination of natives under the colonial government. Super and Wright, Food, Politics and Society in Latin America, p 17","The title of David Landes' seminal 1998 work 'The Wealth and Poverty of Nations: Why some are so rich and some so poor' encapsulates the prevalent question among modern society of why there is such a great disparity of wealth between countries. With the benefit of hindsight, events such as the Industrial Revolution have contributed to a Eurocentric opinion of western superiority, at least until the development of modern North America. However, in the year 1300 this was not the case as in actual fact East Asia possessed a more advanced society than anywhere in the West. During the early modern period, Asian technological and agricultural innovations were envied by Europe, for at this point Europe was no more likely than East Asia to experience an industrialisation process. The economic divergence between East and West has been greatly researched, with historians providing explanations ranging from inherent climatic differences and oppressive state politics to trade and the advantages of European territorial expansion in order to explain East Asia's lack of economic progress compared to Europe. However, many of these claims are rather over-enthusiastic in their emphasis. Whilst European climate was more suitable to agricultural productivity, Asia's technological advantage counteracted this through the creation of irrigation systems. In fact, the West adopted several of their inventions. The first two sections of this essay will focus upon demonstrating that these two factors can be encompassed- like many other explanations- within the greatest reason for slow Asian economic progress: attitude. The comparative superiority of East Asia in 1300 meant that competitive spirit was not fostered in the same manner as Europe, and that attempts to regulate their admirable systems enforced an introverted society based on centralised state control, Sino centric governmental policies and hierarchy. The final section portrays how the effects of this attitude accumulated across most facets of East Asian society creating a rather stagnant economy, not conducive to the burgeoning trade and emerging global networks that Europe was able to exploit. For the purposes of this essay, at times East Asia shall be abbreviated to 'East', and Europe shall be substituted for 'West'. i- Environmental DifferencesDavid Landes claims that Europe's emerging economic superiority was due to its climate: the temperate weather, the even supply of rainfall all year round, and freedom from extreme natural phenomena such as flooding made the continent conducive to agricultural and commercial success. He notes that 'tropical areas generally average enough rainfall, but the timing is often irregular and unpredictable, the downpours anything but gentle' and therefore in terms of producing food for personal consumption and profit, Europe was in a greater position. Whilst this is a sensible observation, Landes extends his discourse to claim that the European climate was not merely the finest in practical terms, but that it was the most invigorating and therefore productive environment in the world, quoting the comments of a Bangladeshi diplomat: David Landes, The Wealth and Poverty of Nations: Why some are so rich and some so poor, (London, 1998), p. 13 'In countries like India, Pakistan, Indonesia, Nigeria and Ghana I have always felt enervated by the slightest physical or mental exertion, whereas in the UK, France, Germany or the US I have felt reinforced and stimulated by the temperate climate'David Landes, The Wealth and Poverty of Nations, p. 15 The moderate environment of Europe may have been beneficial in terms of agricultural production, but it is somewhat over-zealous to claim that the inherent climatic differences between the East and West were responsible for their divergence. Steady annual rainfall aided European agriculture, but the rice-oriented Asian societies developed canals and irrigation systems to counteract their natural disadvantage, creating a more technologically advanced society (which will be discussed in greater detail later). The Chinese invented the chain pump in the first century AD, which allowed water to be transferred from lower to higher ground, permitting the irrigation of crops, as picture one shows, and increasing their yield. These devices meant that, as E.L Jones notes, 'by the thirteenth century China...(had) probably the most sophisticated agriculture in the world'. This superiority was necessary though as unlike Europe, which had a variety of staple dietary products (predominantly wheat, but maize and potatoes from the New World after 1492), Asia depended on rice as its major source of nutrients. Landes claims that this was another hindrance, as rice has a lower nutritional content than other staples. The main disadvantages concerning rice were, though, its land-intensive and labour-intensive cultivation requirements. As C.A. Bayly notes, China and South East Asia 'required large efforts by local communities to maintain the irrigation systems which watered the crops'. Irrigation problems were two-fold; firstly, the wet harvesting of rice subjected workers to parasites such as schitosomes, which cause snail fever. This problem was especially apparent in the stagnant water of canals, such as the Chinese Grand Canal built in 605 AD. Secondly, there were political ramifications. In South East Asia Muslim Emperors were responsible for the maintenance of canal systems, and in China the Emperors managed the irrigation system of the Yellow River and controlled the Grand Canal. State control of the water systems caused (as Karl Wittfogel terms it) a ""hydraulic society"". The necessary waterways were managed by the government, which led to a centralisation of power. Ruler control of canals in Asia granted the state ultimate authority regarding rice cultivation and distribution and therefore workers were restricted in the amount of enterprise they could exercise. By 1800 the Grand Canal and Yellow River dyke systems were all in a state of decay, and there was little that farmers could do to improve the situation; the state considered irrigation a necessity in terms of subsistence, not a vehicle for entrepreneurship and commercialisation. E.L.Jones as cited in Landes, The Wealth and Poverty of Nations, p. 26 C.A. Bayly, The Birth of the Modern World 1780-1914: Global Connections and Comparisons, (Oxford, 2004), p. 28 State attitude towards irrigation was also due to ecological constraints in East Asia. Demographic changes were of much greater concern in the East than Europe, as it was experiencing a period of rapid population growth- in 1750 Japan had a population of approximately twenty eight million, and the Chinese population grew from approximately 100-150 million in 1650, to 200-250 million by 1750. This increase created a rapidly escalating need for food, therefore the states' primary concern was provision rather than profit. Demographic growth was also somewhat self-perpetuating; increased demand for food necessitated extra manpower for cultivation. This created a culture in which early marriage and many children were encouraged, which led to yet more food and therefore manpower being needed. As P.H.H. Vries notes 'China's internal transport with its densely knit system of waterways probably was the most efficient in the world', but this was because it had to be in order to provide subsistence for a large country. Efficiency was essential, and therefore demography hampered the East Asia's chances of large-scale commercialisation and industrialisation more than the environment itself. Landes emphasises climatic differences too fervently, for it was resulting ecological constraints the state's response and that hindered economic progress. As the next section explains, East Asia was able to overcome natural disadvantage through technology; however, their success was brief, for technical superiority highlighted problems similar to those posed by state control over water. Landes, The Wealth and Poverty of Nations, p. 23 P.H.H. Vries, 'Are Coal and Colonies Really Crucial? Kenneth Pomeranz and the Great Divergence', Journal of World History, 12 (2001), p. 415 ii- TechnologyTechnology is a representative case of how the potential for development in East Asia was not wholly fulfilled, whereas Europe learned from Asian inventions and progressed ahead of them. The East experienced many technological developments, such as the aforementioned irrigation system, prior to Europe and by the early modern period they possessed a level of superiority. For instance, China explored rudimentary time measurement with water clocks during the Song period (960-1279). Also, agriculture was advanced not only through hydraulic engineering such as ditches and dykes, but through the development of an iron plough during the sixth century, which could adjust the depth of the furrow. Asia, in fact, was responsible for some of the most fundamental inventions in early history: the compass (960 AD), gunpowder (tenth century), paper (100 AD) and printing. As Joel Mokyr notes 'by the end of the middle ages, it appears that China was about ready to undergo a process eerily similar to the great British Industrial Revolution'. However, this prophetic stage of development was not fulfilled in the manner that European technological development succeeded. Joel Mokyr, The Lever of Riches: Technological creativity and Economic Progress, (Oxford, 1990), pp. 212-213 The main problems with Eastern technological development were that either significant inventions were forgotten, or other advancements were not enhanced to their full potential. In the first instance, time measurement is an excellent example. Achievements such as Su Sung's forty-foot high clock, produced in 1086, had faded by the sixteenth century. Nothing like the European weight-driven clocks were developed, and in the seventeenth century the Jesuits were able to use the novelty of western timepieces as an attraction towards their appeal in the East. Also, in agriculture, many of the superior Asian techniques were forgotten even though they had been recorded. For instance, Wang Chen's work 'Treatise on Agriculture', published in 1331, had only one copy left by 1530. As Mokyr concludes 'a gradual deceleration took place that by the nineteenth century resulted in unmistakable backwardness compared to Europe', as Asia failed to exploit some of its early seeds of innovation to their full potential. The second factor is more important in explaining divergence as many of East Asia's inventions were adapted by the west, which they developed further in order to enhance industrial progress. For example, although the Chinese invented printing, the process that they created was xylography- a wooden block with the text carved on it on reverse, which was then imprinted onto paper. Europeans adopted this technique throughout the fifteenth and sixteenth centuries, however they progressed to using moveable letters when printing documents, which allowed for articles to be printed and subsequently circulated with higher frequency. Although Korea used metal moveable type from 1240 onwards, Asia as a whole failed to develop a similar system in parallel to the West. Another example of Eastern loss of technological superiority is warfare. The Chinese invented gunpowder, but it was used predominantly in rockets and bombs. Overall their weaponry consisted of more antiquated equipment such as bows and arrows. Europe, however, brought gunpowder back to the west and developed cannons. The East then had to learn how to use them from their European counterparts. Even then there were differences in reception throughout East Asia; Japan embraced western inventions, whereas throughout the sixteenth century Chinese officials had to repeatedly ask for the Chinese army to adopt the cannon before it was introduced. Mokyr, The Lever of Riches, p. 219 Overall, in explaining East Asia's dwindling economic progress E.L. Jones' claim that 'the technological category of explanation is probably a diversion' is true in that inventions themselves did not cause a divergence, but the East's decline in technological innovation reveals a far more important cause of economic developmental discrepancy: attitude. Europe's success lay in its interest in Eastern inventions and their subsequent desire to improve upon them. East Asia's attitude, on the other hand, seemed to be the antithesis of Europe, for they had little interest in what the West had to offer unless it became a vital acquisition (such as the cannon). Creativity was not encouraged within the East, for in China even the most prosperous emperors such as K'ang Chi (1622-1722) and Ch'ien Lung (1736-1795), as Mokyr notes, 'discouraged the kind of dynamism that was throbbing throughout Europe at that time'. As the next section of this essay will demonstrate, technology is an illustrative example of an Eastern ethos that restricted growth in many aspects of economy and society, an ethos which can be labelled as the most prominent cause for the slowing of East Asian economic progress. E.L. Jones, The Record of Global Economic Development, (Cheltenham, 2002),p 26 Ibid, p. 237 iii- AttitudesOne of the greatest areas where attitude constricted eastern development was trade. Interest in long-distance trade had prevailed during the late twelfth and early thirteenth centuries, but this was not a lasting feature of Asian history- after 1431-33 there were no more economic or diplomatic voyages made by China during the early modern period. The East's loss of interest in the West was the opposite of European attitude; in 1498 Vasco de Gama discovered a sea trade route between Portugal and India, the first Portuguese ships reached China by 1514, and the Spanish had occupied the Philippines by 1565. Whilst the East had retreated, the West had attempted to encroach upon their land and exploit newfound areas for luxury goods. Asia's loss of interest in foreign voyages was symptomatic of their attitude towards burgeoning foreign trade, for they lacked demand for foreign goods. Europe, on the other hand, sought many products, importing tea, porcelain, lacquerware, spices and sugar from the East. Between the beginning of the eighteenth century and 1800 Britain's import of tea increased from 400,000 lb per annum to 23,000,000 lb per annum, and import duties on Chinese tea provided the British government with a tenth of their total revenue. As Western products did not hold the same appeal in Asia, the main attraction for international trade was the acquisition of silver, particularly in China. China, though, did not necessarily have to engage in European trade as there was a great demand for silks in Japan, which had a large supply of precious metals. It is worth noting, however, that in some instances European merchants acted as a middleman between China and Japan. Also, the Chinese accumulation of silver could have been utilised more effectively, for it was traded as bullion rather than being minted, which meant that the unit of account varied between trades and regions. The great interest in Chinese products led to the commodity economy became monetised, but the government failed to fully exploit the potential for economic growth. Whilst competition between Spain, Portugal, England and the Netherlands drove economic and territorial expansion in the West, East Asian policies began to restrict the dividends of European interest. In China, as Fairbank continues, 'mercantile interests were felt to be inherently in conflict with those of society and the state and had to be curbed as far as possible'. Sea voyages were restricted to small vessels and the Ming dynasty (1368-1644) banned private trade abroad. Similarly, Japan only operated with go-shuinsen ships, official trading vessels, and merchants required the approval of the Shogun. After 1639 they extended this attitude to a Jakoku or 'closed country' policy; The Japanese could not leave the country, no catholic country could enter, and foreign trade had to be conducted via Nagasaki. However, C.A. Bayly notes that the early modern period 'saw the rise of the European chartered companies...in the Asian world', demonstrating how the west was gaining both territorial and economic power. China and Japan may have proved difficult to infiltrate, but the lack of competition that they provided allowed Europe to maintain acquisitions like the Spanish occupation of the Philippines, and capitalise upon available resources such as control of the Indian textiles market. The Western exploitative attitude towards trade allowed them to gain products that improved their wealth and economic progress, whereas East Asia had little to gain in return except silver, which it did not mint to create commercial unity or ease of transaction. John King Fairbank, China: A New History, (1992, London), p. 135 Christopher Bayly, The Birth of the Modern World, 1780-1914: Global Connections and Comparisons, (Oxford, 2004), p. 44 As well as expanding eastwards, Europe also gained an extremely valuable advantage above Asia through Columbus' discovery of the Americas in 1492. Acquisition of the New World supplied the West with new food products, a territorial outlet for an expanding population, and resources such as coal. In terms of food, the Americas provided protein-rich fish from the Caribbean and alternative staples to wheat such as maize and potatoes. The Spanish introduced cattle to Latin America, which meant that Europe was able to import beef as well. Asia did adopt maize, potatoes, sweet potatoes and peanuts from the New World into their diet, which increased the land under cultivation by thirty five percent between 1660 and 1760, as these new products could be grown in drier conditions than rice. However, these lands were a lower yield than the paddy fields, and deforestation to plant the new crops caused soil erosion and flooding into the low lying, fertile areas. As Bayly concludes, the New World was of most benefit to the west because it provided 'new, nutritious varieties of food...(which) spread across the Old World in the wake of the Spanish and Portuguese ""discoveries"", improving fertility and resistance to disease'. Better diet enabled a stronger workforce, conducive to increasing production. Bayly, The Birth of the Modern World, p. 49 The New World was also an important asset in terms of population; migration relieved the pressure of demographic growth, and colonisation allowed the exploitation of indigenous people and imported African slaves in order to provide cheap labour. This was particularly useful for the extraction of one of America's most useful resources to the west: coal. Kenneth Pomeranz, in particular, stresses the importance of coal to Europe, as the transition from a timber to coal fuelled society 'relaxed the land constraint in a more fundamental way than any other innovation before turn-of-the-century chemicals and electricity'. Although Pomeranz disputes that divergence occurred before 1800, Europe definitely exploited American resources from the moment of discovery. East Asia had coalmines but they were not as accessible as those in the New World; China's best coal deposits were in Shaanxi, 700 miles from the country's main area of production the Yangzi Delta. Parthasarathi concludes that 'the massive use of coal certainly made possible an energy economy that far surpassed one based on timber and made possible huge iron output, the steam engine and eventually railways', thus providing Europe with the physical resources it needed to power the inventions of the Industrial Revolution. The Americas gave the West nutrients, fuel and territory that it could exploit and command, altogether creating the potential for an energised society capable of increased economic production and therefore greater prosperity. Whilst Pomeranz's conclusion stressing the cruciality of coal and colonies is both overzealous and too late in history to explain economic divergence by 1800, the New World provides a good example of how Western attitude to trade and exploitation helped them to progress ahead of the East. Kenneth Pomeranz, 'Political Economy and Ecology on the Eve of Industrialization: Europe, China and the Global Conjuncture', American Historical Review, 107 (2002), p. 441 Prasannan Parthasarathi, 'Review Article: The Great Divergence', Past and Present, 176 (2002), p. 282 The backward attitude of the East extended further than their approach to trade; Asian government and state policies were centred on hierarchy and repressive central control. In China, as Fairbank notes, the government: 'operated vertically, from the state upon the individual, more than it did horizontally, to resolve conflicts between one individual and another'. This made deviation very difficult, especially since the law was viewed as ultimate in sanctioning behaviour, with legislation predominantly concerning public activities. As Stephen Haw notes, 'the courts were intended to be terrifying places, which good citizens would avoid at almost all costs' and therefore society felt compelled to side with the interests of the state. The later Ming dynasty advocated repressive rule as they sought a controllable environment, and the successive Qing rule extended Ming foundations by ceasing all public services. Without a formal police force, commercial law or road maintenance, China was denied some essential elements for economic development. This was further exacerbated by the fact that the local governments under central Qing rule were somewhat ineffective, for the population of around two hundred million was divided into only eighteen provinces. An imperial bureaucracy based upon centralised control left little capacity for the accumulation of private profit. Similarly, Japanese politics was based on a philosophy of absolute government. Mason and Caiger observe that the Shogun held 'a right to command which could extend, if need be, to each man, woman or child in the country', and the Tokugawa administration (1600-1868) was underpinned with regulations and restrictions for important social groups; for instance, after 1615 nobles were not allowed to leave their ancestral cities. This detailed level of control fostered an environment in which obedience was key. Across East Asia the state 'oversaw, regulated and repressed' and whilst this did not necessarily mean the absence of autonomy, it did entail the desires of the state being valued above personal ambition. Fairbank, China, p. 184 Stephen Haw, China: A Cultural History, (London, 1990), p. 123 R.H.P. Mason and J.G. Caiger, A History of Japan, (Tokyo, 1973), p. 158 Landes, The Wealth and Poverty of Nations, p. 35 Europe, on the other hand, possessed an advantage in its state control: it was politically fragmented. Differences in governments created competition between countries, which led to the desire for economic superiority and the emergence of city-states. As Van Zanden observes, only in Europe did city-states 'manage to dominate the socio-political and economic life of large regions', and this meant that mercantile interests were of greater concern to European governments than they were in Asia. Many merchants believed in the phrase stadtloft macht frei (city air makes one free), and governments benefited as urban migration created a new workforce to fuel the growing manufacturing industries. England and the Netherlands- perhaps the most advanced European states during this period- established an equilibrium between central authority and commercial interest groups, providing them with the ability 'to mobilise resources on an unprecedented scale'. The mutual benefits of compromise between government and mercantile interests influenced European legislation, for both English common law and European Roman law protected property rights. This removed some of the vulnerability of entrepreneurial nous and thus encouraged economic development. Such an attitude was enhanced by financial changes; In East Asian government monetary matters were controlled by individual rulers, whereas European financial institutions were independent both of government whims and the fortunes of individual merchants. The Bank of England provided independent checks on the state of the economy and concept of partnership came into fruition with the Dutch pioneering of a joint-stock company. As Van Zanden concludes 'social and political relationships in Western Europe appear to have been less hierarchical than in large parts of Asia' and this enabled mercantile interests to prosper. Repressive government in an increasingly global economy was not conducive to economic progress. Jan Luiten Van Zanden, 'The Great Convergence from a West-European Perspective, Some Thoughts and Hypotheses', Itinerario, xxiv (2000), p. 15  Ibid, p. 18 Van Zanden, 'The Great Convergence from a West-European Perspective', Itinerario, xxiv, p. 21 Attitude affected the economic progress of East and West on more levels than political policies, for Europe and the Orient possessed very different mentalities on the whole. Whilst the West was dominated by Christianity, which provided moral reason and fuelled war, competition and change, the East was more secular, being dictated by Confucianism and natural philosophy. These differing mentalities contributed to their economic divergence. Religious prevalence, in particular, demonstrates the importance of attitude; in Europe, the philosophical separation of Church and State gave scope for free economic thought. Although faith played a great part in the monarchy, it supported rather than oppressed mercantilism. Max Weber claimed that Protestant ethic encouraged men to apply themselves rationally to their work, thus promoting economic progress. Also, Europe experienced religious wars during the early modern period, such as disputes between Catholics and Protestants in France from 1562-98. Conflicts of faith and the emergence of groups such as Lutheranism encouraged individual thought within religion, an ideology that could be extended to all aspects of life. In Asia, however, there was no established faith. As Bayly notes 'in Muslim and Asian societies a broad recognition of the supremacy of the emperor's cult, not uniformity of belief, was what was required' and therefore there was no separation of the Church; the state was all-powerful. Both the Chinese and Japanese states were highly influenced by the ideology of Confucianism, though this was never established as a religion. Mason and Caiger agree that Tokugawa policies were also Confucian in nature. Confucianism stressed love within the family (Hsiao) and loyalty to the state (Chung), which meant that businesses were kept within the family unit rather than sharing mercantile interests like Europe, and government attitude to politics and trade was respected rather than disputed. As Landes claims, the hierarchy that such an ideology impressed upon East Asia meant that Confucianism produced 'a culturally and intellectually homeostatic society'. This stagnation produced a desire for the east to maintain the status quo they had achieved, so major changes such as foreign inventions and trade were viewed with fear rather than interest. As Fairbank notes, China tried to extend economic possibilities with expansion of old systems rather than creating new ones: 'the tradition in China had been not to build a better mousetrap but to get the official mouse monopoly'. Bayly, The Birth of the Modern World, p. 32 Landes, The Wealth and Poverty of Nations, p. 38 Fairbank, China, p. 181 The East was hindered further by other cultural influences such as the role of women in society. Early modern Europe experienced the growth of cottage industry through utilising the skills of women. They were able to spin and weave from their homes and the sale of their products supplemented family income, allowing an improved living standard. Asia, as aforementioned, encouraged early marriage followed by many children; Confucianism not only restricted growth in terms of foreign interaction, it restricted their use of human resources where Europe employed initiative to further their progress. Also, China never developed a system of formal logic. This has obvious organisational repercussions, but it is also symptomatic of East Asian culture as a whole. The West was cultivating an atmosphere of change, innovation and progress, whilst the East strived to reinforce structure, control, and remain somewhat introverted. As Landes concludes, 'if we learn anything from the history of economic development, it is that culture makes all the difference' and the contradictory mentalities of the two continents goes a long way in explaining the divergence that occurred between them during the early modern period. Landes, The Wealth and Poverty of Nations, p. 516 ConclusionWith the benefit of hindsight it could be easy to view the slow progress of the East Asian economy as inevitable; the early modern period was an era in which trade networks were cemented across the world, where new continents were discovered and where new resources and techniques created the potential for both industrialisation and the expansion of consumerism. European religious and political beliefs formed an ethos of utilisation, competition and conscientiousness, all favourable for embracing these changes and thus encouraging economic growth. The West changed its diet to incorporate food from the New World, modified decorum to embrace Chinese porcelain, and sought fresh energy resources such as coal in order to increase their prosperity. On the other hand, East Asia had the advantage at the beginning of the period: It had overcome environmental problems such as water distribution with irrigation systems, and experimented with technological developments to create an accomplished agricultural system and efficient methods of chinaware production. However, this supremacy led to an environment in which foreign interaction was viewed with hostility. Instead of exploiting the full potential of European trade, they resisted it; instead of adopting western technology, they ignored it; instead of allowing the flourish of mercantile interests, they repressed them. In the move towards modernisation, the West cultivated the seeds of capitalism whereas the East retained Confucianism, and in an increasingly material society, Europe fared better. It would be erroneous to suggest that the East Asian economy remained stagnant by 1800, for it did change and develop, but the inherent mentality of state control imposed limitations upon the extent to which new thought and direction could emerge. As Bayly concludes 'Europe's...""exceptionalism"" was to be found not in one fact, but in an...accumulation of many characteristics seen separately in other parts of the world', characteristics that the West learned from their encounters and promoted within Europe. Equally, East Asia's slow progress can be seen as an accumulation of characteristics- although these were predominantly from within their society- that led to a situation in which the East became economically inferior to Europe. Bayly, The Birth of the Modern World, p. 71",False
62,"The Black Death of 1347 to 1352 and the subsequent outbreaks of the plague were 'the most devastating natural disasters ever to strike Europe'. Although it is impossible to calculate the exact death toll of the Black Death, as there is limited evidence, 'to maintain that one European in three died... cannot be wildly far from the truth'. As there were a number of social, economic and political changes that occurred around that time, the Black Death can be viewed as a turning point in the history of Europe; F. A. Gasquet believed it to have formed 'the real close of the Medieval period and the beginning of our Modern age'. On the other hand, there are worries that too great a significance has been placed on the impact of the Black Death. Philip Ziegler, for example, argues that 'in the long run things would have followed the same course, even though there had never been a plague'. Herlihy, David, The Black Death and the Transformation of the West, ed. Samuel K. Cohn, Jr. (Cambridge, Mass., 1997) p.17. Ziegler, Philip, The Black Death, 2nd ed. (Harmondsworth, 1998) p.231. Gasquet, F. A, Great Pestilence, (London, 1893) p.xvi. Ziegler, The Black Death, p.269. There were a number of economic changes that occurred as a result of the Black Death. As Ziegler observes ' one third of a country's population cannot be eliminated... without considerable dislocation to its economy and its social structure'. The initial response of many was to flee from the plague. Others preferred to indulge in life's pleasures rather than work. This, along with the high death toll, meant that posts went unfilled and services unperformed. On the other hand, more jobs were created, both in towns and in agriculture. For example, gravediggers and physicians were in greater demand than before. Also, there were more jobs available on the land; where before there had been an excess of labour, there was now a shortage. This had a major effect on the established system as 'labour began... to understand its value and assert its power'. Workers could demand higher wages and better conditions because, if the landlord refused, they could easily find work elsewhere. Ziegler, The Black Death, p.232. Horrox, Rosemary (ed and trans. ), The Black Death (Manchester, 1994) p.229. Thorold Rogers suggests that the ' Black Death was a stimulus towards... the disintegration of the manorial system'. Villeins began to feel that their positions were unfair as they could see the high wages of those not bound to their lord by obligation. The 'scales were... tipped against the land owner'. Laws such as the Statute of Labourers were passed to protect the landlord by checking increased wages and the free movement of labour. These laws were largely ignored. Landlords also tried to maintain power by stepping up the level of fines in their courts. As a result, 'relations... became more confrontational' between the workers and the landowners. In this way, it can be argued that the Black Death caused the Peasants' Revolt of 1381. As Ziegler maintains, 'if there had been no Black Death, tension and bitterness would never have risen by 1381 to the level that it did'. The situation after the outbreak of the plague highlighted existing grievances of the peasants and showed up the flaws in the existing system. Ziegler, The Black Death, p.247. Ziegler, The Black Death, p.239. Horrox, The Black Death, p.239. Ziegler, The Black Death, p.250. Although the peasants revolted against the landowners in 1381, there was an increase in the standard of living of many as a result of the Black Death. The increased availability of jobs and higher wages meant that workers had more money to spend on luxury goods. For example, a ploughman in Cuxham was earning around 2s before the Black Death but this increased to around 10s 6d afterwards. David Herlihy comments on the fact that silk became more sought after than wool, 'indicating smaller, but richer markets'. There was also an increase in the brewing of beer, which 'indicates an improved standard of living'. People had better diets than before because there was an increase in the production of meat and dairy produce. With less of a demand for food, land that had previously been used for crops was freed for use as pasturage or forests. The economy had become more diversified as a result of the Black Death. Ziegler, The Black Death, p.243. Herlihy, The Black Death, p.48. Herlihy, The Black Death, p.47. The time after the Black Death was ' a period of impressive technological achievement'. As the population had decreased, labour costs were high which meant that great rewards were given to inventors of labour-saving devices. Also, with less demand for food there was more room for innovation and experimentation. The depression of the fifteenth century led to the development of the labour-saving, light, one-slit plough. Away from agriculture, there were advancements in other areas such as the invention of Johann Gutenberg's printing press in 1453. This replaced the large teams of monastic copyists whose numbers were decreased significantly during the Black Death. Herlihy, The Black Death, p.49. A major effect of the plague was that it 'undermined confidence in the Church's spiritual leadership'. It was observed that priests were just as likely to die of the plague than anybody else. As the plague was widely held to be a punishment from God for the sins of man, it was suggested that the Church was as corrupt as the laity. People felt let down by the Church, which they felt should have given them some warning about God's wrath rather than waiting till afterwards to condemn their wrongdoings. The depleting numbers of priests put a strain on the Church. They were unable to deal effectively with revolts or protest movements. There was a rush to replenish the priesthood. The usual rules of ordination were discarded, resulting in unsuitable candidates being accepted. The new recruits were less spiritual and not as well educated as former priests. They were not prepared for positions of responsibility and were ill equipped to give lead to a straying flock. Although the Black Death did not cause the Reformation, it created a state of mind in which doctrines were more easily doubted. The Church's 'unquestioned authority... was never to be recovered'. Herlihy, The Black Death,p.66. Ziegler, The Black Death, p.270. Paradoxically, at the same time as the Church's authority was being undermined, there was a 'growth of religious fervour'. Pope Clement VI declared a year of jubilee in 1350 and many churches were built such as the Cathedral of Milan. There was an increase in the number of chantry chapels, which weakened the position of the monasteries. Ziegler asserts that the Black Death was the 'most important element in... [the monasteries] decline'. There was an increase in lay piety instead of people turning to the established Church. The Flagellants were a group that emerged as a result of the Black Death. They would punish themselves for the sins of man by sleeping outdoors and whipping each other in public in the hope that God would take away his punishment of the plague. Groups like the Flagellants succeeded in 'supplanting the clergy in the role of intermediaries between heaven and earth'. Ziegler, The Black Death, p.267. Ziegler, The Black Death, p.266. Herlihy, The Black Death,p.68. The psychological effect of the Black Death on the people was great. This is reflected in the artwork of the time, which shows an increased preoccupation with death. The motif of Three Living and Three Dead was common in pre-plague painting but became more plentiful and more shocking afterwards. The dead warn the living: ""As you are now, so once were we. As we are now, so shall ye be."" People were constantly reminded of their mortality. They were scared and wanted somebody to blame. For many this blame fell onto the Jews. There had always been tense relations with Jews in Europe but things intensified during the plague years. For example, in Strasbourg on the fourteenth of February 1349, nine hundred Jews were burned. Platt, Colin, King Death: The Black Death and its Aftermath in Late Medieval England (London, 1996) p.151. Herlihy, The Black Death, p.66 Although many changes appear to have happened as a result of the Black Death, its true impact may have been exaggerated. Michan claimed that 'the plague of the fourteenth century was no different to those which preceded or followed it'. In other words, it was not the nature of the Black Death that caused such great changes in Europe, but the 'man-made social factors' that were present at the time. By the middle of the thirteenth century, Europe was becoming 'uncomfortably crowded'. Villages expanded into towns and cities of ten or twenty thousand inhabitants. This overpopulation led to poor living standards, over-extended planting of wheat and a fall of productivity as the goodness was drained from the soil. Moreover, in the early fourteenth century there was a drop of around one degree Celsius in the temperature, resulting in a succession of wet summers and a loss of suitable farmland, as higher ground was too cold. This led to a series of famines; between 1315 and 1319 almost every country in Europe lost at least one harvest. There was a population decline of around ten per cent. Ziegler advocates that 'before the Black Death, therefore, most of Europe was in a in recession'. The fairs of Champaign, which were a sign of commercial success, significantly declined. The Black Death merely speeded up this degeneration. The Black Death can be viewed as a 'Malthusian Check' on the population of Europe that was necessary, and had such a great effect, because of the conditions of the time. Michon quoted in Ziegler, The Black Death, p.30. Herlihy, The Black Death, p.3. Ziegler, The Black Death, p.31. Ziegler, The Black Death, p.33. A number of the effects of the Black Death were only short term and evened out after a few years. For example, inflation of agricultural produce after the Black Death subsided and prices regained their previous levels. Wages also fell in a few years back to more usual levels. Although there were a number of technological advancements after the Black Death, many of the major breakthroughs of the time came before the plague such as windmills and complex field systems in agriculture and developments in wool and silk manufacture. Gutenberg's printing press was an advancement that came as a result of the population shortage after the Black Death; however, printing did not really become popular until the 1470s when the population was increasing. Also, the most important centers of printing were Venice, Rome and the Southern German cities where there was the fastest demographic growth. This suggests that the Black Death did not have a great impact on the technological advancements of the time. Ziegler, The Black Death, p.248. Herlihy, The Black Death, p.12. Historians examining the Medieval period had previously over looked the Black Death. Henry only wrote fourteen lines on the subject in a study of twelve volumes. This indicates that the Black Death was not considered to have had a great impact on the history of Europe. Other explanations have been put forward for the decrease in population and the subsequent economic, social and political changes. Colin Platt suggests that 'it was famine not plague, after 1349, that remained the biggest killer' in more northern countries. He also comments on the fact that later marriages and families with fewer children was becoming normal practice and that this kept the population down more than the effects of the plague. As there is limited evidence from the period, it is impossible to discern exactly what was killing off so much of the population. Undeniably the plague was present, but its effects may have been exaggerated. Platt claims that 'more monks undoubtedly died of over-weight or of liver conditions than ever succumbed to the plague'. Graham Twigg, a zoologist, further suggests that the spread of anthrax was the predominant cause of death in 1348. Herlihy examined the Acta Sanctorum, which contained accounts of supposed plague victims. He found that freckles, which were more common to other diseases such as anthrax, not buboes, were the most common symptom described. Platt, King Death, p.190. Platt, King Death, p190. Herlihy, The Black Death, p.7. Herlihy, The Black Death, p.7. There have been a number of different interpretations on how significant an impact the Black Death had on the history of Europe. The population and commercialization of Europe was already in decline and 'continued deterioration... would have been likely, even if... [the Black Death] had never occurred'. It was a catalyst for events such as the Peasant's Revolt, the Reformation and the fall of manorialism, but was not a direct cause. It appears that 'the Black Death did not initiate any major social or economic trends but it accelerated and modified- sometimes drastically- those which already existed'. Ziegler, The Black Death, p.235. Ziegler, The Black Death, p.250.","Charles Darwin provided the first explanation for the means by which evolution takes place. This was by the process of natural selection. A key aspect of Darwin's ideas was that species were not immutable, that they could change in their essential characteristics. It had been a common belief that species were immutable because of the writings in the Bible. Genesis mentioned the species that God had created and it was assumed that only God could create new species. Darwin was worried about the implications of his ideas. In January 1844 he wrote to J. D. Hooker claiming that it was 'like confessing a murder' to suggest that species were not immutable. However, many of Darwin's ideas had been in discussion by earlier natural historians and Romantic thinkers in the decades before he published his Origin of Species by means of Natural Selection in 1859. Romanticism began in the mid-eighteenth century and reached its height in the nineteenth century. The Romantics questioned old beliefs, requiring proof before they believed in something. They would study nature closely and believed that man was closely linked with the natural world. In this way Darwin was also a Romantic thinker. Nichols, Ashton, 'The Anxiety of Species: Toward a Romantic Natural History', Romantic Natural History, 2000,  URL  (21 December 2003). Darwin's theory of evolution involves natural selection, which comes as a result of 'a severe struggle for life'. Those animals with characteristics better adapted to their environment will survive to breed and pass on those characteristics while the less well adapted will die off; 'nature was seen as discarding as well as developing various forms of life'. H. Honour suggests that Darwin's theory of evolution was 'based on a distinctly Romantic notion of conflict'. The Romantics rejected classical notions of harmony and order, instead celebrating the wild and chaotic aspects of nature. Darwin, Charles, On the Origin of Species. A Facsimile of the First Edition, with an introduction by Ernst Mayr, (Cambridge, 1976) p.126. Williams, Raymond, Keywords, (Glasgow, 1983) p.121. Honour, H, Romanticism, (London, 1979) p.311. Instead of merely accepting old beliefs, Romantics looked for other answers. They looked to nature as a tool for learning instead of books written by the ancients. Wordsworth takes this view in his poem ""The Tables Turned"". He writes: 'Books! 'tis a dull and endless strife: / ... Let Nature be your teacher'. Darwin adopted this philosophy, studying nature closely. This is how he arrived at his theory of evolution. While employed as a naturalist on the H.M.S. Beagle, Darwin observed the wildlife on the Galapagos Islands. He noticed that there were thirteen species of finch, all with slight variations. He saw that the finches had different shapes and sizes of beaks depending on what their food source was. Those finches eating insects and small seeds had beaks that were small and thin, ideal for catching insects and picking up small seeds. Those finches eating large seeds, on the other hand, had large, short and wide beaks that were capable of crushing the large seeds. These observations of the natural world helped Darwin come up with the idea of natural selection. The finches with beaks better adapted to the food supply were more likely to survive and pass on the same characteristics to successive generations. Wordsworth, William, 'The Tables Turned', Representative Poetry Online,  URL  (21 December 2003) Darwin's grandfather, Erasmus Darwin, had ideas on the mutability and evolution of species before his grandson. In The Temple of Nature in 1803, Erasmus writes: '... as successive generations bloom, / New powers acquire, and larger limbs assume'. Erasmus' ideas on evolution and the natural world 'powerfully influenced' the Romantics. Samuel Taylor Coleridge complimented him as having 'perhaps, a greater range of knowledge than any other man in Europe'. Mary Shelley referred to the 'experiments of Dr. Darwin' in the introduction to the 1831 edition of Frankenstein. Mary Shelley was influenced by Erasmus' ideas that new species can be created. Victor Frankenstein, in the opening pages of the novel, says to Captain Walton: 'A new species would bless me as its creator'. She is questioning the old beliefs that God was the only creator of species. Darwin, Erasmus, The Temple of Nature, (London, 1803) quoted in 'The Anxiety of Species'. Nichols, 'The Anxiety of Species'. Nichols, Ashton, 'Erasmus Darwin', Romantic Natural History, 2000,  URL  (21 December 2003) Shelley, Mary, Frankenstein, (London, 1988) p.12. Shelley, Frankenstein, p.54. The Romantics believed that if you understood nature, you would attain a greater understanding of yourself. 'Romantic poets and writers were hinting at the biological connectedness of all living things'; for example, Coleridge spoke of the 'one life within us and abroad' in ""The Eolian Harp"". It was believed that Man, the animals and all of inanimate nature were linked. This went against the ancient idea of the Great Chain of Being where man was above nature in the hierarchy. In the Romantic view of nature 'animals were regarded not so much as sub-human as other-than-human'. The natural world was not as ordered as was once believed. Darwin's theory of evolution concurs with this line of thinking. He asserted that 'man and the lower animals do not differ in kind'. J. Howard notes how 'Darwin... used human behaviour to illustrate aspects of animal behaviour', further suggesting their link. The Romantics also attributed human characteristics and emotions to nature. William Wordsworth, in ""Lines Written in Early Spring"", humanised nature when he mused that 'every flower / Enjoys the air it breathes', and John Keats personified a nightingale in ""Ode to a Nightingale"" by giving it human feelings with its 'soul abroad / In such an ecstasy'. William Blake also 'hints at a link between human and nonhuman nature'. In ""The Human Abstract"" he writes: Nichols, 'The Anxiety of Species'. Coleridge, Samuel. Taylor, 'The Eolian Harp' quoted in Nichols, 'The Anxiety of Species'. Honour, Romanticism, p.311. Darwin, Charles, The Descent of Man and Selection in Relation to Sex, vol. I, (London, 1871) p.186 quoted in Howard, J, Darwin: A Very Short Introduction, (New York, 1982) p.76. Howard, J, Darwin: A Very Short Introduction, (New York, 1982) p.75. Wordsworth, William, 'Lines Written in Early Spring', Lyrical Ballads, (London, 1798) p. Keats, John, 'Ode to a Nightingale', Odes of 1819 by John Keats,  URL  (21 December 2003) Nichols, 'The Anxiety of Species'. The Gods of the earth and sea Sought thro' Nature to find this Tree, But their search was all in vain: There grows one in the Human Brain.Blake, William, 'The Human Abstract' quoted in Nichols, 'The Anxiety of Species'. The metaphorical tree in the human brain unites man to inanimate nature. This relates to Darwin's theory that Man evolved from more primitive forms of nature. After looking at all the evidence for evolution, Darwin speculated that 'probably all the organic beings which have ever lived on this earth have descended from some one primordial form'. This concept has been alluded to in the writings of some of the Romantics. For example, in his ""Ode to the West Wind"", Percy Shelley writes: 'The sapless foliage of the ocean, know / Thy voice, and suddenly grow grey, with fear'. The plants of the sea respond in the same way to the changing seasons brought on by the west wind as the plants on the land. This is a 'typically Romantic image of sympathetic interactions across... boundaries'. It is implied that sea plants and land plants behave in similar ways because they have a common origin. The Romantics were not evolutionary thinkers, but their view of nature was similar to Darwin's in the way that they could see 'one grand natural system' instead of individual species with no interaction or variation. Darwin, On the Origin of Species, p.484. Shelley, Percy, 'Ode to the West Wind', quoted in Nichols, 'The Anxiety of Species'. Nichols, 'The Anxiety of Species'. Darwin, On the Origin of Species, p.329. Some of the Romantics believed that understanding nature and becoming one with the natural world would bring them closer to the divine. Lord Alfred Tennyson, for example, in a poem entitled ""Flower in the Crannied Wall"" writes: Little Flower - but if I could understand What you are, root and all, and all in all, I should know what God and man is. Tennyson, Lord Alfred, 'Flower in the Crannied Wall'  URL  Wall.html (21 December 2003) They believed that all of nature was divinely ordered and they thought that evolution could have been part of God's plan. This is where Darwin's theory countered this Romantic view. In his Origin of Species he explains how natural selection does not proliferate characteristics that are beneficial to another species, claiming that he 'would almost as soon believe that the cat curls the end of its tail when preparing to spring, in order to warn the doomed mouse'. Natural selection is selfish. It does nothing for the good of another species. For Darwin, this detail 'discriminated between providential creation and evolution by natural selection'. Also, the randomness of natural selection with chance mutations and the struggle for survival 'sounds more like a game of genetic Russian Roulette that the kind of mechanism that would have been instituted by a wise and benevolent Creator'. Darwin describes evolution in a purely naturalistic manner, removing the supernatural element. In explaining the world 'the naturalist has abolished... God'. However, not all Romantic thinkers still attributed the workings of nature to God. Keats' ode ""To Autumn"", Darwin, On the Origin of Species, p.201. Howard, Darwin, p.29. 'From Darwin to Modern Darwinism: An Overview', p.6. Williams, Keywords, p.217. Season of mists and mellow fruitfulness, Close bosom-friend of the maturing sun; Conspiring with him how to load and bless With fruit the vines that round the thatch-eves run, Keats, John, 'To Autumn' Odes of 1819 by John Keats,  URL  (21 December 2003) contains no allusions to the divine. Nature controls it's own processes. 'Keats's ""nature"", like Erasmus [and Charles] Darwin's, exists without need for, or appeal to, any form of ""super nature"".' Nichols, 'The Anxiety of Species'. 'In the century before Charles Darwin, a wide range of scientists and writers saw human beings as organisms with important connections to their environment'. This included the Romantics. Darwin's theory of evolution did not counter the Romantic view of nature. There were many similarities between Darwin's ideas and those of the Romantics. They could see that nature was not static and unchanging, but that it was a dynamic system that ever changing. The Romantics had a sense that Man was connected to the natural world. Darwin took this idea a step further in his theory of evolution. Although Darwin's purely naturalistic and secular theory opposed the views of some of the Romantic thinkers such as Tennyson who believed that nature was God's creation, certain Romantics like Keats where questioning the divine nature of world. Nichols, 'The Anxiety of Species'.",True
63,"Charles Darwin provided the first explanation for the means by which evolution takes place. This was by the process of natural selection. A key aspect of Darwin's ideas was that species were not immutable, that they could change in their essential characteristics. It had been a common belief that species were immutable because of the writings in the Bible. Genesis mentioned the species that God had created and it was assumed that only God could create new species. Darwin was worried about the implications of his ideas. In January 1844 he wrote to J. D. Hooker claiming that it was 'like confessing a murder' to suggest that species were not immutable. However, many of Darwin's ideas had been in discussion by earlier natural historians and Romantic thinkers in the decades before he published his Origin of Species by means of Natural Selection in 1859. Romanticism began in the mid-eighteenth century and reached its height in the nineteenth century. The Romantics questioned old beliefs, requiring proof before they believed in something. They would study nature closely and believed that man was closely linked with the natural world. In this way Darwin was also a Romantic thinker. Nichols, Ashton, 'The Anxiety of Species: Toward a Romantic Natural History', Romantic Natural History, 2000,  URL  (21 December 2003). Darwin's theory of evolution involves natural selection, which comes as a result of 'a severe struggle for life'. Those animals with characteristics better adapted to their environment will survive to breed and pass on those characteristics while the less well adapted will die off; 'nature was seen as discarding as well as developing various forms of life'. H. Honour suggests that Darwin's theory of evolution was 'based on a distinctly Romantic notion of conflict'. The Romantics rejected classical notions of harmony and order, instead celebrating the wild and chaotic aspects of nature. Darwin, Charles, On the Origin of Species. A Facsimile of the First Edition, with an introduction by Ernst Mayr, (Cambridge, 1976) p.126. Williams, Raymond, Keywords, (Glasgow, 1983) p.121. Honour, H, Romanticism, (London, 1979) p.311. Instead of merely accepting old beliefs, Romantics looked for other answers. They looked to nature as a tool for learning instead of books written by the ancients. Wordsworth takes this view in his poem ""The Tables Turned"". He writes: 'Books! 'tis a dull and endless strife: / ... Let Nature be your teacher'. Darwin adopted this philosophy, studying nature closely. This is how he arrived at his theory of evolution. While employed as a naturalist on the H.M.S. Beagle, Darwin observed the wildlife on the Galapagos Islands. He noticed that there were thirteen species of finch, all with slight variations. He saw that the finches had different shapes and sizes of beaks depending on what their food source was. Those finches eating insects and small seeds had beaks that were small and thin, ideal for catching insects and picking up small seeds. Those finches eating large seeds, on the other hand, had large, short and wide beaks that were capable of crushing the large seeds. These observations of the natural world helped Darwin come up with the idea of natural selection. The finches with beaks better adapted to the food supply were more likely to survive and pass on the same characteristics to successive generations. Wordsworth, William, 'The Tables Turned', Representative Poetry Online,  URL  (21 December 2003) Darwin's grandfather, Erasmus Darwin, had ideas on the mutability and evolution of species before his grandson. In The Temple of Nature in 1803, Erasmus writes: '... as successive generations bloom, / New powers acquire, and larger limbs assume'. Erasmus' ideas on evolution and the natural world 'powerfully influenced' the Romantics. Samuel Taylor Coleridge complimented him as having 'perhaps, a greater range of knowledge than any other man in Europe'. Mary Shelley referred to the 'experiments of Dr. Darwin' in the introduction to the 1831 edition of Frankenstein. Mary Shelley was influenced by Erasmus' ideas that new species can be created. Victor Frankenstein, in the opening pages of the novel, says to Captain Walton: 'A new species would bless me as its creator'. She is questioning the old beliefs that God was the only creator of species. Darwin, Erasmus, The Temple of Nature, (London, 1803) quoted in 'The Anxiety of Species'. Nichols, 'The Anxiety of Species'. Nichols, Ashton, 'Erasmus Darwin', Romantic Natural History, 2000,  URL  (21 December 2003) Shelley, Mary, Frankenstein, (London, 1988) p.12. Shelley, Frankenstein, p.54. The Romantics believed that if you understood nature, you would attain a greater understanding of yourself. 'Romantic poets and writers were hinting at the biological connectedness of all living things'; for example, Coleridge spoke of the 'one life within us and abroad' in ""The Eolian Harp"". It was believed that Man, the animals and all of inanimate nature were linked. This went against the ancient idea of the Great Chain of Being where man was above nature in the hierarchy. In the Romantic view of nature 'animals were regarded not so much as sub-human as other-than-human'. The natural world was not as ordered as was once believed. Darwin's theory of evolution concurs with this line of thinking. He asserted that 'man and the lower animals do not differ in kind'. J. Howard notes how 'Darwin... used human behaviour to illustrate aspects of animal behaviour', further suggesting their link. The Romantics also attributed human characteristics and emotions to nature. William Wordsworth, in ""Lines Written in Early Spring"", humanised nature when he mused that 'every flower / Enjoys the air it breathes', and John Keats personified a nightingale in ""Ode to a Nightingale"" by giving it human feelings with its 'soul abroad / In such an ecstasy'. William Blake also 'hints at a link between human and nonhuman nature'. In ""The Human Abstract"" he writes: Nichols, 'The Anxiety of Species'. Coleridge, Samuel. Taylor, 'The Eolian Harp' quoted in Nichols, 'The Anxiety of Species'. Honour, Romanticism, p.311. Darwin, Charles, The Descent of Man and Selection in Relation to Sex, vol. I, (London, 1871) p.186 quoted in Howard, J, Darwin: A Very Short Introduction, (New York, 1982) p.76. Howard, J, Darwin: A Very Short Introduction, (New York, 1982) p.75. Wordsworth, William, 'Lines Written in Early Spring', Lyrical Ballads, (London, 1798) p. Keats, John, 'Ode to a Nightingale', Odes of 1819 by John Keats,  URL  (21 December 2003) Nichols, 'The Anxiety of Species'. The Gods of the earth and sea Sought thro' Nature to find this Tree, But their search was all in vain: There grows one in the Human Brain.Blake, William, 'The Human Abstract' quoted in Nichols, 'The Anxiety of Species'. The metaphorical tree in the human brain unites man to inanimate nature. This relates to Darwin's theory that Man evolved from more primitive forms of nature. After looking at all the evidence for evolution, Darwin speculated that 'probably all the organic beings which have ever lived on this earth have descended from some one primordial form'. This concept has been alluded to in the writings of some of the Romantics. For example, in his ""Ode to the West Wind"", Percy Shelley writes: 'The sapless foliage of the ocean, know / Thy voice, and suddenly grow grey, with fear'. The plants of the sea respond in the same way to the changing seasons brought on by the west wind as the plants on the land. This is a 'typically Romantic image of sympathetic interactions across... boundaries'. It is implied that sea plants and land plants behave in similar ways because they have a common origin. The Romantics were not evolutionary thinkers, but their view of nature was similar to Darwin's in the way that they could see 'one grand natural system' instead of individual species with no interaction or variation. Darwin, On the Origin of Species, p.484. Shelley, Percy, 'Ode to the West Wind', quoted in Nichols, 'The Anxiety of Species'. Nichols, 'The Anxiety of Species'. Darwin, On the Origin of Species, p.329. Some of the Romantics believed that understanding nature and becoming one with the natural world would bring them closer to the divine. Lord Alfred Tennyson, for example, in a poem entitled ""Flower in the Crannied Wall"" writes: Little Flower - but if I could understand What you are, root and all, and all in all, I should know what God and man is. Tennyson, Lord Alfred, 'Flower in the Crannied Wall'  URL  Wall.html (21 December 2003) They believed that all of nature was divinely ordered and they thought that evolution could have been part of God's plan. This is where Darwin's theory countered this Romantic view. In his Origin of Species he explains how natural selection does not proliferate characteristics that are beneficial to another species, claiming that he 'would almost as soon believe that the cat curls the end of its tail when preparing to spring, in order to warn the doomed mouse'. Natural selection is selfish. It does nothing for the good of another species. For Darwin, this detail 'discriminated between providential creation and evolution by natural selection'. Also, the randomness of natural selection with chance mutations and the struggle for survival 'sounds more like a game of genetic Russian Roulette that the kind of mechanism that would have been instituted by a wise and benevolent Creator'. Darwin describes evolution in a purely naturalistic manner, removing the supernatural element. In explaining the world 'the naturalist has abolished... God'. However, not all Romantic thinkers still attributed the workings of nature to God. Keats' ode ""To Autumn"", Darwin, On the Origin of Species, p.201. Howard, Darwin, p.29. 'From Darwin to Modern Darwinism: An Overview', p.6. Williams, Keywords, p.217. Season of mists and mellow fruitfulness, Close bosom-friend of the maturing sun; Conspiring with him how to load and bless With fruit the vines that round the thatch-eves run, Keats, John, 'To Autumn' Odes of 1819 by John Keats,  URL  (21 December 2003) contains no allusions to the divine. Nature controls it's own processes. 'Keats's ""nature"", like Erasmus [and Charles] Darwin's, exists without need for, or appeal to, any form of ""super nature"".' Nichols, 'The Anxiety of Species'. 'In the century before Charles Darwin, a wide range of scientists and writers saw human beings as organisms with important connections to their environment'. This included the Romantics. Darwin's theory of evolution did not counter the Romantic view of nature. There were many similarities between Darwin's ideas and those of the Romantics. They could see that nature was not static and unchanging, but that it was a dynamic system that ever changing. The Romantics had a sense that Man was connected to the natural world. Darwin took this idea a step further in his theory of evolution. Although Darwin's purely naturalistic and secular theory opposed the views of some of the Romantic thinkers such as Tennyson who believed that nature was God's creation, certain Romantics like Keats where questioning the divine nature of world. Nichols, 'The Anxiety of Species'.","The Black Death of 1347 to 1352 and the subsequent outbreaks of the plague were 'the most devastating natural disasters ever to strike Europe'. Although it is impossible to calculate the exact death toll of the Black Death, as there is limited evidence, 'to maintain that one European in three died... cannot be wildly far from the truth'. As there were a number of social, economic and political changes that occurred around that time, the Black Death can be viewed as a turning point in the history of Europe; F. A. Gasquet believed it to have formed 'the real close of the Medieval period and the beginning of our Modern age'. On the other hand, there are worries that too great a significance has been placed on the impact of the Black Death. Philip Ziegler, for example, argues that 'in the long run things would have followed the same course, even though there had never been a plague'. Herlihy, David, The Black Death and the Transformation of the West, ed. Samuel K. Cohn, Jr. (Cambridge, Mass., 1997) p.17. Ziegler, Philip, The Black Death, 2nd ed. (Harmondsworth, 1998) p.231. Gasquet, F. A, Great Pestilence, (London, 1893) p.xvi. Ziegler, The Black Death, p.269. There were a number of economic changes that occurred as a result of the Black Death. As Ziegler observes ' one third of a country's population cannot be eliminated... without considerable dislocation to its economy and its social structure'. The initial response of many was to flee from the plague. Others preferred to indulge in life's pleasures rather than work. This, along with the high death toll, meant that posts went unfilled and services unperformed. On the other hand, more jobs were created, both in towns and in agriculture. For example, gravediggers and physicians were in greater demand than before. Also, there were more jobs available on the land; where before there had been an excess of labour, there was now a shortage. This had a major effect on the established system as 'labour began... to understand its value and assert its power'. Workers could demand higher wages and better conditions because, if the landlord refused, they could easily find work elsewhere. Ziegler, The Black Death, p.232. Horrox, Rosemary (ed and trans. ), The Black Death (Manchester, 1994) p.229. Thorold Rogers suggests that the ' Black Death was a stimulus towards... the disintegration of the manorial system'. Villeins began to feel that their positions were unfair as they could see the high wages of those not bound to their lord by obligation. The 'scales were... tipped against the land owner'. Laws such as the Statute of Labourers were passed to protect the landlord by checking increased wages and the free movement of labour. These laws were largely ignored. Landlords also tried to maintain power by stepping up the level of fines in their courts. As a result, 'relations... became more confrontational' between the workers and the landowners. In this way, it can be argued that the Black Death caused the Peasants' Revolt of 1381. As Ziegler maintains, 'if there had been no Black Death, tension and bitterness would never have risen by 1381 to the level that it did'. The situation after the outbreak of the plague highlighted existing grievances of the peasants and showed up the flaws in the existing system. Ziegler, The Black Death, p.247. Ziegler, The Black Death, p.239. Horrox, The Black Death, p.239. Ziegler, The Black Death, p.250. Although the peasants revolted against the landowners in 1381, there was an increase in the standard of living of many as a result of the Black Death. The increased availability of jobs and higher wages meant that workers had more money to spend on luxury goods. For example, a ploughman in Cuxham was earning around 2s before the Black Death but this increased to around 10s 6d afterwards. David Herlihy comments on the fact that silk became more sought after than wool, 'indicating smaller, but richer markets'. There was also an increase in the brewing of beer, which 'indicates an improved standard of living'. People had better diets than before because there was an increase in the production of meat and dairy produce. With less of a demand for food, land that had previously been used for crops was freed for use as pasturage or forests. The economy had become more diversified as a result of the Black Death. Ziegler, The Black Death, p.243. Herlihy, The Black Death, p.48. Herlihy, The Black Death, p.47. The time after the Black Death was ' a period of impressive technological achievement'. As the population had decreased, labour costs were high which meant that great rewards were given to inventors of labour-saving devices. Also, with less demand for food there was more room for innovation and experimentation. The depression of the fifteenth century led to the development of the labour-saving, light, one-slit plough. Away from agriculture, there were advancements in other areas such as the invention of Johann Gutenberg's printing press in 1453. This replaced the large teams of monastic copyists whose numbers were decreased significantly during the Black Death. Herlihy, The Black Death, p.49. A major effect of the plague was that it 'undermined confidence in the Church's spiritual leadership'. It was observed that priests were just as likely to die of the plague than anybody else. As the plague was widely held to be a punishment from God for the sins of man, it was suggested that the Church was as corrupt as the laity. People felt let down by the Church, which they felt should have given them some warning about God's wrath rather than waiting till afterwards to condemn their wrongdoings. The depleting numbers of priests put a strain on the Church. They were unable to deal effectively with revolts or protest movements. There was a rush to replenish the priesthood. The usual rules of ordination were discarded, resulting in unsuitable candidates being accepted. The new recruits were less spiritual and not as well educated as former priests. They were not prepared for positions of responsibility and were ill equipped to give lead to a straying flock. Although the Black Death did not cause the Reformation, it created a state of mind in which doctrines were more easily doubted. The Church's 'unquestioned authority... was never to be recovered'. Herlihy, The Black Death,p.66. Ziegler, The Black Death, p.270. Paradoxically, at the same time as the Church's authority was being undermined, there was a 'growth of religious fervour'. Pope Clement VI declared a year of jubilee in 1350 and many churches were built such as the Cathedral of Milan. There was an increase in the number of chantry chapels, which weakened the position of the monasteries. Ziegler asserts that the Black Death was the 'most important element in... [the monasteries] decline'. There was an increase in lay piety instead of people turning to the established Church. The Flagellants were a group that emerged as a result of the Black Death. They would punish themselves for the sins of man by sleeping outdoors and whipping each other in public in the hope that God would take away his punishment of the plague. Groups like the Flagellants succeeded in 'supplanting the clergy in the role of intermediaries between heaven and earth'. Ziegler, The Black Death, p.267. Ziegler, The Black Death, p.266. Herlihy, The Black Death,p.68. The psychological effect of the Black Death on the people was great. This is reflected in the artwork of the time, which shows an increased preoccupation with death. The motif of Three Living and Three Dead was common in pre-plague painting but became more plentiful and more shocking afterwards. The dead warn the living: ""As you are now, so once were we. As we are now, so shall ye be."" People were constantly reminded of their mortality. They were scared and wanted somebody to blame. For many this blame fell onto the Jews. There had always been tense relations with Jews in Europe but things intensified during the plague years. For example, in Strasbourg on the fourteenth of February 1349, nine hundred Jews were burned. Platt, Colin, King Death: The Black Death and its Aftermath in Late Medieval England (London, 1996) p.151. Herlihy, The Black Death, p.66 Although many changes appear to have happened as a result of the Black Death, its true impact may have been exaggerated. Michan claimed that 'the plague of the fourteenth century was no different to those which preceded or followed it'. In other words, it was not the nature of the Black Death that caused such great changes in Europe, but the 'man-made social factors' that were present at the time. By the middle of the thirteenth century, Europe was becoming 'uncomfortably crowded'. Villages expanded into towns and cities of ten or twenty thousand inhabitants. This overpopulation led to poor living standards, over-extended planting of wheat and a fall of productivity as the goodness was drained from the soil. Moreover, in the early fourteenth century there was a drop of around one degree Celsius in the temperature, resulting in a succession of wet summers and a loss of suitable farmland, as higher ground was too cold. This led to a series of famines; between 1315 and 1319 almost every country in Europe lost at least one harvest. There was a population decline of around ten per cent. Ziegler advocates that 'before the Black Death, therefore, most of Europe was in a in recession'. The fairs of Champaign, which were a sign of commercial success, significantly declined. The Black Death merely speeded up this degeneration. The Black Death can be viewed as a 'Malthusian Check' on the population of Europe that was necessary, and had such a great effect, because of the conditions of the time. Michon quoted in Ziegler, The Black Death, p.30. Herlihy, The Black Death, p.3. Ziegler, The Black Death, p.31. Ziegler, The Black Death, p.33. A number of the effects of the Black Death were only short term and evened out after a few years. For example, inflation of agricultural produce after the Black Death subsided and prices regained their previous levels. Wages also fell in a few years back to more usual levels. Although there were a number of technological advancements after the Black Death, many of the major breakthroughs of the time came before the plague such as windmills and complex field systems in agriculture and developments in wool and silk manufacture. Gutenberg's printing press was an advancement that came as a result of the population shortage after the Black Death; however, printing did not really become popular until the 1470s when the population was increasing. Also, the most important centers of printing were Venice, Rome and the Southern German cities where there was the fastest demographic growth. This suggests that the Black Death did not have a great impact on the technological advancements of the time. Ziegler, The Black Death, p.248. Herlihy, The Black Death, p.12. Historians examining the Medieval period had previously over looked the Black Death. Henry only wrote fourteen lines on the subject in a study of twelve volumes. This indicates that the Black Death was not considered to have had a great impact on the history of Europe. Other explanations have been put forward for the decrease in population and the subsequent economic, social and political changes. Colin Platt suggests that 'it was famine not plague, after 1349, that remained the biggest killer' in more northern countries. He also comments on the fact that later marriages and families with fewer children was becoming normal practice and that this kept the population down more than the effects of the plague. As there is limited evidence from the period, it is impossible to discern exactly what was killing off so much of the population. Undeniably the plague was present, but its effects may have been exaggerated. Platt claims that 'more monks undoubtedly died of over-weight or of liver conditions than ever succumbed to the plague'. Graham Twigg, a zoologist, further suggests that the spread of anthrax was the predominant cause of death in 1348. Herlihy examined the Acta Sanctorum, which contained accounts of supposed plague victims. He found that freckles, which were more common to other diseases such as anthrax, not buboes, were the most common symptom described. Platt, King Death, p.190. Platt, King Death, p190. Herlihy, The Black Death, p.7. Herlihy, The Black Death, p.7. There have been a number of different interpretations on how significant an impact the Black Death had on the history of Europe. The population and commercialization of Europe was already in decline and 'continued deterioration... would have been likely, even if... [the Black Death] had never occurred'. It was a catalyst for events such as the Peasant's Revolt, the Reformation and the fall of manorialism, but was not a direct cause. It appears that 'the Black Death did not initiate any major social or economic trends but it accelerated and modified- sometimes drastically- those which already existed'. Ziegler, The Black Death, p.235. Ziegler, The Black Death, p.250.",False
64,"Charles Darwin provided the first explanation for the means by which evolution takes place. This was by the process of natural selection. A key aspect of Darwin's ideas was that species were not immutable, that they could change in their essential characteristics. It had been a common belief that species were immutable because of the writings in the Bible. Genesis mentioned the species that God had created and it was assumed that only God could create new species. Darwin was worried about the implications of his ideas. In January 1844 he wrote to J. D. Hooker claiming that it was 'like confessing a murder' to suggest that species were not immutable. However, many of Darwin's ideas had been in discussion by earlier natural historians and Romantic thinkers in the decades before he published his Origin of Species by means of Natural Selection in 1859. Romanticism began in the mid-eighteenth century and reached its height in the nineteenth century. The Romantics questioned old beliefs, requiring proof before they believed in something. They would study nature closely and believed that man was closely linked with the natural world. In this way Darwin was also a Romantic thinker. Nichols, Ashton, 'The Anxiety of Species: Toward a Romantic Natural History', Romantic Natural History, 2000,  URL  (21 December 2003). Darwin's theory of evolution involves natural selection, which comes as a result of 'a severe struggle for life'. Those animals with characteristics better adapted to their environment will survive to breed and pass on those characteristics while the less well adapted will die off; 'nature was seen as discarding as well as developing various forms of life'. H. Honour suggests that Darwin's theory of evolution was 'based on a distinctly Romantic notion of conflict'. The Romantics rejected classical notions of harmony and order, instead celebrating the wild and chaotic aspects of nature. Darwin, Charles, On the Origin of Species. A Facsimile of the First Edition, with an introduction by Ernst Mayr, (Cambridge, 1976) p.126. Williams, Raymond, Keywords, (Glasgow, 1983) p.121. Honour, H, Romanticism, (London, 1979) p.311. Instead of merely accepting old beliefs, Romantics looked for other answers. They looked to nature as a tool for learning instead of books written by the ancients. Wordsworth takes this view in his poem ""The Tables Turned"". He writes: 'Books! 'tis a dull and endless strife: / ... Let Nature be your teacher'. Darwin adopted this philosophy, studying nature closely. This is how he arrived at his theory of evolution. While employed as a naturalist on the H.M.S. Beagle, Darwin observed the wildlife on the Galapagos Islands. He noticed that there were thirteen species of finch, all with slight variations. He saw that the finches had different shapes and sizes of beaks depending on what their food source was. Those finches eating insects and small seeds had beaks that were small and thin, ideal for catching insects and picking up small seeds. Those finches eating large seeds, on the other hand, had large, short and wide beaks that were capable of crushing the large seeds. These observations of the natural world helped Darwin come up with the idea of natural selection. The finches with beaks better adapted to the food supply were more likely to survive and pass on the same characteristics to successive generations. Wordsworth, William, 'The Tables Turned', Representative Poetry Online,  URL  (21 December 2003) Darwin's grandfather, Erasmus Darwin, had ideas on the mutability and evolution of species before his grandson. In The Temple of Nature in 1803, Erasmus writes: '... as successive generations bloom, / New powers acquire, and larger limbs assume'. Erasmus' ideas on evolution and the natural world 'powerfully influenced' the Romantics. Samuel Taylor Coleridge complimented him as having 'perhaps, a greater range of knowledge than any other man in Europe'. Mary Shelley referred to the 'experiments of Dr. Darwin' in the introduction to the 1831 edition of Frankenstein. Mary Shelley was influenced by Erasmus' ideas that new species can be created. Victor Frankenstein, in the opening pages of the novel, says to Captain Walton: 'A new species would bless me as its creator'. She is questioning the old beliefs that God was the only creator of species. Darwin, Erasmus, The Temple of Nature, (London, 1803) quoted in 'The Anxiety of Species'. Nichols, 'The Anxiety of Species'. Nichols, Ashton, 'Erasmus Darwin', Romantic Natural History, 2000,  URL  (21 December 2003) Shelley, Mary, Frankenstein, (London, 1988) p.12. Shelley, Frankenstein, p.54. The Romantics believed that if you understood nature, you would attain a greater understanding of yourself. 'Romantic poets and writers were hinting at the biological connectedness of all living things'; for example, Coleridge spoke of the 'one life within us and abroad' in ""The Eolian Harp"". It was believed that Man, the animals and all of inanimate nature were linked. This went against the ancient idea of the Great Chain of Being where man was above nature in the hierarchy. In the Romantic view of nature 'animals were regarded not so much as sub-human as other-than-human'. The natural world was not as ordered as was once believed. Darwin's theory of evolution concurs with this line of thinking. He asserted that 'man and the lower animals do not differ in kind'. J. Howard notes how 'Darwin... used human behaviour to illustrate aspects of animal behaviour', further suggesting their link. The Romantics also attributed human characteristics and emotions to nature. William Wordsworth, in ""Lines Written in Early Spring"", humanised nature when he mused that 'every flower / Enjoys the air it breathes', and John Keats personified a nightingale in ""Ode to a Nightingale"" by giving it human feelings with its 'soul abroad / In such an ecstasy'. William Blake also 'hints at a link between human and nonhuman nature'. In ""The Human Abstract"" he writes: Nichols, 'The Anxiety of Species'. Coleridge, Samuel. Taylor, 'The Eolian Harp' quoted in Nichols, 'The Anxiety of Species'. Honour, Romanticism, p.311. Darwin, Charles, The Descent of Man and Selection in Relation to Sex, vol. I, (London, 1871) p.186 quoted in Howard, J, Darwin: A Very Short Introduction, (New York, 1982) p.76. Howard, J, Darwin: A Very Short Introduction, (New York, 1982) p.75. Wordsworth, William, 'Lines Written in Early Spring', Lyrical Ballads, (London, 1798) p. Keats, John, 'Ode to a Nightingale', Odes of 1819 by John Keats,  URL  (21 December 2003) Nichols, 'The Anxiety of Species'. The Gods of the earth and sea Sought thro' Nature to find this Tree, But their search was all in vain: There grows one in the Human Brain.Blake, William, 'The Human Abstract' quoted in Nichols, 'The Anxiety of Species'. The metaphorical tree in the human brain unites man to inanimate nature. This relates to Darwin's theory that Man evolved from more primitive forms of nature. After looking at all the evidence for evolution, Darwin speculated that 'probably all the organic beings which have ever lived on this earth have descended from some one primordial form'. This concept has been alluded to in the writings of some of the Romantics. For example, in his ""Ode to the West Wind"", Percy Shelley writes: 'The sapless foliage of the ocean, know / Thy voice, and suddenly grow grey, with fear'. The plants of the sea respond in the same way to the changing seasons brought on by the west wind as the plants on the land. This is a 'typically Romantic image of sympathetic interactions across... boundaries'. It is implied that sea plants and land plants behave in similar ways because they have a common origin. The Romantics were not evolutionary thinkers, but their view of nature was similar to Darwin's in the way that they could see 'one grand natural system' instead of individual species with no interaction or variation. Darwin, On the Origin of Species, p.484. Shelley, Percy, 'Ode to the West Wind', quoted in Nichols, 'The Anxiety of Species'. Nichols, 'The Anxiety of Species'. Darwin, On the Origin of Species, p.329. Some of the Romantics believed that understanding nature and becoming one with the natural world would bring them closer to the divine. Lord Alfred Tennyson, for example, in a poem entitled ""Flower in the Crannied Wall"" writes: Little Flower - but if I could understand What you are, root and all, and all in all, I should know what God and man is. Tennyson, Lord Alfred, 'Flower in the Crannied Wall'  URL  Wall.html (21 December 2003) They believed that all of nature was divinely ordered and they thought that evolution could have been part of God's plan. This is where Darwin's theory countered this Romantic view. In his Origin of Species he explains how natural selection does not proliferate characteristics that are beneficial to another species, claiming that he 'would almost as soon believe that the cat curls the end of its tail when preparing to spring, in order to warn the doomed mouse'. Natural selection is selfish. It does nothing for the good of another species. For Darwin, this detail 'discriminated between providential creation and evolution by natural selection'. Also, the randomness of natural selection with chance mutations and the struggle for survival 'sounds more like a game of genetic Russian Roulette that the kind of mechanism that would have been instituted by a wise and benevolent Creator'. Darwin describes evolution in a purely naturalistic manner, removing the supernatural element. In explaining the world 'the naturalist has abolished... God'. However, not all Romantic thinkers still attributed the workings of nature to God. Keats' ode ""To Autumn"", Darwin, On the Origin of Species, p.201. Howard, Darwin, p.29. 'From Darwin to Modern Darwinism: An Overview', p.6. Williams, Keywords, p.217. Season of mists and mellow fruitfulness, Close bosom-friend of the maturing sun; Conspiring with him how to load and bless With fruit the vines that round the thatch-eves run, Keats, John, 'To Autumn' Odes of 1819 by John Keats,  URL  (21 December 2003) contains no allusions to the divine. Nature controls it's own processes. 'Keats's ""nature"", like Erasmus [and Charles] Darwin's, exists without need for, or appeal to, any form of ""super nature"".' Nichols, 'The Anxiety of Species'. 'In the century before Charles Darwin, a wide range of scientists and writers saw human beings as organisms with important connections to their environment'. This included the Romantics. Darwin's theory of evolution did not counter the Romantic view of nature. There were many similarities between Darwin's ideas and those of the Romantics. They could see that nature was not static and unchanging, but that it was a dynamic system that ever changing. The Romantics had a sense that Man was connected to the natural world. Darwin took this idea a step further in his theory of evolution. Although Darwin's purely naturalistic and secular theory opposed the views of some of the Romantic thinkers such as Tennyson who believed that nature was God's creation, certain Romantics like Keats where questioning the divine nature of world. Nichols, 'The Anxiety of Species'.","The term Renaissance comes from the Italian word rinascita, meaning rebirth. The movement looked back to the superior aspects of ancient times and integrated them with the new developments of the time. The Italian Renaissance is generally associated with the fifteenth century. This period, however, can be stretched in either direction. For example, the term ""Renaissance"" has been used to describe some thirteenth-century artists, and the great artists of the Renaissance were active until the end of the sixteenth century. The rebirth of the visual arts was one aspect of the Renaissance but the development of humanism and the classical studies of the universities were as important. There were also advancements in music, literature, science and government practices during this period. How far Florence was responsible for these developments can be debated. Robert Black asserts that 'the elitism of Florentine society and the monopoly which the Florentine patriciate had over classical culture made Florence the first and greatest capital of the Renaissance'. Jocelyn Hunt, on the other hand, believes that 'the assumption that Florence was pre-eminent is... Burckhardtian, since, it is argued, the scholars of the Renaissance were based in Rome, and in the other universities'. Porter, Roy, and Teich, Mikulas (eds. ), The Renaissance in National Context, (Cambridge, 1992), p.37. Hunt, Jocelyn, The Renaissance, (London, 1999), p.ix. Burckhardt identified Florence as the 'cradle of the Renaissance'. It appears as though 'the components for a successful intellectual movement were all to be found in Florence at the start of the fifteenth century'. The city was wealthy, banking was highly developed, and papal revenues were often invested in Florence rather than the volatile and dangerous city of Rome. Florence had a high population. It was active in manufacture, trade and commerce. Textile production and metalworking were successful industries in Florence. The city was in a prime position for trade; it was close enough to the coast for sea trade but far enough to keep it safe from pirate attacks. It was also able to trade over the Alps and was close to the Middle East so merchants could bring back the luxury goods available there, such as silk and spices. Florence was primarily a city orientated towards craft-industrial production rather than trade like Venice. This appears to have been a more favourable environment for artists to be produced as 'it was only when Venice turned from trade to industry, at the end of the fifteenth century, that Venetian art caught up with that of Florence'. Hunt, The Renaissance, p.5. Hunt, The Renaissance, p.8. Burke, Peter, The Italian Renaissance: Culture and Society in Italy, (Oxford, 1986), pp.49-50. Giorgio Vasari, a contemporary historian addressed the issue of 'the outsized contribution of Florence to the arts'. He believed that there were three factors that made Florence different from other Italian towns. Firstly, Florentines were free and, therefore, became critical and would not accept things that were not of the highest quality. Gene A. Brucker notes that ' this appreciation of quality, and a corresponding disdain for the shoddy and inferior, became a characteristic feature of the Florentine mentality'. Secondly, as Gregorio Dati, a contemporary Florentine, explains, 'Florence is situated in a naturally wild and sterile place', so that the people had to work hard, using their intelligence in order to live comfortably. They would travel and were innovative and experimental. Thirdly, the people had a 'greed for honour and glory which that air generates in men of every occupation'. Burke, The Italian Renaissance, p.29. Brucker, Gene A, Renaissance Florence, (London, 1969), p.220. Hunt, The Renaissance, p.13. Burke, The Italian Renaissance, p.29. Wealthy citizens would spend their riches on art and culture. The amount they spent was seen to be an indication of their power and status. Florentine men did not own armies, rather hired mercenaries when they were needed. This meant that they had more to spend on luxuries. Giovanni Rucellai, a Florentine merchant, wrote in 1473 that 'earning and spending are among the greatest pleasures that men enjoy in this life'. He, himself, spent much of his wealth on the buildings of Florence, including his house and the loggia opposite, the church of Santa Maria Novella and the church of San Pancrazio. Commissioning artists gave them the opportunities to try out new styles and techniques as well as copying the styles of the ancients. Florence was important in the development of the Italian Renaissance because the artists working there were given these opportunities. Giovanni Rucellai quoted in Hunt, The Renaissance, p.13. The Florentine political system was inspired by antiquity. It compared to those of the ancient Greek city-states or, as Evelyn Welch observes, 'Florence and Venice found their heritage in the Roman Republic'. The king, the Church and the nobility did not rule the Florentine people as in other places. Instead they ruled themselves, either by electing nine men for the Signoria or by holding office themselves for the two-month term. Committees were appointed to oversee administration. The government viewed everybody as equals, with nobody being above the law and everything being done to benefit the whole community. Michelangelo's statue of David was commissioned to depict the strength of Florence. David is an example of the Renaissance style of realism in art. Vincent Cronin has discussed the fact that Burckhardt took Renaissance Italy as a whole and made no distinction between what happened in republican Florence and other cities which were ruler over by tyrants. He argues that 'such a distinction is essential to an understanding of all but the most general points'. The political freedom that Florentine men enjoyed serves to explain Florence's leading role in the development of the Italian Renaissance. Cronin states that 'Florence was the power-house which generated far and away the largest number of influential ideas'. Welch, Evelyn, Art and Society in Italy 1350-1500, (Oxford, 1997), p.241. Cronin, Vincent, The Florentine Renaissance, (London, 1967), p.313. Cronin, The Florentine Renaissance, p.313. In the late fifteenth century, the Medici family took control of most of the councils and committees, threatening the old system. They were firmly established in Florence by the second half of the fifteenth century. Cronin describes the fact that the Medici patronized art as a 'remarkable innovation, since Italy had no tradition of individual lay patronage'. Florence led the way in this new fashion. The Medici employed artists to enhance their status. Lorenzo de'Medici invested in art that would 'make the city of Florence famous'. The freestanding dome of Florence Cathedral, built by Filippo Brunelleschi, was the first of its kind. It was copied in other places such as Rome with the construction of St. Peter's. The dome was inspired by ancient architecture such as the Pantheon in Rome. Brunelleschi used the system of arithmetic proportions, which he had taken from the work of Vitruvius, in his design. Brunelleschi also used new techniques including line point perspective and innovations in engineering that inspired other Renaissance artists and architects. Florence's contribution to the Italian Renaissance's attempt to turn away from the medieval gothic style in architecture and return to the ways of ancient Rome and Greece 'was little short of astounding'. Cronin, The Florentine Renaissance, p.176. Hunt, The Renaissance, p.10. Porter and Teich (eds. ), The Renaissance in National Context, p.21. Another reason why Florence took a leading role in the developments of the Italian Renaissance was the relationship between patrons and artists in Florence which was 'remarkable by comparison with conditions in other circles'. There was a friendly understanding between them which was in part due to classical texts which exalted the artist and emphasized the esteem that they received in ancient times. These texts 'were not lost on the Florentines'. The relationship between Cosimo and Donatello 'went much deeper than that of employer and employee'. Cosimo gave Donatello a pension in his old age, which was the first known instance of this type of help given to an artist. The 'championing of artists by Florence's most prominent citizen naturally raised their status'. Artists in Florence were allowed to use their own genius. Leonardo was educated in Medici Florence and was 'the culmination of the versatile artist whose rise took place in the city by the Arno'. When he moved to Milan, however, he was not understood. He painted the Last Supper in Milan but the Dominican Prior wanted him to work quickly and complained when he spent half a day looking at the painting without adding to it. Leonardo replied that 'men of genius sometimes accomplish most when they work least'. Similarly, in Rome, relations between patron and artist were not as amiable as in Florence. Cronin notes how 'painters were lumped together with carters and grooms' and were often given poor food. The artist Davide Ghirlandaio was served uneatable food while working in the abbey of Passignano and was so angry that he dumped his soup over the friar and beat him with the loaf of bread. Cronin, The Florentine Renaissance, p.179. Cronin, The Florentine Renaissance, p.177. Cronin, The Florentine Renaissance, p.178. Cronin, The Florentine Renaissance, p.180. Cronin, The Florentine Renaissance, p.188. Cronin, The Florentine Renaissance, p.188. Cronin, The Florentine Renaissance, p.179. Cronin, The Florentine Renaissance, p.180. Dati said of Florence: 'so great is the number of talented and rich men that they are unequalled in the world'. The talented men produced the great works of the Italian Renaissance and the rich men enabled them to do so. Florentine artists and sculptors had 'immeasurable influence... on the future development of the visual arts in the renaissance'. Masaccio, a Florentine, was the first painter known to have adopted Brunelleschi's system of line-point perspective and make use of lighting and shading in his works such as the frescoes in the churches of Santa Maria Novella. Florentines also led the way in the revival of classical sculpture. Donatello was responsible for the revival of the classical art of large-scale bronze casting. He was also the first artist since antiquity to study anatomy and represent the nude in his works. Gregorio Dati quoted in Hunt, The Renaissance, p.13. Porter and Teich (eds. ), The Renaissance in National Context, p.23. In Italy, painters, sculptors and masons often belonged to guilds. In some places, such as Milan and Florence, painters had their own guilds. Most of these were quite restrictive. For example, the guild in Padua forbade members to give or sell to non-members and did not allow work to be brought from another district to be sold in Padua. The guild in Venice was also restrictive. Albrecht Dürer commented on the strict nature of the guild when he visited Venice in 1504, describing how they summoned him before the magistrates three times and had him pay four florins to the guild. The guilds in Florence, on the other hand, were not as powerful. The government held that not every craftsman had to join and it allowed people from other places to work in Florence. Peter Burke believes that 'this more liberal policy, which exposed local tradition to stimuli from outside, may help to explain Florence's cultural lead'. Burke, The Italian Renaissance, p.68. Burke, The Italian Renaissance, p.68. Florence's important contribution to the arts can be seen in the proportion of Italy's cultural elite, the people whose creative abilities are recognized in society, which originated from around Florence. Burke has calculated that around 26 per cent of the elite came from Tuscany, which had only 10 per cent of the population, while the Veneto had 20 per cent of the population but only around 23 per cent of the elite. Burke believes that 'Rome's poor contribution needs emphasis' as it produced less than its share of Italy's cultural elite. Rome was no longer the beautiful, classical city that it was in ancient times. 'The Middle Ages reversed Augustus's boast that he had found Rome a city of brick and left it one of marble'. Materials from ancient monuments had been taken for other purposes and marble had been burnt for lime. Rome was in ruins with woods and undergrowth which sheltered foxes, wolves and hares. Rome's population was smaller then Florence's and was much poorer. This helps to explain Florence's dominance in the development of the Italian Renaissance. Burke, The Italian Renaissance, p.44. Burke, The Italian Renaissance, p.45. Cronin, The Florentine Renaissance, p.167. Dante was an important figure in the development of Renaissance literature. He learnt the universal ideals explained by Thomas Aquinas in the Florentine schools. He wrote the Divine Comedy in the local Tuscan dialect instead of Latin. Brucker describes the Divine Comedy as 'a Florentine poem, replete with the particular values, emotions, and concerns of that community'. Florence was important to Dante and Dante was a challenge and inspiration for later generations of Florentine intellectuals. Giotto di Bondone was a Florentine artist in the fourteenth century who, like Dante, influenced later generations of Florentines. He was innovative in the way he painted frescos; he created naturalistic and lifelike scenes that were also grandiose and monumental by humanizing the wooden, stylised figures of Byzantine art. His scenes in Santa Croce from the life of Saint Francis are examples of this style. His frescos had 'a profound impact upon the revolutionary generation of Florentine artists in the early Quattocento'. Brucker, Renaissance Florence, p.215. Brucker, Renaissance Florence, p.216. 'The Florentine contribution to Renaissance culture was not limited to a few specialized areas; it encompassed many fields and disciplines'. The city's jurists, physicians and theologians made considerable contributions to their fields. In many aspects of the Renaissance, 'Florence led the way, Italy followed'. The work of the Florentines: Coluccio, Bruni, Poggio and Alberti, paved the way for educational, political and moral discussions in Ferrara; Bruni and Ficino's translations of Plato enabled Isabella d'Este's circle in Mantua to discuss Platonism; and Antonello drew on the geometrical notions evolved in Florence when painting his Virgin Annunciate. Lionello Venturi summed it up by his comment that 'the new Renaissance style emerged in northern Italy a generation later than in Florence, and as the direct result of work done on the spot by Tuscan artists'. Brucker, Renaissance Florence, p.213. Cronin, The Florentine Renaissance, p.313. Cronin, The Florentine Renaissance, p.314. Florentine citizens were concerned with the study of the classics in the second half of the fifteenth century. Burckhardt asserts that the Renaissance came about because of 'those citizens, mostly Florentines, who made antiquarian interests one of the chief objects of their lives'. Niccolo Niccoli, a Florentine contemporary, told the young Piero de'Pazzi: 'if you do not study the classics, you will be considered a nothing'. In the study of the classics, 'Florence was ahead of the rest of Italy'. By the middle of the fifteenth century, nearly every prominent family in Florence, such as Strozzi, Medici and Rossi, had a humanist scholar among their numbers. It was also in Florence that the old Gothic script of angular closely spaced lettering was rejected in favour of the more rounded and spaced out script known as humanist miniscule. This style was adopted by the rest of Renaissance Italy. Burckhardt, Jacob, The Civilisation of the Renaissance in Itlay, (Harmondsworth, 1990), p.143. Niccolo Niccoli quoted in Hunt, The Renaissance, p.14. Porter and Teich (eds. ), The Renaissance in National Context, p.36. Florence was an important center for the development of history writing until the sixteenth century. The Florentine Leonardo Bruni was a key figure in this development. He rejected the medieval style of writing history chronicles and revived the Roman historian, Livy's, model of writing the history of a city-state with an introductory book on the origins and the early history of the city followed by a detailed account of events, organized as a year-to-year narrative and grouped into books. He wrote the History of the Florentine People in this style. Bruni was also responsible for reviving the Ciceronian dialogue, text written as though in conversation, which was used by Cicero in ancient times. He wrote Dialogues to Pietro Paolo Vergerio of Istria in this style, which became the standard form of Renaissance literature. Marsilio Ficino, leader of the Platonic Academy of Florence, was responsible for translating the works of Plato which were written in Greek. He also wrote his own philosophical works such as Platonic Theology Concerning the Immortality of the Soul. James Hankins has commented on the 'leading role played by Marsilio Ficino and Neoplatonic Philosophy on the cultural life of the High Renaissance'. The Platonic Academy of Florence was Ficino's primary vehicle for the spread of his ideas. There has been some debate over the significance of the Academy. Gustavo Uzielli, a Toscanelli scholar, has even suggested that the Academy was nothing but a fable. Arnaldo della Torre, on the other hand, wrote Storia dell'Academia Platonica di Firenze in 1902 in which the importance of the Academy was stressed. The authenticity of Ficino's letters, which Uzielli had suggested to be forgeries, was defended. Della Torre describes how the Academy was founded by Cosimo de'Medici in 1462 and how Ficino was appointed the head. According to Della Torra, the Academy held regular activities such as lectures, disputations and banquets. The Academy was made up of the Medici patrons as well as around one hundred prominent statesmen, poets, orators, doctors, lawyers and ecclesiastics of the later fifteenth century. James Hankins, however, suggests tah 'Ficino did have an academy of a sort, but if so, it was a thing quite different, and much less important, than has generally been thought'. Hankins, James, 'The Myth of the Platonic Academy of Florence', Renaissance Quarterly, 44, (1991), p.429. Hankins, 'The Myth of the Platonic Academy of Florence', p.430. Hankins, 'The Myth of the Platonic Academy of Florence', p.432. Hankins, 'The Myth of the Platonic Academy of Florence', p.433. Coluccio Salutati, the owner of the largest library of ancient manuscripts in Italy after the death of Petrarch, lived in Florence. Attracting new talent into the city, 'he was perhaps the greatest cultural asset enjoyed by the city'. He was appointed first chancellor in 1375. Salutati's disciples included some of the next generation's leading humanistic scholars such as Leonardo Bruni, Pietro Paul Vergerio and Poggio Bracciolini. Being a statesman and a scholar, Salutati bridged the gap between the world of learning and the world of commerce and politics. Humanists would point to Salutati's reputation and political influence when critics would question the value of classical studies. Slautati was important in the development of the Italian Renaissance and he was a Florentine. Brown, Alison, The Renaissance, (London, 1988), p.36. Florence had a key role in the development of the Italian Renaissance because its cultural pre-eminence coincided with the largest territorial expansion of the time. Between 1350 and 1490 Florence was in control of most of Tuscany. Florence fought against the papacy, Milan and Naples. In 1415, Bruni compared the victory of Florence over Milan, the capture of Pisa, and the defeat of the Ladislas, to the victory of the Roman Republic over Carthage. Antiquity was used to legitimize conquests and to establish and maintain authority by comparing the Florentine government with the successful Roman Republic. Porter and Teich (eds. ), The Renaissance in National Context, p.27. Towards the end of the period of the Italian Renaissance, Florence was still a place of importance. Machiavelli was at work in Florence writing about how men should not only admire the ancients but attempt to put the Roman model into real practical use. His political thought was highly influential. Florence also had influence on the scientific developments of the Renaissance. Copernicus claimed that the earth revolved around the sun, not the other way round which is what had previously been believed. Cronin asserts that this claim 'was a choice, not a discovery, and the basis of Copernicus's choice-as later those of Kepler and Galileo-was the Platonic cosmology revived in Florence'. Cronin, The Florentine Renaissance, p.144. Florence appears to have had much influence on the developments of the Italian Renaissance. Jocelyn Hunt, however, has argued that 'it may be the case that historians have examined the features which made Florence exceptional, and calculated that these are the essential preconditions for the Renaissance'. In this case, it would appear that the importance of Florence has been exaggerated. In the early stages of the Italian Renaissance, in the late thirteenth to early fourteenth centuries, 'Florence had a relatively minor role'. Florence had no unique connection with antiquity as Rome did. Its claims to antiquity were 'easily matched or surpassed by many other Italian cities'. It has been suggested that 'the new age began in Padua and the other urban communes of northern Italy in the 14th century, where lawyers and notaries imitated ancient Latin style and studied Roman archaeology'. Petrarch, perhaps the most influential figure of the Renaissance, never lived and worked in Florence. He achieved much greater fame as a writer than his Florentine equivalent, Salutati. Hunt, The Renaissance, p.8. Porter and Teich (eds. ), The Renaissance in National Context, p.21. Porter and Teich (eds. ), The Renaissance in National Context, p.21. Pioch, Nicolas, 'La Renaissance', Web Museum, 14 October 2002. URL  (1 April 2004) Petrarch recognized that Rome was the capital of the ancient world and, as it was the interest in the achievements of the ancient world that characterised the Renaissance, it has been suggested that Rome was the most important center of the Italian Renaissance. 'The classical revival was at its strongest in Rome, where the largest number of classical buildings remained to inspire the artist'. The pope was the most influential patron. Rome was important 'as a center of patronage which attracted creative individuals from other parts of Italy'. Nobody could outbid the papacy when commissioning the work of an artist. The pope's private library in the Vatican increased in size, containing 3,600 works by 1484, as Italian scholars began to search for the works of classical writers. Rome was an important center where 'the literary and artistic renaissance worked closely together'. For example, Raphael and his pupils painted a series of frescos in Agostino Chigi's Villa Farnesina near the Tiber depicting the story of Amor and Psyche from Apuleius' Golden Ass. Both form and content were taken from classical Roman art and literature. Fritz Saxl said that 'the Amor and Psyche paintings represent the greatest effort to illustrate pagan myths in the spirit of the classical marbles as they reappeared from beneath the Roman soil'. Hunt, The Renaissance, p.10. Burke, The Italian Renaissance, p.45. Porter and Teich (eds. ), The Renaissance in National Context, p.44. Porter and Teich (eds. ), The Renaissance in National Context, p.44. Although Rome was left in ruins after the Middle Ages, it was still an inspiration to artists from all over Italy. Brunelleschi, a Florentine architect, chose to study in Rome instead of Florence. This was partly due to the fact that, during the period at the beginning of the fifteenth century, Florence was in a critical struggle with Giangaleazzo. He and his friend Donatello would systematically inspect and excavate the ruins of Rome with teams of labourers. Brunelleschi wished to discover how the Romans had built their vast constructions. He measured the thickness of walls, columns, arches, bricks and blocks of marble. The Pantheon was a huge inspiration, as it had not been touched since ancient times. Brunelleschi would remove tiles from the roof to study the ribbing of the shallow cupola. He looked at the number of square holes in large stones and decided that the Romans must have used some sort of machine to move them. This gave him the idea of the ulivella, a machine to move large stones by a crane without cords. Brunelleschi studied the ancient buildings of Rome for about twelve years. When the dome to Florence cathedral was being planned, a number of problems arose. The diameter of space to be spanned was 138 feet. The only cupola of a similar dimension was the Pantheon which Brunelleschi had been studying. Brunelleschi came up with a design for the dome and was awarded the commission of building it. The dome of Florence cathedral may have been the first of its kind but the inspiration and the savoir-faire came from Rome. Brucker believes that 'Florentine intellectual life... was matched, to some degree, by Milan, Venice and Naples'. Rulers all over Italy tried to attract the great Renaissance artists and the other Italian cities, such as Milan, Urbino, Ferrara, Venice, Padua and Naples, soon became 'powerful rivals in the spreading wave of change'. Leon Battista Alberti worked on some of the most progressive architecture of the new Humanism in Rimini and Mantua, and Andrea Mantegna's paintings in Padua portrayed a personal formation of linear perspective, antiquarianism and realistic technique. The Venetian school was growing in strength with developments such as Giovanni Bellini's poetic classicism. Other cities took the lead over Florence in certain areas. Venice, for example, became the center of the new printing industry that enabled Renaissance literature to be widely circulated. Burke believes that 'the Venetian cultural achievement of the period... long received considerably less than its due'. He explains that this has happened because there were less studies of the social history of the arts in Venice than there were of Florence. Vasari, a Renaissance historian, is said to have had a 'Tuscan bias', which may have exaggerated Florence's role. There was no material to counter this view. There was an attempt in the sixteenth century by a Venetian to collect material on the lives of painters around Venice but this attempt was never completed and published. There was also no Venetian equivalent to Wackernagel's book on Florence. Burke has made an effort in The Italian Renaissance: Culture and Society in Italy, 'to avoid giving the Florentines more than their fair share of the limelight', by discussing important artists and writers from all over Italy. Brucker, Renaissance Florence, p.215. Pioch, Nicolas, 'La Renaissance: Italy', Web Museum, 14 October 2002. URL  (1 April 2004) Burke, The Italian Renaissance, p.39. Burke, The Italian Renaissance, p.39. Burke, The Italian Renaissance, p.39. Venice was also the 'musical center of Italy' during the high Renaissance. Venice was the center of trade with the east because of its prime position on the coast. It was a wealthy, powerful and cosmopolitan city. It was at a 'cultural pinnacle' during the sixteenth century with composers like Andrea and Giovanni Gabrieli, Adrian Willaert and Claudio Merlo thriving in 'a musical environment that called for grand works that reflected the glory of Venice'. There were processions, ceremonies and festivals where composers could exhibit their music. St. Mark's cathedral was the center of Venetian music where new styles and techniques were developed. The cathedral was built in the style of eastern basilicas with two choir lofts and two organs. Composers had to write for separate choirs and they used the effects that this could produce. Composers also had to write music that was more chordal with emphasis on sound and clarity of text to counter the effects of the acoustics in the cathedral. Composers used instruments in the choirs which was unusual. Justin Klotz asserts that 'throughout the... Renaissance, Venice, the ""Most Serene Republic"", was one of the most exciting cities in Europe'. Klotz, Justin, 'The Venetian School', MUSL 242, 11 October 1998. URL  (1 April 2004) Klotz, 'The Venetian School'. Klotz, 'The Venetian School'. Klotz, 'The Venetian School'. Florence did not lead the way in humanist and classical education. It was 'the custom of the city' for young boys to learn 'enough to work as a merchant' rather than learn about the humanist subjects of grammar, rhetoric, poetry, history and moral philosophy. Key humanist teachers of the Renaissance included Gasparino Barzizza, Guarino Guarini da Verona and Vittorino da Feltre, who were all active in Northern Italy, not Florence. Alison Brown observes how 'in one respect Florence was - or at any rate seemed to be - worse off than many of her neighbours: she lacked a well-established university'. A university had been founded in 1348 but it never achieved the status of the universities of Bologna and Padua. For a long time it only offered vocational training for local students in medicine, law and theology. It was always competing with the abacus schools which Florentine merchants favoured more highly. Humanist education did not become important in Florence until the second half of the fifteenth century and then it was predominantly for elite families. Porter and Teich (eds. ), The Renaissance in National Context, p.32. Brown, The Renaissance, p.25. During the Renaissance, the Medieval Latin language and spelling was rejected, as there was a desire to return to the purer Latin of ancient Rome. The leading centers of the rebirth of Latin in Italy were Padua, Arezzo, Bologna and Verona. It did not begin in Florence, only reaching it by around 1400. By the mid-fifteenth century Rome was dominant in the development of the new language. However, Florence did hold some importance because of the arrival of the Greek Manual Chrysolaras at the turn of the fifteenth century who taught and inspired many students including Leonardo Bruni and Poggio Bracciolini, who were to become important Renaissance thinkers. Bruni produced new translations of Aristotle's Greek text Politics and Ethics. Latin was derived from Greek so Chrysolaras, being Greek, was knowledgeable and influential. By the end of the fifteenth century, Florence became dominant because of important Florentine thinkers such as Angelo Poliziano. In the late fifteenth and sixteenth century, Florence lost much of its influence as great artists such as Leonardo and Michelangelo were tempted away from the city and produced many of their masterpieces abroad. Leonardo had invented a number of military machines. After Florence had ended the war with Naples in 1480, Florentine politicians were only interested in keeping the peace and so Leonardo left Florence in 1482 to sell his war machines to other cities such as Milan. Although other places in Italy such as Rome and Venice had important roles to play in the development of the Italian Renaissance, 'the names most clearly associated with the Renaissance remain those of... artists from Florence'. The conditions in Florence at the time made it the ideal place for artists and scholars to work. There were many wealthy patrons, a large collection of talented men, and a system of government that encouraged cultural development. As Hunt observes, 'Florence is certain to remain at the heart of any serious study of the origins and development of the Italian Renaissance'. The importance of Florence in the development of the Italian Renaissance has not been exaggerated. 'Every important Italian City had its Renaissance, but the Florentine, because it was the most complete and the most influential, has a special claim to be considered representative of the whole'. Hunt, The Renaissance, p.ix. Hunt, The Renaissance, p.10. Cronin, The Florentine Renaissance, p.314.",True
65,"The term Renaissance comes from the Italian word rinascita, meaning rebirth. The movement looked back to the superior aspects of ancient times and integrated them with the new developments of the time. The Italian Renaissance is generally associated with the fifteenth century. This period, however, can be stretched in either direction. For example, the term ""Renaissance"" has been used to describe some thirteenth-century artists, and the great artists of the Renaissance were active until the end of the sixteenth century. The rebirth of the visual arts was one aspect of the Renaissance but the development of humanism and the classical studies of the universities were as important. There were also advancements in music, literature, science and government practices during this period. How far Florence was responsible for these developments can be debated. Robert Black asserts that 'the elitism of Florentine society and the monopoly which the Florentine patriciate had over classical culture made Florence the first and greatest capital of the Renaissance'. Jocelyn Hunt, on the other hand, believes that 'the assumption that Florence was pre-eminent is... Burckhardtian, since, it is argued, the scholars of the Renaissance were based in Rome, and in the other universities'. Porter, Roy, and Teich, Mikulas (eds. ), The Renaissance in National Context, (Cambridge, 1992), p.37. Hunt, Jocelyn, The Renaissance, (London, 1999), p.ix. Burckhardt identified Florence as the 'cradle of the Renaissance'. It appears as though 'the components for a successful intellectual movement were all to be found in Florence at the start of the fifteenth century'. The city was wealthy, banking was highly developed, and papal revenues were often invested in Florence rather than the volatile and dangerous city of Rome. Florence had a high population. It was active in manufacture, trade and commerce. Textile production and metalworking were successful industries in Florence. The city was in a prime position for trade; it was close enough to the coast for sea trade but far enough to keep it safe from pirate attacks. It was also able to trade over the Alps and was close to the Middle East so merchants could bring back the luxury goods available there, such as silk and spices. Florence was primarily a city orientated towards craft-industrial production rather than trade like Venice. This appears to have been a more favourable environment for artists to be produced as 'it was only when Venice turned from trade to industry, at the end of the fifteenth century, that Venetian art caught up with that of Florence'. Hunt, The Renaissance, p.5. Hunt, The Renaissance, p.8. Burke, Peter, The Italian Renaissance: Culture and Society in Italy, (Oxford, 1986), pp.49-50. Giorgio Vasari, a contemporary historian addressed the issue of 'the outsized contribution of Florence to the arts'. He believed that there were three factors that made Florence different from other Italian towns. Firstly, Florentines were free and, therefore, became critical and would not accept things that were not of the highest quality. Gene A. Brucker notes that ' this appreciation of quality, and a corresponding disdain for the shoddy and inferior, became a characteristic feature of the Florentine mentality'. Secondly, as Gregorio Dati, a contemporary Florentine, explains, 'Florence is situated in a naturally wild and sterile place', so that the people had to work hard, using their intelligence in order to live comfortably. They would travel and were innovative and experimental. Thirdly, the people had a 'greed for honour and glory which that air generates in men of every occupation'. Burke, The Italian Renaissance, p.29. Brucker, Gene A, Renaissance Florence, (London, 1969), p.220. Hunt, The Renaissance, p.13. Burke, The Italian Renaissance, p.29. Wealthy citizens would spend their riches on art and culture. The amount they spent was seen to be an indication of their power and status. Florentine men did not own armies, rather hired mercenaries when they were needed. This meant that they had more to spend on luxuries. Giovanni Rucellai, a Florentine merchant, wrote in 1473 that 'earning and spending are among the greatest pleasures that men enjoy in this life'. He, himself, spent much of his wealth on the buildings of Florence, including his house and the loggia opposite, the church of Santa Maria Novella and the church of San Pancrazio. Commissioning artists gave them the opportunities to try out new styles and techniques as well as copying the styles of the ancients. Florence was important in the development of the Italian Renaissance because the artists working there were given these opportunities. Giovanni Rucellai quoted in Hunt, The Renaissance, p.13. The Florentine political system was inspired by antiquity. It compared to those of the ancient Greek city-states or, as Evelyn Welch observes, 'Florence and Venice found their heritage in the Roman Republic'. The king, the Church and the nobility did not rule the Florentine people as in other places. Instead they ruled themselves, either by electing nine men for the Signoria or by holding office themselves for the two-month term. Committees were appointed to oversee administration. The government viewed everybody as equals, with nobody being above the law and everything being done to benefit the whole community. Michelangelo's statue of David was commissioned to depict the strength of Florence. David is an example of the Renaissance style of realism in art. Vincent Cronin has discussed the fact that Burckhardt took Renaissance Italy as a whole and made no distinction between what happened in republican Florence and other cities which were ruler over by tyrants. He argues that 'such a distinction is essential to an understanding of all but the most general points'. The political freedom that Florentine men enjoyed serves to explain Florence's leading role in the development of the Italian Renaissance. Cronin states that 'Florence was the power-house which generated far and away the largest number of influential ideas'. Welch, Evelyn, Art and Society in Italy 1350-1500, (Oxford, 1997), p.241. Cronin, Vincent, The Florentine Renaissance, (London, 1967), p.313. Cronin, The Florentine Renaissance, p.313. In the late fifteenth century, the Medici family took control of most of the councils and committees, threatening the old system. They were firmly established in Florence by the second half of the fifteenth century. Cronin describes the fact that the Medici patronized art as a 'remarkable innovation, since Italy had no tradition of individual lay patronage'. Florence led the way in this new fashion. The Medici employed artists to enhance their status. Lorenzo de'Medici invested in art that would 'make the city of Florence famous'. The freestanding dome of Florence Cathedral, built by Filippo Brunelleschi, was the first of its kind. It was copied in other places such as Rome with the construction of St. Peter's. The dome was inspired by ancient architecture such as the Pantheon in Rome. Brunelleschi used the system of arithmetic proportions, which he had taken from the work of Vitruvius, in his design. Brunelleschi also used new techniques including line point perspective and innovations in engineering that inspired other Renaissance artists and architects. Florence's contribution to the Italian Renaissance's attempt to turn away from the medieval gothic style in architecture and return to the ways of ancient Rome and Greece 'was little short of astounding'. Cronin, The Florentine Renaissance, p.176. Hunt, The Renaissance, p.10. Porter and Teich (eds. ), The Renaissance in National Context, p.21. Another reason why Florence took a leading role in the developments of the Italian Renaissance was the relationship between patrons and artists in Florence which was 'remarkable by comparison with conditions in other circles'. There was a friendly understanding between them which was in part due to classical texts which exalted the artist and emphasized the esteem that they received in ancient times. These texts 'were not lost on the Florentines'. The relationship between Cosimo and Donatello 'went much deeper than that of employer and employee'. Cosimo gave Donatello a pension in his old age, which was the first known instance of this type of help given to an artist. The 'championing of artists by Florence's most prominent citizen naturally raised their status'. Artists in Florence were allowed to use their own genius. Leonardo was educated in Medici Florence and was 'the culmination of the versatile artist whose rise took place in the city by the Arno'. When he moved to Milan, however, he was not understood. He painted the Last Supper in Milan but the Dominican Prior wanted him to work quickly and complained when he spent half a day looking at the painting without adding to it. Leonardo replied that 'men of genius sometimes accomplish most when they work least'. Similarly, in Rome, relations between patron and artist were not as amiable as in Florence. Cronin notes how 'painters were lumped together with carters and grooms' and were often given poor food. The artist Davide Ghirlandaio was served uneatable food while working in the abbey of Passignano and was so angry that he dumped his soup over the friar and beat him with the loaf of bread. Cronin, The Florentine Renaissance, p.179. Cronin, The Florentine Renaissance, p.177. Cronin, The Florentine Renaissance, p.178. Cronin, The Florentine Renaissance, p.180. Cronin, The Florentine Renaissance, p.188. Cronin, The Florentine Renaissance, p.188. Cronin, The Florentine Renaissance, p.179. Cronin, The Florentine Renaissance, p.180. Dati said of Florence: 'so great is the number of talented and rich men that they are unequalled in the world'. The talented men produced the great works of the Italian Renaissance and the rich men enabled them to do so. Florentine artists and sculptors had 'immeasurable influence... on the future development of the visual arts in the renaissance'. Masaccio, a Florentine, was the first painter known to have adopted Brunelleschi's system of line-point perspective and make use of lighting and shading in his works such as the frescoes in the churches of Santa Maria Novella. Florentines also led the way in the revival of classical sculpture. Donatello was responsible for the revival of the classical art of large-scale bronze casting. He was also the first artist since antiquity to study anatomy and represent the nude in his works. Gregorio Dati quoted in Hunt, The Renaissance, p.13. Porter and Teich (eds. ), The Renaissance in National Context, p.23. In Italy, painters, sculptors and masons often belonged to guilds. In some places, such as Milan and Florence, painters had their own guilds. Most of these were quite restrictive. For example, the guild in Padua forbade members to give or sell to non-members and did not allow work to be brought from another district to be sold in Padua. The guild in Venice was also restrictive. Albrecht Dürer commented on the strict nature of the guild when he visited Venice in 1504, describing how they summoned him before the magistrates three times and had him pay four florins to the guild. The guilds in Florence, on the other hand, were not as powerful. The government held that not every craftsman had to join and it allowed people from other places to work in Florence. Peter Burke believes that 'this more liberal policy, which exposed local tradition to stimuli from outside, may help to explain Florence's cultural lead'. Burke, The Italian Renaissance, p.68. Burke, The Italian Renaissance, p.68. Florence's important contribution to the arts can be seen in the proportion of Italy's cultural elite, the people whose creative abilities are recognized in society, which originated from around Florence. Burke has calculated that around 26 per cent of the elite came from Tuscany, which had only 10 per cent of the population, while the Veneto had 20 per cent of the population but only around 23 per cent of the elite. Burke believes that 'Rome's poor contribution needs emphasis' as it produced less than its share of Italy's cultural elite. Rome was no longer the beautiful, classical city that it was in ancient times. 'The Middle Ages reversed Augustus's boast that he had found Rome a city of brick and left it one of marble'. Materials from ancient monuments had been taken for other purposes and marble had been burnt for lime. Rome was in ruins with woods and undergrowth which sheltered foxes, wolves and hares. Rome's population was smaller then Florence's and was much poorer. This helps to explain Florence's dominance in the development of the Italian Renaissance. Burke, The Italian Renaissance, p.44. Burke, The Italian Renaissance, p.45. Cronin, The Florentine Renaissance, p.167. Dante was an important figure in the development of Renaissance literature. He learnt the universal ideals explained by Thomas Aquinas in the Florentine schools. He wrote the Divine Comedy in the local Tuscan dialect instead of Latin. Brucker describes the Divine Comedy as 'a Florentine poem, replete with the particular values, emotions, and concerns of that community'. Florence was important to Dante and Dante was a challenge and inspiration for later generations of Florentine intellectuals. Giotto di Bondone was a Florentine artist in the fourteenth century who, like Dante, influenced later generations of Florentines. He was innovative in the way he painted frescos; he created naturalistic and lifelike scenes that were also grandiose and monumental by humanizing the wooden, stylised figures of Byzantine art. His scenes in Santa Croce from the life of Saint Francis are examples of this style. His frescos had 'a profound impact upon the revolutionary generation of Florentine artists in the early Quattocento'. Brucker, Renaissance Florence, p.215. Brucker, Renaissance Florence, p.216. 'The Florentine contribution to Renaissance culture was not limited to a few specialized areas; it encompassed many fields and disciplines'. The city's jurists, physicians and theologians made considerable contributions to their fields. In many aspects of the Renaissance, 'Florence led the way, Italy followed'. The work of the Florentines: Coluccio, Bruni, Poggio and Alberti, paved the way for educational, political and moral discussions in Ferrara; Bruni and Ficino's translations of Plato enabled Isabella d'Este's circle in Mantua to discuss Platonism; and Antonello drew on the geometrical notions evolved in Florence when painting his Virgin Annunciate. Lionello Venturi summed it up by his comment that 'the new Renaissance style emerged in northern Italy a generation later than in Florence, and as the direct result of work done on the spot by Tuscan artists'. Brucker, Renaissance Florence, p.213. Cronin, The Florentine Renaissance, p.313. Cronin, The Florentine Renaissance, p.314. Florentine citizens were concerned with the study of the classics in the second half of the fifteenth century. Burckhardt asserts that the Renaissance came about because of 'those citizens, mostly Florentines, who made antiquarian interests one of the chief objects of their lives'. Niccolo Niccoli, a Florentine contemporary, told the young Piero de'Pazzi: 'if you do not study the classics, you will be considered a nothing'. In the study of the classics, 'Florence was ahead of the rest of Italy'. By the middle of the fifteenth century, nearly every prominent family in Florence, such as Strozzi, Medici and Rossi, had a humanist scholar among their numbers. It was also in Florence that the old Gothic script of angular closely spaced lettering was rejected in favour of the more rounded and spaced out script known as humanist miniscule. This style was adopted by the rest of Renaissance Italy. Burckhardt, Jacob, The Civilisation of the Renaissance in Itlay, (Harmondsworth, 1990), p.143. Niccolo Niccoli quoted in Hunt, The Renaissance, p.14. Porter and Teich (eds. ), The Renaissance in National Context, p.36. Florence was an important center for the development of history writing until the sixteenth century. The Florentine Leonardo Bruni was a key figure in this development. He rejected the medieval style of writing history chronicles and revived the Roman historian, Livy's, model of writing the history of a city-state with an introductory book on the origins and the early history of the city followed by a detailed account of events, organized as a year-to-year narrative and grouped into books. He wrote the History of the Florentine People in this style. Bruni was also responsible for reviving the Ciceronian dialogue, text written as though in conversation, which was used by Cicero in ancient times. He wrote Dialogues to Pietro Paolo Vergerio of Istria in this style, which became the standard form of Renaissance literature. Marsilio Ficino, leader of the Platonic Academy of Florence, was responsible for translating the works of Plato which were written in Greek. He also wrote his own philosophical works such as Platonic Theology Concerning the Immortality of the Soul. James Hankins has commented on the 'leading role played by Marsilio Ficino and Neoplatonic Philosophy on the cultural life of the High Renaissance'. The Platonic Academy of Florence was Ficino's primary vehicle for the spread of his ideas. There has been some debate over the significance of the Academy. Gustavo Uzielli, a Toscanelli scholar, has even suggested that the Academy was nothing but a fable. Arnaldo della Torre, on the other hand, wrote Storia dell'Academia Platonica di Firenze in 1902 in which the importance of the Academy was stressed. The authenticity of Ficino's letters, which Uzielli had suggested to be forgeries, was defended. Della Torre describes how the Academy was founded by Cosimo de'Medici in 1462 and how Ficino was appointed the head. According to Della Torra, the Academy held regular activities such as lectures, disputations and banquets. The Academy was made up of the Medici patrons as well as around one hundred prominent statesmen, poets, orators, doctors, lawyers and ecclesiastics of the later fifteenth century. James Hankins, however, suggests tah 'Ficino did have an academy of a sort, but if so, it was a thing quite different, and much less important, than has generally been thought'. Hankins, James, 'The Myth of the Platonic Academy of Florence', Renaissance Quarterly, 44, (1991), p.429. Hankins, 'The Myth of the Platonic Academy of Florence', p.430. Hankins, 'The Myth of the Platonic Academy of Florence', p.432. Hankins, 'The Myth of the Platonic Academy of Florence', p.433. Coluccio Salutati, the owner of the largest library of ancient manuscripts in Italy after the death of Petrarch, lived in Florence. Attracting new talent into the city, 'he was perhaps the greatest cultural asset enjoyed by the city'. He was appointed first chancellor in 1375. Salutati's disciples included some of the next generation's leading humanistic scholars such as Leonardo Bruni, Pietro Paul Vergerio and Poggio Bracciolini. Being a statesman and a scholar, Salutati bridged the gap between the world of learning and the world of commerce and politics. Humanists would point to Salutati's reputation and political influence when critics would question the value of classical studies. Slautati was important in the development of the Italian Renaissance and he was a Florentine. Brown, Alison, The Renaissance, (London, 1988), p.36. Florence had a key role in the development of the Italian Renaissance because its cultural pre-eminence coincided with the largest territorial expansion of the time. Between 1350 and 1490 Florence was in control of most of Tuscany. Florence fought against the papacy, Milan and Naples. In 1415, Bruni compared the victory of Florence over Milan, the capture of Pisa, and the defeat of the Ladislas, to the victory of the Roman Republic over Carthage. Antiquity was used to legitimize conquests and to establish and maintain authority by comparing the Florentine government with the successful Roman Republic. Porter and Teich (eds. ), The Renaissance in National Context, p.27. Towards the end of the period of the Italian Renaissance, Florence was still a place of importance. Machiavelli was at work in Florence writing about how men should not only admire the ancients but attempt to put the Roman model into real practical use. His political thought was highly influential. Florence also had influence on the scientific developments of the Renaissance. Copernicus claimed that the earth revolved around the sun, not the other way round which is what had previously been believed. Cronin asserts that this claim 'was a choice, not a discovery, and the basis of Copernicus's choice-as later those of Kepler and Galileo-was the Platonic cosmology revived in Florence'. Cronin, The Florentine Renaissance, p.144. Florence appears to have had much influence on the developments of the Italian Renaissance. Jocelyn Hunt, however, has argued that 'it may be the case that historians have examined the features which made Florence exceptional, and calculated that these are the essential preconditions for the Renaissance'. In this case, it would appear that the importance of Florence has been exaggerated. In the early stages of the Italian Renaissance, in the late thirteenth to early fourteenth centuries, 'Florence had a relatively minor role'. Florence had no unique connection with antiquity as Rome did. Its claims to antiquity were 'easily matched or surpassed by many other Italian cities'. It has been suggested that 'the new age began in Padua and the other urban communes of northern Italy in the 14th century, where lawyers and notaries imitated ancient Latin style and studied Roman archaeology'. Petrarch, perhaps the most influential figure of the Renaissance, never lived and worked in Florence. He achieved much greater fame as a writer than his Florentine equivalent, Salutati. Hunt, The Renaissance, p.8. Porter and Teich (eds. ), The Renaissance in National Context, p.21. Porter and Teich (eds. ), The Renaissance in National Context, p.21. Pioch, Nicolas, 'La Renaissance', Web Museum, 14 October 2002. URL  (1 April 2004) Petrarch recognized that Rome was the capital of the ancient world and, as it was the interest in the achievements of the ancient world that characterised the Renaissance, it has been suggested that Rome was the most important center of the Italian Renaissance. 'The classical revival was at its strongest in Rome, where the largest number of classical buildings remained to inspire the artist'. The pope was the most influential patron. Rome was important 'as a center of patronage which attracted creative individuals from other parts of Italy'. Nobody could outbid the papacy when commissioning the work of an artist. The pope's private library in the Vatican increased in size, containing 3,600 works by 1484, as Italian scholars began to search for the works of classical writers. Rome was an important center where 'the literary and artistic renaissance worked closely together'. For example, Raphael and his pupils painted a series of frescos in Agostino Chigi's Villa Farnesina near the Tiber depicting the story of Amor and Psyche from Apuleius' Golden Ass. Both form and content were taken from classical Roman art and literature. Fritz Saxl said that 'the Amor and Psyche paintings represent the greatest effort to illustrate pagan myths in the spirit of the classical marbles as they reappeared from beneath the Roman soil'. Hunt, The Renaissance, p.10. Burke, The Italian Renaissance, p.45. Porter and Teich (eds. ), The Renaissance in National Context, p.44. Porter and Teich (eds. ), The Renaissance in National Context, p.44. Although Rome was left in ruins after the Middle Ages, it was still an inspiration to artists from all over Italy. Brunelleschi, a Florentine architect, chose to study in Rome instead of Florence. This was partly due to the fact that, during the period at the beginning of the fifteenth century, Florence was in a critical struggle with Giangaleazzo. He and his friend Donatello would systematically inspect and excavate the ruins of Rome with teams of labourers. Brunelleschi wished to discover how the Romans had built their vast constructions. He measured the thickness of walls, columns, arches, bricks and blocks of marble. The Pantheon was a huge inspiration, as it had not been touched since ancient times. Brunelleschi would remove tiles from the roof to study the ribbing of the shallow cupola. He looked at the number of square holes in large stones and decided that the Romans must have used some sort of machine to move them. This gave him the idea of the ulivella, a machine to move large stones by a crane without cords. Brunelleschi studied the ancient buildings of Rome for about twelve years. When the dome to Florence cathedral was being planned, a number of problems arose. The diameter of space to be spanned was 138 feet. The only cupola of a similar dimension was the Pantheon which Brunelleschi had been studying. Brunelleschi came up with a design for the dome and was awarded the commission of building it. The dome of Florence cathedral may have been the first of its kind but the inspiration and the savoir-faire came from Rome. Brucker believes that 'Florentine intellectual life... was matched, to some degree, by Milan, Venice and Naples'. Rulers all over Italy tried to attract the great Renaissance artists and the other Italian cities, such as Milan, Urbino, Ferrara, Venice, Padua and Naples, soon became 'powerful rivals in the spreading wave of change'. Leon Battista Alberti worked on some of the most progressive architecture of the new Humanism in Rimini and Mantua, and Andrea Mantegna's paintings in Padua portrayed a personal formation of linear perspective, antiquarianism and realistic technique. The Venetian school was growing in strength with developments such as Giovanni Bellini's poetic classicism. Other cities took the lead over Florence in certain areas. Venice, for example, became the center of the new printing industry that enabled Renaissance literature to be widely circulated. Burke believes that 'the Venetian cultural achievement of the period... long received considerably less than its due'. He explains that this has happened because there were less studies of the social history of the arts in Venice than there were of Florence. Vasari, a Renaissance historian, is said to have had a 'Tuscan bias', which may have exaggerated Florence's role. There was no material to counter this view. There was an attempt in the sixteenth century by a Venetian to collect material on the lives of painters around Venice but this attempt was never completed and published. There was also no Venetian equivalent to Wackernagel's book on Florence. Burke has made an effort in The Italian Renaissance: Culture and Society in Italy, 'to avoid giving the Florentines more than their fair share of the limelight', by discussing important artists and writers from all over Italy. Brucker, Renaissance Florence, p.215. Pioch, Nicolas, 'La Renaissance: Italy', Web Museum, 14 October 2002. URL  (1 April 2004) Burke, The Italian Renaissance, p.39. Burke, The Italian Renaissance, p.39. Burke, The Italian Renaissance, p.39. Venice was also the 'musical center of Italy' during the high Renaissance. Venice was the center of trade with the east because of its prime position on the coast. It was a wealthy, powerful and cosmopolitan city. It was at a 'cultural pinnacle' during the sixteenth century with composers like Andrea and Giovanni Gabrieli, Adrian Willaert and Claudio Merlo thriving in 'a musical environment that called for grand works that reflected the glory of Venice'. There were processions, ceremonies and festivals where composers could exhibit their music. St. Mark's cathedral was the center of Venetian music where new styles and techniques were developed. The cathedral was built in the style of eastern basilicas with two choir lofts and two organs. Composers had to write for separate choirs and they used the effects that this could produce. Composers also had to write music that was more chordal with emphasis on sound and clarity of text to counter the effects of the acoustics in the cathedral. Composers used instruments in the choirs which was unusual. Justin Klotz asserts that 'throughout the... Renaissance, Venice, the ""Most Serene Republic"", was one of the most exciting cities in Europe'. Klotz, Justin, 'The Venetian School', MUSL 242, 11 October 1998. URL  (1 April 2004) Klotz, 'The Venetian School'. Klotz, 'The Venetian School'. Klotz, 'The Venetian School'. Florence did not lead the way in humanist and classical education. It was 'the custom of the city' for young boys to learn 'enough to work as a merchant' rather than learn about the humanist subjects of grammar, rhetoric, poetry, history and moral philosophy. Key humanist teachers of the Renaissance included Gasparino Barzizza, Guarino Guarini da Verona and Vittorino da Feltre, who were all active in Northern Italy, not Florence. Alison Brown observes how 'in one respect Florence was - or at any rate seemed to be - worse off than many of her neighbours: she lacked a well-established university'. A university had been founded in 1348 but it never achieved the status of the universities of Bologna and Padua. For a long time it only offered vocational training for local students in medicine, law and theology. It was always competing with the abacus schools which Florentine merchants favoured more highly. Humanist education did not become important in Florence until the second half of the fifteenth century and then it was predominantly for elite families. Porter and Teich (eds. ), The Renaissance in National Context, p.32. Brown, The Renaissance, p.25. During the Renaissance, the Medieval Latin language and spelling was rejected, as there was a desire to return to the purer Latin of ancient Rome. The leading centers of the rebirth of Latin in Italy were Padua, Arezzo, Bologna and Verona. It did not begin in Florence, only reaching it by around 1400. By the mid-fifteenth century Rome was dominant in the development of the new language. However, Florence did hold some importance because of the arrival of the Greek Manual Chrysolaras at the turn of the fifteenth century who taught and inspired many students including Leonardo Bruni and Poggio Bracciolini, who were to become important Renaissance thinkers. Bruni produced new translations of Aristotle's Greek text Politics and Ethics. Latin was derived from Greek so Chrysolaras, being Greek, was knowledgeable and influential. By the end of the fifteenth century, Florence became dominant because of important Florentine thinkers such as Angelo Poliziano. In the late fifteenth and sixteenth century, Florence lost much of its influence as great artists such as Leonardo and Michelangelo were tempted away from the city and produced many of their masterpieces abroad. Leonardo had invented a number of military machines. After Florence had ended the war with Naples in 1480, Florentine politicians were only interested in keeping the peace and so Leonardo left Florence in 1482 to sell his war machines to other cities such as Milan. Although other places in Italy such as Rome and Venice had important roles to play in the development of the Italian Renaissance, 'the names most clearly associated with the Renaissance remain those of... artists from Florence'. The conditions in Florence at the time made it the ideal place for artists and scholars to work. There were many wealthy patrons, a large collection of talented men, and a system of government that encouraged cultural development. As Hunt observes, 'Florence is certain to remain at the heart of any serious study of the origins and development of the Italian Renaissance'. The importance of Florence in the development of the Italian Renaissance has not been exaggerated. 'Every important Italian City had its Renaissance, but the Florentine, because it was the most complete and the most influential, has a special claim to be considered representative of the whole'. Hunt, The Renaissance, p.ix. Hunt, The Renaissance, p.10. Cronin, The Florentine Renaissance, p.314.","Charles Darwin provided the first explanation for the means by which evolution takes place. This was by the process of natural selection. A key aspect of Darwin's ideas was that species were not immutable, that they could change in their essential characteristics. It had been a common belief that species were immutable because of the writings in the Bible. Genesis mentioned the species that God had created and it was assumed that only God could create new species. Darwin was worried about the implications of his ideas. In January 1844 he wrote to J. D. Hooker claiming that it was 'like confessing a murder' to suggest that species were not immutable. However, many of Darwin's ideas had been in discussion by earlier natural historians and Romantic thinkers in the decades before he published his Origin of Species by means of Natural Selection in 1859. Romanticism began in the mid-eighteenth century and reached its height in the nineteenth century. The Romantics questioned old beliefs, requiring proof before they believed in something. They would study nature closely and believed that man was closely linked with the natural world. In this way Darwin was also a Romantic thinker. Nichols, Ashton, 'The Anxiety of Species: Toward a Romantic Natural History', Romantic Natural History, 2000,  URL  (21 December 2003). Darwin's theory of evolution involves natural selection, which comes as a result of 'a severe struggle for life'. Those animals with characteristics better adapted to their environment will survive to breed and pass on those characteristics while the less well adapted will die off; 'nature was seen as discarding as well as developing various forms of life'. H. Honour suggests that Darwin's theory of evolution was 'based on a distinctly Romantic notion of conflict'. The Romantics rejected classical notions of harmony and order, instead celebrating the wild and chaotic aspects of nature. Darwin, Charles, On the Origin of Species. A Facsimile of the First Edition, with an introduction by Ernst Mayr, (Cambridge, 1976) p.126. Williams, Raymond, Keywords, (Glasgow, 1983) p.121. Honour, H, Romanticism, (London, 1979) p.311. Instead of merely accepting old beliefs, Romantics looked for other answers. They looked to nature as a tool for learning instead of books written by the ancients. Wordsworth takes this view in his poem ""The Tables Turned"". He writes: 'Books! 'tis a dull and endless strife: / ... Let Nature be your teacher'. Darwin adopted this philosophy, studying nature closely. This is how he arrived at his theory of evolution. While employed as a naturalist on the H.M.S. Beagle, Darwin observed the wildlife on the Galapagos Islands. He noticed that there were thirteen species of finch, all with slight variations. He saw that the finches had different shapes and sizes of beaks depending on what their food source was. Those finches eating insects and small seeds had beaks that were small and thin, ideal for catching insects and picking up small seeds. Those finches eating large seeds, on the other hand, had large, short and wide beaks that were capable of crushing the large seeds. These observations of the natural world helped Darwin come up with the idea of natural selection. The finches with beaks better adapted to the food supply were more likely to survive and pass on the same characteristics to successive generations. Wordsworth, William, 'The Tables Turned', Representative Poetry Online,  URL  (21 December 2003) Darwin's grandfather, Erasmus Darwin, had ideas on the mutability and evolution of species before his grandson. In The Temple of Nature in 1803, Erasmus writes: '... as successive generations bloom, / New powers acquire, and larger limbs assume'. Erasmus' ideas on evolution and the natural world 'powerfully influenced' the Romantics. Samuel Taylor Coleridge complimented him as having 'perhaps, a greater range of knowledge than any other man in Europe'. Mary Shelley referred to the 'experiments of Dr. Darwin' in the introduction to the 1831 edition of Frankenstein. Mary Shelley was influenced by Erasmus' ideas that new species can be created. Victor Frankenstein, in the opening pages of the novel, says to Captain Walton: 'A new species would bless me as its creator'. She is questioning the old beliefs that God was the only creator of species. Darwin, Erasmus, The Temple of Nature, (London, 1803) quoted in 'The Anxiety of Species'. Nichols, 'The Anxiety of Species'. Nichols, Ashton, 'Erasmus Darwin', Romantic Natural History, 2000,  URL  (21 December 2003) Shelley, Mary, Frankenstein, (London, 1988) p.12. Shelley, Frankenstein, p.54. The Romantics believed that if you understood nature, you would attain a greater understanding of yourself. 'Romantic poets and writers were hinting at the biological connectedness of all living things'; for example, Coleridge spoke of the 'one life within us and abroad' in ""The Eolian Harp"". It was believed that Man, the animals and all of inanimate nature were linked. This went against the ancient idea of the Great Chain of Being where man was above nature in the hierarchy. In the Romantic view of nature 'animals were regarded not so much as sub-human as other-than-human'. The natural world was not as ordered as was once believed. Darwin's theory of evolution concurs with this line of thinking. He asserted that 'man and the lower animals do not differ in kind'. J. Howard notes how 'Darwin... used human behaviour to illustrate aspects of animal behaviour', further suggesting their link. The Romantics also attributed human characteristics and emotions to nature. William Wordsworth, in ""Lines Written in Early Spring"", humanised nature when he mused that 'every flower / Enjoys the air it breathes', and John Keats personified a nightingale in ""Ode to a Nightingale"" by giving it human feelings with its 'soul abroad / In such an ecstasy'. William Blake also 'hints at a link between human and nonhuman nature'. In ""The Human Abstract"" he writes: Nichols, 'The Anxiety of Species'. Coleridge, Samuel. Taylor, 'The Eolian Harp' quoted in Nichols, 'The Anxiety of Species'. Honour, Romanticism, p.311. Darwin, Charles, The Descent of Man and Selection in Relation to Sex, vol. I, (London, 1871) p.186 quoted in Howard, J, Darwin: A Very Short Introduction, (New York, 1982) p.76. Howard, J, Darwin: A Very Short Introduction, (New York, 1982) p.75. Wordsworth, William, 'Lines Written in Early Spring', Lyrical Ballads, (London, 1798) p. Keats, John, 'Ode to a Nightingale', Odes of 1819 by John Keats,  URL  (21 December 2003) Nichols, 'The Anxiety of Species'. The Gods of the earth and sea Sought thro' Nature to find this Tree, But their search was all in vain: There grows one in the Human Brain.Blake, William, 'The Human Abstract' quoted in Nichols, 'The Anxiety of Species'. The metaphorical tree in the human brain unites man to inanimate nature. This relates to Darwin's theory that Man evolved from more primitive forms of nature. After looking at all the evidence for evolution, Darwin speculated that 'probably all the organic beings which have ever lived on this earth have descended from some one primordial form'. This concept has been alluded to in the writings of some of the Romantics. For example, in his ""Ode to the West Wind"", Percy Shelley writes: 'The sapless foliage of the ocean, know / Thy voice, and suddenly grow grey, with fear'. The plants of the sea respond in the same way to the changing seasons brought on by the west wind as the plants on the land. This is a 'typically Romantic image of sympathetic interactions across... boundaries'. It is implied that sea plants and land plants behave in similar ways because they have a common origin. The Romantics were not evolutionary thinkers, but their view of nature was similar to Darwin's in the way that they could see 'one grand natural system' instead of individual species with no interaction or variation. Darwin, On the Origin of Species, p.484. Shelley, Percy, 'Ode to the West Wind', quoted in Nichols, 'The Anxiety of Species'. Nichols, 'The Anxiety of Species'. Darwin, On the Origin of Species, p.329. Some of the Romantics believed that understanding nature and becoming one with the natural world would bring them closer to the divine. Lord Alfred Tennyson, for example, in a poem entitled ""Flower in the Crannied Wall"" writes: Little Flower - but if I could understand What you are, root and all, and all in all, I should know what God and man is. Tennyson, Lord Alfred, 'Flower in the Crannied Wall'  URL  Wall.html (21 December 2003) They believed that all of nature was divinely ordered and they thought that evolution could have been part of God's plan. This is where Darwin's theory countered this Romantic view. In his Origin of Species he explains how natural selection does not proliferate characteristics that are beneficial to another species, claiming that he 'would almost as soon believe that the cat curls the end of its tail when preparing to spring, in order to warn the doomed mouse'. Natural selection is selfish. It does nothing for the good of another species. For Darwin, this detail 'discriminated between providential creation and evolution by natural selection'. Also, the randomness of natural selection with chance mutations and the struggle for survival 'sounds more like a game of genetic Russian Roulette that the kind of mechanism that would have been instituted by a wise and benevolent Creator'. Darwin describes evolution in a purely naturalistic manner, removing the supernatural element. In explaining the world 'the naturalist has abolished... God'. However, not all Romantic thinkers still attributed the workings of nature to God. Keats' ode ""To Autumn"", Darwin, On the Origin of Species, p.201. Howard, Darwin, p.29. 'From Darwin to Modern Darwinism: An Overview', p.6. Williams, Keywords, p.217. Season of mists and mellow fruitfulness, Close bosom-friend of the maturing sun; Conspiring with him how to load and bless With fruit the vines that round the thatch-eves run, Keats, John, 'To Autumn' Odes of 1819 by John Keats,  URL  (21 December 2003) contains no allusions to the divine. Nature controls it's own processes. 'Keats's ""nature"", like Erasmus [and Charles] Darwin's, exists without need for, or appeal to, any form of ""super nature"".' Nichols, 'The Anxiety of Species'. 'In the century before Charles Darwin, a wide range of scientists and writers saw human beings as organisms with important connections to their environment'. This included the Romantics. Darwin's theory of evolution did not counter the Romantic view of nature. There were many similarities between Darwin's ideas and those of the Romantics. They could see that nature was not static and unchanging, but that it was a dynamic system that ever changing. The Romantics had a sense that Man was connected to the natural world. Darwin took this idea a step further in his theory of evolution. Although Darwin's purely naturalistic and secular theory opposed the views of some of the Romantic thinkers such as Tennyson who believed that nature was God's creation, certain Romantics like Keats where questioning the divine nature of world. Nichols, 'The Anxiety of Species'.",False
66,"The term Renaissance comes from the Italian word rinascita, meaning rebirth. The movement looked back to the superior aspects of ancient times and integrated them with the new developments of the time. The Italian Renaissance is generally associated with the fifteenth century. This period, however, can be stretched in either direction. For example, the term ""Renaissance"" has been used to describe some thirteenth-century artists, and the great artists of the Renaissance were active until the end of the sixteenth century. The rebirth of the visual arts was one aspect of the Renaissance but the development of humanism and the classical studies of the universities were as important. There were also advancements in music, literature, science and government practices during this period. How far Florence was responsible for these developments can be debated. Robert Black asserts that 'the elitism of Florentine society and the monopoly which the Florentine patriciate had over classical culture made Florence the first and greatest capital of the Renaissance'. Jocelyn Hunt, on the other hand, believes that 'the assumption that Florence was pre-eminent is... Burckhardtian, since, it is argued, the scholars of the Renaissance were based in Rome, and in the other universities'. Porter, Roy, and Teich, Mikulas (eds. ), The Renaissance in National Context, (Cambridge, 1992), p.37. Hunt, Jocelyn, The Renaissance, (London, 1999), p.ix. Burckhardt identified Florence as the 'cradle of the Renaissance'. It appears as though 'the components for a successful intellectual movement were all to be found in Florence at the start of the fifteenth century'. The city was wealthy, banking was highly developed, and papal revenues were often invested in Florence rather than the volatile and dangerous city of Rome. Florence had a high population. It was active in manufacture, trade and commerce. Textile production and metalworking were successful industries in Florence. The city was in a prime position for trade; it was close enough to the coast for sea trade but far enough to keep it safe from pirate attacks. It was also able to trade over the Alps and was close to the Middle East so merchants could bring back the luxury goods available there, such as silk and spices. Florence was primarily a city orientated towards craft-industrial production rather than trade like Venice. This appears to have been a more favourable environment for artists to be produced as 'it was only when Venice turned from trade to industry, at the end of the fifteenth century, that Venetian art caught up with that of Florence'. Hunt, The Renaissance, p.5. Hunt, The Renaissance, p.8. Burke, Peter, The Italian Renaissance: Culture and Society in Italy, (Oxford, 1986), pp.49-50. Giorgio Vasari, a contemporary historian addressed the issue of 'the outsized contribution of Florence to the arts'. He believed that there were three factors that made Florence different from other Italian towns. Firstly, Florentines were free and, therefore, became critical and would not accept things that were not of the highest quality. Gene A. Brucker notes that ' this appreciation of quality, and a corresponding disdain for the shoddy and inferior, became a characteristic feature of the Florentine mentality'. Secondly, as Gregorio Dati, a contemporary Florentine, explains, 'Florence is situated in a naturally wild and sterile place', so that the people had to work hard, using their intelligence in order to live comfortably. They would travel and were innovative and experimental. Thirdly, the people had a 'greed for honour and glory which that air generates in men of every occupation'. Burke, The Italian Renaissance, p.29. Brucker, Gene A, Renaissance Florence, (London, 1969), p.220. Hunt, The Renaissance, p.13. Burke, The Italian Renaissance, p.29. Wealthy citizens would spend their riches on art and culture. The amount they spent was seen to be an indication of their power and status. Florentine men did not own armies, rather hired mercenaries when they were needed. This meant that they had more to spend on luxuries. Giovanni Rucellai, a Florentine merchant, wrote in 1473 that 'earning and spending are among the greatest pleasures that men enjoy in this life'. He, himself, spent much of his wealth on the buildings of Florence, including his house and the loggia opposite, the church of Santa Maria Novella and the church of San Pancrazio. Commissioning artists gave them the opportunities to try out new styles and techniques as well as copying the styles of the ancients. Florence was important in the development of the Italian Renaissance because the artists working there were given these opportunities. Giovanni Rucellai quoted in Hunt, The Renaissance, p.13. The Florentine political system was inspired by antiquity. It compared to those of the ancient Greek city-states or, as Evelyn Welch observes, 'Florence and Venice found their heritage in the Roman Republic'. The king, the Church and the nobility did not rule the Florentine people as in other places. Instead they ruled themselves, either by electing nine men for the Signoria or by holding office themselves for the two-month term. Committees were appointed to oversee administration. The government viewed everybody as equals, with nobody being above the law and everything being done to benefit the whole community. Michelangelo's statue of David was commissioned to depict the strength of Florence. David is an example of the Renaissance style of realism in art. Vincent Cronin has discussed the fact that Burckhardt took Renaissance Italy as a whole and made no distinction between what happened in republican Florence and other cities which were ruler over by tyrants. He argues that 'such a distinction is essential to an understanding of all but the most general points'. The political freedom that Florentine men enjoyed serves to explain Florence's leading role in the development of the Italian Renaissance. Cronin states that 'Florence was the power-house which generated far and away the largest number of influential ideas'. Welch, Evelyn, Art and Society in Italy 1350-1500, (Oxford, 1997), p.241. Cronin, Vincent, The Florentine Renaissance, (London, 1967), p.313. Cronin, The Florentine Renaissance, p.313. In the late fifteenth century, the Medici family took control of most of the councils and committees, threatening the old system. They were firmly established in Florence by the second half of the fifteenth century. Cronin describes the fact that the Medici patronized art as a 'remarkable innovation, since Italy had no tradition of individual lay patronage'. Florence led the way in this new fashion. The Medici employed artists to enhance their status. Lorenzo de'Medici invested in art that would 'make the city of Florence famous'. The freestanding dome of Florence Cathedral, built by Filippo Brunelleschi, was the first of its kind. It was copied in other places such as Rome with the construction of St. Peter's. The dome was inspired by ancient architecture such as the Pantheon in Rome. Brunelleschi used the system of arithmetic proportions, which he had taken from the work of Vitruvius, in his design. Brunelleschi also used new techniques including line point perspective and innovations in engineering that inspired other Renaissance artists and architects. Florence's contribution to the Italian Renaissance's attempt to turn away from the medieval gothic style in architecture and return to the ways of ancient Rome and Greece 'was little short of astounding'. Cronin, The Florentine Renaissance, p.176. Hunt, The Renaissance, p.10. Porter and Teich (eds. ), The Renaissance in National Context, p.21. Another reason why Florence took a leading role in the developments of the Italian Renaissance was the relationship between patrons and artists in Florence which was 'remarkable by comparison with conditions in other circles'. There was a friendly understanding between them which was in part due to classical texts which exalted the artist and emphasized the esteem that they received in ancient times. These texts 'were not lost on the Florentines'. The relationship between Cosimo and Donatello 'went much deeper than that of employer and employee'. Cosimo gave Donatello a pension in his old age, which was the first known instance of this type of help given to an artist. The 'championing of artists by Florence's most prominent citizen naturally raised their status'. Artists in Florence were allowed to use their own genius. Leonardo was educated in Medici Florence and was 'the culmination of the versatile artist whose rise took place in the city by the Arno'. When he moved to Milan, however, he was not understood. He painted the Last Supper in Milan but the Dominican Prior wanted him to work quickly and complained when he spent half a day looking at the painting without adding to it. Leonardo replied that 'men of genius sometimes accomplish most when they work least'. Similarly, in Rome, relations between patron and artist were not as amiable as in Florence. Cronin notes how 'painters were lumped together with carters and grooms' and were often given poor food. The artist Davide Ghirlandaio was served uneatable food while working in the abbey of Passignano and was so angry that he dumped his soup over the friar and beat him with the loaf of bread. Cronin, The Florentine Renaissance, p.179. Cronin, The Florentine Renaissance, p.177. Cronin, The Florentine Renaissance, p.178. Cronin, The Florentine Renaissance, p.180. Cronin, The Florentine Renaissance, p.188. Cronin, The Florentine Renaissance, p.188. Cronin, The Florentine Renaissance, p.179. Cronin, The Florentine Renaissance, p.180. Dati said of Florence: 'so great is the number of talented and rich men that they are unequalled in the world'. The talented men produced the great works of the Italian Renaissance and the rich men enabled them to do so. Florentine artists and sculptors had 'immeasurable influence... on the future development of the visual arts in the renaissance'. Masaccio, a Florentine, was the first painter known to have adopted Brunelleschi's system of line-point perspective and make use of lighting and shading in his works such as the frescoes in the churches of Santa Maria Novella. Florentines also led the way in the revival of classical sculpture. Donatello was responsible for the revival of the classical art of large-scale bronze casting. He was also the first artist since antiquity to study anatomy and represent the nude in his works. Gregorio Dati quoted in Hunt, The Renaissance, p.13. Porter and Teich (eds. ), The Renaissance in National Context, p.23. In Italy, painters, sculptors and masons often belonged to guilds. In some places, such as Milan and Florence, painters had their own guilds. Most of these were quite restrictive. For example, the guild in Padua forbade members to give or sell to non-members and did not allow work to be brought from another district to be sold in Padua. The guild in Venice was also restrictive. Albrecht Dürer commented on the strict nature of the guild when he visited Venice in 1504, describing how they summoned him before the magistrates three times and had him pay four florins to the guild. The guilds in Florence, on the other hand, were not as powerful. The government held that not every craftsman had to join and it allowed people from other places to work in Florence. Peter Burke believes that 'this more liberal policy, which exposed local tradition to stimuli from outside, may help to explain Florence's cultural lead'. Burke, The Italian Renaissance, p.68. Burke, The Italian Renaissance, p.68. Florence's important contribution to the arts can be seen in the proportion of Italy's cultural elite, the people whose creative abilities are recognized in society, which originated from around Florence. Burke has calculated that around 26 per cent of the elite came from Tuscany, which had only 10 per cent of the population, while the Veneto had 20 per cent of the population but only around 23 per cent of the elite. Burke believes that 'Rome's poor contribution needs emphasis' as it produced less than its share of Italy's cultural elite. Rome was no longer the beautiful, classical city that it was in ancient times. 'The Middle Ages reversed Augustus's boast that he had found Rome a city of brick and left it one of marble'. Materials from ancient monuments had been taken for other purposes and marble had been burnt for lime. Rome was in ruins with woods and undergrowth which sheltered foxes, wolves and hares. Rome's population was smaller then Florence's and was much poorer. This helps to explain Florence's dominance in the development of the Italian Renaissance. Burke, The Italian Renaissance, p.44. Burke, The Italian Renaissance, p.45. Cronin, The Florentine Renaissance, p.167. Dante was an important figure in the development of Renaissance literature. He learnt the universal ideals explained by Thomas Aquinas in the Florentine schools. He wrote the Divine Comedy in the local Tuscan dialect instead of Latin. Brucker describes the Divine Comedy as 'a Florentine poem, replete with the particular values, emotions, and concerns of that community'. Florence was important to Dante and Dante was a challenge and inspiration for later generations of Florentine intellectuals. Giotto di Bondone was a Florentine artist in the fourteenth century who, like Dante, influenced later generations of Florentines. He was innovative in the way he painted frescos; he created naturalistic and lifelike scenes that were also grandiose and monumental by humanizing the wooden, stylised figures of Byzantine art. His scenes in Santa Croce from the life of Saint Francis are examples of this style. His frescos had 'a profound impact upon the revolutionary generation of Florentine artists in the early Quattocento'. Brucker, Renaissance Florence, p.215. Brucker, Renaissance Florence, p.216. 'The Florentine contribution to Renaissance culture was not limited to a few specialized areas; it encompassed many fields and disciplines'. The city's jurists, physicians and theologians made considerable contributions to their fields. In many aspects of the Renaissance, 'Florence led the way, Italy followed'. The work of the Florentines: Coluccio, Bruni, Poggio and Alberti, paved the way for educational, political and moral discussions in Ferrara; Bruni and Ficino's translations of Plato enabled Isabella d'Este's circle in Mantua to discuss Platonism; and Antonello drew on the geometrical notions evolved in Florence when painting his Virgin Annunciate. Lionello Venturi summed it up by his comment that 'the new Renaissance style emerged in northern Italy a generation later than in Florence, and as the direct result of work done on the spot by Tuscan artists'. Brucker, Renaissance Florence, p.213. Cronin, The Florentine Renaissance, p.313. Cronin, The Florentine Renaissance, p.314. Florentine citizens were concerned with the study of the classics in the second half of the fifteenth century. Burckhardt asserts that the Renaissance came about because of 'those citizens, mostly Florentines, who made antiquarian interests one of the chief objects of their lives'. Niccolo Niccoli, a Florentine contemporary, told the young Piero de'Pazzi: 'if you do not study the classics, you will be considered a nothing'. In the study of the classics, 'Florence was ahead of the rest of Italy'. By the middle of the fifteenth century, nearly every prominent family in Florence, such as Strozzi, Medici and Rossi, had a humanist scholar among their numbers. It was also in Florence that the old Gothic script of angular closely spaced lettering was rejected in favour of the more rounded and spaced out script known as humanist miniscule. This style was adopted by the rest of Renaissance Italy. Burckhardt, Jacob, The Civilisation of the Renaissance in Itlay, (Harmondsworth, 1990), p.143. Niccolo Niccoli quoted in Hunt, The Renaissance, p.14. Porter and Teich (eds. ), The Renaissance in National Context, p.36. Florence was an important center for the development of history writing until the sixteenth century. The Florentine Leonardo Bruni was a key figure in this development. He rejected the medieval style of writing history chronicles and revived the Roman historian, Livy's, model of writing the history of a city-state with an introductory book on the origins and the early history of the city followed by a detailed account of events, organized as a year-to-year narrative and grouped into books. He wrote the History of the Florentine People in this style. Bruni was also responsible for reviving the Ciceronian dialogue, text written as though in conversation, which was used by Cicero in ancient times. He wrote Dialogues to Pietro Paolo Vergerio of Istria in this style, which became the standard form of Renaissance literature. Marsilio Ficino, leader of the Platonic Academy of Florence, was responsible for translating the works of Plato which were written in Greek. He also wrote his own philosophical works such as Platonic Theology Concerning the Immortality of the Soul. James Hankins has commented on the 'leading role played by Marsilio Ficino and Neoplatonic Philosophy on the cultural life of the High Renaissance'. The Platonic Academy of Florence was Ficino's primary vehicle for the spread of his ideas. There has been some debate over the significance of the Academy. Gustavo Uzielli, a Toscanelli scholar, has even suggested that the Academy was nothing but a fable. Arnaldo della Torre, on the other hand, wrote Storia dell'Academia Platonica di Firenze in 1902 in which the importance of the Academy was stressed. The authenticity of Ficino's letters, which Uzielli had suggested to be forgeries, was defended. Della Torre describes how the Academy was founded by Cosimo de'Medici in 1462 and how Ficino was appointed the head. According to Della Torra, the Academy held regular activities such as lectures, disputations and banquets. The Academy was made up of the Medici patrons as well as around one hundred prominent statesmen, poets, orators, doctors, lawyers and ecclesiastics of the later fifteenth century. James Hankins, however, suggests tah 'Ficino did have an academy of a sort, but if so, it was a thing quite different, and much less important, than has generally been thought'. Hankins, James, 'The Myth of the Platonic Academy of Florence', Renaissance Quarterly, 44, (1991), p.429. Hankins, 'The Myth of the Platonic Academy of Florence', p.430. Hankins, 'The Myth of the Platonic Academy of Florence', p.432. Hankins, 'The Myth of the Platonic Academy of Florence', p.433. Coluccio Salutati, the owner of the largest library of ancient manuscripts in Italy after the death of Petrarch, lived in Florence. Attracting new talent into the city, 'he was perhaps the greatest cultural asset enjoyed by the city'. He was appointed first chancellor in 1375. Salutati's disciples included some of the next generation's leading humanistic scholars such as Leonardo Bruni, Pietro Paul Vergerio and Poggio Bracciolini. Being a statesman and a scholar, Salutati bridged the gap between the world of learning and the world of commerce and politics. Humanists would point to Salutati's reputation and political influence when critics would question the value of classical studies. Slautati was important in the development of the Italian Renaissance and he was a Florentine. Brown, Alison, The Renaissance, (London, 1988), p.36. Florence had a key role in the development of the Italian Renaissance because its cultural pre-eminence coincided with the largest territorial expansion of the time. Between 1350 and 1490 Florence was in control of most of Tuscany. Florence fought against the papacy, Milan and Naples. In 1415, Bruni compared the victory of Florence over Milan, the capture of Pisa, and the defeat of the Ladislas, to the victory of the Roman Republic over Carthage. Antiquity was used to legitimize conquests and to establish and maintain authority by comparing the Florentine government with the successful Roman Republic. Porter and Teich (eds. ), The Renaissance in National Context, p.27. Towards the end of the period of the Italian Renaissance, Florence was still a place of importance. Machiavelli was at work in Florence writing about how men should not only admire the ancients but attempt to put the Roman model into real practical use. His political thought was highly influential. Florence also had influence on the scientific developments of the Renaissance. Copernicus claimed that the earth revolved around the sun, not the other way round which is what had previously been believed. Cronin asserts that this claim 'was a choice, not a discovery, and the basis of Copernicus's choice-as later those of Kepler and Galileo-was the Platonic cosmology revived in Florence'. Cronin, The Florentine Renaissance, p.144. Florence appears to have had much influence on the developments of the Italian Renaissance. Jocelyn Hunt, however, has argued that 'it may be the case that historians have examined the features which made Florence exceptional, and calculated that these are the essential preconditions for the Renaissance'. In this case, it would appear that the importance of Florence has been exaggerated. In the early stages of the Italian Renaissance, in the late thirteenth to early fourteenth centuries, 'Florence had a relatively minor role'. Florence had no unique connection with antiquity as Rome did. Its claims to antiquity were 'easily matched or surpassed by many other Italian cities'. It has been suggested that 'the new age began in Padua and the other urban communes of northern Italy in the 14th century, where lawyers and notaries imitated ancient Latin style and studied Roman archaeology'. Petrarch, perhaps the most influential figure of the Renaissance, never lived and worked in Florence. He achieved much greater fame as a writer than his Florentine equivalent, Salutati. Hunt, The Renaissance, p.8. Porter and Teich (eds. ), The Renaissance in National Context, p.21. Porter and Teich (eds. ), The Renaissance in National Context, p.21. Pioch, Nicolas, 'La Renaissance', Web Museum, 14 October 2002. URL  (1 April 2004) Petrarch recognized that Rome was the capital of the ancient world and, as it was the interest in the achievements of the ancient world that characterised the Renaissance, it has been suggested that Rome was the most important center of the Italian Renaissance. 'The classical revival was at its strongest in Rome, where the largest number of classical buildings remained to inspire the artist'. The pope was the most influential patron. Rome was important 'as a center of patronage which attracted creative individuals from other parts of Italy'. Nobody could outbid the papacy when commissioning the work of an artist. The pope's private library in the Vatican increased in size, containing 3,600 works by 1484, as Italian scholars began to search for the works of classical writers. Rome was an important center where 'the literary and artistic renaissance worked closely together'. For example, Raphael and his pupils painted a series of frescos in Agostino Chigi's Villa Farnesina near the Tiber depicting the story of Amor and Psyche from Apuleius' Golden Ass. Both form and content were taken from classical Roman art and literature. Fritz Saxl said that 'the Amor and Psyche paintings represent the greatest effort to illustrate pagan myths in the spirit of the classical marbles as they reappeared from beneath the Roman soil'. Hunt, The Renaissance, p.10. Burke, The Italian Renaissance, p.45. Porter and Teich (eds. ), The Renaissance in National Context, p.44. Porter and Teich (eds. ), The Renaissance in National Context, p.44. Although Rome was left in ruins after the Middle Ages, it was still an inspiration to artists from all over Italy. Brunelleschi, a Florentine architect, chose to study in Rome instead of Florence. This was partly due to the fact that, during the period at the beginning of the fifteenth century, Florence was in a critical struggle with Giangaleazzo. He and his friend Donatello would systematically inspect and excavate the ruins of Rome with teams of labourers. Brunelleschi wished to discover how the Romans had built their vast constructions. He measured the thickness of walls, columns, arches, bricks and blocks of marble. The Pantheon was a huge inspiration, as it had not been touched since ancient times. Brunelleschi would remove tiles from the roof to study the ribbing of the shallow cupola. He looked at the number of square holes in large stones and decided that the Romans must have used some sort of machine to move them. This gave him the idea of the ulivella, a machine to move large stones by a crane without cords. Brunelleschi studied the ancient buildings of Rome for about twelve years. When the dome to Florence cathedral was being planned, a number of problems arose. The diameter of space to be spanned was 138 feet. The only cupola of a similar dimension was the Pantheon which Brunelleschi had been studying. Brunelleschi came up with a design for the dome and was awarded the commission of building it. The dome of Florence cathedral may have been the first of its kind but the inspiration and the savoir-faire came from Rome. Brucker believes that 'Florentine intellectual life... was matched, to some degree, by Milan, Venice and Naples'. Rulers all over Italy tried to attract the great Renaissance artists and the other Italian cities, such as Milan, Urbino, Ferrara, Venice, Padua and Naples, soon became 'powerful rivals in the spreading wave of change'. Leon Battista Alberti worked on some of the most progressive architecture of the new Humanism in Rimini and Mantua, and Andrea Mantegna's paintings in Padua portrayed a personal formation of linear perspective, antiquarianism and realistic technique. The Venetian school was growing in strength with developments such as Giovanni Bellini's poetic classicism. Other cities took the lead over Florence in certain areas. Venice, for example, became the center of the new printing industry that enabled Renaissance literature to be widely circulated. Burke believes that 'the Venetian cultural achievement of the period... long received considerably less than its due'. He explains that this has happened because there were less studies of the social history of the arts in Venice than there were of Florence. Vasari, a Renaissance historian, is said to have had a 'Tuscan bias', which may have exaggerated Florence's role. There was no material to counter this view. There was an attempt in the sixteenth century by a Venetian to collect material on the lives of painters around Venice but this attempt was never completed and published. There was also no Venetian equivalent to Wackernagel's book on Florence. Burke has made an effort in The Italian Renaissance: Culture and Society in Italy, 'to avoid giving the Florentines more than their fair share of the limelight', by discussing important artists and writers from all over Italy. Brucker, Renaissance Florence, p.215. Pioch, Nicolas, 'La Renaissance: Italy', Web Museum, 14 October 2002. URL  (1 April 2004) Burke, The Italian Renaissance, p.39. Burke, The Italian Renaissance, p.39. Burke, The Italian Renaissance, p.39. Venice was also the 'musical center of Italy' during the high Renaissance. Venice was the center of trade with the east because of its prime position on the coast. It was a wealthy, powerful and cosmopolitan city. It was at a 'cultural pinnacle' during the sixteenth century with composers like Andrea and Giovanni Gabrieli, Adrian Willaert and Claudio Merlo thriving in 'a musical environment that called for grand works that reflected the glory of Venice'. There were processions, ceremonies and festivals where composers could exhibit their music. St. Mark's cathedral was the center of Venetian music where new styles and techniques were developed. The cathedral was built in the style of eastern basilicas with two choir lofts and two organs. Composers had to write for separate choirs and they used the effects that this could produce. Composers also had to write music that was more chordal with emphasis on sound and clarity of text to counter the effects of the acoustics in the cathedral. Composers used instruments in the choirs which was unusual. Justin Klotz asserts that 'throughout the... Renaissance, Venice, the ""Most Serene Republic"", was one of the most exciting cities in Europe'. Klotz, Justin, 'The Venetian School', MUSL 242, 11 October 1998. URL  (1 April 2004) Klotz, 'The Venetian School'. Klotz, 'The Venetian School'. Klotz, 'The Venetian School'. Florence did not lead the way in humanist and classical education. It was 'the custom of the city' for young boys to learn 'enough to work as a merchant' rather than learn about the humanist subjects of grammar, rhetoric, poetry, history and moral philosophy. Key humanist teachers of the Renaissance included Gasparino Barzizza, Guarino Guarini da Verona and Vittorino da Feltre, who were all active in Northern Italy, not Florence. Alison Brown observes how 'in one respect Florence was - or at any rate seemed to be - worse off than many of her neighbours: she lacked a well-established university'. A university had been founded in 1348 but it never achieved the status of the universities of Bologna and Padua. For a long time it only offered vocational training for local students in medicine, law and theology. It was always competing with the abacus schools which Florentine merchants favoured more highly. Humanist education did not become important in Florence until the second half of the fifteenth century and then it was predominantly for elite families. Porter and Teich (eds. ), The Renaissance in National Context, p.32. Brown, The Renaissance, p.25. During the Renaissance, the Medieval Latin language and spelling was rejected, as there was a desire to return to the purer Latin of ancient Rome. The leading centers of the rebirth of Latin in Italy were Padua, Arezzo, Bologna and Verona. It did not begin in Florence, only reaching it by around 1400. By the mid-fifteenth century Rome was dominant in the development of the new language. However, Florence did hold some importance because of the arrival of the Greek Manual Chrysolaras at the turn of the fifteenth century who taught and inspired many students including Leonardo Bruni and Poggio Bracciolini, who were to become important Renaissance thinkers. Bruni produced new translations of Aristotle's Greek text Politics and Ethics. Latin was derived from Greek so Chrysolaras, being Greek, was knowledgeable and influential. By the end of the fifteenth century, Florence became dominant because of important Florentine thinkers such as Angelo Poliziano. In the late fifteenth and sixteenth century, Florence lost much of its influence as great artists such as Leonardo and Michelangelo were tempted away from the city and produced many of their masterpieces abroad. Leonardo had invented a number of military machines. After Florence had ended the war with Naples in 1480, Florentine politicians were only interested in keeping the peace and so Leonardo left Florence in 1482 to sell his war machines to other cities such as Milan. Although other places in Italy such as Rome and Venice had important roles to play in the development of the Italian Renaissance, 'the names most clearly associated with the Renaissance remain those of... artists from Florence'. The conditions in Florence at the time made it the ideal place for artists and scholars to work. There were many wealthy patrons, a large collection of talented men, and a system of government that encouraged cultural development. As Hunt observes, 'Florence is certain to remain at the heart of any serious study of the origins and development of the Italian Renaissance'. The importance of Florence in the development of the Italian Renaissance has not been exaggerated. 'Every important Italian City had its Renaissance, but the Florentine, because it was the most complete and the most influential, has a special claim to be considered representative of the whole'. Hunt, The Renaissance, p.ix. Hunt, The Renaissance, p.10. Cronin, The Florentine Renaissance, p.314.","London was growing rapidly during the Tudor and Stuart period. As the population of the country as a whole doubled, the population of London quadrupled. In 1500 the population was just 50,000, but this had risen to 490,000 by 1700. There was a low fertility rate as only a small proportion of people were married and the poor living conditions in the capital resulted in a high mortality rate; yet the population continued to rise. The explanation for this comes from the high level of migration into the city. In order to counter the effects of low fertility and high mortality rates on the population, London needed around 7,000 migrants entering the city every year to grow at the rate it did. E.A Wrigley estimated that one in six people had lived in London at some stage in their life by the end of the seventeenth century. A. L. Beier and R. Finlay have raised the question of why it was London that attracted so many people. There were between 700 and 800 market towns in England, including a dozen major provincial cities, so 'the country was not devoid of possible competitors to London'. The motivations for migration into the capital are numerous. Some people's reasons to move to London were to get away from the countryside and smaller towns (push factors) and others were motivated by what the city had to offer (pull factors). E. A. Wrigley, 'A Simple model of London's importance in changing English society and economy 1650-1750, Past and Present, (1976) p. 37. A. L Beier, and R. Finlay, 'The Significance of the Metropolis', A. L Beier, and R. Finlay (eds. ), London 1500-1700, (London, 1986) p. 4. As Beier and Finlay have asserted, 'London evidently exerted a tremendous pull on migrants from an early date'. One explanation for this may have been the unique situation in London in which the capital city was also a great port. One of the main factors that encouraged migration was the centralisation of England's economic life in London. The shift from Mediterranean to Atlantic trade was beneficial to England, and hence to London which was one of the major ports. By 1700, London dominated English trade. It dealt with eighty per cent of imports and sixty-nine per cent of exports. By the mid-1540s London's cloth exports accounted for eighty-six per cent of the country's total woollen exports. Professor J. H. Hexter comments on the number of merchants who 'lived in the town but were not of it, and yet won there that beatitude of riches that enabled them to go back to where they came from'. Men would come to London to make their fortunes as merchants. The cloth trade 'unquestionably stimulated London's economy and migration to the capital'. Beier, and Finlay, 'The Significance of the Metropolis', p. 10. R.G. Lang, 'Social Origins and Social Aspirations of Jacobean London Merchants', Economic History Review, 27 (1974) p. 91-2. Beier, and Finlay, 'The Significance of the Metropolis', p. 15. It could be argued that trade did not have much of a direct impact on migration as it only involved a small number of merchants. However, it did have considerable impact in other ways. Profits from trade were invested in other industries such as manufacture. For example, a trader called Thomas Myddleton set up a sugar refinery and a draper called Thomas Cullum financed a dying business. This sort of activity created job opportunities which were major pull factors for migrants. Beier, and Finlay, 'The Significance of the Metropolis', p. 15. There were many job opportunities in the city for professionals such as doctors and lawyers. There were also many jobs involving unskilled labour such as shipping and building. People could find jobs as street sellers or as water men taking people up and down the Thames in boats. Most migration was by younger people wising to find placements as apprentices or domestic servants. By 1600 the number of apprentices migrating to the city was between 4,000 and 5,000. During the period new craft guilds were established and membership to traditional ones increased. Production spread to the suburbs and provided thousands of jobs for migrants. It became an industrial area and specialised in the production of luxury goods in the mid-seventeenth century. 'These changes in turn attracted migrants' both for the job opportunities and the goods produced. Wages were a huge incentive as they were fifty per cent higher in London than in provincial areas. Moving to London for the economic benefits of more work and better wages was betterment migration. People found opportunities to rise to freemen, masters and girl wardens. For example, John Browne claimed in 1631 that he could not make a living dressing hemp in Leicester, but after going to London he was profitably self-employed and a freeman of the city and of the company of merchant tailors. People would follow their neighbours to London to see if they could have the same success. The story of Dick Whittington became part of English folk-lore at the end of the sixteenth century and Beier and Finlay note how there was 'a popular belief that the city's streets were paved with gold for the immigrant'. Beier, and Finlay, 'The Significance of the Metropolis', p. 15. P. Clark and P. Slack, English Towns in Transition 1500-1700, (London, 1976) p. 65. Beier, and Finlay, 'The Significance of the Metropolis', p. 17. As well as being the centre of economic life, London was also the centre of political action. This centralisation was a major pull factor for migration to the city. 'Generations of historians have connected the rise of the modern capital with its political role'. The number of Peers increased from around 30 in the 1560s to 90 in the 1630s. They would bring with them their family and servants which added to the growing population. The Royal household grew under the Reign of Charles I. While under Elizabeth there were around 1,000 in Court, this increased to around 2,600 under Charles I. The Court used to move around the realm in the High Middle Ages, but from the reign of Edward IV it remained in or around London. This 'attracted to the city the custom and residence of county landowners' who sought favour and office. The number of resident gentlemen in London increased and the number of MP's rose from 300 in the early sixteenth century to 500 during the mid-seventeenth. The Court also provided patronage for musicians and artists which drew them into the city. Beier, and Finlay, 'The Significance of the Metropolis', p. 11. Clark and Slack, English Towns in Transition, p. 63. However, Beier and Finlay believe that 'the direct influence of Government on London's growth has probably been overstated'. The size of the central government does not change much between 1580 and 1640 when London experienced its biggest population increase. The increase in officials came after 1650. By 1725 the number of officials had risen to around 2,700 from only 400 under Elizabeth. Nevertheless, even 2,700 officials and their family and servants would have only taken up around 'five and a half per cent' of the population of London; therefore, government membership cannot be considered one of the major driving factors for migration. Beier, and Finlay, 'The Significance of the Metropolis', p. 11. Beier, and Finlay, 'The Significance of the Metropolis', p. 12. London was the legal centre of the country. The four Inns of Court, which were like England's third university, experienced a five-fold increase in admissions between 1550 and 1650. During term time there were around 1,000 students populating the capital. The Inns also employed many staff such as ushers, agents and scribes. The judicial centre of the country was Westminster, which observed a huge expansion in legal business in the sixteenth century. The land market expanded as church land was seized after the Reformation. This generated much legal business as the land was fought over. Women also engaged in legal business which brought them to London. They brought cases such as defamation, sexual offences and marriage disputes. This was a pull factor that drove migrants to the capital. During the Tudor and Stuart period 'the landed upper classes increasingly resorted to London for business and pleasure'. The London Season was from October to June. Rural landowners would travel into the city for entertainment and shopping. Road transport was improving in the seventeenth century and the stagecoach was introduced which made it easier for frequent travel into the city. London was a commercial city where items that were not available elsewhere could be obtained, including such exotic goods as tea, coffee, wine, spirits and tobacco. The West End became an area of high quality shops in the late seventeenth century and attracted many rich landowners. The merchants helped to finance the extravagant spending of the aristocracy, encouraging them to come to the capital, by providing them with loans and pawnbroking arrangements. The elites were also enticed to live in London by the newly built mansions and squares. This 'influx of the landed classes boosted the city's population'. Beier, and Finlay, 'The Significance of the Metropolis', p. 11. Beier, and Finlay, 'The Significance of the Metropolis', p. 13. It was not only the rich who came to London for the social aspects of city life. Many would come to see family and friends. There was much entertainment that was available to the poor as well as the rich. There was bull baiting and theatre productions in the Swan and the Globe. Brothels were also part of London life. Songs, jests and fables were produced in London and were discussed in the markets and alehouses. The alehouses were important places where migrants could procure food, drink, shelter, loans and contacts for finding work. London coffee houses also became popular towards the end of the seventeenth century, which were important in the development of the bourgeois public sphere as described by Jürgen Habermas. They were places where members of the general public would meet to discuss politics. People would move to London to be nearer this type of cultural movement. Young women migrated to London as it was a place 'where the rigid controls of patriarchy... [were] more easily evaded'. Single women could socialise and court more freely than in the smaller, more kin-based communities of the rest of the country. These women migrants often married later then the women from where they originated. They often worked and earned independently for a few years before marriage. The anonymity of the city allowed some women to start anew if their reputation had been damaged at home. This type of migration involved both push and pull factors as the women were escaping the restrictions of the smaller towns and countryside and were attracted by the freer lifestyle in London. Laura Gowing, '""The Freedom of the Streets"": Women and Social Space, 1560-1640' in P. Griffiths and M. Jenner (eds. ), Londinopolis, Essays in the Cultural and Social Writing of Early Modern London, (Manchester, 2000) p. 132. Some people came to London for the specialist medical practitioners. By 1704 there were around 1,000 London apothecaries. People came to consult other specialists such as astrologers like William Lilly for which 'the Stuart city was also famous for'. Other people migrated to London for the educational opportunities available. The literacy levels in London were higher than the national average for men and women. Schools taught a humanist curriculum and there were public lectures that anybody could attend. Books were more readily available in London as it is where the printing presses were. Ballads and cheap pamphlets were produced on a large scale and provided writers with a regular, if low, income. Clark and Slack, English Towns in Transition, p. 68. Beier and Finlay assert that 'people were pushed to London as well as pulled'. As well as going to the city to better themselves, many were pushed there by poverty. The latter became increasingly common between 1500 and 1700, whereas in medieval period migration primarily involved reputable workers wanting to improve their situation by becoming apprentices in the guilds. Clark and Slack have argued that it was the poor and destitute who were seeking labour and charity that 'probably formed the bulk of the 8,000 or so net immigrants whom late Stuart London absorbed every year'. This suggests that migration into early modern London was driven primarily by push factors. Beier, and Finlay, 'The Significance of the Metropolis', p. 10. Clark and Slack, English Towns in Transition, p. 64. As London increased in size, it needed more food. This placed an increasing demand on the agrarian economy which was forced to make improvements to increase production. These changes included the introduction of enclosure and engrossing, which led to depopulation in the countryside as less labour were needed. Beier notes how these changes 'pushed surplus labour out of advanced farming regions to London'. Most people in the countryside probably migrated to other areas of the countryside or to smaller towns before moving to London. However, this still effected the migration levels into the capital because increasing populations in small towns from migration and general national population growth meant worsening living standards and higher levels of poverty and unemployment. These were push factors that made people want to leave other areas of England and move to London where there were more job opportunities and higher wages. This was often a form of subsistence migration because people would have to move to London as they could not support themselves or their family where they were. Beier, and Finlay, 'The Significance of the Metropolis', p. 10. The success of London's trade and manufacture 'had wide ramifications'. Provincial out ports such as Southampton suffered as trade became centralised in London. Clark and Slack have observed that, 'as London continued to consolidate its lead in overseas and domestic trade, many provincial towns were forced to recognise the evident disadvantages of their position'. It may have also frustrated the depression of inland towns such as Coventry because of London's ready access to Low Countries' markets. 'Poverty not infrequently threatened to overwhelm English towns' and pushed people out and into the capital. Beier, and Finlay, 'The Significance of the Metropolis', p. 15. Clark and Slack, English Towns in Transition, p. 103. Clark and Slack, English Towns in Transition, p. 121. 'London's massive growth... took place mainly because of two developments which encouraged migration: first, the centralisation in London of the nation's political and economic life... secondly, upheavals in the provincial economies', which have been discussed above. This explanation contains both push and pull factors that drove migrants to the capital. It was a combination of the two that encouraged so many to migrate to London in the early modern period. Beier, and Finlay, 'The Significance of the Metropolis', p. 11.",True
67,"London was growing rapidly during the Tudor and Stuart period. As the population of the country as a whole doubled, the population of London quadrupled. In 1500 the population was just 50,000, but this had risen to 490,000 by 1700. There was a low fertility rate as only a small proportion of people were married and the poor living conditions in the capital resulted in a high mortality rate; yet the population continued to rise. The explanation for this comes from the high level of migration into the city. In order to counter the effects of low fertility and high mortality rates on the population, London needed around 7,000 migrants entering the city every year to grow at the rate it did. E.A Wrigley estimated that one in six people had lived in London at some stage in their life by the end of the seventeenth century. A. L. Beier and R. Finlay have raised the question of why it was London that attracted so many people. There were between 700 and 800 market towns in England, including a dozen major provincial cities, so 'the country was not devoid of possible competitors to London'. The motivations for migration into the capital are numerous. Some people's reasons to move to London were to get away from the countryside and smaller towns (push factors) and others were motivated by what the city had to offer (pull factors). E. A. Wrigley, 'A Simple model of London's importance in changing English society and economy 1650-1750, Past and Present, (1976) p. 37. A. L Beier, and R. Finlay, 'The Significance of the Metropolis', A. L Beier, and R. Finlay (eds. ), London 1500-1700, (London, 1986) p. 4. As Beier and Finlay have asserted, 'London evidently exerted a tremendous pull on migrants from an early date'. One explanation for this may have been the unique situation in London in which the capital city was also a great port. One of the main factors that encouraged migration was the centralisation of England's economic life in London. The shift from Mediterranean to Atlantic trade was beneficial to England, and hence to London which was one of the major ports. By 1700, London dominated English trade. It dealt with eighty per cent of imports and sixty-nine per cent of exports. By the mid-1540s London's cloth exports accounted for eighty-six per cent of the country's total woollen exports. Professor J. H. Hexter comments on the number of merchants who 'lived in the town but were not of it, and yet won there that beatitude of riches that enabled them to go back to where they came from'. Men would come to London to make their fortunes as merchants. The cloth trade 'unquestionably stimulated London's economy and migration to the capital'. Beier, and Finlay, 'The Significance of the Metropolis', p. 10. R.G. Lang, 'Social Origins and Social Aspirations of Jacobean London Merchants', Economic History Review, 27 (1974) p. 91-2. Beier, and Finlay, 'The Significance of the Metropolis', p. 15. It could be argued that trade did not have much of a direct impact on migration as it only involved a small number of merchants. However, it did have considerable impact in other ways. Profits from trade were invested in other industries such as manufacture. For example, a trader called Thomas Myddleton set up a sugar refinery and a draper called Thomas Cullum financed a dying business. This sort of activity created job opportunities which were major pull factors for migrants. Beier, and Finlay, 'The Significance of the Metropolis', p. 15. There were many job opportunities in the city for professionals such as doctors and lawyers. There were also many jobs involving unskilled labour such as shipping and building. People could find jobs as street sellers or as water men taking people up and down the Thames in boats. Most migration was by younger people wising to find placements as apprentices or domestic servants. By 1600 the number of apprentices migrating to the city was between 4,000 and 5,000. During the period new craft guilds were established and membership to traditional ones increased. Production spread to the suburbs and provided thousands of jobs for migrants. It became an industrial area and specialised in the production of luxury goods in the mid-seventeenth century. 'These changes in turn attracted migrants' both for the job opportunities and the goods produced. Wages were a huge incentive as they were fifty per cent higher in London than in provincial areas. Moving to London for the economic benefits of more work and better wages was betterment migration. People found opportunities to rise to freemen, masters and girl wardens. For example, John Browne claimed in 1631 that he could not make a living dressing hemp in Leicester, but after going to London he was profitably self-employed and a freeman of the city and of the company of merchant tailors. People would follow their neighbours to London to see if they could have the same success. The story of Dick Whittington became part of English folk-lore at the end of the sixteenth century and Beier and Finlay note how there was 'a popular belief that the city's streets were paved with gold for the immigrant'. Beier, and Finlay, 'The Significance of the Metropolis', p. 15. P. Clark and P. Slack, English Towns in Transition 1500-1700, (London, 1976) p. 65. Beier, and Finlay, 'The Significance of the Metropolis', p. 17. As well as being the centre of economic life, London was also the centre of political action. This centralisation was a major pull factor for migration to the city. 'Generations of historians have connected the rise of the modern capital with its political role'. The number of Peers increased from around 30 in the 1560s to 90 in the 1630s. They would bring with them their family and servants which added to the growing population. The Royal household grew under the Reign of Charles I. While under Elizabeth there were around 1,000 in Court, this increased to around 2,600 under Charles I. The Court used to move around the realm in the High Middle Ages, but from the reign of Edward IV it remained in or around London. This 'attracted to the city the custom and residence of county landowners' who sought favour and office. The number of resident gentlemen in London increased and the number of MP's rose from 300 in the early sixteenth century to 500 during the mid-seventeenth. The Court also provided patronage for musicians and artists which drew them into the city. Beier, and Finlay, 'The Significance of the Metropolis', p. 11. Clark and Slack, English Towns in Transition, p. 63. However, Beier and Finlay believe that 'the direct influence of Government on London's growth has probably been overstated'. The size of the central government does not change much between 1580 and 1640 when London experienced its biggest population increase. The increase in officials came after 1650. By 1725 the number of officials had risen to around 2,700 from only 400 under Elizabeth. Nevertheless, even 2,700 officials and their family and servants would have only taken up around 'five and a half per cent' of the population of London; therefore, government membership cannot be considered one of the major driving factors for migration. Beier, and Finlay, 'The Significance of the Metropolis', p. 11. Beier, and Finlay, 'The Significance of the Metropolis', p. 12. London was the legal centre of the country. The four Inns of Court, which were like England's third university, experienced a five-fold increase in admissions between 1550 and 1650. During term time there were around 1,000 students populating the capital. The Inns also employed many staff such as ushers, agents and scribes. The judicial centre of the country was Westminster, which observed a huge expansion in legal business in the sixteenth century. The land market expanded as church land was seized after the Reformation. This generated much legal business as the land was fought over. Women also engaged in legal business which brought them to London. They brought cases such as defamation, sexual offences and marriage disputes. This was a pull factor that drove migrants to the capital. During the Tudor and Stuart period 'the landed upper classes increasingly resorted to London for business and pleasure'. The London Season was from October to June. Rural landowners would travel into the city for entertainment and shopping. Road transport was improving in the seventeenth century and the stagecoach was introduced which made it easier for frequent travel into the city. London was a commercial city where items that were not available elsewhere could be obtained, including such exotic goods as tea, coffee, wine, spirits and tobacco. The West End became an area of high quality shops in the late seventeenth century and attracted many rich landowners. The merchants helped to finance the extravagant spending of the aristocracy, encouraging them to come to the capital, by providing them with loans and pawnbroking arrangements. The elites were also enticed to live in London by the newly built mansions and squares. This 'influx of the landed classes boosted the city's population'. Beier, and Finlay, 'The Significance of the Metropolis', p. 11. Beier, and Finlay, 'The Significance of the Metropolis', p. 13. It was not only the rich who came to London for the social aspects of city life. Many would come to see family and friends. There was much entertainment that was available to the poor as well as the rich. There was bull baiting and theatre productions in the Swan and the Globe. Brothels were also part of London life. Songs, jests and fables were produced in London and were discussed in the markets and alehouses. The alehouses were important places where migrants could procure food, drink, shelter, loans and contacts for finding work. London coffee houses also became popular towards the end of the seventeenth century, which were important in the development of the bourgeois public sphere as described by Jürgen Habermas. They were places where members of the general public would meet to discuss politics. People would move to London to be nearer this type of cultural movement. Young women migrated to London as it was a place 'where the rigid controls of patriarchy... [were] more easily evaded'. Single women could socialise and court more freely than in the smaller, more kin-based communities of the rest of the country. These women migrants often married later then the women from where they originated. They often worked and earned independently for a few years before marriage. The anonymity of the city allowed some women to start anew if their reputation had been damaged at home. This type of migration involved both push and pull factors as the women were escaping the restrictions of the smaller towns and countryside and were attracted by the freer lifestyle in London. Laura Gowing, '""The Freedom of the Streets"": Women and Social Space, 1560-1640' in P. Griffiths and M. Jenner (eds. ), Londinopolis, Essays in the Cultural and Social Writing of Early Modern London, (Manchester, 2000) p. 132. Some people came to London for the specialist medical practitioners. By 1704 there were around 1,000 London apothecaries. People came to consult other specialists such as astrologers like William Lilly for which 'the Stuart city was also famous for'. Other people migrated to London for the educational opportunities available. The literacy levels in London were higher than the national average for men and women. Schools taught a humanist curriculum and there were public lectures that anybody could attend. Books were more readily available in London as it is where the printing presses were. Ballads and cheap pamphlets were produced on a large scale and provided writers with a regular, if low, income. Clark and Slack, English Towns in Transition, p. 68. Beier and Finlay assert that 'people were pushed to London as well as pulled'. As well as going to the city to better themselves, many were pushed there by poverty. The latter became increasingly common between 1500 and 1700, whereas in medieval period migration primarily involved reputable workers wanting to improve their situation by becoming apprentices in the guilds. Clark and Slack have argued that it was the poor and destitute who were seeking labour and charity that 'probably formed the bulk of the 8,000 or so net immigrants whom late Stuart London absorbed every year'. This suggests that migration into early modern London was driven primarily by push factors. Beier, and Finlay, 'The Significance of the Metropolis', p. 10. Clark and Slack, English Towns in Transition, p. 64. As London increased in size, it needed more food. This placed an increasing demand on the agrarian economy which was forced to make improvements to increase production. These changes included the introduction of enclosure and engrossing, which led to depopulation in the countryside as less labour were needed. Beier notes how these changes 'pushed surplus labour out of advanced farming regions to London'. Most people in the countryside probably migrated to other areas of the countryside or to smaller towns before moving to London. However, this still effected the migration levels into the capital because increasing populations in small towns from migration and general national population growth meant worsening living standards and higher levels of poverty and unemployment. These were push factors that made people want to leave other areas of England and move to London where there were more job opportunities and higher wages. This was often a form of subsistence migration because people would have to move to London as they could not support themselves or their family where they were. Beier, and Finlay, 'The Significance of the Metropolis', p. 10. The success of London's trade and manufacture 'had wide ramifications'. Provincial out ports such as Southampton suffered as trade became centralised in London. Clark and Slack have observed that, 'as London continued to consolidate its lead in overseas and domestic trade, many provincial towns were forced to recognise the evident disadvantages of their position'. It may have also frustrated the depression of inland towns such as Coventry because of London's ready access to Low Countries' markets. 'Poverty not infrequently threatened to overwhelm English towns' and pushed people out and into the capital. Beier, and Finlay, 'The Significance of the Metropolis', p. 15. Clark and Slack, English Towns in Transition, p. 103. Clark and Slack, English Towns in Transition, p. 121. 'London's massive growth... took place mainly because of two developments which encouraged migration: first, the centralisation in London of the nation's political and economic life... secondly, upheavals in the provincial economies', which have been discussed above. This explanation contains both push and pull factors that drove migrants to the capital. It was a combination of the two that encouraged so many to migrate to London in the early modern period. Beier, and Finlay, 'The Significance of the Metropolis', p. 11.","The term Renaissance comes from the Italian word rinascita, meaning rebirth. The movement looked back to the superior aspects of ancient times and integrated them with the new developments of the time. The Italian Renaissance is generally associated with the fifteenth century. This period, however, can be stretched in either direction. For example, the term ""Renaissance"" has been used to describe some thirteenth-century artists, and the great artists of the Renaissance were active until the end of the sixteenth century. The rebirth of the visual arts was one aspect of the Renaissance but the development of humanism and the classical studies of the universities were as important. There were also advancements in music, literature, science and government practices during this period. How far Florence was responsible for these developments can be debated. Robert Black asserts that 'the elitism of Florentine society and the monopoly which the Florentine patriciate had over classical culture made Florence the first and greatest capital of the Renaissance'. Jocelyn Hunt, on the other hand, believes that 'the assumption that Florence was pre-eminent is... Burckhardtian, since, it is argued, the scholars of the Renaissance were based in Rome, and in the other universities'. Porter, Roy, and Teich, Mikulas (eds. ), The Renaissance in National Context, (Cambridge, 1992), p.37. Hunt, Jocelyn, The Renaissance, (London, 1999), p.ix. Burckhardt identified Florence as the 'cradle of the Renaissance'. It appears as though 'the components for a successful intellectual movement were all to be found in Florence at the start of the fifteenth century'. The city was wealthy, banking was highly developed, and papal revenues were often invested in Florence rather than the volatile and dangerous city of Rome. Florence had a high population. It was active in manufacture, trade and commerce. Textile production and metalworking were successful industries in Florence. The city was in a prime position for trade; it was close enough to the coast for sea trade but far enough to keep it safe from pirate attacks. It was also able to trade over the Alps and was close to the Middle East so merchants could bring back the luxury goods available there, such as silk and spices. Florence was primarily a city orientated towards craft-industrial production rather than trade like Venice. This appears to have been a more favourable environment for artists to be produced as 'it was only when Venice turned from trade to industry, at the end of the fifteenth century, that Venetian art caught up with that of Florence'. Hunt, The Renaissance, p.5. Hunt, The Renaissance, p.8. Burke, Peter, The Italian Renaissance: Culture and Society in Italy, (Oxford, 1986), pp.49-50. Giorgio Vasari, a contemporary historian addressed the issue of 'the outsized contribution of Florence to the arts'. He believed that there were three factors that made Florence different from other Italian towns. Firstly, Florentines were free and, therefore, became critical and would not accept things that were not of the highest quality. Gene A. Brucker notes that ' this appreciation of quality, and a corresponding disdain for the shoddy and inferior, became a characteristic feature of the Florentine mentality'. Secondly, as Gregorio Dati, a contemporary Florentine, explains, 'Florence is situated in a naturally wild and sterile place', so that the people had to work hard, using their intelligence in order to live comfortably. They would travel and were innovative and experimental. Thirdly, the people had a 'greed for honour and glory which that air generates in men of every occupation'. Burke, The Italian Renaissance, p.29. Brucker, Gene A, Renaissance Florence, (London, 1969), p.220. Hunt, The Renaissance, p.13. Burke, The Italian Renaissance, p.29. Wealthy citizens would spend their riches on art and culture. The amount they spent was seen to be an indication of their power and status. Florentine men did not own armies, rather hired mercenaries when they were needed. This meant that they had more to spend on luxuries. Giovanni Rucellai, a Florentine merchant, wrote in 1473 that 'earning and spending are among the greatest pleasures that men enjoy in this life'. He, himself, spent much of his wealth on the buildings of Florence, including his house and the loggia opposite, the church of Santa Maria Novella and the church of San Pancrazio. Commissioning artists gave them the opportunities to try out new styles and techniques as well as copying the styles of the ancients. Florence was important in the development of the Italian Renaissance because the artists working there were given these opportunities. Giovanni Rucellai quoted in Hunt, The Renaissance, p.13. The Florentine political system was inspired by antiquity. It compared to those of the ancient Greek city-states or, as Evelyn Welch observes, 'Florence and Venice found their heritage in the Roman Republic'. The king, the Church and the nobility did not rule the Florentine people as in other places. Instead they ruled themselves, either by electing nine men for the Signoria or by holding office themselves for the two-month term. Committees were appointed to oversee administration. The government viewed everybody as equals, with nobody being above the law and everything being done to benefit the whole community. Michelangelo's statue of David was commissioned to depict the strength of Florence. David is an example of the Renaissance style of realism in art. Vincent Cronin has discussed the fact that Burckhardt took Renaissance Italy as a whole and made no distinction between what happened in republican Florence and other cities which were ruler over by tyrants. He argues that 'such a distinction is essential to an understanding of all but the most general points'. The political freedom that Florentine men enjoyed serves to explain Florence's leading role in the development of the Italian Renaissance. Cronin states that 'Florence was the power-house which generated far and away the largest number of influential ideas'. Welch, Evelyn, Art and Society in Italy 1350-1500, (Oxford, 1997), p.241. Cronin, Vincent, The Florentine Renaissance, (London, 1967), p.313. Cronin, The Florentine Renaissance, p.313. In the late fifteenth century, the Medici family took control of most of the councils and committees, threatening the old system. They were firmly established in Florence by the second half of the fifteenth century. Cronin describes the fact that the Medici patronized art as a 'remarkable innovation, since Italy had no tradition of individual lay patronage'. Florence led the way in this new fashion. The Medici employed artists to enhance their status. Lorenzo de'Medici invested in art that would 'make the city of Florence famous'. The freestanding dome of Florence Cathedral, built by Filippo Brunelleschi, was the first of its kind. It was copied in other places such as Rome with the construction of St. Peter's. The dome was inspired by ancient architecture such as the Pantheon in Rome. Brunelleschi used the system of arithmetic proportions, which he had taken from the work of Vitruvius, in his design. Brunelleschi also used new techniques including line point perspective and innovations in engineering that inspired other Renaissance artists and architects. Florence's contribution to the Italian Renaissance's attempt to turn away from the medieval gothic style in architecture and return to the ways of ancient Rome and Greece 'was little short of astounding'. Cronin, The Florentine Renaissance, p.176. Hunt, The Renaissance, p.10. Porter and Teich (eds. ), The Renaissance in National Context, p.21. Another reason why Florence took a leading role in the developments of the Italian Renaissance was the relationship between patrons and artists in Florence which was 'remarkable by comparison with conditions in other circles'. There was a friendly understanding between them which was in part due to classical texts which exalted the artist and emphasized the esteem that they received in ancient times. These texts 'were not lost on the Florentines'. The relationship between Cosimo and Donatello 'went much deeper than that of employer and employee'. Cosimo gave Donatello a pension in his old age, which was the first known instance of this type of help given to an artist. The 'championing of artists by Florence's most prominent citizen naturally raised their status'. Artists in Florence were allowed to use their own genius. Leonardo was educated in Medici Florence and was 'the culmination of the versatile artist whose rise took place in the city by the Arno'. When he moved to Milan, however, he was not understood. He painted the Last Supper in Milan but the Dominican Prior wanted him to work quickly and complained when he spent half a day looking at the painting without adding to it. Leonardo replied that 'men of genius sometimes accomplish most when they work least'. Similarly, in Rome, relations between patron and artist were not as amiable as in Florence. Cronin notes how 'painters were lumped together with carters and grooms' and were often given poor food. The artist Davide Ghirlandaio was served uneatable food while working in the abbey of Passignano and was so angry that he dumped his soup over the friar and beat him with the loaf of bread. Cronin, The Florentine Renaissance, p.179. Cronin, The Florentine Renaissance, p.177. Cronin, The Florentine Renaissance, p.178. Cronin, The Florentine Renaissance, p.180. Cronin, The Florentine Renaissance, p.188. Cronin, The Florentine Renaissance, p.188. Cronin, The Florentine Renaissance, p.179. Cronin, The Florentine Renaissance, p.180. Dati said of Florence: 'so great is the number of talented and rich men that they are unequalled in the world'. The talented men produced the great works of the Italian Renaissance and the rich men enabled them to do so. Florentine artists and sculptors had 'immeasurable influence... on the future development of the visual arts in the renaissance'. Masaccio, a Florentine, was the first painter known to have adopted Brunelleschi's system of line-point perspective and make use of lighting and shading in his works such as the frescoes in the churches of Santa Maria Novella. Florentines also led the way in the revival of classical sculpture. Donatello was responsible for the revival of the classical art of large-scale bronze casting. He was also the first artist since antiquity to study anatomy and represent the nude in his works. Gregorio Dati quoted in Hunt, The Renaissance, p.13. Porter and Teich (eds. ), The Renaissance in National Context, p.23. In Italy, painters, sculptors and masons often belonged to guilds. In some places, such as Milan and Florence, painters had their own guilds. Most of these were quite restrictive. For example, the guild in Padua forbade members to give or sell to non-members and did not allow work to be brought from another district to be sold in Padua. The guild in Venice was also restrictive. Albrecht Dürer commented on the strict nature of the guild when he visited Venice in 1504, describing how they summoned him before the magistrates three times and had him pay four florins to the guild. The guilds in Florence, on the other hand, were not as powerful. The government held that not every craftsman had to join and it allowed people from other places to work in Florence. Peter Burke believes that 'this more liberal policy, which exposed local tradition to stimuli from outside, may help to explain Florence's cultural lead'. Burke, The Italian Renaissance, p.68. Burke, The Italian Renaissance, p.68. Florence's important contribution to the arts can be seen in the proportion of Italy's cultural elite, the people whose creative abilities are recognized in society, which originated from around Florence. Burke has calculated that around 26 per cent of the elite came from Tuscany, which had only 10 per cent of the population, while the Veneto had 20 per cent of the population but only around 23 per cent of the elite. Burke believes that 'Rome's poor contribution needs emphasis' as it produced less than its share of Italy's cultural elite. Rome was no longer the beautiful, classical city that it was in ancient times. 'The Middle Ages reversed Augustus's boast that he had found Rome a city of brick and left it one of marble'. Materials from ancient monuments had been taken for other purposes and marble had been burnt for lime. Rome was in ruins with woods and undergrowth which sheltered foxes, wolves and hares. Rome's population was smaller then Florence's and was much poorer. This helps to explain Florence's dominance in the development of the Italian Renaissance. Burke, The Italian Renaissance, p.44. Burke, The Italian Renaissance, p.45. Cronin, The Florentine Renaissance, p.167. Dante was an important figure in the development of Renaissance literature. He learnt the universal ideals explained by Thomas Aquinas in the Florentine schools. He wrote the Divine Comedy in the local Tuscan dialect instead of Latin. Brucker describes the Divine Comedy as 'a Florentine poem, replete with the particular values, emotions, and concerns of that community'. Florence was important to Dante and Dante was a challenge and inspiration for later generations of Florentine intellectuals. Giotto di Bondone was a Florentine artist in the fourteenth century who, like Dante, influenced later generations of Florentines. He was innovative in the way he painted frescos; he created naturalistic and lifelike scenes that were also grandiose and monumental by humanizing the wooden, stylised figures of Byzantine art. His scenes in Santa Croce from the life of Saint Francis are examples of this style. His frescos had 'a profound impact upon the revolutionary generation of Florentine artists in the early Quattocento'. Brucker, Renaissance Florence, p.215. Brucker, Renaissance Florence, p.216. 'The Florentine contribution to Renaissance culture was not limited to a few specialized areas; it encompassed many fields and disciplines'. The city's jurists, physicians and theologians made considerable contributions to their fields. In many aspects of the Renaissance, 'Florence led the way, Italy followed'. The work of the Florentines: Coluccio, Bruni, Poggio and Alberti, paved the way for educational, political and moral discussions in Ferrara; Bruni and Ficino's translations of Plato enabled Isabella d'Este's circle in Mantua to discuss Platonism; and Antonello drew on the geometrical notions evolved in Florence when painting his Virgin Annunciate. Lionello Venturi summed it up by his comment that 'the new Renaissance style emerged in northern Italy a generation later than in Florence, and as the direct result of work done on the spot by Tuscan artists'. Brucker, Renaissance Florence, p.213. Cronin, The Florentine Renaissance, p.313. Cronin, The Florentine Renaissance, p.314. Florentine citizens were concerned with the study of the classics in the second half of the fifteenth century. Burckhardt asserts that the Renaissance came about because of 'those citizens, mostly Florentines, who made antiquarian interests one of the chief objects of their lives'. Niccolo Niccoli, a Florentine contemporary, told the young Piero de'Pazzi: 'if you do not study the classics, you will be considered a nothing'. In the study of the classics, 'Florence was ahead of the rest of Italy'. By the middle of the fifteenth century, nearly every prominent family in Florence, such as Strozzi, Medici and Rossi, had a humanist scholar among their numbers. It was also in Florence that the old Gothic script of angular closely spaced lettering was rejected in favour of the more rounded and spaced out script known as humanist miniscule. This style was adopted by the rest of Renaissance Italy. Burckhardt, Jacob, The Civilisation of the Renaissance in Itlay, (Harmondsworth, 1990), p.143. Niccolo Niccoli quoted in Hunt, The Renaissance, p.14. Porter and Teich (eds. ), The Renaissance in National Context, p.36. Florence was an important center for the development of history writing until the sixteenth century. The Florentine Leonardo Bruni was a key figure in this development. He rejected the medieval style of writing history chronicles and revived the Roman historian, Livy's, model of writing the history of a city-state with an introductory book on the origins and the early history of the city followed by a detailed account of events, organized as a year-to-year narrative and grouped into books. He wrote the History of the Florentine People in this style. Bruni was also responsible for reviving the Ciceronian dialogue, text written as though in conversation, which was used by Cicero in ancient times. He wrote Dialogues to Pietro Paolo Vergerio of Istria in this style, which became the standard form of Renaissance literature. Marsilio Ficino, leader of the Platonic Academy of Florence, was responsible for translating the works of Plato which were written in Greek. He also wrote his own philosophical works such as Platonic Theology Concerning the Immortality of the Soul. James Hankins has commented on the 'leading role played by Marsilio Ficino and Neoplatonic Philosophy on the cultural life of the High Renaissance'. The Platonic Academy of Florence was Ficino's primary vehicle for the spread of his ideas. There has been some debate over the significance of the Academy. Gustavo Uzielli, a Toscanelli scholar, has even suggested that the Academy was nothing but a fable. Arnaldo della Torre, on the other hand, wrote Storia dell'Academia Platonica di Firenze in 1902 in which the importance of the Academy was stressed. The authenticity of Ficino's letters, which Uzielli had suggested to be forgeries, was defended. Della Torre describes how the Academy was founded by Cosimo de'Medici in 1462 and how Ficino was appointed the head. According to Della Torra, the Academy held regular activities such as lectures, disputations and banquets. The Academy was made up of the Medici patrons as well as around one hundred prominent statesmen, poets, orators, doctors, lawyers and ecclesiastics of the later fifteenth century. James Hankins, however, suggests tah 'Ficino did have an academy of a sort, but if so, it was a thing quite different, and much less important, than has generally been thought'. Hankins, James, 'The Myth of the Platonic Academy of Florence', Renaissance Quarterly, 44, (1991), p.429. Hankins, 'The Myth of the Platonic Academy of Florence', p.430. Hankins, 'The Myth of the Platonic Academy of Florence', p.432. Hankins, 'The Myth of the Platonic Academy of Florence', p.433. Coluccio Salutati, the owner of the largest library of ancient manuscripts in Italy after the death of Petrarch, lived in Florence. Attracting new talent into the city, 'he was perhaps the greatest cultural asset enjoyed by the city'. He was appointed first chancellor in 1375. Salutati's disciples included some of the next generation's leading humanistic scholars such as Leonardo Bruni, Pietro Paul Vergerio and Poggio Bracciolini. Being a statesman and a scholar, Salutati bridged the gap between the world of learning and the world of commerce and politics. Humanists would point to Salutati's reputation and political influence when critics would question the value of classical studies. Slautati was important in the development of the Italian Renaissance and he was a Florentine. Brown, Alison, The Renaissance, (London, 1988), p.36. Florence had a key role in the development of the Italian Renaissance because its cultural pre-eminence coincided with the largest territorial expansion of the time. Between 1350 and 1490 Florence was in control of most of Tuscany. Florence fought against the papacy, Milan and Naples. In 1415, Bruni compared the victory of Florence over Milan, the capture of Pisa, and the defeat of the Ladislas, to the victory of the Roman Republic over Carthage. Antiquity was used to legitimize conquests and to establish and maintain authority by comparing the Florentine government with the successful Roman Republic. Porter and Teich (eds. ), The Renaissance in National Context, p.27. Towards the end of the period of the Italian Renaissance, Florence was still a place of importance. Machiavelli was at work in Florence writing about how men should not only admire the ancients but attempt to put the Roman model into real practical use. His political thought was highly influential. Florence also had influence on the scientific developments of the Renaissance. Copernicus claimed that the earth revolved around the sun, not the other way round which is what had previously been believed. Cronin asserts that this claim 'was a choice, not a discovery, and the basis of Copernicus's choice-as later those of Kepler and Galileo-was the Platonic cosmology revived in Florence'. Cronin, The Florentine Renaissance, p.144. Florence appears to have had much influence on the developments of the Italian Renaissance. Jocelyn Hunt, however, has argued that 'it may be the case that historians have examined the features which made Florence exceptional, and calculated that these are the essential preconditions for the Renaissance'. In this case, it would appear that the importance of Florence has been exaggerated. In the early stages of the Italian Renaissance, in the late thirteenth to early fourteenth centuries, 'Florence had a relatively minor role'. Florence had no unique connection with antiquity as Rome did. Its claims to antiquity were 'easily matched or surpassed by many other Italian cities'. It has been suggested that 'the new age began in Padua and the other urban communes of northern Italy in the 14th century, where lawyers and notaries imitated ancient Latin style and studied Roman archaeology'. Petrarch, perhaps the most influential figure of the Renaissance, never lived and worked in Florence. He achieved much greater fame as a writer than his Florentine equivalent, Salutati. Hunt, The Renaissance, p.8. Porter and Teich (eds. ), The Renaissance in National Context, p.21. Porter and Teich (eds. ), The Renaissance in National Context, p.21. Pioch, Nicolas, 'La Renaissance', Web Museum, 14 October 2002. URL  (1 April 2004) Petrarch recognized that Rome was the capital of the ancient world and, as it was the interest in the achievements of the ancient world that characterised the Renaissance, it has been suggested that Rome was the most important center of the Italian Renaissance. 'The classical revival was at its strongest in Rome, where the largest number of classical buildings remained to inspire the artist'. The pope was the most influential patron. Rome was important 'as a center of patronage which attracted creative individuals from other parts of Italy'. Nobody could outbid the papacy when commissioning the work of an artist. The pope's private library in the Vatican increased in size, containing 3,600 works by 1484, as Italian scholars began to search for the works of classical writers. Rome was an important center where 'the literary and artistic renaissance worked closely together'. For example, Raphael and his pupils painted a series of frescos in Agostino Chigi's Villa Farnesina near the Tiber depicting the story of Amor and Psyche from Apuleius' Golden Ass. Both form and content were taken from classical Roman art and literature. Fritz Saxl said that 'the Amor and Psyche paintings represent the greatest effort to illustrate pagan myths in the spirit of the classical marbles as they reappeared from beneath the Roman soil'. Hunt, The Renaissance, p.10. Burke, The Italian Renaissance, p.45. Porter and Teich (eds. ), The Renaissance in National Context, p.44. Porter and Teich (eds. ), The Renaissance in National Context, p.44. Although Rome was left in ruins after the Middle Ages, it was still an inspiration to artists from all over Italy. Brunelleschi, a Florentine architect, chose to study in Rome instead of Florence. This was partly due to the fact that, during the period at the beginning of the fifteenth century, Florence was in a critical struggle with Giangaleazzo. He and his friend Donatello would systematically inspect and excavate the ruins of Rome with teams of labourers. Brunelleschi wished to discover how the Romans had built their vast constructions. He measured the thickness of walls, columns, arches, bricks and blocks of marble. The Pantheon was a huge inspiration, as it had not been touched since ancient times. Brunelleschi would remove tiles from the roof to study the ribbing of the shallow cupola. He looked at the number of square holes in large stones and decided that the Romans must have used some sort of machine to move them. This gave him the idea of the ulivella, a machine to move large stones by a crane without cords. Brunelleschi studied the ancient buildings of Rome for about twelve years. When the dome to Florence cathedral was being planned, a number of problems arose. The diameter of space to be spanned was 138 feet. The only cupola of a similar dimension was the Pantheon which Brunelleschi had been studying. Brunelleschi came up with a design for the dome and was awarded the commission of building it. The dome of Florence cathedral may have been the first of its kind but the inspiration and the savoir-faire came from Rome. Brucker believes that 'Florentine intellectual life... was matched, to some degree, by Milan, Venice and Naples'. Rulers all over Italy tried to attract the great Renaissance artists and the other Italian cities, such as Milan, Urbino, Ferrara, Venice, Padua and Naples, soon became 'powerful rivals in the spreading wave of change'. Leon Battista Alberti worked on some of the most progressive architecture of the new Humanism in Rimini and Mantua, and Andrea Mantegna's paintings in Padua portrayed a personal formation of linear perspective, antiquarianism and realistic technique. The Venetian school was growing in strength with developments such as Giovanni Bellini's poetic classicism. Other cities took the lead over Florence in certain areas. Venice, for example, became the center of the new printing industry that enabled Renaissance literature to be widely circulated. Burke believes that 'the Venetian cultural achievement of the period... long received considerably less than its due'. He explains that this has happened because there were less studies of the social history of the arts in Venice than there were of Florence. Vasari, a Renaissance historian, is said to have had a 'Tuscan bias', which may have exaggerated Florence's role. There was no material to counter this view. There was an attempt in the sixteenth century by a Venetian to collect material on the lives of painters around Venice but this attempt was never completed and published. There was also no Venetian equivalent to Wackernagel's book on Florence. Burke has made an effort in The Italian Renaissance: Culture and Society in Italy, 'to avoid giving the Florentines more than their fair share of the limelight', by discussing important artists and writers from all over Italy. Brucker, Renaissance Florence, p.215. Pioch, Nicolas, 'La Renaissance: Italy', Web Museum, 14 October 2002. URL  (1 April 2004) Burke, The Italian Renaissance, p.39. Burke, The Italian Renaissance, p.39. Burke, The Italian Renaissance, p.39. Venice was also the 'musical center of Italy' during the high Renaissance. Venice was the center of trade with the east because of its prime position on the coast. It was a wealthy, powerful and cosmopolitan city. It was at a 'cultural pinnacle' during the sixteenth century with composers like Andrea and Giovanni Gabrieli, Adrian Willaert and Claudio Merlo thriving in 'a musical environment that called for grand works that reflected the glory of Venice'. There were processions, ceremonies and festivals where composers could exhibit their music. St. Mark's cathedral was the center of Venetian music where new styles and techniques were developed. The cathedral was built in the style of eastern basilicas with two choir lofts and two organs. Composers had to write for separate choirs and they used the effects that this could produce. Composers also had to write music that was more chordal with emphasis on sound and clarity of text to counter the effects of the acoustics in the cathedral. Composers used instruments in the choirs which was unusual. Justin Klotz asserts that 'throughout the... Renaissance, Venice, the ""Most Serene Republic"", was one of the most exciting cities in Europe'. Klotz, Justin, 'The Venetian School', MUSL 242, 11 October 1998. URL  (1 April 2004) Klotz, 'The Venetian School'. Klotz, 'The Venetian School'. Klotz, 'The Venetian School'. Florence did not lead the way in humanist and classical education. It was 'the custom of the city' for young boys to learn 'enough to work as a merchant' rather than learn about the humanist subjects of grammar, rhetoric, poetry, history and moral philosophy. Key humanist teachers of the Renaissance included Gasparino Barzizza, Guarino Guarini da Verona and Vittorino da Feltre, who were all active in Northern Italy, not Florence. Alison Brown observes how 'in one respect Florence was - or at any rate seemed to be - worse off than many of her neighbours: she lacked a well-established university'. A university had been founded in 1348 but it never achieved the status of the universities of Bologna and Padua. For a long time it only offered vocational training for local students in medicine, law and theology. It was always competing with the abacus schools which Florentine merchants favoured more highly. Humanist education did not become important in Florence until the second half of the fifteenth century and then it was predominantly for elite families. Porter and Teich (eds. ), The Renaissance in National Context, p.32. Brown, The Renaissance, p.25. During the Renaissance, the Medieval Latin language and spelling was rejected, as there was a desire to return to the purer Latin of ancient Rome. The leading centers of the rebirth of Latin in Italy were Padua, Arezzo, Bologna and Verona. It did not begin in Florence, only reaching it by around 1400. By the mid-fifteenth century Rome was dominant in the development of the new language. However, Florence did hold some importance because of the arrival of the Greek Manual Chrysolaras at the turn of the fifteenth century who taught and inspired many students including Leonardo Bruni and Poggio Bracciolini, who were to become important Renaissance thinkers. Bruni produced new translations of Aristotle's Greek text Politics and Ethics. Latin was derived from Greek so Chrysolaras, being Greek, was knowledgeable and influential. By the end of the fifteenth century, Florence became dominant because of important Florentine thinkers such as Angelo Poliziano. In the late fifteenth and sixteenth century, Florence lost much of its influence as great artists such as Leonardo and Michelangelo were tempted away from the city and produced many of their masterpieces abroad. Leonardo had invented a number of military machines. After Florence had ended the war with Naples in 1480, Florentine politicians were only interested in keeping the peace and so Leonardo left Florence in 1482 to sell his war machines to other cities such as Milan. Although other places in Italy such as Rome and Venice had important roles to play in the development of the Italian Renaissance, 'the names most clearly associated with the Renaissance remain those of... artists from Florence'. The conditions in Florence at the time made it the ideal place for artists and scholars to work. There were many wealthy patrons, a large collection of talented men, and a system of government that encouraged cultural development. As Hunt observes, 'Florence is certain to remain at the heart of any serious study of the origins and development of the Italian Renaissance'. The importance of Florence in the development of the Italian Renaissance has not been exaggerated. 'Every important Italian City had its Renaissance, but the Florentine, because it was the most complete and the most influential, has a special claim to be considered representative of the whole'. Hunt, The Renaissance, p.ix. Hunt, The Renaissance, p.10. Cronin, The Florentine Renaissance, p.314.",False
68,"Qualitative research is associated with meanings and interpretation. It is not reliant upon numerical data as quantitative research is and focuses more upon the context in which behaviour is recorded. The reliability and validity of qualitative research has often been questioned as the analysis of its findings are often open to interpretation (Burman, 1994). Because of this it has previously been claimed that the method is not reliable or valid, although these two terms have always been defined in terms of quantitative research. It is possible that qualitative research is at least as reliable and valid as quantitative research when the definition of what is ""reliable"" and what is ""valid"" is altered to reflect the specialist nature of the qualitative method (Merrick, 1993). In quantitative terms reliability is defined as being the extent to which a particular study could be replicated and the same results achieved (Colman, 2001). Validity can be defined as the extent to which a test measures that which it claims to (Colman, 2001). Using these traditional definitions it is difficult to claim that qualitative research is either reliable or valid. This is because the main tool of the qualitative researcher is discourse analysis and the conclusions made are often open to interpretation as it is the job of the researcher to make inferences as opposed to statistics. There is often a large diversity of meanings that we can attribute to differing interpretations (Parker, 1994). This imposes the confounding variable of researcher bias, therefore reducing the validity of the analysis. It is also claimed that qualitative research lacks reliability, as due to the individualised nature of the methodology it would be difficult to repeat a study and gain exactly the same results (Merrick, 1999). It is however arguable that the quantitative definitions of reliability and validity are not appropriate measures to judge the quality of qualitative research. Qualitative research investigates issues that are too complex for quantitative analysis and does not reduce human beings to numbers and statistics (Henwood and Pigeon, 1992). Because of its unique methodology it should not be labelled as ""unreliable"" or ""invalid"" because it does not fit the criteria set for measuring the quality of numerical quantitative data. It should be judged against a criteria relevant to itself rather than that of another method (Smith, 1996). In an attempt to redefine the measure of quality in qualitative research Lincoln and Guba 1985 (cited in Merrick, 1999) suggest that the term reliability should be replaced with ""credibility"". This is more appropriate as it still means the data is reliable but without the strict criterion that a replica study should yield the same results. To create ""credibility"" there should be more than one analyst interpreting the data to reduce researcher bias and increase validity (Merrick, 1999). Wilkinson suggests there should be documentation of reflexivity where the researcher should continually record when interpretations begin to form and on what basis (Merrick, 1999). This allows the pattern of interpretation to be followed and continuous critical evaluation, therefore once again increasing validity and reducing bias. It is also suggested that interpretations should be shared with the participant to see if they agree with the conclusions drawn (Henwood and Pigeon, 1992). This is controversial however and does introduce confounding variables as the participant may publically agree with the researcher's interpretations despite privately disagreeing because they see them as holding a position of power (Smith, 1996). Also participants may disagree with negative interpretations due to social desirability bias. These are all ways to increase the reliability and validity of qualitative research without adhering to the quantitative definitions although still ensuring quality and trustworthiness of research. To further reduce researcher bias and increase validity the concept of Grounded Theory has been developed to help remove researcher bias. It suggests that theory should be built up from the observations rather than prior theory (Henwood and Pigeon, 1992). This reduces the chance of the researcher trying to fit the observations made to a prior theory, and also increases the chance that what is being measured is actually there rather than what is expected to be there (Henwood and Pigeon, 1992). There are however problems with this method as no researchers can approach an investigation without prior expectations. The terms ""reliability"" and ""validity"" as defined by quantitative methods are not fitting to qualitative research. To try and impose them onto qualitative analysis reduces the individuality and significance of the qualitative method. The studies often involve analysis of individual transcripts and because you do not get identical individuals it is unreasonable to expect the study to be repeated and identical results achieved (Merrick, 1999). This does not however mean that the research is not of high quality and trustworthy. Once measured against a criterion more fitting to the qualitative method, the quality of the research can be fully established. With validity it is possible to establish whether the researcher is actually measuring what is there by using techniques to reduce researcher bias. In conclusion Psychology is the study of the phenomena of behaviour and mental processes. An advantage of using qualitative methods is that we can study behaviour in the context of which it occurs which leads to a greater understanding to the meaning behind the behaviour (Billig, 1997). Providing measures are taken to ensure the quality and trustworthiness of the analysis of data collected, the qualitative method of studying psychology is highly valid and reliable when defined in relation to its own methodology.","AbstractThis study investigated whether first year university females had significantly more disordered eating than a control group of non-university females. It was hypothesised that low self esteem as a result of leaving home for the first time, and having to integrate a new peer group would be the cause of this. Questionnaires were used to measure participants eating behaviour, body shape satisfaction and self esteem. Attitudes towards the new peer group were measured using specific questions on the self esteem questionnaire. It was found that the first year students did have significantly more disordered eating than the control group, although having low self esteem and integrating a new peer group were not significant causes of this. It may be that low self esteem and having to integrate a new peer group play a part in increasing eating deviancy in first year students, although these may not have been shown to be significant because they combine with other factors associated with university life such as stress and depression. It was clearly found, however, that first year females have far more problems with their eating than females who are not at university. IntroductionEating disorders are becoming an increasing problem. The concept of the thin body ideal is being internalised through the social setting and causing more people, particularly adolescent females, to resort to severe dieting techniques (Markham, Thompson, Bowling, 2005). Unhealthy eating attitudes are an immensely important area of research as it has been shown that up to 20% of young women participate in intense dieting practices at some point in their lives (Kurth, Krahn, Nain and Drewnowski, 1995). Eating disorders are an issue affecting millions of people worldwide, with 3.4% of the population having a diagnosis (Kjelsan, Bjornstrom and Gotestom, 2004). It is likely the incidence rate for eating disorders is higher than this, although many people suffering from them deny the problem and therefore do not seek diagnosis or treatment. Research in this area is particularly important as it appears that more and more people are developing the disorders and concealing them. Once the causes for eating disorders have been fully investigated it may be possible to accurately identify individuals who are predisposed to suffer with their eating behaviour. There are many factors which may trigger an eating disorder. Lieberman, Gauvin, Bukowski and White (2001) found significant causes to be peer pressure to be thin, social reinforcement and body related teasing. They present the argument that as peer relations are very important in adolescence, if the social group values the thin body ideal highly then individuals aspire to achieve this, even through deviant eating patterns. In addition to this Blowers, Loxton, Grady-Flesser and Dawe (2003) found that the media plays an important role in the internalisation of the thin body ideal and increasing body dissatisfaction, and therefore, contributing to the development of eating disorders. These can be considered social influences over eating behaviour and are directly linked to how the individual perceives themselves in relation to others. This appears to be a highly influential factor for eating pathology because if a person's evaluation of their body does not meet the real or imagined body ideal set by peers, or by the media, then deviant eating behaviour seems to be the coping technique adopted to achieve what is believed to be desirable (Gordon, 1990). Believing that your body is inadequate and not acceptable to social standards is captured overall by low self-esteem. It has been shown by McLaren, Gauvin and Steigler (2001) that low self esteem is a significant contributor to eating pathology. It appears that it is a combination of low self esteem, peer and media pressure to be thin, and an internalisation of the thin body ideal that predominantly triggers eating pathology. The present study is concerned with investigating the combination of both internal and external factors within a particularly vulnerable group of individuals, first year university students. Moving away from home for the first time is one of the most stressful life changes a person will ever have to adjust to (Seligman, Walker, Rosenhan, 2001). This study intends to investigate whether eating behaviours become more deviant during the first year of living away from home at university compared to a control group of non-university participants. Moving into halls of residence at university entails integration into a new peer group, which may heighten feelings of insecurity and low self esteem. It has already been shown that low self esteem is a significant contributor to disordered eating behaviour, and this may combine with students seeking the thin body ideal in order to successfully integrate with their new group of friends. When adolescent girls are beginning new friendships they believe it is important to be thin, and compare themselves to other members of the group (Guendouzi, 2004). Attainment of the thin body ideal was also found to be a common topic of conversation (Guendouzi, 2004). This is particularly important when investigating the effect integration of a new peer group has on eating behaviour as it is likely that when girls first start university and are making new friendships, this is a safe topic of conversation as body concerns an issue many girls have as a common ground, which may increase pressure to be thin. It is likely that the low self esteem associated with moving away from home for the first time will enhance the desire to be slim to make a more favourable impression on the new peers. This is because many believe the more attractive you are, the more likely you are to be popular. Langlois and Stephan (1977) (as cited in Lieberman et al, 2001) found that physical attractiveness is linked to more positive peer relations and social acceptance. Students may resort to disordered eating to increase their chances of achieving the thin body ideal, and therefore, increase their chances of being popular within the new group of friends they are living with, and also to counterbalance low self esteem. It is also important to study first year students as the majority of previous research has centred upon participants aged between 14 and 16 years. It has been shown however that eating disorders are prevalent up until the age of 25 (Vanderlinden, Grave, Vandereycken, Noorduin, 2001). It is therefore important to study eating behaviour of older adolescents (hence first year university students), as this seems to be an area which needs enhancing. Assessments will be made using the Eating Attitudes Test (EAT) (Garner and Garfinkel, 1979, as cited in Corcoran and Fisher, 2000) to measure eating deviancy, the Self Esteem Rating Scale (SERS) (Nugent and Thomas, 1993, as cited in Corcoran and Fisher, 2000) to measure self esteem levels and the Body Shape Questionnaire (BSQ) (Cooper, Taylor, Cooper and Fairburn, 1987, as cited in Corcoran and Fisher, 2000) to measure body shape dissatisfaction. Peer group influence over self esteem will be measured using a revised version of the Self Esteem Rating Scale (as detailed in the method section). It is hypothesised that having to integrate a new peer group and low self esteem will lead to more deviant eating behaviour in first year university girls compared to a control group of matched age participants. Females only were used as eating disorders predominantly affects this sex by a ratio of 9:1 females to males (Gordon, 1990). MethodParticipantsAn opportunity sample of 66 female volunteers participated in the experimental condition. All were first year students living away from home for the first time in halls of residences at three higher education institutions, the University of Warwick, University of Central England, Birmingham, University of Coventry and The Italia Conti Academy of Theatre Arts, aged between 18 and 21. An opportunity sample of 66 female volunteers also participated in the control condition, aged between 18 and 21 and were at university. Written consent was obtained from all participants. MaterialsEating deviancy was assessed using EAT (see appendix) and Self-esteem was measured using SERS. This questionnaire was altered slightly for the experimental group to measure the effect the new peer group was having over self esteem levels at university. The word ""others"" in the original questionnaire was changed to ""my new friends"" for the experimental group only, for example, question two originally presents ""I feel others do things much better than I do"" and this was retained for the control group. For the experimental group however the question was changed to ""I feel my new friends do things much better than I do"". Participants were informed, using prose instruction, that the term ""my new friends"" meant the new friends they had made since arriving at university (see appendix for a copy of both questionnaires). Body dissatisfaction was measured using BSQ (see appendix). One questionnaire entailed all three components, counterbalanced to reduce any order effects, creating six types of questionnaire. ProcedureOf the 132 participants, eleven in each group completed each type of questionnaire, with the experimental group receiving the revised questions for SERS. Instructions as to how participants should answer the questionnaires and use the provided scales were issued in written form at the beginning of each questionnaire. Before participants were asked to participate, they were asked to sign a written consent form fully explaining ethical concerns and withdrawal rights. Data was collected from participants in isolation to ensure privacy, and collected from the University of Warwick, University of Central England, Birmingham, Coventry University and The Italia Conti Academy of Theatre Arts. Four venues were chosen to increase generalisability across the university environment. All questionnaires were scored according to the original criteria. Second marking only occurred on the second round of scoring for SERS, where only the questions referring to ""others"" and the ""new group of friends"" were assessed. This was done to more fully investigate the effect a new peer group had over the individual's self esteem levels (and therefore their eating behaviour). For included and excluded questions please see appendix. Data was collected between week one and week three of term in order for new peer groups to be classed as ""new"". Data AnalysisIn order to investigate whether self-esteem levels were lower and deviant eating behaviours were higher in first year university girls than a control group of non university girls, between subjects multivariate analysis of variance (MANOVA) and Pearson's correlation co-efficient were used to analyse data. To check the assumptions of the MANOVA could be relied upon, a Mahalanobis distance identified one outlier, it was decided to keep this in however as one outlier is not going to effect the assumptions of the MANOVA. To more fully investigate the effect integrating a new peer group may have on self-esteem levels, and therefore eating patterns, an independent samples t-test was carried out between the means for the experimental and control group for SERS. This time however only the questions referring to ""others"" or ""new friends"" were included, 22 out of the original 40 questions. This was to see whether self esteem levels relating to peers were significantly different between the two groups. One sample t-tests were also conducted to investigate whether there was a significant difference between the experimental group and the normative values. A Pearson's product moment correlation co-efficient was used to investigate the relationship between the dependant variables. To measure the strength of the correlations Cohan's (1988) criteria was used (as cited in Pallant, 2002). ResultsThe graph below compares the means and standard deviations for the experimental group, control group, and the normative means for the three dependant variables; EAT, BSQ and SERS (for a table of exact findings see appendix). It can immediately be seen from this that there are higher levels of eating deviancy and body shape dissatisfaction for the experimental group although the control group have slightly lower levels of self esteem. The one-way between-subjects MANOVA was performed to investigate the three dependant variables; eating attitudes, self esteem and body shape satisfaction. The independent variable was whether participants were first year university student living away from home (experimental group), or matched age non university student (control group). There was a significant difference between the experimental group and the control group on the combined dependant variables: F (3,128) = 4.08 , p= 0.008; Wilks' Lambda = 0.91. When the results for the dependant variables were considered separately, the only difference to reach statistical significance using a Bonferroni adjusted level of 0.17, was eating attitudes: f (1,130) = 9.54 p 0.079. Self esteem levels were also not significant using a Bonferroni adjusted level of 0.17 or using a significance level of 0.05: f (1,130) = 0.042 p> 0.838. An independent t test for SERS including only the questions relating to ""new friends"" or ""others"" found no significant difference between the means for the experimental group (M=4.53, SD=18.59) or the control group (M=2.65, SD=16.82) where t (132) = 0.609, p> 0.544. The magnitude of difference in the means was very small (eta squared = 0.002). A one sample t test compared the means for the experimental group (M=19.88, SD=10.66) with normative means for EAT (M=15.6, SD=9.3). This showed that the experimental group had significantly more deviant eating behaviours than the normative group t (66) = 3.262, p = 0.002. A one samples t test showed a significant difference for BSQ between the experimental group (M=102.27 SD=34.08) and the normative group (M=81.5 SD=28.4) where t (66) = 4.952, p = 0.000. A one samples t-test also showed a significant difference for SERS between the experimental group (M =19.97 SD= 27.74) and the normative group (M=0 SD=0) where t (66) = 5.849, p = 0.000. This illustrates that all three dependant variables for the experimental group were significant when compared with the established normative values. A Pearson product moment correlation coefficient was used to investigate the relationship between the three variables measured by EAT, BSQ and SERS. For the experimental group there was a medium positive correlation between the scores for EAT and BSQ (r = 0.476, n = 66 ,p DiscussionThe most general conclusion which can be drawn from the MANOVA is that there was an overall significance between the experimental and control group on the three dependant variables; scores for EAT, BSQ and SERS. There was however only a significant main effect for EAT, which means that there was only a significant difference between the groups in their eating behaviour and not in their self-esteem levels or body shape dissatisfaction. When an independent samples t-test was conducted between the control and experimental group for the revised measure of SERS, where only questionnaires relating to the new peer group were included, there was still no significant difference between the two. This suggests that experimental participants had no more self esteem issues regarding their ""new friends"" than control participants had with ""others"". There was, however, only a non significant relationship for self-esteem between the experimental and control group used in this study. When a one sample t test was conducted between the experimental group in this study, and the established normative mean specified by Nugent and Thomas (1993), there was a highly significant difference. After consulting the means it was clear that the experimental group had lower self esteem than their non clinical control (although the experimental group did not have lower self esteem levels than the control group used in this study). It may be that low self esteem does contribute to disordered eating behaviour in first year university girls, although it was not shown to be significant with the MANOVA due to generally low levels of self esteem amongst the control group. In first year students however this may combine with several other factors linked with disordered eating not investigated in the study, for example, stress and depression (Vanderlinden et al, 2001). The control group of participants (not at university) may not resort to disordered eating to try and counteract feelings of low self esteem as they may not also have to cope with depression and stress and being away from home for the first time. Data for the experimental group was also collected from The Italia Conti Academy of Theatre Arts. This is a performing arts theatre which may encourage outgoing, confident students. It may be that this had an effect on the overall self esteem levels for the experimental group and caused them to be higher than they would be if a non performing arts university had been selected. If a non performing arts institution had been selected then first year university students may have had significantly lower self esteem levels than the control group. It is therefore clear from the results that although first year students do have significantly more deviant eating behaviours than a control group of matched aged participants, it is not attributable to low self esteem and having to integrate a new peer group, as the t test investigating the influence new friends had proved also not to be significant. After correlating the results using a Pearson's product moment co-efficient it can be seen that there was a medium strength negative correlation between scores for EAT and SERS, showing that high eating deviancy did correlate with low self esteem levels. Although this did not prove to be significant in the MANOVA there was 19.98% shared variance with r = -0.44. When EAT and SERS were correlated for the control group there was only a small negative correlation between the two with only 8.1% shared variance with r = -0.28. Correlations do not indicate cause although it does show that low self esteem correlates more strongly with disordered eating for the experimental group. The fact that there was no main effect for BSQ on the MANOVA suggests that the disordered eating patterns are not caused by body image. This might be because being at university does not increase body dissatisfaction and therefore cause more deviant eating behaviours. Although there was a stronger correlation between EAT and BSQ for the experimental than control group, body image alone cannot be considered a significant difference in disordered eating between the experimental and control group. Although low self esteem and integrating a new peer group have not been shown to be significant factors in causing disordered eating behaviour for first year university students, it has been shown that they do have significantly more deviant eating behaviour than the control group. This may still be indirectly a result of being in a new peer group, as shown by the findings of Langlois and Stephan (1977) (as cited in Lieberman et al, 2001) who found that physical attractiveness is linked to more positive peer relations and social acceptance. Attitudes towards attractiveness and the benefits attractiveness can bring are established from an early age. This tends to be well known and seems to have internalised as a concept in young girls (Markham, Thompson and Bowling, 2005). This was also supported by Leiberman et al (2001) who found that more popular girls are more likely to have eating disorders. This suggests that girls who want to be popular, or want to maintain popularity tend to resort to severe eating behaviour and are under the assumption that slimness and attractiveness equals popularity. If this is the case then integrating a new peer group may be a significant factor on eating behaviour in first year students. This study investigated the affect new peer group had over eating by measuring insecurity felt amongst new friends. It may not be that individuals feel insecure with their friends, but that they feel competitive amongst their new friends and engage in dieting tactics in order to establish themselves as the most attractive, thereby increasing their chances of being popular. This could not be established from this study which concentrated on the effect new friends had on self esteem levels. Lieberman et al (2001) found that peer group influence was the single biggest influence over disordered eating. Another explanation as to why first year students may have significantly more deviant eating behaviour may reside in the element of perceived control. Individuals with the propensity to suffer from disordered eating tend to have a deficiency in their ability to influence their own environment (Gordon, 1990). They feel that they cannot control their surroundings or their lives and so resort to controlling the one thing that they feel they do have control over, their eating behaviour. Being independent and moving away to university may increase the sense of lost control and therefore present a crisis in self worth (Gordon, 1990). This may increase disordered eating as the individual may increase their control over their dieting behaviour as they feel a decrease in control of their circumstances. If this were the case, then it may be that the individuals already have the eating problems and it is just that moving away from university, and the challenge and stress accompanying this, enhances rather than causes the eating deviancy. It may be beneficial to compose a longitudinal study investigating how university affects eating patterns before and during university as opposed to comparison with a control group. It may be that in this study self-esteem levels and integrating the new peer group were not significant due to a number of confounding variables. The SERS questionnaire was established in 1993. As it is now nearly ten years later it may be that there has been an increase in the levels of self esteem of the population in general. This could be why when the experimental group was compared with the control group there was no significant difference, but when the experimental group was compared with the normative values Nugent and Thomas (1993) identified, there is a highly significant difference. It may be that the questionnaire needs to be modified to account for this possibility. It is also possible that social desirability bias may have played a role in self esteem and peer group influence not being significant, as nobody wishes to portray themselves as having low self esteem and participants may not have liked to admit that they did feel inferior to their friends, despite the fact that the questionnaires are anonymous. People who are low might not want to recognise or express this due to reaction formation. The format of the questionnaires may also have proved a problem. To increase reliability and validity three established questionnaires were used to create one. The main problem here is that all three used a slightly different scale. This may have caused confusion for participants, who might have used the wrong scales whilst answering questions. Experimental care was however taken by explaining in prose form at the beginning of each questionnaire that a different scale was being used for each section. The scales were also repeated every time questions continued onto a new page so participants would not have to rely on memory to use the scales. Human error may however have meant some participants may have used the wrong scale for the questionnaires. Other issues may have influenced results such as whether participants had a boyfriend at university. This may have increased self esteem levels and reduced the effect a new peer group may have had over individuals who had boyfriends. Also the distance from home participants were may have had an effect. Participants living closer to the university may not have had such low self esteem as they are still relatively near to their parents. In further research it may be beneficial to account for these individual factors. Due to the fact that integrating a new peer group and low self esteem have not been shown to be significant causes of the difference in eating deviancy between the control group and the experimental group, there appears to be plenty of scope for further research. It may be beneficial to stay with a new peer group and self esteem levels but use different methodology to ascertain what influence these have over disordered eating practices. It is clear that although these were not shown to be significant they do have an effect and it may also be beneficial to look at the effect of a new peer group in a different context. Instead of viewing it from the stand point of a feeling of inferiority to peers as this study does, it may be possible to study it from a competition of popularity standpoint, led by the research of Langlois at al (1977) and Leiberman et al (2001). Depression and family factors may also play a role in increasing disordered eating behaviour in students and more research may be needed in this area. It may be that a new questionnaire needs to be established particularly looking at these issues to ensure validity and to ensure it is up to date and relevant to this generation. Further research may also look at the prevalence and causes of disordered eating for male students. It is however important to stay within the region of students as clearly there is an effect of going to university on students eating behaviour. This can also be supported by Kjelsan et al (2004) who found 6% of undergraduate women think they have an eating disorder, which is far higher than the national predicted prevalence rate of 3.4%.",True
69,"AbstractThis study investigated whether first year university females had significantly more disordered eating than a control group of non-university females. It was hypothesised that low self esteem as a result of leaving home for the first time, and having to integrate a new peer group would be the cause of this. Questionnaires were used to measure participants eating behaviour, body shape satisfaction and self esteem. Attitudes towards the new peer group were measured using specific questions on the self esteem questionnaire. It was found that the first year students did have significantly more disordered eating than the control group, although having low self esteem and integrating a new peer group were not significant causes of this. It may be that low self esteem and having to integrate a new peer group play a part in increasing eating deviancy in first year students, although these may not have been shown to be significant because they combine with other factors associated with university life such as stress and depression. It was clearly found, however, that first year females have far more problems with their eating than females who are not at university. IntroductionEating disorders are becoming an increasing problem. The concept of the thin body ideal is being internalised through the social setting and causing more people, particularly adolescent females, to resort to severe dieting techniques (Markham, Thompson, Bowling, 2005). Unhealthy eating attitudes are an immensely important area of research as it has been shown that up to 20% of young women participate in intense dieting practices at some point in their lives (Kurth, Krahn, Nain and Drewnowski, 1995). Eating disorders are an issue affecting millions of people worldwide, with 3.4% of the population having a diagnosis (Kjelsan, Bjornstrom and Gotestom, 2004). It is likely the incidence rate for eating disorders is higher than this, although many people suffering from them deny the problem and therefore do not seek diagnosis or treatment. Research in this area is particularly important as it appears that more and more people are developing the disorders and concealing them. Once the causes for eating disorders have been fully investigated it may be possible to accurately identify individuals who are predisposed to suffer with their eating behaviour. There are many factors which may trigger an eating disorder. Lieberman, Gauvin, Bukowski and White (2001) found significant causes to be peer pressure to be thin, social reinforcement and body related teasing. They present the argument that as peer relations are very important in adolescence, if the social group values the thin body ideal highly then individuals aspire to achieve this, even through deviant eating patterns. In addition to this Blowers, Loxton, Grady-Flesser and Dawe (2003) found that the media plays an important role in the internalisation of the thin body ideal and increasing body dissatisfaction, and therefore, contributing to the development of eating disorders. These can be considered social influences over eating behaviour and are directly linked to how the individual perceives themselves in relation to others. This appears to be a highly influential factor for eating pathology because if a person's evaluation of their body does not meet the real or imagined body ideal set by peers, or by the media, then deviant eating behaviour seems to be the coping technique adopted to achieve what is believed to be desirable (Gordon, 1990). Believing that your body is inadequate and not acceptable to social standards is captured overall by low self-esteem. It has been shown by McLaren, Gauvin and Steigler (2001) that low self esteem is a significant contributor to eating pathology. It appears that it is a combination of low self esteem, peer and media pressure to be thin, and an internalisation of the thin body ideal that predominantly triggers eating pathology. The present study is concerned with investigating the combination of both internal and external factors within a particularly vulnerable group of individuals, first year university students. Moving away from home for the first time is one of the most stressful life changes a person will ever have to adjust to (Seligman, Walker, Rosenhan, 2001). This study intends to investigate whether eating behaviours become more deviant during the first year of living away from home at university compared to a control group of non-university participants. Moving into halls of residence at university entails integration into a new peer group, which may heighten feelings of insecurity and low self esteem. It has already been shown that low self esteem is a significant contributor to disordered eating behaviour, and this may combine with students seeking the thin body ideal in order to successfully integrate with their new group of friends. When adolescent girls are beginning new friendships they believe it is important to be thin, and compare themselves to other members of the group (Guendouzi, 2004). Attainment of the thin body ideal was also found to be a common topic of conversation (Guendouzi, 2004). This is particularly important when investigating the effect integration of a new peer group has on eating behaviour as it is likely that when girls first start university and are making new friendships, this is a safe topic of conversation as body concerns an issue many girls have as a common ground, which may increase pressure to be thin. It is likely that the low self esteem associated with moving away from home for the first time will enhance the desire to be slim to make a more favourable impression on the new peers. This is because many believe the more attractive you are, the more likely you are to be popular. Langlois and Stephan (1977) (as cited in Lieberman et al, 2001) found that physical attractiveness is linked to more positive peer relations and social acceptance. Students may resort to disordered eating to increase their chances of achieving the thin body ideal, and therefore, increase their chances of being popular within the new group of friends they are living with, and also to counterbalance low self esteem. It is also important to study first year students as the majority of previous research has centred upon participants aged between 14 and 16 years. It has been shown however that eating disorders are prevalent up until the age of 25 (Vanderlinden, Grave, Vandereycken, Noorduin, 2001). It is therefore important to study eating behaviour of older adolescents (hence first year university students), as this seems to be an area which needs enhancing. Assessments will be made using the Eating Attitudes Test (EAT) (Garner and Garfinkel, 1979, as cited in Corcoran and Fisher, 2000) to measure eating deviancy, the Self Esteem Rating Scale (SERS) (Nugent and Thomas, 1993, as cited in Corcoran and Fisher, 2000) to measure self esteem levels and the Body Shape Questionnaire (BSQ) (Cooper, Taylor, Cooper and Fairburn, 1987, as cited in Corcoran and Fisher, 2000) to measure body shape dissatisfaction. Peer group influence over self esteem will be measured using a revised version of the Self Esteem Rating Scale (as detailed in the method section). It is hypothesised that having to integrate a new peer group and low self esteem will lead to more deviant eating behaviour in first year university girls compared to a control group of matched age participants. Females only were used as eating disorders predominantly affects this sex by a ratio of 9:1 females to males (Gordon, 1990). MethodParticipantsAn opportunity sample of 66 female volunteers participated in the experimental condition. All were first year students living away from home for the first time in halls of residences at three higher education institutions, the University of Warwick, University of Central England, Birmingham, University of Coventry and The Italia Conti Academy of Theatre Arts, aged between 18 and 21. An opportunity sample of 66 female volunteers also participated in the control condition, aged between 18 and 21 and were at university. Written consent was obtained from all participants. MaterialsEating deviancy was assessed using EAT (see appendix) and Self-esteem was measured using SERS. This questionnaire was altered slightly for the experimental group to measure the effect the new peer group was having over self esteem levels at university. The word ""others"" in the original questionnaire was changed to ""my new friends"" for the experimental group only, for example, question two originally presents ""I feel others do things much better than I do"" and this was retained for the control group. For the experimental group however the question was changed to ""I feel my new friends do things much better than I do"". Participants were informed, using prose instruction, that the term ""my new friends"" meant the new friends they had made since arriving at university (see appendix for a copy of both questionnaires). Body dissatisfaction was measured using BSQ (see appendix). One questionnaire entailed all three components, counterbalanced to reduce any order effects, creating six types of questionnaire. ProcedureOf the 132 participants, eleven in each group completed each type of questionnaire, with the experimental group receiving the revised questions for SERS. Instructions as to how participants should answer the questionnaires and use the provided scales were issued in written form at the beginning of each questionnaire. Before participants were asked to participate, they were asked to sign a written consent form fully explaining ethical concerns and withdrawal rights. Data was collected from participants in isolation to ensure privacy, and collected from the University of Warwick, University of Central England, Birmingham, Coventry University and The Italia Conti Academy of Theatre Arts. Four venues were chosen to increase generalisability across the university environment. All questionnaires were scored according to the original criteria. Second marking only occurred on the second round of scoring for SERS, where only the questions referring to ""others"" and the ""new group of friends"" were assessed. This was done to more fully investigate the effect a new peer group had over the individual's self esteem levels (and therefore their eating behaviour). For included and excluded questions please see appendix. Data was collected between week one and week three of term in order for new peer groups to be classed as ""new"". Data AnalysisIn order to investigate whether self-esteem levels were lower and deviant eating behaviours were higher in first year university girls than a control group of non university girls, between subjects multivariate analysis of variance (MANOVA) and Pearson's correlation co-efficient were used to analyse data. To check the assumptions of the MANOVA could be relied upon, a Mahalanobis distance identified one outlier, it was decided to keep this in however as one outlier is not going to effect the assumptions of the MANOVA. To more fully investigate the effect integrating a new peer group may have on self-esteem levels, and therefore eating patterns, an independent samples t-test was carried out between the means for the experimental and control group for SERS. This time however only the questions referring to ""others"" or ""new friends"" were included, 22 out of the original 40 questions. This was to see whether self esteem levels relating to peers were significantly different between the two groups. One sample t-tests were also conducted to investigate whether there was a significant difference between the experimental group and the normative values. A Pearson's product moment correlation co-efficient was used to investigate the relationship between the dependant variables. To measure the strength of the correlations Cohan's (1988) criteria was used (as cited in Pallant, 2002). ResultsThe graph below compares the means and standard deviations for the experimental group, control group, and the normative means for the three dependant variables; EAT, BSQ and SERS (for a table of exact findings see appendix). It can immediately be seen from this that there are higher levels of eating deviancy and body shape dissatisfaction for the experimental group although the control group have slightly lower levels of self esteem. The one-way between-subjects MANOVA was performed to investigate the three dependant variables; eating attitudes, self esteem and body shape satisfaction. The independent variable was whether participants were first year university student living away from home (experimental group), or matched age non university student (control group). There was a significant difference between the experimental group and the control group on the combined dependant variables: F (3,128) = 4.08 , p= 0.008; Wilks' Lambda = 0.91. When the results for the dependant variables were considered separately, the only difference to reach statistical significance using a Bonferroni adjusted level of 0.17, was eating attitudes: f (1,130) = 9.54 p 0.079. Self esteem levels were also not significant using a Bonferroni adjusted level of 0.17 or using a significance level of 0.05: f (1,130) = 0.042 p> 0.838. An independent t test for SERS including only the questions relating to ""new friends"" or ""others"" found no significant difference between the means for the experimental group (M=4.53, SD=18.59) or the control group (M=2.65, SD=16.82) where t (132) = 0.609, p> 0.544. The magnitude of difference in the means was very small (eta squared = 0.002). A one sample t test compared the means for the experimental group (M=19.88, SD=10.66) with normative means for EAT (M=15.6, SD=9.3). This showed that the experimental group had significantly more deviant eating behaviours than the normative group t (66) = 3.262, p = 0.002. A one samples t test showed a significant difference for BSQ between the experimental group (M=102.27 SD=34.08) and the normative group (M=81.5 SD=28.4) where t (66) = 4.952, p = 0.000. A one samples t-test also showed a significant difference for SERS between the experimental group (M =19.97 SD= 27.74) and the normative group (M=0 SD=0) where t (66) = 5.849, p = 0.000. This illustrates that all three dependant variables for the experimental group were significant when compared with the established normative values. A Pearson product moment correlation coefficient was used to investigate the relationship between the three variables measured by EAT, BSQ and SERS. For the experimental group there was a medium positive correlation between the scores for EAT and BSQ (r = 0.476, n = 66 ,p DiscussionThe most general conclusion which can be drawn from the MANOVA is that there was an overall significance between the experimental and control group on the three dependant variables; scores for EAT, BSQ and SERS. There was however only a significant main effect for EAT, which means that there was only a significant difference between the groups in their eating behaviour and not in their self-esteem levels or body shape dissatisfaction. When an independent samples t-test was conducted between the control and experimental group for the revised measure of SERS, where only questionnaires relating to the new peer group were included, there was still no significant difference between the two. This suggests that experimental participants had no more self esteem issues regarding their ""new friends"" than control participants had with ""others"". There was, however, only a non significant relationship for self-esteem between the experimental and control group used in this study. When a one sample t test was conducted between the experimental group in this study, and the established normative mean specified by Nugent and Thomas (1993), there was a highly significant difference. After consulting the means it was clear that the experimental group had lower self esteem than their non clinical control (although the experimental group did not have lower self esteem levels than the control group used in this study). It may be that low self esteem does contribute to disordered eating behaviour in first year university girls, although it was not shown to be significant with the MANOVA due to generally low levels of self esteem amongst the control group. In first year students however this may combine with several other factors linked with disordered eating not investigated in the study, for example, stress and depression (Vanderlinden et al, 2001). The control group of participants (not at university) may not resort to disordered eating to try and counteract feelings of low self esteem as they may not also have to cope with depression and stress and being away from home for the first time. Data for the experimental group was also collected from The Italia Conti Academy of Theatre Arts. This is a performing arts theatre which may encourage outgoing, confident students. It may be that this had an effect on the overall self esteem levels for the experimental group and caused them to be higher than they would be if a non performing arts university had been selected. If a non performing arts institution had been selected then first year university students may have had significantly lower self esteem levels than the control group. It is therefore clear from the results that although first year students do have significantly more deviant eating behaviours than a control group of matched aged participants, it is not attributable to low self esteem and having to integrate a new peer group, as the t test investigating the influence new friends had proved also not to be significant. After correlating the results using a Pearson's product moment co-efficient it can be seen that there was a medium strength negative correlation between scores for EAT and SERS, showing that high eating deviancy did correlate with low self esteem levels. Although this did not prove to be significant in the MANOVA there was 19.98% shared variance with r = -0.44. When EAT and SERS were correlated for the control group there was only a small negative correlation between the two with only 8.1% shared variance with r = -0.28. Correlations do not indicate cause although it does show that low self esteem correlates more strongly with disordered eating for the experimental group. The fact that there was no main effect for BSQ on the MANOVA suggests that the disordered eating patterns are not caused by body image. This might be because being at university does not increase body dissatisfaction and therefore cause more deviant eating behaviours. Although there was a stronger correlation between EAT and BSQ for the experimental than control group, body image alone cannot be considered a significant difference in disordered eating between the experimental and control group. Although low self esteem and integrating a new peer group have not been shown to be significant factors in causing disordered eating behaviour for first year university students, it has been shown that they do have significantly more deviant eating behaviour than the control group. This may still be indirectly a result of being in a new peer group, as shown by the findings of Langlois and Stephan (1977) (as cited in Lieberman et al, 2001) who found that physical attractiveness is linked to more positive peer relations and social acceptance. Attitudes towards attractiveness and the benefits attractiveness can bring are established from an early age. This tends to be well known and seems to have internalised as a concept in young girls (Markham, Thompson and Bowling, 2005). This was also supported by Leiberman et al (2001) who found that more popular girls are more likely to have eating disorders. This suggests that girls who want to be popular, or want to maintain popularity tend to resort to severe eating behaviour and are under the assumption that slimness and attractiveness equals popularity. If this is the case then integrating a new peer group may be a significant factor on eating behaviour in first year students. This study investigated the affect new peer group had over eating by measuring insecurity felt amongst new friends. It may not be that individuals feel insecure with their friends, but that they feel competitive amongst their new friends and engage in dieting tactics in order to establish themselves as the most attractive, thereby increasing their chances of being popular. This could not be established from this study which concentrated on the effect new friends had on self esteem levels. Lieberman et al (2001) found that peer group influence was the single biggest influence over disordered eating. Another explanation as to why first year students may have significantly more deviant eating behaviour may reside in the element of perceived control. Individuals with the propensity to suffer from disordered eating tend to have a deficiency in their ability to influence their own environment (Gordon, 1990). They feel that they cannot control their surroundings or their lives and so resort to controlling the one thing that they feel they do have control over, their eating behaviour. Being independent and moving away to university may increase the sense of lost control and therefore present a crisis in self worth (Gordon, 1990). This may increase disordered eating as the individual may increase their control over their dieting behaviour as they feel a decrease in control of their circumstances. If this were the case, then it may be that the individuals already have the eating problems and it is just that moving away from university, and the challenge and stress accompanying this, enhances rather than causes the eating deviancy. It may be beneficial to compose a longitudinal study investigating how university affects eating patterns before and during university as opposed to comparison with a control group. It may be that in this study self-esteem levels and integrating the new peer group were not significant due to a number of confounding variables. The SERS questionnaire was established in 1993. As it is now nearly ten years later it may be that there has been an increase in the levels of self esteem of the population in general. This could be why when the experimental group was compared with the control group there was no significant difference, but when the experimental group was compared with the normative values Nugent and Thomas (1993) identified, there is a highly significant difference. It may be that the questionnaire needs to be modified to account for this possibility. It is also possible that social desirability bias may have played a role in self esteem and peer group influence not being significant, as nobody wishes to portray themselves as having low self esteem and participants may not have liked to admit that they did feel inferior to their friends, despite the fact that the questionnaires are anonymous. People who are low might not want to recognise or express this due to reaction formation. The format of the questionnaires may also have proved a problem. To increase reliability and validity three established questionnaires were used to create one. The main problem here is that all three used a slightly different scale. This may have caused confusion for participants, who might have used the wrong scales whilst answering questions. Experimental care was however taken by explaining in prose form at the beginning of each questionnaire that a different scale was being used for each section. The scales were also repeated every time questions continued onto a new page so participants would not have to rely on memory to use the scales. Human error may however have meant some participants may have used the wrong scale for the questionnaires. Other issues may have influenced results such as whether participants had a boyfriend at university. This may have increased self esteem levels and reduced the effect a new peer group may have had over individuals who had boyfriends. Also the distance from home participants were may have had an effect. Participants living closer to the university may not have had such low self esteem as they are still relatively near to their parents. In further research it may be beneficial to account for these individual factors. Due to the fact that integrating a new peer group and low self esteem have not been shown to be significant causes of the difference in eating deviancy between the control group and the experimental group, there appears to be plenty of scope for further research. It may be beneficial to stay with a new peer group and self esteem levels but use different methodology to ascertain what influence these have over disordered eating practices. It is clear that although these were not shown to be significant they do have an effect and it may also be beneficial to look at the effect of a new peer group in a different context. Instead of viewing it from the stand point of a feeling of inferiority to peers as this study does, it may be possible to study it from a competition of popularity standpoint, led by the research of Langlois at al (1977) and Leiberman et al (2001). Depression and family factors may also play a role in increasing disordered eating behaviour in students and more research may be needed in this area. It may be that a new questionnaire needs to be established particularly looking at these issues to ensure validity and to ensure it is up to date and relevant to this generation. Further research may also look at the prevalence and causes of disordered eating for male students. It is however important to stay within the region of students as clearly there is an effect of going to university on students eating behaviour. This can also be supported by Kjelsan et al (2004) who found 6% of undergraduate women think they have an eating disorder, which is far higher than the national predicted prevalence rate of 3.4%.","Qualitative research is associated with meanings and interpretation. It is not reliant upon numerical data as quantitative research is and focuses more upon the context in which behaviour is recorded. The reliability and validity of qualitative research has often been questioned as the analysis of its findings are often open to interpretation (Burman, 1994). Because of this it has previously been claimed that the method is not reliable or valid, although these two terms have always been defined in terms of quantitative research. It is possible that qualitative research is at least as reliable and valid as quantitative research when the definition of what is ""reliable"" and what is ""valid"" is altered to reflect the specialist nature of the qualitative method (Merrick, 1993). In quantitative terms reliability is defined as being the extent to which a particular study could be replicated and the same results achieved (Colman, 2001). Validity can be defined as the extent to which a test measures that which it claims to (Colman, 2001). Using these traditional definitions it is difficult to claim that qualitative research is either reliable or valid. This is because the main tool of the qualitative researcher is discourse analysis and the conclusions made are often open to interpretation as it is the job of the researcher to make inferences as opposed to statistics. There is often a large diversity of meanings that we can attribute to differing interpretations (Parker, 1994). This imposes the confounding variable of researcher bias, therefore reducing the validity of the analysis. It is also claimed that qualitative research lacks reliability, as due to the individualised nature of the methodology it would be difficult to repeat a study and gain exactly the same results (Merrick, 1999). It is however arguable that the quantitative definitions of reliability and validity are not appropriate measures to judge the quality of qualitative research. Qualitative research investigates issues that are too complex for quantitative analysis and does not reduce human beings to numbers and statistics (Henwood and Pigeon, 1992). Because of its unique methodology it should not be labelled as ""unreliable"" or ""invalid"" because it does not fit the criteria set for measuring the quality of numerical quantitative data. It should be judged against a criteria relevant to itself rather than that of another method (Smith, 1996). In an attempt to redefine the measure of quality in qualitative research Lincoln and Guba 1985 (cited in Merrick, 1999) suggest that the term reliability should be replaced with ""credibility"". This is more appropriate as it still means the data is reliable but without the strict criterion that a replica study should yield the same results. To create ""credibility"" there should be more than one analyst interpreting the data to reduce researcher bias and increase validity (Merrick, 1999). Wilkinson suggests there should be documentation of reflexivity where the researcher should continually record when interpretations begin to form and on what basis (Merrick, 1999). This allows the pattern of interpretation to be followed and continuous critical evaluation, therefore once again increasing validity and reducing bias. It is also suggested that interpretations should be shared with the participant to see if they agree with the conclusions drawn (Henwood and Pigeon, 1992). This is controversial however and does introduce confounding variables as the participant may publically agree with the researcher's interpretations despite privately disagreeing because they see them as holding a position of power (Smith, 1996). Also participants may disagree with negative interpretations due to social desirability bias. These are all ways to increase the reliability and validity of qualitative research without adhering to the quantitative definitions although still ensuring quality and trustworthiness of research. To further reduce researcher bias and increase validity the concept of Grounded Theory has been developed to help remove researcher bias. It suggests that theory should be built up from the observations rather than prior theory (Henwood and Pigeon, 1992). This reduces the chance of the researcher trying to fit the observations made to a prior theory, and also increases the chance that what is being measured is actually there rather than what is expected to be there (Henwood and Pigeon, 1992). There are however problems with this method as no researchers can approach an investigation without prior expectations. The terms ""reliability"" and ""validity"" as defined by quantitative methods are not fitting to qualitative research. To try and impose them onto qualitative analysis reduces the individuality and significance of the qualitative method. The studies often involve analysis of individual transcripts and because you do not get identical individuals it is unreasonable to expect the study to be repeated and identical results achieved (Merrick, 1999). This does not however mean that the research is not of high quality and trustworthy. Once measured against a criterion more fitting to the qualitative method, the quality of the research can be fully established. With validity it is possible to establish whether the researcher is actually measuring what is there by using techniques to reduce researcher bias. In conclusion Psychology is the study of the phenomena of behaviour and mental processes. An advantage of using qualitative methods is that we can study behaviour in the context of which it occurs which leads to a greater understanding to the meaning behind the behaviour (Billig, 1997). Providing measures are taken to ensure the quality and trustworthiness of the analysis of data collected, the qualitative method of studying psychology is highly valid and reliable when defined in relation to its own methodology.",False
70,"AbstractThis study investigated whether first year university females had significantly more disordered eating than a control group of non-university females. It was hypothesised that low self esteem as a result of leaving home for the first time, and having to integrate a new peer group would be the cause of this. Questionnaires were used to measure participants eating behaviour, body shape satisfaction and self esteem. Attitudes towards the new peer group were measured using specific questions on the self esteem questionnaire. It was found that the first year students did have significantly more disordered eating than the control group, although having low self esteem and integrating a new peer group were not significant causes of this. It may be that low self esteem and having to integrate a new peer group play a part in increasing eating deviancy in first year students, although these may not have been shown to be significant because they combine with other factors associated with university life such as stress and depression. It was clearly found, however, that first year females have far more problems with their eating than females who are not at university. IntroductionEating disorders are becoming an increasing problem. The concept of the thin body ideal is being internalised through the social setting and causing more people, particularly adolescent females, to resort to severe dieting techniques (Markham, Thompson, Bowling, 2005). Unhealthy eating attitudes are an immensely important area of research as it has been shown that up to 20% of young women participate in intense dieting practices at some point in their lives (Kurth, Krahn, Nain and Drewnowski, 1995). Eating disorders are an issue affecting millions of people worldwide, with 3.4% of the population having a diagnosis (Kjelsan, Bjornstrom and Gotestom, 2004). It is likely the incidence rate for eating disorders is higher than this, although many people suffering from them deny the problem and therefore do not seek diagnosis or treatment. Research in this area is particularly important as it appears that more and more people are developing the disorders and concealing them. Once the causes for eating disorders have been fully investigated it may be possible to accurately identify individuals who are predisposed to suffer with their eating behaviour. There are many factors which may trigger an eating disorder. Lieberman, Gauvin, Bukowski and White (2001) found significant causes to be peer pressure to be thin, social reinforcement and body related teasing. They present the argument that as peer relations are very important in adolescence, if the social group values the thin body ideal highly then individuals aspire to achieve this, even through deviant eating patterns. In addition to this Blowers, Loxton, Grady-Flesser and Dawe (2003) found that the media plays an important role in the internalisation of the thin body ideal and increasing body dissatisfaction, and therefore, contributing to the development of eating disorders. These can be considered social influences over eating behaviour and are directly linked to how the individual perceives themselves in relation to others. This appears to be a highly influential factor for eating pathology because if a person's evaluation of their body does not meet the real or imagined body ideal set by peers, or by the media, then deviant eating behaviour seems to be the coping technique adopted to achieve what is believed to be desirable (Gordon, 1990). Believing that your body is inadequate and not acceptable to social standards is captured overall by low self-esteem. It has been shown by McLaren, Gauvin and Steigler (2001) that low self esteem is a significant contributor to eating pathology. It appears that it is a combination of low self esteem, peer and media pressure to be thin, and an internalisation of the thin body ideal that predominantly triggers eating pathology. The present study is concerned with investigating the combination of both internal and external factors within a particularly vulnerable group of individuals, first year university students. Moving away from home for the first time is one of the most stressful life changes a person will ever have to adjust to (Seligman, Walker, Rosenhan, 2001). This study intends to investigate whether eating behaviours become more deviant during the first year of living away from home at university compared to a control group of non-university participants. Moving into halls of residence at university entails integration into a new peer group, which may heighten feelings of insecurity and low self esteem. It has already been shown that low self esteem is a significant contributor to disordered eating behaviour, and this may combine with students seeking the thin body ideal in order to successfully integrate with their new group of friends. When adolescent girls are beginning new friendships they believe it is important to be thin, and compare themselves to other members of the group (Guendouzi, 2004). Attainment of the thin body ideal was also found to be a common topic of conversation (Guendouzi, 2004). This is particularly important when investigating the effect integration of a new peer group has on eating behaviour as it is likely that when girls first start university and are making new friendships, this is a safe topic of conversation as body concerns an issue many girls have as a common ground, which may increase pressure to be thin. It is likely that the low self esteem associated with moving away from home for the first time will enhance the desire to be slim to make a more favourable impression on the new peers. This is because many believe the more attractive you are, the more likely you are to be popular. Langlois and Stephan (1977) (as cited in Lieberman et al, 2001) found that physical attractiveness is linked to more positive peer relations and social acceptance. Students may resort to disordered eating to increase their chances of achieving the thin body ideal, and therefore, increase their chances of being popular within the new group of friends they are living with, and also to counterbalance low self esteem. It is also important to study first year students as the majority of previous research has centred upon participants aged between 14 and 16 years. It has been shown however that eating disorders are prevalent up until the age of 25 (Vanderlinden, Grave, Vandereycken, Noorduin, 2001). It is therefore important to study eating behaviour of older adolescents (hence first year university students), as this seems to be an area which needs enhancing. Assessments will be made using the Eating Attitudes Test (EAT) (Garner and Garfinkel, 1979, as cited in Corcoran and Fisher, 2000) to measure eating deviancy, the Self Esteem Rating Scale (SERS) (Nugent and Thomas, 1993, as cited in Corcoran and Fisher, 2000) to measure self esteem levels and the Body Shape Questionnaire (BSQ) (Cooper, Taylor, Cooper and Fairburn, 1987, as cited in Corcoran and Fisher, 2000) to measure body shape dissatisfaction. Peer group influence over self esteem will be measured using a revised version of the Self Esteem Rating Scale (as detailed in the method section). It is hypothesised that having to integrate a new peer group and low self esteem will lead to more deviant eating behaviour in first year university girls compared to a control group of matched age participants. Females only were used as eating disorders predominantly affects this sex by a ratio of 9:1 females to males (Gordon, 1990). MethodParticipantsAn opportunity sample of 66 female volunteers participated in the experimental condition. All were first year students living away from home for the first time in halls of residences at three higher education institutions, the University of Warwick, University of Central England, Birmingham, University of Coventry and The Italia Conti Academy of Theatre Arts, aged between 18 and 21. An opportunity sample of 66 female volunteers also participated in the control condition, aged between 18 and 21 and were at university. Written consent was obtained from all participants. MaterialsEating deviancy was assessed using EAT (see appendix) and Self-esteem was measured using SERS. This questionnaire was altered slightly for the experimental group to measure the effect the new peer group was having over self esteem levels at university. The word ""others"" in the original questionnaire was changed to ""my new friends"" for the experimental group only, for example, question two originally presents ""I feel others do things much better than I do"" and this was retained for the control group. For the experimental group however the question was changed to ""I feel my new friends do things much better than I do"". Participants were informed, using prose instruction, that the term ""my new friends"" meant the new friends they had made since arriving at university (see appendix for a copy of both questionnaires). Body dissatisfaction was measured using BSQ (see appendix). One questionnaire entailed all three components, counterbalanced to reduce any order effects, creating six types of questionnaire. ProcedureOf the 132 participants, eleven in each group completed each type of questionnaire, with the experimental group receiving the revised questions for SERS. Instructions as to how participants should answer the questionnaires and use the provided scales were issued in written form at the beginning of each questionnaire. Before participants were asked to participate, they were asked to sign a written consent form fully explaining ethical concerns and withdrawal rights. Data was collected from participants in isolation to ensure privacy, and collected from the University of Warwick, University of Central England, Birmingham, Coventry University and The Italia Conti Academy of Theatre Arts. Four venues were chosen to increase generalisability across the university environment. All questionnaires were scored according to the original criteria. Second marking only occurred on the second round of scoring for SERS, where only the questions referring to ""others"" and the ""new group of friends"" were assessed. This was done to more fully investigate the effect a new peer group had over the individual's self esteem levels (and therefore their eating behaviour). For included and excluded questions please see appendix. Data was collected between week one and week three of term in order for new peer groups to be classed as ""new"". Data AnalysisIn order to investigate whether self-esteem levels were lower and deviant eating behaviours were higher in first year university girls than a control group of non university girls, between subjects multivariate analysis of variance (MANOVA) and Pearson's correlation co-efficient were used to analyse data. To check the assumptions of the MANOVA could be relied upon, a Mahalanobis distance identified one outlier, it was decided to keep this in however as one outlier is not going to effect the assumptions of the MANOVA. To more fully investigate the effect integrating a new peer group may have on self-esteem levels, and therefore eating patterns, an independent samples t-test was carried out between the means for the experimental and control group for SERS. This time however only the questions referring to ""others"" or ""new friends"" were included, 22 out of the original 40 questions. This was to see whether self esteem levels relating to peers were significantly different between the two groups. One sample t-tests were also conducted to investigate whether there was a significant difference between the experimental group and the normative values. A Pearson's product moment correlation co-efficient was used to investigate the relationship between the dependant variables. To measure the strength of the correlations Cohan's (1988) criteria was used (as cited in Pallant, 2002). ResultsThe graph below compares the means and standard deviations for the experimental group, control group, and the normative means for the three dependant variables; EAT, BSQ and SERS (for a table of exact findings see appendix). It can immediately be seen from this that there are higher levels of eating deviancy and body shape dissatisfaction for the experimental group although the control group have slightly lower levels of self esteem. The one-way between-subjects MANOVA was performed to investigate the three dependant variables; eating attitudes, self esteem and body shape satisfaction. The independent variable was whether participants were first year university student living away from home (experimental group), or matched age non university student (control group). There was a significant difference between the experimental group and the control group on the combined dependant variables: F (3,128) = 4.08 , p= 0.008; Wilks' Lambda = 0.91. When the results for the dependant variables were considered separately, the only difference to reach statistical significance using a Bonferroni adjusted level of 0.17, was eating attitudes: f (1,130) = 9.54 p 0.079. Self esteem levels were also not significant using a Bonferroni adjusted level of 0.17 or using a significance level of 0.05: f (1,130) = 0.042 p> 0.838. An independent t test for SERS including only the questions relating to ""new friends"" or ""others"" found no significant difference between the means for the experimental group (M=4.53, SD=18.59) or the control group (M=2.65, SD=16.82) where t (132) = 0.609, p> 0.544. The magnitude of difference in the means was very small (eta squared = 0.002). A one sample t test compared the means for the experimental group (M=19.88, SD=10.66) with normative means for EAT (M=15.6, SD=9.3). This showed that the experimental group had significantly more deviant eating behaviours than the normative group t (66) = 3.262, p = 0.002. A one samples t test showed a significant difference for BSQ between the experimental group (M=102.27 SD=34.08) and the normative group (M=81.5 SD=28.4) where t (66) = 4.952, p = 0.000. A one samples t-test also showed a significant difference for SERS between the experimental group (M =19.97 SD= 27.74) and the normative group (M=0 SD=0) where t (66) = 5.849, p = 0.000. This illustrates that all three dependant variables for the experimental group were significant when compared with the established normative values. A Pearson product moment correlation coefficient was used to investigate the relationship between the three variables measured by EAT, BSQ and SERS. For the experimental group there was a medium positive correlation between the scores for EAT and BSQ (r = 0.476, n = 66 ,p DiscussionThe most general conclusion which can be drawn from the MANOVA is that there was an overall significance between the experimental and control group on the three dependant variables; scores for EAT, BSQ and SERS. There was however only a significant main effect for EAT, which means that there was only a significant difference between the groups in their eating behaviour and not in their self-esteem levels or body shape dissatisfaction. When an independent samples t-test was conducted between the control and experimental group for the revised measure of SERS, where only questionnaires relating to the new peer group were included, there was still no significant difference between the two. This suggests that experimental participants had no more self esteem issues regarding their ""new friends"" than control participants had with ""others"". There was, however, only a non significant relationship for self-esteem between the experimental and control group used in this study. When a one sample t test was conducted between the experimental group in this study, and the established normative mean specified by Nugent and Thomas (1993), there was a highly significant difference. After consulting the means it was clear that the experimental group had lower self esteem than their non clinical control (although the experimental group did not have lower self esteem levels than the control group used in this study). It may be that low self esteem does contribute to disordered eating behaviour in first year university girls, although it was not shown to be significant with the MANOVA due to generally low levels of self esteem amongst the control group. In first year students however this may combine with several other factors linked with disordered eating not investigated in the study, for example, stress and depression (Vanderlinden et al, 2001). The control group of participants (not at university) may not resort to disordered eating to try and counteract feelings of low self esteem as they may not also have to cope with depression and stress and being away from home for the first time. Data for the experimental group was also collected from The Italia Conti Academy of Theatre Arts. This is a performing arts theatre which may encourage outgoing, confident students. It may be that this had an effect on the overall self esteem levels for the experimental group and caused them to be higher than they would be if a non performing arts university had been selected. If a non performing arts institution had been selected then first year university students may have had significantly lower self esteem levels than the control group. It is therefore clear from the results that although first year students do have significantly more deviant eating behaviours than a control group of matched aged participants, it is not attributable to low self esteem and having to integrate a new peer group, as the t test investigating the influence new friends had proved also not to be significant. After correlating the results using a Pearson's product moment co-efficient it can be seen that there was a medium strength negative correlation between scores for EAT and SERS, showing that high eating deviancy did correlate with low self esteem levels. Although this did not prove to be significant in the MANOVA there was 19.98% shared variance with r = -0.44. When EAT and SERS were correlated for the control group there was only a small negative correlation between the two with only 8.1% shared variance with r = -0.28. Correlations do not indicate cause although it does show that low self esteem correlates more strongly with disordered eating for the experimental group. The fact that there was no main effect for BSQ on the MANOVA suggests that the disordered eating patterns are not caused by body image. This might be because being at university does not increase body dissatisfaction and therefore cause more deviant eating behaviours. Although there was a stronger correlation between EAT and BSQ for the experimental than control group, body image alone cannot be considered a significant difference in disordered eating between the experimental and control group. Although low self esteem and integrating a new peer group have not been shown to be significant factors in causing disordered eating behaviour for first year university students, it has been shown that they do have significantly more deviant eating behaviour than the control group. This may still be indirectly a result of being in a new peer group, as shown by the findings of Langlois and Stephan (1977) (as cited in Lieberman et al, 2001) who found that physical attractiveness is linked to more positive peer relations and social acceptance. Attitudes towards attractiveness and the benefits attractiveness can bring are established from an early age. This tends to be well known and seems to have internalised as a concept in young girls (Markham, Thompson and Bowling, 2005). This was also supported by Leiberman et al (2001) who found that more popular girls are more likely to have eating disorders. This suggests that girls who want to be popular, or want to maintain popularity tend to resort to severe eating behaviour and are under the assumption that slimness and attractiveness equals popularity. If this is the case then integrating a new peer group may be a significant factor on eating behaviour in first year students. This study investigated the affect new peer group had over eating by measuring insecurity felt amongst new friends. It may not be that individuals feel insecure with their friends, but that they feel competitive amongst their new friends and engage in dieting tactics in order to establish themselves as the most attractive, thereby increasing their chances of being popular. This could not be established from this study which concentrated on the effect new friends had on self esteem levels. Lieberman et al (2001) found that peer group influence was the single biggest influence over disordered eating. Another explanation as to why first year students may have significantly more deviant eating behaviour may reside in the element of perceived control. Individuals with the propensity to suffer from disordered eating tend to have a deficiency in their ability to influence their own environment (Gordon, 1990). They feel that they cannot control their surroundings or their lives and so resort to controlling the one thing that they feel they do have control over, their eating behaviour. Being independent and moving away to university may increase the sense of lost control and therefore present a crisis in self worth (Gordon, 1990). This may increase disordered eating as the individual may increase their control over their dieting behaviour as they feel a decrease in control of their circumstances. If this were the case, then it may be that the individuals already have the eating problems and it is just that moving away from university, and the challenge and stress accompanying this, enhances rather than causes the eating deviancy. It may be beneficial to compose a longitudinal study investigating how university affects eating patterns before and during university as opposed to comparison with a control group. It may be that in this study self-esteem levels and integrating the new peer group were not significant due to a number of confounding variables. The SERS questionnaire was established in 1993. As it is now nearly ten years later it may be that there has been an increase in the levels of self esteem of the population in general. This could be why when the experimental group was compared with the control group there was no significant difference, but when the experimental group was compared with the normative values Nugent and Thomas (1993) identified, there is a highly significant difference. It may be that the questionnaire needs to be modified to account for this possibility. It is also possible that social desirability bias may have played a role in self esteem and peer group influence not being significant, as nobody wishes to portray themselves as having low self esteem and participants may not have liked to admit that they did feel inferior to their friends, despite the fact that the questionnaires are anonymous. People who are low might not want to recognise or express this due to reaction formation. The format of the questionnaires may also have proved a problem. To increase reliability and validity three established questionnaires were used to create one. The main problem here is that all three used a slightly different scale. This may have caused confusion for participants, who might have used the wrong scales whilst answering questions. Experimental care was however taken by explaining in prose form at the beginning of each questionnaire that a different scale was being used for each section. The scales were also repeated every time questions continued onto a new page so participants would not have to rely on memory to use the scales. Human error may however have meant some participants may have used the wrong scale for the questionnaires. Other issues may have influenced results such as whether participants had a boyfriend at university. This may have increased self esteem levels and reduced the effect a new peer group may have had over individuals who had boyfriends. Also the distance from home participants were may have had an effect. Participants living closer to the university may not have had such low self esteem as they are still relatively near to their parents. In further research it may be beneficial to account for these individual factors. Due to the fact that integrating a new peer group and low self esteem have not been shown to be significant causes of the difference in eating deviancy between the control group and the experimental group, there appears to be plenty of scope for further research. It may be beneficial to stay with a new peer group and self esteem levels but use different methodology to ascertain what influence these have over disordered eating practices. It is clear that although these were not shown to be significant they do have an effect and it may also be beneficial to look at the effect of a new peer group in a different context. Instead of viewing it from the stand point of a feeling of inferiority to peers as this study does, it may be possible to study it from a competition of popularity standpoint, led by the research of Langlois at al (1977) and Leiberman et al (2001). Depression and family factors may also play a role in increasing disordered eating behaviour in students and more research may be needed in this area. It may be that a new questionnaire needs to be established particularly looking at these issues to ensure validity and to ensure it is up to date and relevant to this generation. Further research may also look at the prevalence and causes of disordered eating for male students. It is however important to stay within the region of students as clearly there is an effect of going to university on students eating behaviour. This can also be supported by Kjelsan et al (2004) who found 6% of undergraduate women think they have an eating disorder, which is far higher than the national predicted prevalence rate of 3.4%.","Popularity is an extremely difficult concept to measure as popularity is variable across cultures and ages. Characteristics defining a child as popular at the age of 8 may not be the same characteristics which define a child as popular at 14. Another problem arises distinguishing friendship from popularity. A child may be perceived as unpopular but still have a few close friends, and it is important to consider this when looking at psychological development because popularity is multi-faceted as opposed to one dimensional. It has been shown that unpopular children are more likely to have psychological difficulties such as schizophrenia, anxiety disorders, mood disorders and criminal behaviour (Kupersmidt, Coie and Dodge, 1990). Because of this, an important issue when looking at the effect of popularity on psychological development is cause and effect. This issue is covered extensively in the essay as it is valuable, yet difficult, to try and determine whether being unpopular causes problems with psychological development, or whether underlying psychological difficulties cause the child to behave inappropriately and become unpopular. Popularity is an extremely important issue affecting all children. It is more important now than ever as parents tend to return earlier to work so children tend to spend more time with peers in nurseries and after school wrap-around classes. Characteristics of popular and unpopular children have been widely investigated, and there is some variation in the results found. Most commonly however it has been found in primary school aged children, between the ages of 4 and 11, the children regarded as popular tend to be co-operative, helpful, have leadership skills and actively engage in prosocial behaviour (Dekovic and Gerris, 1994). Unpopular children however tend to be aggressive, possessive, verbally aversive and display hostility (Dekovic and Gerris, 1994). To study the effect popularity has on psychological development however it is important to break the notion of ""unpopularity"" down into further components. Asher (1990) defines two types of unpopular children. The first are ""rejected"" and these are children who are actively disliked by their fellow peers (Asher, 1990). The second type are ""neglected"" and these are not actively disliked but tend to be timid and shy and are almost forgotten by peers and also appear to have a lack of friends (Asher, 1990). It is important to make this distinction as psychological development is influenced by whether you are considered ""rejected"" or ""neglected"", it appears that neglected children know how to behave socially but have problems putting this into practice, rejected children have difficulties with both (Asher, 1990). It has been found by Coie and Dodge (1983) (as cited in Asher, 1990) that rejection is a stable process and once you have been rejected you are likely to stay rejected for at least the next five years. This is why it is important to look at the implications for this on the child's psychological development as it is likely the child will remain in their rejected situation until at least adulthood. An important aspect to consider with respect to this is labelling theory and the self fulfilling prophecy. When cause and effect are taken into account a child may remain unpopular because they believe themselves to be unpopular and therefore act in a way appropriate to the label they have been given. Also children are very aware of building alliances to secure their own popularity and if one child is labelled undesirable other children may not want to be associated with the labelled child thus securing their own alliances and popularity. This may account for why children remain unpopular but it does not explain why the child may become unpopular or undesirable in the first place. One possible reason causing a child to be unpopular may be due to them having an autistic spectrum disorder (Hobson, 1995). Unfortunately many of the characteristics displayed by unpopular children are also displayed by children suffering from an autistic spectrum disorder such as Autism or Asberger's Syndrome. Children who are unpopular tend to only view situations from their own perspective, do not value others opinions and cannot empathise with their peers (Dekovic et al, 1994). Autism is a disorder where it is believed children lack a ""Theory of Mind"". This means that they find it very hard to see the world from any perspective but their own and fail to understand others emotions or social cues (Hobson, 1995). This may lead to the child becoming unpopular as perspective seeing is essential to being popular as are the communication and cognitive skills that someone with autism may lack. This is one reason a child may become unpopular as autism affects 5 in every 10,000 children (Hobson, 1995). If autism is not diagnosed until the later years the child will be in main stream education and more than likely be rejected by peers and possibly suffer the psychological problems associated with rejection which will be discussed later. Another cause for a child being unpopular is associated with the family (Henggeler, Edwards, Cohen and Summerville, 1991). The majority of children spend their first few years learning from their parents so it is not surprising that these early years contribute to the child's personality and whether they become popular or unpopular. Ainsworth and Wittig (1969) (as cited in Henggeler et al, 1991) investigated types of attachment between mother and infant. They identified three types of attachment as these have been investigated with regards to child's future popularity. Vandell and Wilson (1987) (as cited in Henggeler et al 1991) found that children who are securely attached become more popular than those who are not securely attached and have more positive peer interactions. This is likely to be because these children have a secure base from which to explore from and feel confident in themselves to make social advances, knowing that they have support. This inner feeling of security may facilitate confidence and an interpersonal family may produce interactional skills. To support this MacDonald (1987) (as cited in Henggeler, 1991) found that preschool boys who had been assessed as being ""rejected"" had parents who were less interpersonally responsive and were less emotionally positive. These children were more likely to have been considered to have had an attachment type of anxious avoidant or anxious resistant. If child had this attachment type then they will not have had a secure base from which to explore from and particularly anxious resistant children would have been anxious at leaving the caregiver and facilitating early social relationships. This may be a cause in why some children become popular and why some children become unpopular. It is interesting to note however that the majority of research completed on attachment type and popularity occurred in 1987. This is just one year after Maine (as cited in Maine and Cassidy, 1988) introduced the new attachment type insecure disorganised. This means that attachment will have been in the media and well heard of, which may have been an incentive to link attachment with popularity and more may have been made of it than is actually there. Attachment suggests that it is causal in determining popularity although this seems a little reductionist and little has been done to investigate whether securely attached children are still more popular in the older years when there is less reliance on the parents. How this effects children in care would also need to be fully investigated before attachment could be seen as a deciding figure in a child's popularity. Also attachment claims to be a causal factor but it may be the child's temperament that causes attachment and popularity therefore attachment is not the cause, but the child's personality. It may not be as much attachment but more to do with Bandura's social learning theory (1969) (as cited in Grusec 1992) as children who's parents are emotionally responsive and communicative to them learn to be emotionally responsive and communicative to others and therefore more popular. Piaget (as cited in Rubin, LeMare and Lollis, 1990) offers another environmental explanation as to why some children may be more popular than others. He suggests that children who become popular have successfully negotiated mutually engendered behaviours. Because young children are egocentric to learn how to interact and become socially efficient they need to lose the belief that the world centres around them and develop a theory of mind (as cited in Rubin, 1990). Early play and reciprocal friendship allow perspective taking skills and prosocial behaviours to develop, which are important skills to be popular and socially accepted (as cited in Rubin, 1990). This has implications for children who do not have the opportunity to socialise at an early age as may not completely abolish the notion of egocentrism. It also means important cognitive and social skills are not being developed. This is supported by the fact that Dekovic and Gerris (1994) found that rejected children demonstrated more egocentric, selfish behaviours This offers a suggestion as to why children may not exhibit prosocial behaviour but it still does not explicitly offer a cause. Is it opportunity that prevents early socialisation, and if so should this just not occur at preschool, or is it personality which prevents early socialisation or is it attachment type and not feeling secure enough to socialise. It is clear that there are so many different causes which may lead a child to become unpopular. There is also a large number of children who can be considered rejected or neglected and therefore it is important to investigate any implications of this on their psychological development. It is at this stage that it is important to separate the notion ""unpopular"" into rejected and neglected. Rejected children tend to be more actively disliked than neglected and engage in more aggressive and hostile behaviours (Asher, 1990). It is this type of child who is more likely to turn to criminality and anti social behaviours (Kupersmidt et al 1990). This is a dangerous assertation to make however regarding the self fulfilling prophecy and also almost makes it acceptable for rejected children to turn to criminal behaviour. Roff (1972) (as cited in Kupersmidt et al, 1990) found that peer rejection was a significant predictor of delinquency and this once again may be a result of the self fulfilling prophecy in that the child believes they are bad and therefore will only survive by being bad. It may also be an attempt for admiration as anti social behaviour becomes more associated with popularity once the child moves towards adolescence, or it could just be a scream for attention. Neglected children however tend to be less actively disliked by their peers but are more withdrawn and isolated from society (Asher, 1990). This is obviously important for psychological development as there are three disorders noted in DSM IV (American Psychiatric Association, 1995) regarding withdrawal such as ""Shyness Disorder"", ""Adjustment Disorder"" and ""Introverted Disorder"", therefore being withdrawn obviously has psychological disadvantages. Neglected children are also more likely to suffer from internal disorders such as those involving mood and anxiety (Asher, 1990) as opposed to rejected who are more likely to suffer external ones (Asher, 1990). Being neglected however does not necessarily lead to psychological problems however as Coie and Kupersmidt (1983) (as cited in Rubin, 1990) found that neglected children are more confident in smaller groups. There is only a problem however when insecurity and anxiety issues become apparent. Obviously children who are considered unpopular whether rejected or neglected are perceptive enough to pick up on and internalise negative feelings about themselves which has implications for psychological development in itself and can lead to many further complications, particularly that of depression. There has been links made between children who were unpopular at school and a later development of schizophrenia. A feedback study conducted by Bleuler (1950) (as cited in Kupersmidt et al 1990) describes the preschizophrenic child as anxious, excitable, irritable and aggressive. This seems to be descriptive of a rejected rather than a neglected child. There however problems with cause and effect with this as it is not clear whether rejection in itself causes schizophrenia or the schizophrenia causes the antisocial behaviour which leads the child to become unpopular. From the evidence and knowledge about schizophrenia I believe that being unpopular itself doesn't cause schizophrenia. This is because schizophrenia does not tend to develop until late adolescence. Also Watt (1978) showed that for the preschizophrenic child behaviour in childhood is normal and the child does not become unpopular until adolescence with the onset of schizophrenia. This also concurs with evidence that rejection is stable. If a child is unpopular at adolescence then they will have been unpopular during childhood, the pre-schizophrenic child does not show this pattern so it is more likely that schizophrenia causes the unpopularity and not vice versa. In conclusion there could be many factors effecting whether a child turns out to be popular or unpopular. The majority however seem to ignore the child's actual personality which probably plays a large role. There is little doubt however that factors such as attachment type and early experiences with reciprocal friendships are valuable in learning the necessary skills to socialise and which therefore give you an increased chance of being popular and having friends. It has been shown that lack of friends or being unpopular may lead to problems with psychological development and these seem to be more extreme with rejected rather than neglected children. There is plenty still to discover however regarding whether being unpopular causes psychological problems, or the psychological problems are already there and cause unpopularity. It would appear from the conclusions made in this essay that it is a likely combination of the two.",True
71,"Popularity is an extremely difficult concept to measure as popularity is variable across cultures and ages. Characteristics defining a child as popular at the age of 8 may not be the same characteristics which define a child as popular at 14. Another problem arises distinguishing friendship from popularity. A child may be perceived as unpopular but still have a few close friends, and it is important to consider this when looking at psychological development because popularity is multi-faceted as opposed to one dimensional. It has been shown that unpopular children are more likely to have psychological difficulties such as schizophrenia, anxiety disorders, mood disorders and criminal behaviour (Kupersmidt, Coie and Dodge, 1990). Because of this, an important issue when looking at the effect of popularity on psychological development is cause and effect. This issue is covered extensively in the essay as it is valuable, yet difficult, to try and determine whether being unpopular causes problems with psychological development, or whether underlying psychological difficulties cause the child to behave inappropriately and become unpopular. Popularity is an extremely important issue affecting all children. It is more important now than ever as parents tend to return earlier to work so children tend to spend more time with peers in nurseries and after school wrap-around classes. Characteristics of popular and unpopular children have been widely investigated, and there is some variation in the results found. Most commonly however it has been found in primary school aged children, between the ages of 4 and 11, the children regarded as popular tend to be co-operative, helpful, have leadership skills and actively engage in prosocial behaviour (Dekovic and Gerris, 1994). Unpopular children however tend to be aggressive, possessive, verbally aversive and display hostility (Dekovic and Gerris, 1994). To study the effect popularity has on psychological development however it is important to break the notion of ""unpopularity"" down into further components. Asher (1990) defines two types of unpopular children. The first are ""rejected"" and these are children who are actively disliked by their fellow peers (Asher, 1990). The second type are ""neglected"" and these are not actively disliked but tend to be timid and shy and are almost forgotten by peers and also appear to have a lack of friends (Asher, 1990). It is important to make this distinction as psychological development is influenced by whether you are considered ""rejected"" or ""neglected"", it appears that neglected children know how to behave socially but have problems putting this into practice, rejected children have difficulties with both (Asher, 1990). It has been found by Coie and Dodge (1983) (as cited in Asher, 1990) that rejection is a stable process and once you have been rejected you are likely to stay rejected for at least the next five years. This is why it is important to look at the implications for this on the child's psychological development as it is likely the child will remain in their rejected situation until at least adulthood. An important aspect to consider with respect to this is labelling theory and the self fulfilling prophecy. When cause and effect are taken into account a child may remain unpopular because they believe themselves to be unpopular and therefore act in a way appropriate to the label they have been given. Also children are very aware of building alliances to secure their own popularity and if one child is labelled undesirable other children may not want to be associated with the labelled child thus securing their own alliances and popularity. This may account for why children remain unpopular but it does not explain why the child may become unpopular or undesirable in the first place. One possible reason causing a child to be unpopular may be due to them having an autistic spectrum disorder (Hobson, 1995). Unfortunately many of the characteristics displayed by unpopular children are also displayed by children suffering from an autistic spectrum disorder such as Autism or Asberger's Syndrome. Children who are unpopular tend to only view situations from their own perspective, do not value others opinions and cannot empathise with their peers (Dekovic et al, 1994). Autism is a disorder where it is believed children lack a ""Theory of Mind"". This means that they find it very hard to see the world from any perspective but their own and fail to understand others emotions or social cues (Hobson, 1995). This may lead to the child becoming unpopular as perspective seeing is essential to being popular as are the communication and cognitive skills that someone with autism may lack. This is one reason a child may become unpopular as autism affects 5 in every 10,000 children (Hobson, 1995). If autism is not diagnosed until the later years the child will be in main stream education and more than likely be rejected by peers and possibly suffer the psychological problems associated with rejection which will be discussed later. Another cause for a child being unpopular is associated with the family (Henggeler, Edwards, Cohen and Summerville, 1991). The majority of children spend their first few years learning from their parents so it is not surprising that these early years contribute to the child's personality and whether they become popular or unpopular. Ainsworth and Wittig (1969) (as cited in Henggeler et al, 1991) investigated types of attachment between mother and infant. They identified three types of attachment as these have been investigated with regards to child's future popularity. Vandell and Wilson (1987) (as cited in Henggeler et al 1991) found that children who are securely attached become more popular than those who are not securely attached and have more positive peer interactions. This is likely to be because these children have a secure base from which to explore from and feel confident in themselves to make social advances, knowing that they have support. This inner feeling of security may facilitate confidence and an interpersonal family may produce interactional skills. To support this MacDonald (1987) (as cited in Henggeler, 1991) found that preschool boys who had been assessed as being ""rejected"" had parents who were less interpersonally responsive and were less emotionally positive. These children were more likely to have been considered to have had an attachment type of anxious avoidant or anxious resistant. If child had this attachment type then they will not have had a secure base from which to explore from and particularly anxious resistant children would have been anxious at leaving the caregiver and facilitating early social relationships. This may be a cause in why some children become popular and why some children become unpopular. It is interesting to note however that the majority of research completed on attachment type and popularity occurred in 1987. This is just one year after Maine (as cited in Maine and Cassidy, 1988) introduced the new attachment type insecure disorganised. This means that attachment will have been in the media and well heard of, which may have been an incentive to link attachment with popularity and more may have been made of it than is actually there. Attachment suggests that it is causal in determining popularity although this seems a little reductionist and little has been done to investigate whether securely attached children are still more popular in the older years when there is less reliance on the parents. How this effects children in care would also need to be fully investigated before attachment could be seen as a deciding figure in a child's popularity. Also attachment claims to be a causal factor but it may be the child's temperament that causes attachment and popularity therefore attachment is not the cause, but the child's personality. It may not be as much attachment but more to do with Bandura's social learning theory (1969) (as cited in Grusec 1992) as children who's parents are emotionally responsive and communicative to them learn to be emotionally responsive and communicative to others and therefore more popular. Piaget (as cited in Rubin, LeMare and Lollis, 1990) offers another environmental explanation as to why some children may be more popular than others. He suggests that children who become popular have successfully negotiated mutually engendered behaviours. Because young children are egocentric to learn how to interact and become socially efficient they need to lose the belief that the world centres around them and develop a theory of mind (as cited in Rubin, 1990). Early play and reciprocal friendship allow perspective taking skills and prosocial behaviours to develop, which are important skills to be popular and socially accepted (as cited in Rubin, 1990). This has implications for children who do not have the opportunity to socialise at an early age as may not completely abolish the notion of egocentrism. It also means important cognitive and social skills are not being developed. This is supported by the fact that Dekovic and Gerris (1994) found that rejected children demonstrated more egocentric, selfish behaviours This offers a suggestion as to why children may not exhibit prosocial behaviour but it still does not explicitly offer a cause. Is it opportunity that prevents early socialisation, and if so should this just not occur at preschool, or is it personality which prevents early socialisation or is it attachment type and not feeling secure enough to socialise. It is clear that there are so many different causes which may lead a child to become unpopular. There is also a large number of children who can be considered rejected or neglected and therefore it is important to investigate any implications of this on their psychological development. It is at this stage that it is important to separate the notion ""unpopular"" into rejected and neglected. Rejected children tend to be more actively disliked than neglected and engage in more aggressive and hostile behaviours (Asher, 1990). It is this type of child who is more likely to turn to criminality and anti social behaviours (Kupersmidt et al 1990). This is a dangerous assertation to make however regarding the self fulfilling prophecy and also almost makes it acceptable for rejected children to turn to criminal behaviour. Roff (1972) (as cited in Kupersmidt et al, 1990) found that peer rejection was a significant predictor of delinquency and this once again may be a result of the self fulfilling prophecy in that the child believes they are bad and therefore will only survive by being bad. It may also be an attempt for admiration as anti social behaviour becomes more associated with popularity once the child moves towards adolescence, or it could just be a scream for attention. Neglected children however tend to be less actively disliked by their peers but are more withdrawn and isolated from society (Asher, 1990). This is obviously important for psychological development as there are three disorders noted in DSM IV (American Psychiatric Association, 1995) regarding withdrawal such as ""Shyness Disorder"", ""Adjustment Disorder"" and ""Introverted Disorder"", therefore being withdrawn obviously has psychological disadvantages. Neglected children are also more likely to suffer from internal disorders such as those involving mood and anxiety (Asher, 1990) as opposed to rejected who are more likely to suffer external ones (Asher, 1990). Being neglected however does not necessarily lead to psychological problems however as Coie and Kupersmidt (1983) (as cited in Rubin, 1990) found that neglected children are more confident in smaller groups. There is only a problem however when insecurity and anxiety issues become apparent. Obviously children who are considered unpopular whether rejected or neglected are perceptive enough to pick up on and internalise negative feelings about themselves which has implications for psychological development in itself and can lead to many further complications, particularly that of depression. There has been links made between children who were unpopular at school and a later development of schizophrenia. A feedback study conducted by Bleuler (1950) (as cited in Kupersmidt et al 1990) describes the preschizophrenic child as anxious, excitable, irritable and aggressive. This seems to be descriptive of a rejected rather than a neglected child. There however problems with cause and effect with this as it is not clear whether rejection in itself causes schizophrenia or the schizophrenia causes the antisocial behaviour which leads the child to become unpopular. From the evidence and knowledge about schizophrenia I believe that being unpopular itself doesn't cause schizophrenia. This is because schizophrenia does not tend to develop until late adolescence. Also Watt (1978) showed that for the preschizophrenic child behaviour in childhood is normal and the child does not become unpopular until adolescence with the onset of schizophrenia. This also concurs with evidence that rejection is stable. If a child is unpopular at adolescence then they will have been unpopular during childhood, the pre-schizophrenic child does not show this pattern so it is more likely that schizophrenia causes the unpopularity and not vice versa. In conclusion there could be many factors effecting whether a child turns out to be popular or unpopular. The majority however seem to ignore the child's actual personality which probably plays a large role. There is little doubt however that factors such as attachment type and early experiences with reciprocal friendships are valuable in learning the necessary skills to socialise and which therefore give you an increased chance of being popular and having friends. It has been shown that lack of friends or being unpopular may lead to problems with psychological development and these seem to be more extreme with rejected rather than neglected children. There is plenty still to discover however regarding whether being unpopular causes psychological problems, or the psychological problems are already there and cause unpopularity. It would appear from the conclusions made in this essay that it is a likely combination of the two.","AbstractThis study investigated whether first year university females had significantly more disordered eating than a control group of non-university females. It was hypothesised that low self esteem as a result of leaving home for the first time, and having to integrate a new peer group would be the cause of this. Questionnaires were used to measure participants eating behaviour, body shape satisfaction and self esteem. Attitudes towards the new peer group were measured using specific questions on the self esteem questionnaire. It was found that the first year students did have significantly more disordered eating than the control group, although having low self esteem and integrating a new peer group were not significant causes of this. It may be that low self esteem and having to integrate a new peer group play a part in increasing eating deviancy in first year students, although these may not have been shown to be significant because they combine with other factors associated with university life such as stress and depression. It was clearly found, however, that first year females have far more problems with their eating than females who are not at university. IntroductionEating disorders are becoming an increasing problem. The concept of the thin body ideal is being internalised through the social setting and causing more people, particularly adolescent females, to resort to severe dieting techniques (Markham, Thompson, Bowling, 2005). Unhealthy eating attitudes are an immensely important area of research as it has been shown that up to 20% of young women participate in intense dieting practices at some point in their lives (Kurth, Krahn, Nain and Drewnowski, 1995). Eating disorders are an issue affecting millions of people worldwide, with 3.4% of the population having a diagnosis (Kjelsan, Bjornstrom and Gotestom, 2004). It is likely the incidence rate for eating disorders is higher than this, although many people suffering from them deny the problem and therefore do not seek diagnosis or treatment. Research in this area is particularly important as it appears that more and more people are developing the disorders and concealing them. Once the causes for eating disorders have been fully investigated it may be possible to accurately identify individuals who are predisposed to suffer with their eating behaviour. There are many factors which may trigger an eating disorder. Lieberman, Gauvin, Bukowski and White (2001) found significant causes to be peer pressure to be thin, social reinforcement and body related teasing. They present the argument that as peer relations are very important in adolescence, if the social group values the thin body ideal highly then individuals aspire to achieve this, even through deviant eating patterns. In addition to this Blowers, Loxton, Grady-Flesser and Dawe (2003) found that the media plays an important role in the internalisation of the thin body ideal and increasing body dissatisfaction, and therefore, contributing to the development of eating disorders. These can be considered social influences over eating behaviour and are directly linked to how the individual perceives themselves in relation to others. This appears to be a highly influential factor for eating pathology because if a person's evaluation of their body does not meet the real or imagined body ideal set by peers, or by the media, then deviant eating behaviour seems to be the coping technique adopted to achieve what is believed to be desirable (Gordon, 1990). Believing that your body is inadequate and not acceptable to social standards is captured overall by low self-esteem. It has been shown by McLaren, Gauvin and Steigler (2001) that low self esteem is a significant contributor to eating pathology. It appears that it is a combination of low self esteem, peer and media pressure to be thin, and an internalisation of the thin body ideal that predominantly triggers eating pathology. The present study is concerned with investigating the combination of both internal and external factors within a particularly vulnerable group of individuals, first year university students. Moving away from home for the first time is one of the most stressful life changes a person will ever have to adjust to (Seligman, Walker, Rosenhan, 2001). This study intends to investigate whether eating behaviours become more deviant during the first year of living away from home at university compared to a control group of non-university participants. Moving into halls of residence at university entails integration into a new peer group, which may heighten feelings of insecurity and low self esteem. It has already been shown that low self esteem is a significant contributor to disordered eating behaviour, and this may combine with students seeking the thin body ideal in order to successfully integrate with their new group of friends. When adolescent girls are beginning new friendships they believe it is important to be thin, and compare themselves to other members of the group (Guendouzi, 2004). Attainment of the thin body ideal was also found to be a common topic of conversation (Guendouzi, 2004). This is particularly important when investigating the effect integration of a new peer group has on eating behaviour as it is likely that when girls first start university and are making new friendships, this is a safe topic of conversation as body concerns an issue many girls have as a common ground, which may increase pressure to be thin. It is likely that the low self esteem associated with moving away from home for the first time will enhance the desire to be slim to make a more favourable impression on the new peers. This is because many believe the more attractive you are, the more likely you are to be popular. Langlois and Stephan (1977) (as cited in Lieberman et al, 2001) found that physical attractiveness is linked to more positive peer relations and social acceptance. Students may resort to disordered eating to increase their chances of achieving the thin body ideal, and therefore, increase their chances of being popular within the new group of friends they are living with, and also to counterbalance low self esteem. It is also important to study first year students as the majority of previous research has centred upon participants aged between 14 and 16 years. It has been shown however that eating disorders are prevalent up until the age of 25 (Vanderlinden, Grave, Vandereycken, Noorduin, 2001). It is therefore important to study eating behaviour of older adolescents (hence first year university students), as this seems to be an area which needs enhancing. Assessments will be made using the Eating Attitudes Test (EAT) (Garner and Garfinkel, 1979, as cited in Corcoran and Fisher, 2000) to measure eating deviancy, the Self Esteem Rating Scale (SERS) (Nugent and Thomas, 1993, as cited in Corcoran and Fisher, 2000) to measure self esteem levels and the Body Shape Questionnaire (BSQ) (Cooper, Taylor, Cooper and Fairburn, 1987, as cited in Corcoran and Fisher, 2000) to measure body shape dissatisfaction. Peer group influence over self esteem will be measured using a revised version of the Self Esteem Rating Scale (as detailed in the method section). It is hypothesised that having to integrate a new peer group and low self esteem will lead to more deviant eating behaviour in first year university girls compared to a control group of matched age participants. Females only were used as eating disorders predominantly affects this sex by a ratio of 9:1 females to males (Gordon, 1990). MethodParticipantsAn opportunity sample of 66 female volunteers participated in the experimental condition. All were first year students living away from home for the first time in halls of residences at three higher education institutions, the University of Warwick, University of Central England, Birmingham, University of Coventry and The Italia Conti Academy of Theatre Arts, aged between 18 and 21. An opportunity sample of 66 female volunteers also participated in the control condition, aged between 18 and 21 and were at university. Written consent was obtained from all participants. MaterialsEating deviancy was assessed using EAT (see appendix) and Self-esteem was measured using SERS. This questionnaire was altered slightly for the experimental group to measure the effect the new peer group was having over self esteem levels at university. The word ""others"" in the original questionnaire was changed to ""my new friends"" for the experimental group only, for example, question two originally presents ""I feel others do things much better than I do"" and this was retained for the control group. For the experimental group however the question was changed to ""I feel my new friends do things much better than I do"". Participants were informed, using prose instruction, that the term ""my new friends"" meant the new friends they had made since arriving at university (see appendix for a copy of both questionnaires). Body dissatisfaction was measured using BSQ (see appendix). One questionnaire entailed all three components, counterbalanced to reduce any order effects, creating six types of questionnaire. ProcedureOf the 132 participants, eleven in each group completed each type of questionnaire, with the experimental group receiving the revised questions for SERS. Instructions as to how participants should answer the questionnaires and use the provided scales were issued in written form at the beginning of each questionnaire. Before participants were asked to participate, they were asked to sign a written consent form fully explaining ethical concerns and withdrawal rights. Data was collected from participants in isolation to ensure privacy, and collected from the University of Warwick, University of Central England, Birmingham, Coventry University and The Italia Conti Academy of Theatre Arts. Four venues were chosen to increase generalisability across the university environment. All questionnaires were scored according to the original criteria. Second marking only occurred on the second round of scoring for SERS, where only the questions referring to ""others"" and the ""new group of friends"" were assessed. This was done to more fully investigate the effect a new peer group had over the individual's self esteem levels (and therefore their eating behaviour). For included and excluded questions please see appendix. Data was collected between week one and week three of term in order for new peer groups to be classed as ""new"". Data AnalysisIn order to investigate whether self-esteem levels were lower and deviant eating behaviours were higher in first year university girls than a control group of non university girls, between subjects multivariate analysis of variance (MANOVA) and Pearson's correlation co-efficient were used to analyse data. To check the assumptions of the MANOVA could be relied upon, a Mahalanobis distance identified one outlier, it was decided to keep this in however as one outlier is not going to effect the assumptions of the MANOVA. To more fully investigate the effect integrating a new peer group may have on self-esteem levels, and therefore eating patterns, an independent samples t-test was carried out between the means for the experimental and control group for SERS. This time however only the questions referring to ""others"" or ""new friends"" were included, 22 out of the original 40 questions. This was to see whether self esteem levels relating to peers were significantly different between the two groups. One sample t-tests were also conducted to investigate whether there was a significant difference between the experimental group and the normative values. A Pearson's product moment correlation co-efficient was used to investigate the relationship between the dependant variables. To measure the strength of the correlations Cohan's (1988) criteria was used (as cited in Pallant, 2002). ResultsThe graph below compares the means and standard deviations for the experimental group, control group, and the normative means for the three dependant variables; EAT, BSQ and SERS (for a table of exact findings see appendix). It can immediately be seen from this that there are higher levels of eating deviancy and body shape dissatisfaction for the experimental group although the control group have slightly lower levels of self esteem. The one-way between-subjects MANOVA was performed to investigate the three dependant variables; eating attitudes, self esteem and body shape satisfaction. The independent variable was whether participants were first year university student living away from home (experimental group), or matched age non university student (control group). There was a significant difference between the experimental group and the control group on the combined dependant variables: F (3,128) = 4.08 , p= 0.008; Wilks' Lambda = 0.91. When the results for the dependant variables were considered separately, the only difference to reach statistical significance using a Bonferroni adjusted level of 0.17, was eating attitudes: f (1,130) = 9.54 p 0.079. Self esteem levels were also not significant using a Bonferroni adjusted level of 0.17 or using a significance level of 0.05: f (1,130) = 0.042 p> 0.838. An independent t test for SERS including only the questions relating to ""new friends"" or ""others"" found no significant difference between the means for the experimental group (M=4.53, SD=18.59) or the control group (M=2.65, SD=16.82) where t (132) = 0.609, p> 0.544. The magnitude of difference in the means was very small (eta squared = 0.002). A one sample t test compared the means for the experimental group (M=19.88, SD=10.66) with normative means for EAT (M=15.6, SD=9.3). This showed that the experimental group had significantly more deviant eating behaviours than the normative group t (66) = 3.262, p = 0.002. A one samples t test showed a significant difference for BSQ between the experimental group (M=102.27 SD=34.08) and the normative group (M=81.5 SD=28.4) where t (66) = 4.952, p = 0.000. A one samples t-test also showed a significant difference for SERS between the experimental group (M =19.97 SD= 27.74) and the normative group (M=0 SD=0) where t (66) = 5.849, p = 0.000. This illustrates that all three dependant variables for the experimental group were significant when compared with the established normative values. A Pearson product moment correlation coefficient was used to investigate the relationship between the three variables measured by EAT, BSQ and SERS. For the experimental group there was a medium positive correlation between the scores for EAT and BSQ (r = 0.476, n = 66 ,p DiscussionThe most general conclusion which can be drawn from the MANOVA is that there was an overall significance between the experimental and control group on the three dependant variables; scores for EAT, BSQ and SERS. There was however only a significant main effect for EAT, which means that there was only a significant difference between the groups in their eating behaviour and not in their self-esteem levels or body shape dissatisfaction. When an independent samples t-test was conducted between the control and experimental group for the revised measure of SERS, where only questionnaires relating to the new peer group were included, there was still no significant difference between the two. This suggests that experimental participants had no more self esteem issues regarding their ""new friends"" than control participants had with ""others"". There was, however, only a non significant relationship for self-esteem between the experimental and control group used in this study. When a one sample t test was conducted between the experimental group in this study, and the established normative mean specified by Nugent and Thomas (1993), there was a highly significant difference. After consulting the means it was clear that the experimental group had lower self esteem than their non clinical control (although the experimental group did not have lower self esteem levels than the control group used in this study). It may be that low self esteem does contribute to disordered eating behaviour in first year university girls, although it was not shown to be significant with the MANOVA due to generally low levels of self esteem amongst the control group. In first year students however this may combine with several other factors linked with disordered eating not investigated in the study, for example, stress and depression (Vanderlinden et al, 2001). The control group of participants (not at university) may not resort to disordered eating to try and counteract feelings of low self esteem as they may not also have to cope with depression and stress and being away from home for the first time. Data for the experimental group was also collected from The Italia Conti Academy of Theatre Arts. This is a performing arts theatre which may encourage outgoing, confident students. It may be that this had an effect on the overall self esteem levels for the experimental group and caused them to be higher than they would be if a non performing arts university had been selected. If a non performing arts institution had been selected then first year university students may have had significantly lower self esteem levels than the control group. It is therefore clear from the results that although first year students do have significantly more deviant eating behaviours than a control group of matched aged participants, it is not attributable to low self esteem and having to integrate a new peer group, as the t test investigating the influence new friends had proved also not to be significant. After correlating the results using a Pearson's product moment co-efficient it can be seen that there was a medium strength negative correlation between scores for EAT and SERS, showing that high eating deviancy did correlate with low self esteem levels. Although this did not prove to be significant in the MANOVA there was 19.98% shared variance with r = -0.44. When EAT and SERS were correlated for the control group there was only a small negative correlation between the two with only 8.1% shared variance with r = -0.28. Correlations do not indicate cause although it does show that low self esteem correlates more strongly with disordered eating for the experimental group. The fact that there was no main effect for BSQ on the MANOVA suggests that the disordered eating patterns are not caused by body image. This might be because being at university does not increase body dissatisfaction and therefore cause more deviant eating behaviours. Although there was a stronger correlation between EAT and BSQ for the experimental than control group, body image alone cannot be considered a significant difference in disordered eating between the experimental and control group. Although low self esteem and integrating a new peer group have not been shown to be significant factors in causing disordered eating behaviour for first year university students, it has been shown that they do have significantly more deviant eating behaviour than the control group. This may still be indirectly a result of being in a new peer group, as shown by the findings of Langlois and Stephan (1977) (as cited in Lieberman et al, 2001) who found that physical attractiveness is linked to more positive peer relations and social acceptance. Attitudes towards attractiveness and the benefits attractiveness can bring are established from an early age. This tends to be well known and seems to have internalised as a concept in young girls (Markham, Thompson and Bowling, 2005). This was also supported by Leiberman et al (2001) who found that more popular girls are more likely to have eating disorders. This suggests that girls who want to be popular, or want to maintain popularity tend to resort to severe eating behaviour and are under the assumption that slimness and attractiveness equals popularity. If this is the case then integrating a new peer group may be a significant factor on eating behaviour in first year students. This study investigated the affect new peer group had over eating by measuring insecurity felt amongst new friends. It may not be that individuals feel insecure with their friends, but that they feel competitive amongst their new friends and engage in dieting tactics in order to establish themselves as the most attractive, thereby increasing their chances of being popular. This could not be established from this study which concentrated on the effect new friends had on self esteem levels. Lieberman et al (2001) found that peer group influence was the single biggest influence over disordered eating. Another explanation as to why first year students may have significantly more deviant eating behaviour may reside in the element of perceived control. Individuals with the propensity to suffer from disordered eating tend to have a deficiency in their ability to influence their own environment (Gordon, 1990). They feel that they cannot control their surroundings or their lives and so resort to controlling the one thing that they feel they do have control over, their eating behaviour. Being independent and moving away to university may increase the sense of lost control and therefore present a crisis in self worth (Gordon, 1990). This may increase disordered eating as the individual may increase their control over their dieting behaviour as they feel a decrease in control of their circumstances. If this were the case, then it may be that the individuals already have the eating problems and it is just that moving away from university, and the challenge and stress accompanying this, enhances rather than causes the eating deviancy. It may be beneficial to compose a longitudinal study investigating how university affects eating patterns before and during university as opposed to comparison with a control group. It may be that in this study self-esteem levels and integrating the new peer group were not significant due to a number of confounding variables. The SERS questionnaire was established in 1993. As it is now nearly ten years later it may be that there has been an increase in the levels of self esteem of the population in general. This could be why when the experimental group was compared with the control group there was no significant difference, but when the experimental group was compared with the normative values Nugent and Thomas (1993) identified, there is a highly significant difference. It may be that the questionnaire needs to be modified to account for this possibility. It is also possible that social desirability bias may have played a role in self esteem and peer group influence not being significant, as nobody wishes to portray themselves as having low self esteem and participants may not have liked to admit that they did feel inferior to their friends, despite the fact that the questionnaires are anonymous. People who are low might not want to recognise or express this due to reaction formation. The format of the questionnaires may also have proved a problem. To increase reliability and validity three established questionnaires were used to create one. The main problem here is that all three used a slightly different scale. This may have caused confusion for participants, who might have used the wrong scales whilst answering questions. Experimental care was however taken by explaining in prose form at the beginning of each questionnaire that a different scale was being used for each section. The scales were also repeated every time questions continued onto a new page so participants would not have to rely on memory to use the scales. Human error may however have meant some participants may have used the wrong scale for the questionnaires. Other issues may have influenced results such as whether participants had a boyfriend at university. This may have increased self esteem levels and reduced the effect a new peer group may have had over individuals who had boyfriends. Also the distance from home participants were may have had an effect. Participants living closer to the university may not have had such low self esteem as they are still relatively near to their parents. In further research it may be beneficial to account for these individual factors. Due to the fact that integrating a new peer group and low self esteem have not been shown to be significant causes of the difference in eating deviancy between the control group and the experimental group, there appears to be plenty of scope for further research. It may be beneficial to stay with a new peer group and self esteem levels but use different methodology to ascertain what influence these have over disordered eating practices. It is clear that although these were not shown to be significant they do have an effect and it may also be beneficial to look at the effect of a new peer group in a different context. Instead of viewing it from the stand point of a feeling of inferiority to peers as this study does, it may be possible to study it from a competition of popularity standpoint, led by the research of Langlois at al (1977) and Leiberman et al (2001). Depression and family factors may also play a role in increasing disordered eating behaviour in students and more research may be needed in this area. It may be that a new questionnaire needs to be established particularly looking at these issues to ensure validity and to ensure it is up to date and relevant to this generation. Further research may also look at the prevalence and causes of disordered eating for male students. It is however important to stay within the region of students as clearly there is an effect of going to university on students eating behaviour. This can also be supported by Kjelsan et al (2004) who found 6% of undergraduate women think they have an eating disorder, which is far higher than the national predicted prevalence rate of 3.4%.",False
72,"Popularity is an extremely difficult concept to measure as popularity is variable across cultures and ages. Characteristics defining a child as popular at the age of 8 may not be the same characteristics which define a child as popular at 14. Another problem arises distinguishing friendship from popularity. A child may be perceived as unpopular but still have a few close friends, and it is important to consider this when looking at psychological development because popularity is multi-faceted as opposed to one dimensional. It has been shown that unpopular children are more likely to have psychological difficulties such as schizophrenia, anxiety disorders, mood disorders and criminal behaviour (Kupersmidt, Coie and Dodge, 1990). Because of this, an important issue when looking at the effect of popularity on psychological development is cause and effect. This issue is covered extensively in the essay as it is valuable, yet difficult, to try and determine whether being unpopular causes problems with psychological development, or whether underlying psychological difficulties cause the child to behave inappropriately and become unpopular. Popularity is an extremely important issue affecting all children. It is more important now than ever as parents tend to return earlier to work so children tend to spend more time with peers in nurseries and after school wrap-around classes. Characteristics of popular and unpopular children have been widely investigated, and there is some variation in the results found. Most commonly however it has been found in primary school aged children, between the ages of 4 and 11, the children regarded as popular tend to be co-operative, helpful, have leadership skills and actively engage in prosocial behaviour (Dekovic and Gerris, 1994). Unpopular children however tend to be aggressive, possessive, verbally aversive and display hostility (Dekovic and Gerris, 1994). To study the effect popularity has on psychological development however it is important to break the notion of ""unpopularity"" down into further components. Asher (1990) defines two types of unpopular children. The first are ""rejected"" and these are children who are actively disliked by their fellow peers (Asher, 1990). The second type are ""neglected"" and these are not actively disliked but tend to be timid and shy and are almost forgotten by peers and also appear to have a lack of friends (Asher, 1990). It is important to make this distinction as psychological development is influenced by whether you are considered ""rejected"" or ""neglected"", it appears that neglected children know how to behave socially but have problems putting this into practice, rejected children have difficulties with both (Asher, 1990). It has been found by Coie and Dodge (1983) (as cited in Asher, 1990) that rejection is a stable process and once you have been rejected you are likely to stay rejected for at least the next five years. This is why it is important to look at the implications for this on the child's psychological development as it is likely the child will remain in their rejected situation until at least adulthood. An important aspect to consider with respect to this is labelling theory and the self fulfilling prophecy. When cause and effect are taken into account a child may remain unpopular because they believe themselves to be unpopular and therefore act in a way appropriate to the label they have been given. Also children are very aware of building alliances to secure their own popularity and if one child is labelled undesirable other children may not want to be associated with the labelled child thus securing their own alliances and popularity. This may account for why children remain unpopular but it does not explain why the child may become unpopular or undesirable in the first place. One possible reason causing a child to be unpopular may be due to them having an autistic spectrum disorder (Hobson, 1995). Unfortunately many of the characteristics displayed by unpopular children are also displayed by children suffering from an autistic spectrum disorder such as Autism or Asberger's Syndrome. Children who are unpopular tend to only view situations from their own perspective, do not value others opinions and cannot empathise with their peers (Dekovic et al, 1994). Autism is a disorder where it is believed children lack a ""Theory of Mind"". This means that they find it very hard to see the world from any perspective but their own and fail to understand others emotions or social cues (Hobson, 1995). This may lead to the child becoming unpopular as perspective seeing is essential to being popular as are the communication and cognitive skills that someone with autism may lack. This is one reason a child may become unpopular as autism affects 5 in every 10,000 children (Hobson, 1995). If autism is not diagnosed until the later years the child will be in main stream education and more than likely be rejected by peers and possibly suffer the psychological problems associated with rejection which will be discussed later. Another cause for a child being unpopular is associated with the family (Henggeler, Edwards, Cohen and Summerville, 1991). The majority of children spend their first few years learning from their parents so it is not surprising that these early years contribute to the child's personality and whether they become popular or unpopular. Ainsworth and Wittig (1969) (as cited in Henggeler et al, 1991) investigated types of attachment between mother and infant. They identified three types of attachment as these have been investigated with regards to child's future popularity. Vandell and Wilson (1987) (as cited in Henggeler et al 1991) found that children who are securely attached become more popular than those who are not securely attached and have more positive peer interactions. This is likely to be because these children have a secure base from which to explore from and feel confident in themselves to make social advances, knowing that they have support. This inner feeling of security may facilitate confidence and an interpersonal family may produce interactional skills. To support this MacDonald (1987) (as cited in Henggeler, 1991) found that preschool boys who had been assessed as being ""rejected"" had parents who were less interpersonally responsive and were less emotionally positive. These children were more likely to have been considered to have had an attachment type of anxious avoidant or anxious resistant. If child had this attachment type then they will not have had a secure base from which to explore from and particularly anxious resistant children would have been anxious at leaving the caregiver and facilitating early social relationships. This may be a cause in why some children become popular and why some children become unpopular. It is interesting to note however that the majority of research completed on attachment type and popularity occurred in 1987. This is just one year after Maine (as cited in Maine and Cassidy, 1988) introduced the new attachment type insecure disorganised. This means that attachment will have been in the media and well heard of, which may have been an incentive to link attachment with popularity and more may have been made of it than is actually there. Attachment suggests that it is causal in determining popularity although this seems a little reductionist and little has been done to investigate whether securely attached children are still more popular in the older years when there is less reliance on the parents. How this effects children in care would also need to be fully investigated before attachment could be seen as a deciding figure in a child's popularity. Also attachment claims to be a causal factor but it may be the child's temperament that causes attachment and popularity therefore attachment is not the cause, but the child's personality. It may not be as much attachment but more to do with Bandura's social learning theory (1969) (as cited in Grusec 1992) as children who's parents are emotionally responsive and communicative to them learn to be emotionally responsive and communicative to others and therefore more popular. Piaget (as cited in Rubin, LeMare and Lollis, 1990) offers another environmental explanation as to why some children may be more popular than others. He suggests that children who become popular have successfully negotiated mutually engendered behaviours. Because young children are egocentric to learn how to interact and become socially efficient they need to lose the belief that the world centres around them and develop a theory of mind (as cited in Rubin, 1990). Early play and reciprocal friendship allow perspective taking skills and prosocial behaviours to develop, which are important skills to be popular and socially accepted (as cited in Rubin, 1990). This has implications for children who do not have the opportunity to socialise at an early age as may not completely abolish the notion of egocentrism. It also means important cognitive and social skills are not being developed. This is supported by the fact that Dekovic and Gerris (1994) found that rejected children demonstrated more egocentric, selfish behaviours This offers a suggestion as to why children may not exhibit prosocial behaviour but it still does not explicitly offer a cause. Is it opportunity that prevents early socialisation, and if so should this just not occur at preschool, or is it personality which prevents early socialisation or is it attachment type and not feeling secure enough to socialise. It is clear that there are so many different causes which may lead a child to become unpopular. There is also a large number of children who can be considered rejected or neglected and therefore it is important to investigate any implications of this on their psychological development. It is at this stage that it is important to separate the notion ""unpopular"" into rejected and neglected. Rejected children tend to be more actively disliked than neglected and engage in more aggressive and hostile behaviours (Asher, 1990). It is this type of child who is more likely to turn to criminality and anti social behaviours (Kupersmidt et al 1990). This is a dangerous assertation to make however regarding the self fulfilling prophecy and also almost makes it acceptable for rejected children to turn to criminal behaviour. Roff (1972) (as cited in Kupersmidt et al, 1990) found that peer rejection was a significant predictor of delinquency and this once again may be a result of the self fulfilling prophecy in that the child believes they are bad and therefore will only survive by being bad. It may also be an attempt for admiration as anti social behaviour becomes more associated with popularity once the child moves towards adolescence, or it could just be a scream for attention. Neglected children however tend to be less actively disliked by their peers but are more withdrawn and isolated from society (Asher, 1990). This is obviously important for psychological development as there are three disorders noted in DSM IV (American Psychiatric Association, 1995) regarding withdrawal such as ""Shyness Disorder"", ""Adjustment Disorder"" and ""Introverted Disorder"", therefore being withdrawn obviously has psychological disadvantages. Neglected children are also more likely to suffer from internal disorders such as those involving mood and anxiety (Asher, 1990) as opposed to rejected who are more likely to suffer external ones (Asher, 1990). Being neglected however does not necessarily lead to psychological problems however as Coie and Kupersmidt (1983) (as cited in Rubin, 1990) found that neglected children are more confident in smaller groups. There is only a problem however when insecurity and anxiety issues become apparent. Obviously children who are considered unpopular whether rejected or neglected are perceptive enough to pick up on and internalise negative feelings about themselves which has implications for psychological development in itself and can lead to many further complications, particularly that of depression. There has been links made between children who were unpopular at school and a later development of schizophrenia. A feedback study conducted by Bleuler (1950) (as cited in Kupersmidt et al 1990) describes the preschizophrenic child as anxious, excitable, irritable and aggressive. This seems to be descriptive of a rejected rather than a neglected child. There however problems with cause and effect with this as it is not clear whether rejection in itself causes schizophrenia or the schizophrenia causes the antisocial behaviour which leads the child to become unpopular. From the evidence and knowledge about schizophrenia I believe that being unpopular itself doesn't cause schizophrenia. This is because schizophrenia does not tend to develop until late adolescence. Also Watt (1978) showed that for the preschizophrenic child behaviour in childhood is normal and the child does not become unpopular until adolescence with the onset of schizophrenia. This also concurs with evidence that rejection is stable. If a child is unpopular at adolescence then they will have been unpopular during childhood, the pre-schizophrenic child does not show this pattern so it is more likely that schizophrenia causes the unpopularity and not vice versa. In conclusion there could be many factors effecting whether a child turns out to be popular or unpopular. The majority however seem to ignore the child's actual personality which probably plays a large role. There is little doubt however that factors such as attachment type and early experiences with reciprocal friendships are valuable in learning the necessary skills to socialise and which therefore give you an increased chance of being popular and having friends. It has been shown that lack of friends or being unpopular may lead to problems with psychological development and these seem to be more extreme with rejected rather than neglected children. There is plenty still to discover however regarding whether being unpopular causes psychological problems, or the psychological problems are already there and cause unpopularity. It would appear from the conclusions made in this essay that it is a likely combination of the two.","Human colour vision can most often be described as trichromatic. Having trichromatic colour vision entails having three types of wavelength specific cones in the retina, each of which are maximally responsive to short, medium or long wavelengths of light in the colour spectrum. Short, medium and long wavelength cones maximally respond to blue, green and red light respectively (Goldstein, 2002). Trichromacy is not a uniform feature of human physiology however. Because colour vision is inherited genetically, with medium and long wavelength specific genes being located on the x chromosome, mutations can occur. These mutations can cause a type of colour blindness called anomalous trichromacy, in which the red photopsin becomes slightly shifted (Sanocki, Shevell, Winderickx, 1993). If a woman inherits the three cones for normal trichromacy on one x chromosome, and the three cones for anomalous trichromacy on the other x chromosomes, she will possess two types of red photopsin and therefore have the possibility of four wavelength specific cones. If having four different wavelength specific cones defines tetrachromacy it is highly likely that there are tetrachromats (Jameson, Highnote and Wasserman, 2001). If, however, tetrachromacy is defined by the visual system acting upon this extra cone and creating the neural mechanisms to allow visual experience to be derived from it, then evidence for tetrachromacy is not so forthcoming (Jordan and Mollen, 1993). It is likely that even if the visual system can derive information from this extra cone containing the shifted red photopsin, this is not going to result in a considerably different visual experience to that of trichromats because the shifted red photopsin is only six nanometres (nm) away from the original red photopsin (Winderickx, Lindsey, Sanocki, Teller, Motulsky, Deeb,1992). The theory of trichromacy is often referred to as the Young-Helmholtz hypothesis. This assumes that colour vision is subsumed by three photosensitive pigments in the retina (Ingler, 1968), where short wavelength specific cones have a peak absorption maxima of 419nm, medium wavelength specific cones of 531nm and long wavelength specific cones of 558nm (Bruce, Green and Georgeson, 2004). Colour vision is inherited genetically with short wavelength specific genes being transmitted on chromosome number seven, and medium and long wavelength specific genes being transmitted on the x chromosome (Bowmaker, 1998). Medium and long wavelength specific genes are highly homologous and it is polymorphism of these which cause variation in colour vision. Winderickx et al (1992) found that a common polymorphism occurs at point 180 on the red opsin, and that this results in the general population having two different types of red opsin, one being alanine, and one being serine. Serine has a peak absorption maxima of 557nm and alanine has a peak absorption maxima of 552nm (Winderickx et al, 1992). This means that if alanine is substituted for serine at point 180 on the red opsin of the long wavelength specific cone, it's peak sensitivity will fall below that expected for a long wavelength sensitive photopsin and the carrier will have a reduced sensitivity to red light (Bowmaker, 1998). The implication of Winderickx et al's (1992) evidence is that if a female inherits the genetic code for alanine on one x chromosome, and the genetic code for serine on the other x chromosome, she is heterozygous and has the propensity to have four different wavelength specific cone cells. The female would have the short, medium and long wavelength sensitive cones required for trichromatic colour vision, and also an extra cone which has been shifted between the medium and long wavelength sensitive cones. Males only have one x chromosome and therefore will only ever inherit alanine or serine, they will not be able to have both types of the red photopsin and therefore cannot become tetrachromatic. Typically however, even if the female inherits both types the shifted red opsin (alanine in the case of long wavelength sensitive cones), will not be expressed, so even if the female is heterozygous she will not be tetrachromatic. There is however a physiological mechanism called x inactivation which acts upon the x chromosome and results in both types of red opsin being expressed. It is this that gives females the propensity to be tetrachromatic. X inactivation is the process by which one or another x chromosome becomes inactivated (Jordan and Mollen, 1993). If x inactivation occurs in heterozygous females, the allelic properties will be separated so some cells will express alanine and some will express serine and the female will have in total, four wavelength specific cones (Jordan and Mollen, 1993). To support this it was predicted by Lyon (1961) (as cited in Cohn, Emmerich and Carlson, 1989) that if a woman had the propensity to express both types of red opsin, some cells should have normal colour vision, and some cells should have below level colour vision as a result of expressing the shifted gene. Cohn et al, (1989) found that when they asked heterozygous carriers with x inactivation to identify the colour of a green light shone on certain patches of the retina, on certain patches they often made mistakes and saw yellow or white light. Significantly more mistakes were made by heterozygous females. This suggests that there is a mosaic pattern of cells on the retina (Cohn et al, 1989), some of which express the three cones for trichromacy. The other cells express two normal trichromatic cones for green and blue and the abnormal slightly shifted red cone, which results in anomalous trichromacy in these cells (Jameson et al, 2001). Cohn et al did find however that the heterozygous carrier can compensate for this as when they were allowed to scan the stimuli, and use the whole of their retina (the fully functional as well as the shifted cones), they had no colour deficiency. The importance of Cohn et al's results is that it demonstrates the expression of two different types of cell, those containing one type of photopigment and those containing a slightly shifted variant. Although the results were gained in contrived laboratory conditions, these results are extremely valuable in demonstrating the process of x inactivation and how it can lead to four different types of wavelength specific cones. Although Cohn et al (1989) found that this can limit the female's colour vision in some respects, it has been shown that the woman can compensate for this by scanning. This evidence can be linked with Winderickx et al's (1992) finding that as well as having short, medium and long wavelength specific cones responding to an absorption maxima of 419nm, 531nm and 558 nm respectively (Bruce et al, 2004), heterozygous females can also demonstrate a fourth wavelength specific cone with an absorption maxima of 552nm (Winderickx et al, 1992). Winderickx et al (1992) and Cohn et al (1989) both show that it is possible for a woman to be a tetrachromat. An important distinction has to be made however between being a tetrachromat and having tetrachromatic vision. The requirement for tetrachromatic vision is not as simple as having four different wavelength specific cones. The brain has to be plastic enough to adapt to the extra cones and create post receptoral neural channels to process the information and interpret it(Jordan and Mollen, 1993). Simply having four wavelength specific cones available is known as weak tetrachromacy, but being able to use them is strong tetrachromacy (Jordan and Mollen, 1993). This however is very difficult to test whilst the person is alive, and behavioural tests do not appear to be sensitive enough to accurately discriminate between tetrachromatic vision and trichromatic vision (Jameson et al, 2001).A problem regarding the sensitivity of behavioural tests is that the shifted fourth cones are possibly too close to original trichromatic cones to be able to accurately distinguish between them. There is only a maximum of a six nanometre difference between alanine an serine, and whereas this may be enough to produce a more detailed, complex vision of the world in females who are tetrachromatic, it may be that it is not enough to always be detected in contrived, laboratory, behavioural tests. There is some evidence however that heterozygous females may not only be tetrachromats, but have tetrachromatic vision (Jameson et al, 2001). The fact that Cohn et al (1989) found that there are normal and defective cells operating in the retina implies that the fourth, shifted opsin is operating and that the brain has adapted to receive the input because if it had not, Cohn et al (1989) would not have found defective patches on the retina. This strongly suggests that as well as having four different cones responding to different wavelengths of light, they are also all being utilised. The notion that humans may possess the ability, not only to be tetrachromats, but to use their tetrachromatic vision can initially be investigated by looking at new world monkeys. Mollen, Bowmaker and Jacobs (1984) found that in a species of typically dichromatic squirrel monkeys, there were the occasional females who showed trichromatic behaviour after training on Rayleigh colour matches. They also found using microspectrophotometer techniques, that squirrel monkeys who displayed trichromatic behaviour could draw two pigments corresponding to medium and long wavelengths as well as pigments corresponding to short wavelengths, illustrating trichromacy physiologically (Mollen et al, 1984). The relevance of this shows how females in a predominantly dichromatic species can develop the ability to enhance their colour vision by developing, and critically using, an extra photopsin. It has also been found that the coding sequences of red and green photopigment genes of certain species of monkey are highly homologous to human coding sequences of red and green photopigment genes (Deeb, Jorgensen, Battisti, Iwanski and Motulsky, 1994). This suggests that if monkeys, who have coding sequences which are highly homologous to human coding sequences, can develop and use extra portions of the visual system. It may be possible to generalise this to humans and anticipate a similar ability. We know humans can develop a fourth type of photopsin (Windericks et al, 1992), and this evidence suggests that if monkeys can do the same (albeit with a third photopsin) and develop the capacity to use their extra photopsin, humans may be able to. Evidence of human ability to create neural mechanisms in order to use the information from the fourth photopsin is not as conclusive as evidence from new world monkeys as it is not possible to use microspectrophotometer techniques. Behavioural evidence has illustrated some possible cases of full tetrachromacy however. Jordan and Mollen (1993) asked heterozygous carries of anomalous trichromacy and trichromatic controls to adjust a mixture of green and orange light, and yellow and red light, to obtain a match. They found no significant difference in behaviour between the two groups, suggesting either that this is not a sensitive enough measure of tetrachromacy, or that people may carry four wavelength sensitive cones, but cannot actually use them due to the structure of the brain. Jordan and Mollen (1993) did find one person however, who they label as cDa7, who found only one combination of the two mixtures that would give a match. The other trichromatic controls and the heterozygous carriers had a greater variability in the repeated responses they gave on the ratio matching tasks, whereas cDa7 repeatedly gave the same response. This implies that she had greater ability to distinguish between the colours to gain a precise match and therefore her visual system was using tetrachromatic vision as opposed to the extra, shifted cone not being utilised. It may be that all the heterozygous carriers used in the study did have tetrachromatic vision, it was just that cDa7 had shifted opsins far enough away from the original cones required from trichromacy, to be detected by such behavioural techniques. Overall however, it suggests that even if all the heterozygous carriers do have tetrachromatic vision which has not been picked up by the tests, their vision cannot be considerably different to that of a trichromatic's vision. To further research this area, Jameson et al, (2001) tried to investigate tetrachromacy using techniques that they considered to be more reliable, and found a far higher incidence rate of tetrachromacy. Jameson et al, (2001) propose that ratio matching is not a reliable technique to measure tetrachromacy because it does not take into consideration real world viewing situations, such as surfaces, textures and surround contexts variations. It was suggested that many colour vision mechanisms only become active when there is sufficient variation and context in the light, which cannot be captured by ratio matches. Instead Jameson et al, (2001) asked participants to identify spectral delineations for a diffracted spectral stimulus. Although this is not entirely reflective of real world viewing conditions, it does offer more variability to the retina than simple ratio matching techniques. It was found from this that heterozygous carriers of the two types of red opsin, delineated more bands of colour in the diffracted spectrum than a control group of non heterozygous trichromatic females. This removes any differences that may be attributed to gender (Jameson et al, 2001) and also implies that the two groups of females are experiencing different perceptions of colour. This also suggests that the heterozygous carriers are experiencing strong tetrachromacy and that their visual system is utilising the fourth photopsin. This evidence demonstrates a stronger tendency for heterozygous females to exhibit tetrachromatic colour vision than Jordan and Mollen (1993) found, and it is likely that this is a more sensitive measure of colour perception. It also suggests that the viewing experiences of women who have the propensity to be tetrachromatic, in this case, is more advanced than women who are not heterozygous than expected by Jordan and Mollen (1993). Further evidence that the human visual system should be able to accommodate a fourth wavelength specific cone comes indirectly from Neitz, Carroll, Youmauchi, Neitz and Williams, (2002). They demonstrated that the visual system is plastic and can adapt to environmental stimuli after it is fully developed. Neitz et al, (2002) asked people to wear coloured filters to alter their chromatic experience. They found that as a result of this there was a shift in colour perception which lasted for nearly two weeks after the filters had been removed. This shows the nervous system's ability to use experience gained from the environment and adjust the inputs to the red and green channel to accommodate this. The implication of this is that if the visual system can adapt and change inputs to the red and green system to external stimuli, it should also be able to do the same to internal stimuli, such as receiving input from an extra photopsin. The majority of the evidence discussed suggests that there are tetrachromats. It has been shown by Winderickx et al, (1992) and Cohn et al, (1989) that physiologically a woman can posses four different types of wavelength specific cones. Whether women in this position have the neural mechanisms to utilise an extra cone is more controversial. It has been shown behaviourally that women with four cones can be distinguished from women who have three, particularly by Jameson et al, (2001), and as this can be demonstrated behaviourally it implies that the women are having a different perceptual experience of colour than women who have three cones. It would also make sense for the visual system to adapt to an extra photopsin because the nervous system can change and update itself, and this has been demonstrated by Neitz et al, (2002). The fact that the extra photopsin is only six nanometres (Winderickx et al, 1992) away from the long wavelength specific cone suggests that if there are tetrachromats, as there appear to be, their colour vision is not likely to be much superior to a trichromats. As it has been shown by Jordan and Mollen, on simple behavioural tasks such as ratio matches there does not appear to be much difference at all. It is likely that cDa7 had a larger shift than the expected six nanometres to show tetrachromatic behaviour on these assessments. It appears conclusive that women can have four different wavelength specific cones, although the extent to which the extra is utilised and an advantage to colour vision still requires further investigation. There have been relatively consistent findings regarding the effects of group decision making compared with individual decision making. Groups tend to polarize towards the given majority and individuals often become more extreme in their preferences after the group discussion compared to before (Myers and Kaplan, 1976). The mediating mechanism operating between the pre and the post decision is the deliberation process. Deliberation has a large impact on jury decision making, which is reliant upon individual jurors discussing the evidence as a group, and returning with an often unanimous verdict. The majority of research on the jury has focused upon how individual jurors reach a verdict before the discussion, such as studies supporting the Story Board Model of juror decision making (Pennington and Hastie, 1992). Although this is a theory investigating how the individual decides upon a verdict, it does have implications for the deliberation process. There have also been attempts to produce Social Decision Schemes to assess the probability of deliberation resulting in a guilty, not guilty or hung verdict, depending upon pre deliberation verdicts (Davis, Kerr, Stasser, Meek and Holt, 1977). Although both offer reasonable attempts to explain or predict jury decision making, neither provide a complete theory of the deliberation process. The Story Board Model of jury decision making proposes that in order for jurors to understand the vast and complex body of evidence presented at trials, they need to interpret the evidence in a way that makes sense to them. (Pennington and Hastie, 1992). In order to do this the juror imposes a narrative structure on the evidence to create a story. This story then acts as causal model to allow the jurors to interpret the evidence in a context from which they can draw their own experiences and evaluations, whilst allowing them to perceive the evidence as a coherent mental representation (Pennington and Hastie, 1992). Stories are created from schemas which are composed from experiences and preconceptions held by the individual and built up over a life time, it is through these that the facts of the case can take on their different meanings (Holstein, 1985). People can create completely different stories for the same evidence which is where the Story Board Model has implications for the deliberation process, as these many individual stories have to be integrated into one, often unanimous, verdict. Unfortunately the majority of research on the Story Board Model does not focus upon how this final verdict is created and concentrates upon how the initial individual pre deliberation verdict is reached. This research will be considered however as inferences can be drawn as to how this might impact upon the deliberation process. Pennington and Hastie (1992) conducted a study in which they hypothesised that presenting the evidence in a fashion that conformed to one particular story, would cause jurors to adopt this story, and depending on the nature of the story, result in a guilty or not guilty verdict. In order to achieve this effect, instead of presenting evidence to the jury in witness order (as would happen in a real trial), Pennington and Hastie (1992) organized the evidence in a way that would make one story easier to construct than another, without altering the nature of the evidence. They found that verdicts returned were consistent with the order in which the evidence was presented, thus implying that stories are a mediating mechanism in jury decision making. There is however a large problem with this evidence regarding it's generalisability to real jury decision making processes. In real trials the evidence is not presented in ""story order"" but is presented in ""witness order"", which produces a complex disorganization of the evidence. All Pennington and Hastie (1992) have shown is that in certain situations, where the evidence is neatly ordered into a coherent story, this is likely to influence the jurors' interpretation of the evidence, and therefore the verdict. This does not contemplate the complex reorganisation the brain has to implement on the evidence and how in this situation stories are created. The whole point of the story board model is to demonstrate how jurors take complex and disordered evidence and make sense of it by imposing order and meaning onto it through the narrative of the story. This is not illustrated in research such as this which already imposes the order. To further investigate the utility of the story board model Huntley and Costanzo (2003) conducted research on how jurors compose stories without manipulating the evidence as Pennington and Hastie (1992) did. Huntley and Costanzo (2003) found that individuals were predisposed to either prosecution or defence stories for a sexual harassment case. This result is presumably as a result of their different schemas. Huntley and Costanzo (2003) do not make this link explicit, although they did find a significant main effect for gender in the way that females were significantly more predisposed to a guilty verdict than males. This can be interpreted in a way that females may have more experience with being sexually harassed than males, and as a result, have schemas which reflect this. This may have caused the creation of prosecution stories which resulted in the females propagating a guilty verdict. Huntley and Costanzo (2003) also present this research as evidence for the story model because whilst evaluating the evidence within the causal model of the story, jurors relied upon extralegal, contextual information about the plaintiff and defendant when there was little hard evidence. This was to create the coverage, coherence and uniqueness which are proposed by Pennington and Hastie (1992) as necessary conditions for a successful and convincing story. Jurors selectively incorporated this biasing information into their stories providing it served as an enhancement to their story. Any biasing information that went against their story was dismissed. This evidence, when taken at face value, appears to support the story board model of jury decision making because it illustrates how jurors favour prosecution or defence stories depending on the interpretive schemes operating within each individual. It is important however, to take care when regarding this research as direct evidence that jurors compose either a prosecution or defence story. Firstly, it is likely that jurors compose more than one story of the evidence, and are not simply ""plaintiff or defense jurors"" (Huntley and Costanzo, 2003, pg35). It is more likely that jurors are more open to the interpretation of the evidence and, as suggested by Morley (1996), develop both prosecution and defense stories and select the story which, consistent with their own criteria, is most satisfactory for coverage, coherence and uniqueness (Pennington and Hastie, 1992). Also the methodology of Huntley and Costanzo's (2003) research is not consistent with what actually occurs in a trail, so the generalisability of the research to suggest that real jurors create stories, may be limited. Huntley and Costanzo (2003) asked jurors systematic questions of their interpretations of the evidence after each attorney had presented their case. In doing this Huntley and Costanzo are surely promoting a Bayesian method of processing where evaluation of the evidence is updated with each new piece of evidence (Hastie, 1993). By asking the jurors to evaluate the evidence in this Bayesian fashion, they are going against the fact that the story model advocates that jurors do not evaluate the evidence in such a systematic fashion, but as a global memory representation (Pennington and Hastie, 1992). Huntley and Costanzo's (2003) finding that different people can create different stories for the same evidence is widely supported however (Stephenson, 1995), and this is where the research is relevant in creating the implications for the deliberation process. Holstein (1985) investigated the effect of story generation on juries and the deliberation process and found that the higher the number of individual stories that were propagated into the deliberation process, the less likely the jury was to return a unanimous verdict. Holstein (1985) demonstrated that as deliberation increased the additional interpretations of the evidence, the task became more complex. This implies that if people do not initially have stories based upon similar assumptions, then the deliberation process is not going to largely alter people's stories, and it is this that makes the deliberation process more complex and a unanimous verdict less likely (Holstein, 1985). Holstein (1985) also found that even when people have two different stories favouring the same verdict, if the underlying assumptions are different, this still reduces the ability of the jury to reach a unanimous verdict and increases the complexity of the deliberation. The hypothesis the jurors compose stories to account for and interpret evidence appears to be a likely resource jurors use for interpreting evidence. Problems arise however in that the evidence used to support the notion that jurors reach verdicts in this way is not always reflective of how real trials work, so inferences made can not be generalised. Problems also arise as to whether the story model is causal. Pennington and Hastie (1992) propose that the story the juror generates determines the verdict. It is not made clear however whether jurors contemplate all the evidence and then decide upon the most appropriate story (Pennington and Hastie, 1992), or whether jurors make a very early decision as to guilt or innocence, and interpret all following evidence in the way of this initially decided verdict, as suggested by Huntley and Costanzo (2003). Less subjective approaches to jury deliberations are social decision schemes. Davis (1973) (as cited in Parks and Kerr, 1999) used social decision schemes which encompassed applying probability matrixes to initial distribution of jurors preferences in order to gain a probability of achieving a particular verdict (Parks and Kerr, 1999). The basic aim of a social decision scheme is to show how initial preference distributions relate to eventual group output, so unlike the story board model, the social decision scheme is directly concerned with the deliberation process and how this creates a single unanimous verdict. Research on group decision making outside the realm of juries has shown that whilst deliberating in a group, people tend to polarize towards the given majority (Myers and Kaplan, 1976). If this assumption were to be applied to jury deliberation processes, a social decision scheme would emerge that would predict that the final verdict would be the verdict of which the majority support. This however is not an accurate assessment of the jury deliberation process however and has been shown not to be a successful predictive matrix (Davis et al, 1977). The main reason for this is that juries have to consider the evidence with a criteria for conviction ""beyond all reasonable doubt"". It is proposed by MacCoun and Kerr (1988) that this produces a leniency bias in jury deliberations which must be accounted for by any accurate social decision scheme. This shows that jury deliberation is far more complex than simple polarization as it was even found by Tanford and Penrod (1986) (as cited in MacCoun and Kerr, 1988), that even when there is an initial majority favouring of conviction, a not guilty verdict can still prevail. Davis et al, (1977) generated fifteen social decision scheme matrices to investigate which provided the most accurate fit to their data. To gain the data Davis et al asked participants to watch a videoed case of a rape trial. Participants were then randomly spilt into six member juries and asked to find a verdict. As a general finding, Davis et al (1977) found that the verdict initially favoured by the majority, was predominantly the jury's final verdict. What is important however is that when there was a three by three spilt in the initial verdict preference, there was a strong tendency for acquittal and a near zero tendency to convict. This illustrates that in most cases a majority rule social decision scheme can be implemented, but it needs to have a sub scheme allowing for this leniency bias (MacCoun and Kerr, 1988) This was termed matrix D15, ""defendant protection, hung otherwise"" (Davis et al, 1977). To support that a social decision scheme of jury deliberations requires a sub scheme allowing for a leniency bias, or an asymmetry effect, MacCoun and Kerr (1988) produced a meta analysis of twelve previous studies of jury deliberations in which there was an equal split of jurors on each jury, initially proposing a guilty or not guilty verdict. They also found that the leniency bias was active when two thirds of the jury initially believe that the verdict should be guilty (MacCoun and Kerr, 1988). In another study by Kerr and MacCoun (1985) they demonstrated that this social decision scheme was also strong enough to consider the size of the jury and that it still produced consistent results whether there were six person or twelve person juries. This mathematical approach to jury deliberation does appear to be an effective way of predicting the effect deliberation will have on the initial set of references. A problem however is that the assumptions of the leniency bias could be explained by the fact that most of the data was collected using students, and it has been shown by Simon and Mahan (1971) (as cited in MacCoun and Kerr, 1988). In an attempt to remedy this as an explanation for the leniency bias, MacCoun and Kerr (1988) conducted a further experiment where student decision making processes were compared to those of the general public. They did not find student juries to be more lenient. In this comparison however, MacCoun and Kerr (1988) only compared student and general public juries consisting of four members. It may have been that four members was not enough to really allow the deliberation process to generate results showing that students were more lenient. Another problem is that the social decision scheme does not take into account situational factors which may effect which scheme is most appropriate. As an example of this, Davis (1975) (as cited in Davis et al, 1977) found that on a different set of data, matrix D15 was not the most effective predictor of the data, matrix D7, the ""simple majority hung"" matrix provided a better fit. This can also be shown by the fact that Schonemann (1979) found that a ridiculous matrix (where the probability of a guilty verdict is inversely related to the number of jurors favouring it), fitted Davis et al's (1977) data perfectly. Schonemann's (1979) point in illustrating this was to show that a social decision scheme could fit the data perfectly without being plausible. Social decision schemes also do not explain how group processes of deliberation are undertaken. It does suggest that the majority will rule unless the majority only slightly favour guilt, or there is an equal split in verdict choice, but it does not explain under which circumstances this is likely to happen. It appears that although social decision schemes are accurate in predicting verdicts after the data has been collected, it is far more difficult to select an appropriate social decision scheme in advance because the social decision scheme does not specify which situation factors will effect it and why. It has also been shown by Sandy and Dillehay (1995) (as cited in Devine, Clayton, Dunford, Seying and Price, 2001) that in real split juries, 71% convict, so they are clearly not showing a leniency bias in this instance which would be predicted by the social decision scheme. There are many difficulties in studying the jury deliberation process, predominantly because it is illegal to view the process, or even to ask jurors about their experiences after a case. As a result of this the majority of the evidence is conducted under contrived circumstances under which the juries face no consequences for their decisions, or pressure as they would in a real jury. It may be because of this that research on mock juries has not been able to produce a unified theory of the jury deliberation process. It may however be that there cannot be a single unified theory of jury deliberation as there are so many situational factors which could be acting upon the process. The fact that shadow jurors fail to consistently reach the same verdict illustrates this (MacCoun and Kerr, 1988). It has been suggested that personality traits such as authoritarianism contribute to how juries deliberate, although there has not been much support for these ideas, (Parks and Kerr, 1999). It is important however not to see the story board model and social decision schemes as being mutually exclusive to each other because they are not. Both contribute to our understanding of how juries deliberate although, as discussed, neither provide a comprehensive theory. It is important to keep researching the area however to improve our legal system and to try to reduce such variability in proceedings.",True
73,"Human colour vision can most often be described as trichromatic. Having trichromatic colour vision entails having three types of wavelength specific cones in the retina, each of which are maximally responsive to short, medium or long wavelengths of light in the colour spectrum. Short, medium and long wavelength cones maximally respond to blue, green and red light respectively (Goldstein, 2002). Trichromacy is not a uniform feature of human physiology however. Because colour vision is inherited genetically, with medium and long wavelength specific genes being located on the x chromosome, mutations can occur. These mutations can cause a type of colour blindness called anomalous trichromacy, in which the red photopsin becomes slightly shifted (Sanocki, Shevell, Winderickx, 1993). If a woman inherits the three cones for normal trichromacy on one x chromosome, and the three cones for anomalous trichromacy on the other x chromosomes, she will possess two types of red photopsin and therefore have the possibility of four wavelength specific cones. If having four different wavelength specific cones defines tetrachromacy it is highly likely that there are tetrachromats (Jameson, Highnote and Wasserman, 2001). If, however, tetrachromacy is defined by the visual system acting upon this extra cone and creating the neural mechanisms to allow visual experience to be derived from it, then evidence for tetrachromacy is not so forthcoming (Jordan and Mollen, 1993). It is likely that even if the visual system can derive information from this extra cone containing the shifted red photopsin, this is not going to result in a considerably different visual experience to that of trichromats because the shifted red photopsin is only six nanometres (nm) away from the original red photopsin (Winderickx, Lindsey, Sanocki, Teller, Motulsky, Deeb,1992). The theory of trichromacy is often referred to as the Young-Helmholtz hypothesis. This assumes that colour vision is subsumed by three photosensitive pigments in the retina (Ingler, 1968), where short wavelength specific cones have a peak absorption maxima of 419nm, medium wavelength specific cones of 531nm and long wavelength specific cones of 558nm (Bruce, Green and Georgeson, 2004). Colour vision is inherited genetically with short wavelength specific genes being transmitted on chromosome number seven, and medium and long wavelength specific genes being transmitted on the x chromosome (Bowmaker, 1998). Medium and long wavelength specific genes are highly homologous and it is polymorphism of these which cause variation in colour vision. Winderickx et al (1992) found that a common polymorphism occurs at point 180 on the red opsin, and that this results in the general population having two different types of red opsin, one being alanine, and one being serine. Serine has a peak absorption maxima of 557nm and alanine has a peak absorption maxima of 552nm (Winderickx et al, 1992). This means that if alanine is substituted for serine at point 180 on the red opsin of the long wavelength specific cone, it's peak sensitivity will fall below that expected for a long wavelength sensitive photopsin and the carrier will have a reduced sensitivity to red light (Bowmaker, 1998). The implication of Winderickx et al's (1992) evidence is that if a female inherits the genetic code for alanine on one x chromosome, and the genetic code for serine on the other x chromosome, she is heterozygous and has the propensity to have four different wavelength specific cone cells. The female would have the short, medium and long wavelength sensitive cones required for trichromatic colour vision, and also an extra cone which has been shifted between the medium and long wavelength sensitive cones. Males only have one x chromosome and therefore will only ever inherit alanine or serine, they will not be able to have both types of the red photopsin and therefore cannot become tetrachromatic. Typically however, even if the female inherits both types the shifted red opsin (alanine in the case of long wavelength sensitive cones), will not be expressed, so even if the female is heterozygous she will not be tetrachromatic. There is however a physiological mechanism called x inactivation which acts upon the x chromosome and results in both types of red opsin being expressed. It is this that gives females the propensity to be tetrachromatic. X inactivation is the process by which one or another x chromosome becomes inactivated (Jordan and Mollen, 1993). If x inactivation occurs in heterozygous females, the allelic properties will be separated so some cells will express alanine and some will express serine and the female will have in total, four wavelength specific cones (Jordan and Mollen, 1993). To support this it was predicted by Lyon (1961) (as cited in Cohn, Emmerich and Carlson, 1989) that if a woman had the propensity to express both types of red opsin, some cells should have normal colour vision, and some cells should have below level colour vision as a result of expressing the shifted gene. Cohn et al, (1989) found that when they asked heterozygous carriers with x inactivation to identify the colour of a green light shone on certain patches of the retina, on certain patches they often made mistakes and saw yellow or white light. Significantly more mistakes were made by heterozygous females. This suggests that there is a mosaic pattern of cells on the retina (Cohn et al, 1989), some of which express the three cones for trichromacy. The other cells express two normal trichromatic cones for green and blue and the abnormal slightly shifted red cone, which results in anomalous trichromacy in these cells (Jameson et al, 2001). Cohn et al did find however that the heterozygous carrier can compensate for this as when they were allowed to scan the stimuli, and use the whole of their retina (the fully functional as well as the shifted cones), they had no colour deficiency. The importance of Cohn et al's results is that it demonstrates the expression of two different types of cell, those containing one type of photopigment and those containing a slightly shifted variant. Although the results were gained in contrived laboratory conditions, these results are extremely valuable in demonstrating the process of x inactivation and how it can lead to four different types of wavelength specific cones. Although Cohn et al (1989) found that this can limit the female's colour vision in some respects, it has been shown that the woman can compensate for this by scanning. This evidence can be linked with Winderickx et al's (1992) finding that as well as having short, medium and long wavelength specific cones responding to an absorption maxima of 419nm, 531nm and 558 nm respectively (Bruce et al, 2004), heterozygous females can also demonstrate a fourth wavelength specific cone with an absorption maxima of 552nm (Winderickx et al, 1992). Winderickx et al (1992) and Cohn et al (1989) both show that it is possible for a woman to be a tetrachromat. An important distinction has to be made however between being a tetrachromat and having tetrachromatic vision. The requirement for tetrachromatic vision is not as simple as having four different wavelength specific cones. The brain has to be plastic enough to adapt to the extra cones and create post receptoral neural channels to process the information and interpret it(Jordan and Mollen, 1993). Simply having four wavelength specific cones available is known as weak tetrachromacy, but being able to use them is strong tetrachromacy (Jordan and Mollen, 1993). This however is very difficult to test whilst the person is alive, and behavioural tests do not appear to be sensitive enough to accurately discriminate between tetrachromatic vision and trichromatic vision (Jameson et al, 2001).A problem regarding the sensitivity of behavioural tests is that the shifted fourth cones are possibly too close to original trichromatic cones to be able to accurately distinguish between them. There is only a maximum of a six nanometre difference between alanine an serine, and whereas this may be enough to produce a more detailed, complex vision of the world in females who are tetrachromatic, it may be that it is not enough to always be detected in contrived, laboratory, behavioural tests. There is some evidence however that heterozygous females may not only be tetrachromats, but have tetrachromatic vision (Jameson et al, 2001). The fact that Cohn et al (1989) found that there are normal and defective cells operating in the retina implies that the fourth, shifted opsin is operating and that the brain has adapted to receive the input because if it had not, Cohn et al (1989) would not have found defective patches on the retina. This strongly suggests that as well as having four different cones responding to different wavelengths of light, they are also all being utilised. The notion that humans may possess the ability, not only to be tetrachromats, but to use their tetrachromatic vision can initially be investigated by looking at new world monkeys. Mollen, Bowmaker and Jacobs (1984) found that in a species of typically dichromatic squirrel monkeys, there were the occasional females who showed trichromatic behaviour after training on Rayleigh colour matches. They also found using microspectrophotometer techniques, that squirrel monkeys who displayed trichromatic behaviour could draw two pigments corresponding to medium and long wavelengths as well as pigments corresponding to short wavelengths, illustrating trichromacy physiologically (Mollen et al, 1984). The relevance of this shows how females in a predominantly dichromatic species can develop the ability to enhance their colour vision by developing, and critically using, an extra photopsin. It has also been found that the coding sequences of red and green photopigment genes of certain species of monkey are highly homologous to human coding sequences of red and green photopigment genes (Deeb, Jorgensen, Battisti, Iwanski and Motulsky, 1994). This suggests that if monkeys, who have coding sequences which are highly homologous to human coding sequences, can develop and use extra portions of the visual system. It may be possible to generalise this to humans and anticipate a similar ability. We know humans can develop a fourth type of photopsin (Windericks et al, 1992), and this evidence suggests that if monkeys can do the same (albeit with a third photopsin) and develop the capacity to use their extra photopsin, humans may be able to. Evidence of human ability to create neural mechanisms in order to use the information from the fourth photopsin is not as conclusive as evidence from new world monkeys as it is not possible to use microspectrophotometer techniques. Behavioural evidence has illustrated some possible cases of full tetrachromacy however. Jordan and Mollen (1993) asked heterozygous carries of anomalous trichromacy and trichromatic controls to adjust a mixture of green and orange light, and yellow and red light, to obtain a match. They found no significant difference in behaviour between the two groups, suggesting either that this is not a sensitive enough measure of tetrachromacy, or that people may carry four wavelength sensitive cones, but cannot actually use them due to the structure of the brain. Jordan and Mollen (1993) did find one person however, who they label as cDa7, who found only one combination of the two mixtures that would give a match. The other trichromatic controls and the heterozygous carriers had a greater variability in the repeated responses they gave on the ratio matching tasks, whereas cDa7 repeatedly gave the same response. This implies that she had greater ability to distinguish between the colours to gain a precise match and therefore her visual system was using tetrachromatic vision as opposed to the extra, shifted cone not being utilised. It may be that all the heterozygous carriers used in the study did have tetrachromatic vision, it was just that cDa7 had shifted opsins far enough away from the original cones required from trichromacy, to be detected by such behavioural techniques. Overall however, it suggests that even if all the heterozygous carriers do have tetrachromatic vision which has not been picked up by the tests, their vision cannot be considerably different to that of a trichromatic's vision. To further research this area, Jameson et al, (2001) tried to investigate tetrachromacy using techniques that they considered to be more reliable, and found a far higher incidence rate of tetrachromacy. Jameson et al, (2001) propose that ratio matching is not a reliable technique to measure tetrachromacy because it does not take into consideration real world viewing situations, such as surfaces, textures and surround contexts variations. It was suggested that many colour vision mechanisms only become active when there is sufficient variation and context in the light, which cannot be captured by ratio matches. Instead Jameson et al, (2001) asked participants to identify spectral delineations for a diffracted spectral stimulus. Although this is not entirely reflective of real world viewing conditions, it does offer more variability to the retina than simple ratio matching techniques. It was found from this that heterozygous carriers of the two types of red opsin, delineated more bands of colour in the diffracted spectrum than a control group of non heterozygous trichromatic females. This removes any differences that may be attributed to gender (Jameson et al, 2001) and also implies that the two groups of females are experiencing different perceptions of colour. This also suggests that the heterozygous carriers are experiencing strong tetrachromacy and that their visual system is utilising the fourth photopsin. This evidence demonstrates a stronger tendency for heterozygous females to exhibit tetrachromatic colour vision than Jordan and Mollen (1993) found, and it is likely that this is a more sensitive measure of colour perception. It also suggests that the viewing experiences of women who have the propensity to be tetrachromatic, in this case, is more advanced than women who are not heterozygous than expected by Jordan and Mollen (1993). Further evidence that the human visual system should be able to accommodate a fourth wavelength specific cone comes indirectly from Neitz, Carroll, Youmauchi, Neitz and Williams, (2002). They demonstrated that the visual system is plastic and can adapt to environmental stimuli after it is fully developed. Neitz et al, (2002) asked people to wear coloured filters to alter their chromatic experience. They found that as a result of this there was a shift in colour perception which lasted for nearly two weeks after the filters had been removed. This shows the nervous system's ability to use experience gained from the environment and adjust the inputs to the red and green channel to accommodate this. The implication of this is that if the visual system can adapt and change inputs to the red and green system to external stimuli, it should also be able to do the same to internal stimuli, such as receiving input from an extra photopsin. The majority of the evidence discussed suggests that there are tetrachromats. It has been shown by Winderickx et al, (1992) and Cohn et al, (1989) that physiologically a woman can posses four different types of wavelength specific cones. Whether women in this position have the neural mechanisms to utilise an extra cone is more controversial. It has been shown behaviourally that women with four cones can be distinguished from women who have three, particularly by Jameson et al, (2001), and as this can be demonstrated behaviourally it implies that the women are having a different perceptual experience of colour than women who have three cones. It would also make sense for the visual system to adapt to an extra photopsin because the nervous system can change and update itself, and this has been demonstrated by Neitz et al, (2002). The fact that the extra photopsin is only six nanometres (Winderickx et al, 1992) away from the long wavelength specific cone suggests that if there are tetrachromats, as there appear to be, their colour vision is not likely to be much superior to a trichromats. As it has been shown by Jordan and Mollen, on simple behavioural tasks such as ratio matches there does not appear to be much difference at all. It is likely that cDa7 had a larger shift than the expected six nanometres to show tetrachromatic behaviour on these assessments. It appears conclusive that women can have four different wavelength specific cones, although the extent to which the extra is utilised and an advantage to colour vision still requires further investigation. There have been relatively consistent findings regarding the effects of group decision making compared with individual decision making. Groups tend to polarize towards the given majority and individuals often become more extreme in their preferences after the group discussion compared to before (Myers and Kaplan, 1976). The mediating mechanism operating between the pre and the post decision is the deliberation process. Deliberation has a large impact on jury decision making, which is reliant upon individual jurors discussing the evidence as a group, and returning with an often unanimous verdict. The majority of research on the jury has focused upon how individual jurors reach a verdict before the discussion, such as studies supporting the Story Board Model of juror decision making (Pennington and Hastie, 1992). Although this is a theory investigating how the individual decides upon a verdict, it does have implications for the deliberation process. There have also been attempts to produce Social Decision Schemes to assess the probability of deliberation resulting in a guilty, not guilty or hung verdict, depending upon pre deliberation verdicts (Davis, Kerr, Stasser, Meek and Holt, 1977). Although both offer reasonable attempts to explain or predict jury decision making, neither provide a complete theory of the deliberation process. The Story Board Model of jury decision making proposes that in order for jurors to understand the vast and complex body of evidence presented at trials, they need to interpret the evidence in a way that makes sense to them. (Pennington and Hastie, 1992). In order to do this the juror imposes a narrative structure on the evidence to create a story. This story then acts as causal model to allow the jurors to interpret the evidence in a context from which they can draw their own experiences and evaluations, whilst allowing them to perceive the evidence as a coherent mental representation (Pennington and Hastie, 1992). Stories are created from schemas which are composed from experiences and preconceptions held by the individual and built up over a life time, it is through these that the facts of the case can take on their different meanings (Holstein, 1985). People can create completely different stories for the same evidence which is where the Story Board Model has implications for the deliberation process, as these many individual stories have to be integrated into one, often unanimous, verdict. Unfortunately the majority of research on the Story Board Model does not focus upon how this final verdict is created and concentrates upon how the initial individual pre deliberation verdict is reached. This research will be considered however as inferences can be drawn as to how this might impact upon the deliberation process. Pennington and Hastie (1992) conducted a study in which they hypothesised that presenting the evidence in a fashion that conformed to one particular story, would cause jurors to adopt this story, and depending on the nature of the story, result in a guilty or not guilty verdict. In order to achieve this effect, instead of presenting evidence to the jury in witness order (as would happen in a real trial), Pennington and Hastie (1992) organized the evidence in a way that would make one story easier to construct than another, without altering the nature of the evidence. They found that verdicts returned were consistent with the order in which the evidence was presented, thus implying that stories are a mediating mechanism in jury decision making. There is however a large problem with this evidence regarding it's generalisability to real jury decision making processes. In real trials the evidence is not presented in ""story order"" but is presented in ""witness order"", which produces a complex disorganization of the evidence. All Pennington and Hastie (1992) have shown is that in certain situations, where the evidence is neatly ordered into a coherent story, this is likely to influence the jurors' interpretation of the evidence, and therefore the verdict. This does not contemplate the complex reorganisation the brain has to implement on the evidence and how in this situation stories are created. The whole point of the story board model is to demonstrate how jurors take complex and disordered evidence and make sense of it by imposing order and meaning onto it through the narrative of the story. This is not illustrated in research such as this which already imposes the order. To further investigate the utility of the story board model Huntley and Costanzo (2003) conducted research on how jurors compose stories without manipulating the evidence as Pennington and Hastie (1992) did. Huntley and Costanzo (2003) found that individuals were predisposed to either prosecution or defence stories for a sexual harassment case. This result is presumably as a result of their different schemas. Huntley and Costanzo (2003) do not make this link explicit, although they did find a significant main effect for gender in the way that females were significantly more predisposed to a guilty verdict than males. This can be interpreted in a way that females may have more experience with being sexually harassed than males, and as a result, have schemas which reflect this. This may have caused the creation of prosecution stories which resulted in the females propagating a guilty verdict. Huntley and Costanzo (2003) also present this research as evidence for the story model because whilst evaluating the evidence within the causal model of the story, jurors relied upon extralegal, contextual information about the plaintiff and defendant when there was little hard evidence. This was to create the coverage, coherence and uniqueness which are proposed by Pennington and Hastie (1992) as necessary conditions for a successful and convincing story. Jurors selectively incorporated this biasing information into their stories providing it served as an enhancement to their story. Any biasing information that went against their story was dismissed. This evidence, when taken at face value, appears to support the story board model of jury decision making because it illustrates how jurors favour prosecution or defence stories depending on the interpretive schemes operating within each individual. It is important however, to take care when regarding this research as direct evidence that jurors compose either a prosecution or defence story. Firstly, it is likely that jurors compose more than one story of the evidence, and are not simply ""plaintiff or defense jurors"" (Huntley and Costanzo, 2003, pg35). It is more likely that jurors are more open to the interpretation of the evidence and, as suggested by Morley (1996), develop both prosecution and defense stories and select the story which, consistent with their own criteria, is most satisfactory for coverage, coherence and uniqueness (Pennington and Hastie, 1992). Also the methodology of Huntley and Costanzo's (2003) research is not consistent with what actually occurs in a trail, so the generalisability of the research to suggest that real jurors create stories, may be limited. Huntley and Costanzo (2003) asked jurors systematic questions of their interpretations of the evidence after each attorney had presented their case. In doing this Huntley and Costanzo are surely promoting a Bayesian method of processing where evaluation of the evidence is updated with each new piece of evidence (Hastie, 1993). By asking the jurors to evaluate the evidence in this Bayesian fashion, they are going against the fact that the story model advocates that jurors do not evaluate the evidence in such a systematic fashion, but as a global memory representation (Pennington and Hastie, 1992). Huntley and Costanzo's (2003) finding that different people can create different stories for the same evidence is widely supported however (Stephenson, 1995), and this is where the research is relevant in creating the implications for the deliberation process. Holstein (1985) investigated the effect of story generation on juries and the deliberation process and found that the higher the number of individual stories that were propagated into the deliberation process, the less likely the jury was to return a unanimous verdict. Holstein (1985) demonstrated that as deliberation increased the additional interpretations of the evidence, the task became more complex. This implies that if people do not initially have stories based upon similar assumptions, then the deliberation process is not going to largely alter people's stories, and it is this that makes the deliberation process more complex and a unanimous verdict less likely (Holstein, 1985). Holstein (1985) also found that even when people have two different stories favouring the same verdict, if the underlying assumptions are different, this still reduces the ability of the jury to reach a unanimous verdict and increases the complexity of the deliberation. The hypothesis the jurors compose stories to account for and interpret evidence appears to be a likely resource jurors use for interpreting evidence. Problems arise however in that the evidence used to support the notion that jurors reach verdicts in this way is not always reflective of how real trials work, so inferences made can not be generalised. Problems also arise as to whether the story model is causal. Pennington and Hastie (1992) propose that the story the juror generates determines the verdict. It is not made clear however whether jurors contemplate all the evidence and then decide upon the most appropriate story (Pennington and Hastie, 1992), or whether jurors make a very early decision as to guilt or innocence, and interpret all following evidence in the way of this initially decided verdict, as suggested by Huntley and Costanzo (2003). Less subjective approaches to jury deliberations are social decision schemes. Davis (1973) (as cited in Parks and Kerr, 1999) used social decision schemes which encompassed applying probability matrixes to initial distribution of jurors preferences in order to gain a probability of achieving a particular verdict (Parks and Kerr, 1999). The basic aim of a social decision scheme is to show how initial preference distributions relate to eventual group output, so unlike the story board model, the social decision scheme is directly concerned with the deliberation process and how this creates a single unanimous verdict. Research on group decision making outside the realm of juries has shown that whilst deliberating in a group, people tend to polarize towards the given majority (Myers and Kaplan, 1976). If this assumption were to be applied to jury deliberation processes, a social decision scheme would emerge that would predict that the final verdict would be the verdict of which the majority support. This however is not an accurate assessment of the jury deliberation process however and has been shown not to be a successful predictive matrix (Davis et al, 1977). The main reason for this is that juries have to consider the evidence with a criteria for conviction ""beyond all reasonable doubt"". It is proposed by MacCoun and Kerr (1988) that this produces a leniency bias in jury deliberations which must be accounted for by any accurate social decision scheme. This shows that jury deliberation is far more complex than simple polarization as it was even found by Tanford and Penrod (1986) (as cited in MacCoun and Kerr, 1988), that even when there is an initial majority favouring of conviction, a not guilty verdict can still prevail. Davis et al, (1977) generated fifteen social decision scheme matrices to investigate which provided the most accurate fit to their data. To gain the data Davis et al asked participants to watch a videoed case of a rape trial. Participants were then randomly spilt into six member juries and asked to find a verdict. As a general finding, Davis et al (1977) found that the verdict initially favoured by the majority, was predominantly the jury's final verdict. What is important however is that when there was a three by three spilt in the initial verdict preference, there was a strong tendency for acquittal and a near zero tendency to convict. This illustrates that in most cases a majority rule social decision scheme can be implemented, but it needs to have a sub scheme allowing for this leniency bias (MacCoun and Kerr, 1988) This was termed matrix D15, ""defendant protection, hung otherwise"" (Davis et al, 1977). To support that a social decision scheme of jury deliberations requires a sub scheme allowing for a leniency bias, or an asymmetry effect, MacCoun and Kerr (1988) produced a meta analysis of twelve previous studies of jury deliberations in which there was an equal split of jurors on each jury, initially proposing a guilty or not guilty verdict. They also found that the leniency bias was active when two thirds of the jury initially believe that the verdict should be guilty (MacCoun and Kerr, 1988). In another study by Kerr and MacCoun (1985) they demonstrated that this social decision scheme was also strong enough to consider the size of the jury and that it still produced consistent results whether there were six person or twelve person juries. This mathematical approach to jury deliberation does appear to be an effective way of predicting the effect deliberation will have on the initial set of references. A problem however is that the assumptions of the leniency bias could be explained by the fact that most of the data was collected using students, and it has been shown by Simon and Mahan (1971) (as cited in MacCoun and Kerr, 1988). In an attempt to remedy this as an explanation for the leniency bias, MacCoun and Kerr (1988) conducted a further experiment where student decision making processes were compared to those of the general public. They did not find student juries to be more lenient. In this comparison however, MacCoun and Kerr (1988) only compared student and general public juries consisting of four members. It may have been that four members was not enough to really allow the deliberation process to generate results showing that students were more lenient. Another problem is that the social decision scheme does not take into account situational factors which may effect which scheme is most appropriate. As an example of this, Davis (1975) (as cited in Davis et al, 1977) found that on a different set of data, matrix D15 was not the most effective predictor of the data, matrix D7, the ""simple majority hung"" matrix provided a better fit. This can also be shown by the fact that Schonemann (1979) found that a ridiculous matrix (where the probability of a guilty verdict is inversely related to the number of jurors favouring it), fitted Davis et al's (1977) data perfectly. Schonemann's (1979) point in illustrating this was to show that a social decision scheme could fit the data perfectly without being plausible. Social decision schemes also do not explain how group processes of deliberation are undertaken. It does suggest that the majority will rule unless the majority only slightly favour guilt, or there is an equal split in verdict choice, but it does not explain under which circumstances this is likely to happen. It appears that although social decision schemes are accurate in predicting verdicts after the data has been collected, it is far more difficult to select an appropriate social decision scheme in advance because the social decision scheme does not specify which situation factors will effect it and why. It has also been shown by Sandy and Dillehay (1995) (as cited in Devine, Clayton, Dunford, Seying and Price, 2001) that in real split juries, 71% convict, so they are clearly not showing a leniency bias in this instance which would be predicted by the social decision scheme. There are many difficulties in studying the jury deliberation process, predominantly because it is illegal to view the process, or even to ask jurors about their experiences after a case. As a result of this the majority of the evidence is conducted under contrived circumstances under which the juries face no consequences for their decisions, or pressure as they would in a real jury. It may be because of this that research on mock juries has not been able to produce a unified theory of the jury deliberation process. It may however be that there cannot be a single unified theory of jury deliberation as there are so many situational factors which could be acting upon the process. The fact that shadow jurors fail to consistently reach the same verdict illustrates this (MacCoun and Kerr, 1988). It has been suggested that personality traits such as authoritarianism contribute to how juries deliberate, although there has not been much support for these ideas, (Parks and Kerr, 1999). It is important however not to see the story board model and social decision schemes as being mutually exclusive to each other because they are not. Both contribute to our understanding of how juries deliberate although, as discussed, neither provide a comprehensive theory. It is important to keep researching the area however to improve our legal system and to try to reduce such variability in proceedings.","Popularity is an extremely difficult concept to measure as popularity is variable across cultures and ages. Characteristics defining a child as popular at the age of 8 may not be the same characteristics which define a child as popular at 14. Another problem arises distinguishing friendship from popularity. A child may be perceived as unpopular but still have a few close friends, and it is important to consider this when looking at psychological development because popularity is multi-faceted as opposed to one dimensional. It has been shown that unpopular children are more likely to have psychological difficulties such as schizophrenia, anxiety disorders, mood disorders and criminal behaviour (Kupersmidt, Coie and Dodge, 1990). Because of this, an important issue when looking at the effect of popularity on psychological development is cause and effect. This issue is covered extensively in the essay as it is valuable, yet difficult, to try and determine whether being unpopular causes problems with psychological development, or whether underlying psychological difficulties cause the child to behave inappropriately and become unpopular. Popularity is an extremely important issue affecting all children. It is more important now than ever as parents tend to return earlier to work so children tend to spend more time with peers in nurseries and after school wrap-around classes. Characteristics of popular and unpopular children have been widely investigated, and there is some variation in the results found. Most commonly however it has been found in primary school aged children, between the ages of 4 and 11, the children regarded as popular tend to be co-operative, helpful, have leadership skills and actively engage in prosocial behaviour (Dekovic and Gerris, 1994). Unpopular children however tend to be aggressive, possessive, verbally aversive and display hostility (Dekovic and Gerris, 1994). To study the effect popularity has on psychological development however it is important to break the notion of ""unpopularity"" down into further components. Asher (1990) defines two types of unpopular children. The first are ""rejected"" and these are children who are actively disliked by their fellow peers (Asher, 1990). The second type are ""neglected"" and these are not actively disliked but tend to be timid and shy and are almost forgotten by peers and also appear to have a lack of friends (Asher, 1990). It is important to make this distinction as psychological development is influenced by whether you are considered ""rejected"" or ""neglected"", it appears that neglected children know how to behave socially but have problems putting this into practice, rejected children have difficulties with both (Asher, 1990). It has been found by Coie and Dodge (1983) (as cited in Asher, 1990) that rejection is a stable process and once you have been rejected you are likely to stay rejected for at least the next five years. This is why it is important to look at the implications for this on the child's psychological development as it is likely the child will remain in their rejected situation until at least adulthood. An important aspect to consider with respect to this is labelling theory and the self fulfilling prophecy. When cause and effect are taken into account a child may remain unpopular because they believe themselves to be unpopular and therefore act in a way appropriate to the label they have been given. Also children are very aware of building alliances to secure their own popularity and if one child is labelled undesirable other children may not want to be associated with the labelled child thus securing their own alliances and popularity. This may account for why children remain unpopular but it does not explain why the child may become unpopular or undesirable in the first place. One possible reason causing a child to be unpopular may be due to them having an autistic spectrum disorder (Hobson, 1995). Unfortunately many of the characteristics displayed by unpopular children are also displayed by children suffering from an autistic spectrum disorder such as Autism or Asberger's Syndrome. Children who are unpopular tend to only view situations from their own perspective, do not value others opinions and cannot empathise with their peers (Dekovic et al, 1994). Autism is a disorder where it is believed children lack a ""Theory of Mind"". This means that they find it very hard to see the world from any perspective but their own and fail to understand others emotions or social cues (Hobson, 1995). This may lead to the child becoming unpopular as perspective seeing is essential to being popular as are the communication and cognitive skills that someone with autism may lack. This is one reason a child may become unpopular as autism affects 5 in every 10,000 children (Hobson, 1995). If autism is not diagnosed until the later years the child will be in main stream education and more than likely be rejected by peers and possibly suffer the psychological problems associated with rejection which will be discussed later. Another cause for a child being unpopular is associated with the family (Henggeler, Edwards, Cohen and Summerville, 1991). The majority of children spend their first few years learning from their parents so it is not surprising that these early years contribute to the child's personality and whether they become popular or unpopular. Ainsworth and Wittig (1969) (as cited in Henggeler et al, 1991) investigated types of attachment between mother and infant. They identified three types of attachment as these have been investigated with regards to child's future popularity. Vandell and Wilson (1987) (as cited in Henggeler et al 1991) found that children who are securely attached become more popular than those who are not securely attached and have more positive peer interactions. This is likely to be because these children have a secure base from which to explore from and feel confident in themselves to make social advances, knowing that they have support. This inner feeling of security may facilitate confidence and an interpersonal family may produce interactional skills. To support this MacDonald (1987) (as cited in Henggeler, 1991) found that preschool boys who had been assessed as being ""rejected"" had parents who were less interpersonally responsive and were less emotionally positive. These children were more likely to have been considered to have had an attachment type of anxious avoidant or anxious resistant. If child had this attachment type then they will not have had a secure base from which to explore from and particularly anxious resistant children would have been anxious at leaving the caregiver and facilitating early social relationships. This may be a cause in why some children become popular and why some children become unpopular. It is interesting to note however that the majority of research completed on attachment type and popularity occurred in 1987. This is just one year after Maine (as cited in Maine and Cassidy, 1988) introduced the new attachment type insecure disorganised. This means that attachment will have been in the media and well heard of, which may have been an incentive to link attachment with popularity and more may have been made of it than is actually there. Attachment suggests that it is causal in determining popularity although this seems a little reductionist and little has been done to investigate whether securely attached children are still more popular in the older years when there is less reliance on the parents. How this effects children in care would also need to be fully investigated before attachment could be seen as a deciding figure in a child's popularity. Also attachment claims to be a causal factor but it may be the child's temperament that causes attachment and popularity therefore attachment is not the cause, but the child's personality. It may not be as much attachment but more to do with Bandura's social learning theory (1969) (as cited in Grusec 1992) as children who's parents are emotionally responsive and communicative to them learn to be emotionally responsive and communicative to others and therefore more popular. Piaget (as cited in Rubin, LeMare and Lollis, 1990) offers another environmental explanation as to why some children may be more popular than others. He suggests that children who become popular have successfully negotiated mutually engendered behaviours. Because young children are egocentric to learn how to interact and become socially efficient they need to lose the belief that the world centres around them and develop a theory of mind (as cited in Rubin, 1990). Early play and reciprocal friendship allow perspective taking skills and prosocial behaviours to develop, which are important skills to be popular and socially accepted (as cited in Rubin, 1990). This has implications for children who do not have the opportunity to socialise at an early age as may not completely abolish the notion of egocentrism. It also means important cognitive and social skills are not being developed. This is supported by the fact that Dekovic and Gerris (1994) found that rejected children demonstrated more egocentric, selfish behaviours This offers a suggestion as to why children may not exhibit prosocial behaviour but it still does not explicitly offer a cause. Is it opportunity that prevents early socialisation, and if so should this just not occur at preschool, or is it personality which prevents early socialisation or is it attachment type and not feeling secure enough to socialise. It is clear that there are so many different causes which may lead a child to become unpopular. There is also a large number of children who can be considered rejected or neglected and therefore it is important to investigate any implications of this on their psychological development. It is at this stage that it is important to separate the notion ""unpopular"" into rejected and neglected. Rejected children tend to be more actively disliked than neglected and engage in more aggressive and hostile behaviours (Asher, 1990). It is this type of child who is more likely to turn to criminality and anti social behaviours (Kupersmidt et al 1990). This is a dangerous assertation to make however regarding the self fulfilling prophecy and also almost makes it acceptable for rejected children to turn to criminal behaviour. Roff (1972) (as cited in Kupersmidt et al, 1990) found that peer rejection was a significant predictor of delinquency and this once again may be a result of the self fulfilling prophecy in that the child believes they are bad and therefore will only survive by being bad. It may also be an attempt for admiration as anti social behaviour becomes more associated with popularity once the child moves towards adolescence, or it could just be a scream for attention. Neglected children however tend to be less actively disliked by their peers but are more withdrawn and isolated from society (Asher, 1990). This is obviously important for psychological development as there are three disorders noted in DSM IV (American Psychiatric Association, 1995) regarding withdrawal such as ""Shyness Disorder"", ""Adjustment Disorder"" and ""Introverted Disorder"", therefore being withdrawn obviously has psychological disadvantages. Neglected children are also more likely to suffer from internal disorders such as those involving mood and anxiety (Asher, 1990) as opposed to rejected who are more likely to suffer external ones (Asher, 1990). Being neglected however does not necessarily lead to psychological problems however as Coie and Kupersmidt (1983) (as cited in Rubin, 1990) found that neglected children are more confident in smaller groups. There is only a problem however when insecurity and anxiety issues become apparent. Obviously children who are considered unpopular whether rejected or neglected are perceptive enough to pick up on and internalise negative feelings about themselves which has implications for psychological development in itself and can lead to many further complications, particularly that of depression. There has been links made between children who were unpopular at school and a later development of schizophrenia. A feedback study conducted by Bleuler (1950) (as cited in Kupersmidt et al 1990) describes the preschizophrenic child as anxious, excitable, irritable and aggressive. This seems to be descriptive of a rejected rather than a neglected child. There however problems with cause and effect with this as it is not clear whether rejection in itself causes schizophrenia or the schizophrenia causes the antisocial behaviour which leads the child to become unpopular. From the evidence and knowledge about schizophrenia I believe that being unpopular itself doesn't cause schizophrenia. This is because schizophrenia does not tend to develop until late adolescence. Also Watt (1978) showed that for the preschizophrenic child behaviour in childhood is normal and the child does not become unpopular until adolescence with the onset of schizophrenia. This also concurs with evidence that rejection is stable. If a child is unpopular at adolescence then they will have been unpopular during childhood, the pre-schizophrenic child does not show this pattern so it is more likely that schizophrenia causes the unpopularity and not vice versa. In conclusion there could be many factors effecting whether a child turns out to be popular or unpopular. The majority however seem to ignore the child's actual personality which probably plays a large role. There is little doubt however that factors such as attachment type and early experiences with reciprocal friendships are valuable in learning the necessary skills to socialise and which therefore give you an increased chance of being popular and having friends. It has been shown that lack of friends or being unpopular may lead to problems with psychological development and these seem to be more extreme with rejected rather than neglected children. There is plenty still to discover however regarding whether being unpopular causes psychological problems, or the psychological problems are already there and cause unpopularity. It would appear from the conclusions made in this essay that it is a likely combination of the two.",False
74,"The Civil War in Russia is often considered as a fight between Reds and Whites. However, many recent historians, such as Radkey, consider the Bolsheviks versus the Greens, a popular movement led by the peasants, as the main Civil War in Russia. How valid is this claim? The Whites were made up of conservatives, landowners, former generals and monarchists. However, because they had such a variety of members there was political disunity amongst them. Although White leaders themselves were strong, there was no one leader of the movement. 'In many ways, the Whites were doomed from the beginning."" Nevertheless, they had the support of foreign countries and held land where there was much grain. However, the Allies support could never have won the Whites the war as 'the Whites...appeared as undignified puppets of Britain and France, while the Bolsheviks seemed independent Russian patriots defending the country against foreign intervention."" The White Army reached a peak of 100,000, compared to the Red Army with over three and a half million members in 1920. It appears that throughout the Civil War the Reds had the upper hand over the Whites, thus the real struggle was not between the Reds and the Whites. Read argues that 'civil war against the Whites was virtually over by the end of 1919."" However, in the south the White threat was especially strong. The Whites had the support of the Cossacks, who had four and a half million members, many of whom were professional soldiers. However, the Cossacks were returning from World War I and only wished to fight to protect their homeland, therefore were not a threat to the Reds across the whole of Russia. Read, C The Making and Breaking of the Soviet System (2001, Hampshire) p. 31 Bradley, J Civil War in Russia 1917 - 1920 (1975, London) p. 178 Read, C From Tsar to Soviets (2001, Eastbourne) p. 258 Much of the Bolshevik strength lay in the land it held. The Reds held highly populated areas, including industrial Petrograd, which they could use for military equipment production and land where railways were already laid, making communication between troops easier. Conversely, the Whites were dispersed across Russia. However, the Reds relied on workers for production and railways, many of whom were members of the Greens. Therefore the Greens could pose a serious threat to the Bolsheviks if they revolted in these areas. The Greens consisted of anarchists, peasants, workers and nationalists. Brovkin says the Greens were 'the most numerous of all parties and movements."" However, the peasants made up eighty percent of the population so this is not surprising. Radkey argues that there were two Civil Wars in Russia. The first being the well-publicised Reds versus Whites and the second was a 'movement of the people itself - and particularly the peasantry - to shake off the tyranny imposed by victorious Red minority."" Reasons for this Green movement were social, political, economic and personal. The peasants had just fought in World War I, they were tired of fighting for others. They wanted to fight for their own cause now. A peasant song from the Civil War illustrates this feeling. Brovkin Behind the Front Lines of the Civil War p.415 Radkey, O The Unknown Civil War in Soviet Russia (1976, California) p. 1 'We'll drive away the Red Army, And take the flour for us. 'Figes, Orlando Peasant Russia, Civil War (1989, Oxford) p. 321 In 1917 many soldiers who previously supported the Bolsheviks, returned to their villages and were reabsorbed into the countryside and joined with the other peasants in their quest for land. Also, Red Army deserters joined the peasants to fight against the Bolsheviks. Thus, the popular movement had support from soldiers who could threaten the Bolsheviks because of their war experiences. The peasants, workers, soldiers and sailors fought in self-defence against the crisis of War Communism occurring in Russia. Peasant food was requisitioned and given to soldiers. If the peasants refused to give up their food the Cheka would brutally intervene. Martyn Latsys, the head of the Cheka, claims there were two hundred and forty five peasant rebellions in 1918 against the intervention of the Cheka. The main areas of popular movements were Nestor Makhno in the Ukraine and Aleksandr Antonov in Tambov Province. Peasants destroyed railways, bridges, ambushed Red Army units and ransacked Soviet military depots. Although War Communism helped the Reds win the Civil War, it was at the cost of the rest of the population. Five million ordinary citizens died in Russia through War Communism. The Bolsheviks had gone against one of their main promises of bread when they ceased power in 1917. 'Some historians believe it was this mistake (of requisitioning) on Lenin's part that led to the civil war against the peasants."" Lenin led a campaign called 'Peace with the Peasants' in 1919 and the Bolshevik - peasant relations looked as though they may improve. However, by 1920 the streets of Russia looked like they had been air raided. The people were starving and it was bitterly cold, they had nothing. Lenin's Secret Police Brovkin Behind the Front Lines of the Civil War p.129 Medvedev in Brovkin Behind the Front Lines of the Civil War p.129 Sometimes the Greens would join the Reds to fight the Whites or vice-versa, but this was always for their own gain. Revisionist historians hold the view that 'The majority of historians, reduce the Civil War to the struggle between the Reds and the Whites... The facts show...that there was a third force upon which fell the biggest blow - the peasant insurrectionist movement. In different periods to a differing degree this movement joined either the Whites or the Reds, but it remained a relatively independent force.' Seliunin in Brovkin Behind the Front Lines of the Civil War p.128 However, Figes argues that 'the defeat of the peasant rebellions was unavoidable' and he claims that the peasantry of central Russia generally remained loyal to the Bolsheviks. Not all the peasantry were united against the Bolsheviks, so the popular movement was not as strong as possible. However, Figes goes on to argue that by 1921, the peasants had been defeated by the Bolsheviks, but had made sufficient gains such as hospitals, a rise in the level of literacy, autonomous communes and elimination of extremes of poverty. These positive aspects surely illustrate that the Bolsheviks must have felt threatened enough by the peasants to fear more uprisings and thus make improvements in the countryside. Figes Peasant Russia, Civil War p. 354 Relations between workers and the State were also dramatically deteriorating. The populations of Moscow fell by a half and Petrograd fell by two thirds between 1917 and 1921, because Lenin wanted one man management and their had been an economic collapse leading to lower bread rations, forcing workers to leave the cities in search of bread; those workers who remained in the cities were demoralised and unreliable. The workers were supposed to be the traditional supporters of the Bolsheviks; the political system was officially defined as a proletariat dictatorship. However the Bolsheviks were moving from being a mass organisation of workers to a body of officials. By 1922 two thirds of the Bolshevik party members were administrators. The Bolsheviks needed the workers support for military and economic reasons as well as a commitment to their original political ideology, meaning workers' discontent with the Bolsheviks was an important struggle for the Reds. Where there was worker unrest the Bolsheviks censored it. However, strikes and riots did take place, more radically in provinces than the capitals. For example, the Aleksandrovskii railway workshops' strike in February 1919. The immediate cause was forty percent of their wages not being paid on time. They also demanded food rations equal to the Red Army soldiers. The Bolsheviks agreed and it was thought that peace would be resumed and the workers would go back to work. This is similar to the 'Peace with the Peasants' campaign, showing that there was a constant struggle among the Bolsheviks and the popular movement. However, the Cheka arrested the leaders of the strike and the workers demanded their release. The economic protest had now turned into a political battle. In June, the Aleksandrovskii railway workers went on strike again and were joined by numerous other railways, who were protesting against the demand that some workers leave and fight for the Red Army. However, 'the growth of the Soviet state, the broadening scope of internal repression and the need for unity against the Whites' reduced the impact of the popular movement in Soviet held areas. 'Part of it (the popular movement) had been absorbed into the party and state, another...against centralisation, and....the largest part, had simply tried to get by in worsening conditions.' Read From Tsar to Soviets p. 260 Read From Tsar to Soviets p. 258 Read From Tsar to Soviets p. 258 The 1920 harvest was a mere sixty percent of the pre-War level, which added to a rise in city and countryside discontent against the Bolsheviks. For example, the Krondstadt Mutiny took place in March 1921. Although it was crushed by the Red Army, their views for free trade of grain, democracy and free speech for socialist parties had been heard. Lenin said that Kronstadt 'was the flash which lit up reality better than anything else."" It lit up the fact that Bolshevism was not going to listen to the people of Russia, and would maintain political control in their own undemocratic way. Lenin thought that the series of peasant wars were 'far more dangerous' than other counter movements. Read From Tsar to Soviets p. 277 Figes Peasant Russia, Civil War p. 321 Propaganda in Russia constantly told the people that the Bolsheviks were in charge of Russia and life would improve. It is difficult to assess how successful the propaganda was because there were few object opinion polls, but the fact that rebellions existed shows that the Bolsheviks did not have control over the population in the way they imagined. However, propaganda may have reduced the threat that popular opinion posed. Although many rebellions posed strong threats it is unlikely that the popular movement would have defeated the Bolsheviks, because they had many spontaneous uprisings and the peasants and workers were spread across Russia with no one leader and the Bolsheviks, especially by the end of 1919, were very strong. 'The Bolsheviks must have been a tiny minority of the Russian population, which through superior organisation, ruthless determination and efficient propaganda snatched victory from an inefficient, divided and silent majority.' Bradley Civil War in Russia p. 184 After defeating the White Army, the Bolsheviks were forced to 'surrender before its own peasantry."" The New Economic Policy was introduced because of the threat mass unpopularity caused to the Bolsheviks, illustrating that the greater struggle was with popular movement because the Bolsheviks were forced to make changes to please the Greens. Although the Whites did pose a threat to the Reds, they were divided, lacked morale and had weak communication lines. The Green and Red fight lasted longer, therefore there was more of a 'struggle'. Brovkin argues that 'military and strategic factors are far less important than popular attitudes in a civil war."" The Red and White war was a far more military and strategic war, and the Green and Red a war of popular opinion. Brovkin concludes that 'the available evidence suggests...the magnitude of the Bolshevik war against the peasants on the internal front eclipsed by far the frontline civil war against the Whites.' Figes Peasant Russia, Civil War p. 321 Brovkin Behind the Front Lines of the Civil War p.91 Brovkin Behind the Front Lines of the Civil War p.127","'The German invasion, launched at dawn on Sunday 22 June 1941, caught the Red Army almost fatally unprepared and the Soviet Union suffered catastrophic defeats."" How was it possible for Soviet Russia to survive and defeat Germany? Many historians argue it was the power and morale of the people that lead to the victory. For example, Chris Ward says 'to put it another way the Great Patriotic War should be characterised as a people's war."" How valid is this claim? Acton, Edward Russia: The Tsarist and Soviet Legacy (London, 1995) p. 243 Ward, Chris Stalin's Russia (London, 1999) p. 221 When Germany attacked Moscow in 1941, it appeared that Stalin immediately gave up. In 1988 Aleksei Kosygin said that 'office doors swung open, papers blew around and rustled underfoot, everywhere phones were ringing...in the absence of clear information rumours and alarms swept through the city and people began to take matters into their own hands."" Looting and rioting begun, order was collapsing. Stalin first addressed the nation after the attack on July 3 rd and order was regained. This illustrates that left to their own devises the Russian people could not defend the capital. Ward Stalin's Russia p. 191 The government created an efficient wartime economy. 'The war years witnessed the greatest triumph of Soviet planning."" Almost 1,500 factories were moved in two months and 3,500 big new factories were built from scratch during the war. Also, by 1945 Russia had more railway lines operating than at any time in their history. This required mass organisation and money. At its peak, more than half the national income was devoted to armaments. Workers had to be housed and fed in 'towns whose size tripled overnight."" Work to complete the factories continued into the freezing night, requiring incredible endurance and therefore it is down to the Russian people that relocation and production were successful. 'In World War II the Soviet economy supplied the eastern front with tens of millions of soldiers and weapons. These soldiers and weapons destroyed Hitler's dream of a German empire in Europe."" Rearmament was very fast. Munitions output doubled in 1942 and peaked in 1944. Soviet production, during the war, amounted to 100,000 tanks, 130,000 aircrafts, 800,000 field guns, half a billion artillery shells, 1.4 million machine guns, six million machine pistols and twelve million rifles. This meant the army had a much superior quantity of equipment to the Axis powers. 'What won the war for the Allies, in the end, was their ability to produce munitions in greater - much greater - quantity than Germany and Japan."" According to some historians, although the quality was similar, the sheer volume of products won the war for Russia. The volume of products created can be attributed to the hard work of the Soviet people, but also the Soviet government for organising the production line so effectively. Ward Stalin's Russia p. 175 Barber, John and Harrison, Mark The Soviet Home Front, 1941-1945: A social and economic history of the USSR in World War II (London, 1991) p. 138 Acton Russia p. 249 House, Jonathan and Glanz, David When Titans clashed: How the Red Army stopped Hitler (Kansas, 1995) p. 71 Harrison, Mark Accounting for War: Soviet production, employment and the defence burden, 1940-1945 (Cambridge, 1996) p. 172 Ward Stalin's Russia p. 197 Barber and Harrison The Soviet Home Front p. 180 Message of Goldsmith 1946 in Barber and Harrison The Soviet Home Front p. 180 As well as demands on the economy increasing, so did demands on the countryside. There was re-collectivisation and in 1942 new minimum work days and extended hours with mandatory three hours overtime were introduced. However, at the same time government relaxed many farming industry rules. People could take their own initiative, which they did, illustrating that the in some aspects the people's effort won the war. However, it could be argued that the Russian people were a resource; it was up to the government to use them efficiently. Without the government the people had no central control and thus no chance of organising themselves effectively in defence of Russia. The Soviet regime disregarded morals, was ruthless and used harsh repression to keep order during the war. If a minority were seen as siding with Germany then the whole community where the minority lived would be deported. The state would punish any soldiers who abandoned their positions. Absenteeism and lateness in defence industry factories were seen as crimes and mass overtime was demanded from offenders. Offenders could also be imprisoned from five to eight years. 'Brute force was applied, and not only to the enemy."" Another way in which the state controlled the Russian people was concealing information from the public. Harrison Accounting for War p. 172 Similarly, there obviously cannot be a state without people, the people make the government's policies happen. Thus although the government made new policies of production and had the ideas of creating new factories, the Russian people fulfilled these policies, which was the key to winning the war. The motivation of the Russian people is important to consider. Many historians believe that the cult of Stalin's personality was vital in this. Some historians consider that the Soviet Union won against Germany because of Stalin. 'Some credit for the Soviet Union's ultimate victory undoubtedly belongs to Stalin. He learnt from his mistakes....he demoted incompetent cronies from the Civil War period....he gradually mastered complex areas of strategy and logic."" Not only was he a great patriotic leader, but also his industrialisation and rearmament policies were seen as successful. However, others suggest that 'perhaps, if a different policy had been followed, the Germans would not have got as far as Stalingrad."" However, Stalin's ability to raise morale and install nationalism should not be underestimated. He was always in propaganda in officer's uniform and was seen as the protector of Russia, a symbol of ultimate power. He was vital to winning the war because he raised patriotism and was seen as a father figure to the nation. In the Great Patriotic War, socialism and communism were forgotten. The state emphasised that it was a fight for Russia as a nation. 'It is necessary to give the party organs who carried out their ideological work their due. They reacted quickly to the mood of the people."" Stalin appealed backwards to the great Princes, such as Alexander Nevsky and great leaders, such as Lenin to gain a sense of patriotism in Russia. Also, people genuinely wanted to believe that there was hope of victory; Stalin gave them this hope. 'During the war years, as the Soviet people were battered by unbelievable miseries, the name of Stalin and faith in him to some degree pulled the Soviet people together, giving them hope of victory."" The cult of Stalin's personality may help to explain how the population survived the horrific invasion of Operation Barbarossa, then endured the hardships of war and come out victorious. Popular belief in Stalin would have made people work harder, more bravely and obey orders. Support for Stalin is believed to have been greater amongst the armed forces over the civilians. Propaganda in the army was more intense. Pictures of Stalin could be seen in army camps. However, naturally there were civilians and soldiers who remained hostile to Stalin and blamed him for the war. Another way the government tried to raise morale and nationalism was through the reintroduction of the Orthodox Church. Religion provided a sense of national pride. This is illustrated in The People's War, which talks about an SD's description of a Minsk clergyman as 'the best propagandist for the German issue."" However, this clergyman changed to support Stalin when Stalin altered his strategy toward the church, meaning support for the Soviet Union was dramatically raised when linked with the church. This is again a combination of government and Soviet people securing war success. Barber and Harrison The Soviet Home Front p. 55 Harrison Accounting for War p. 13 Bonwetsch, Bernd and Thurston, Robert (eds) The People's War: Responses to World War II in the Soviet Union (Urbana, 2000) p. 82 Medvedev, Roy Let History Judge: The Origins and Consequences of Stalinism (New York, 1973) p. 455 Security Service Bonwetsch and Thurston (eds) The People's War p. 19 Patriotism was not just forced upon the nation by the government. The role of spontaneous Soviet patriotism should not be underestimated; there is a clear capacity amongst the Russian people for self-sacrifice. For example, 'soldiers staying put in their trenches while the heavy German tanks were sweeping across them, and then firing them from behind.' Ward Stalin's Russia p. 222 The effort of the individual soldier was also vital. Every soldier had a struggle for survival. Propaganda was successful in installing national pride and hatred of the Nazis, which gave people motivation to fight and people were prepared to work harder and longer hours. People believed they were fighting for a better future for Russia, they wanted to escape the Great Terror. The people were highly motivated to win. Similarly, the role of Russian women is important when considering the Russian people's role in the war. 800,000 young girls and women mobilised in Russia after Operation Barbarossa. They fought hand-to-hand combat as well as the traditional role of cooks and nurses. John Erickson argues that 'there are countless accounts of women's courage and self-sacrifice in the early defeats of Soviet forces."" Women also went down mines and worked in heavy industry, all for the war effort. 'The physical labour was demanding and exhausting."" Women's industrial training expanded and so did the discipline. For example, in Leningrad 500 housewives began work in the Kirov Factory and in Moscow 374,000 housewives begun factory work in August 1941. There was a high demand for war materials, but the total 'manpower' fell by twenty six million. So 800,000 people, including housewives and pensioners, joined the war economy effort by joining a mass voluntary movement at the start of the war. People contributed more to the war victory than the government. Often miners' wives volunteered for work in the mines. However, the Soviet Manpower Committee soon set up compulsory labour forces moving people from light to heavy industries, which means maybe it was government intervention that led to the mass economic production. Garrard, Carol and Garrard, John (eds) World WarII and the Soviet People (Hampshire, 1993) p. 61 Garrard and Garrard (eds) World WarII and the Soviet People p. 53 Garrard and Garrard (eds) World WarII and the Soviet People p. 53 Garrard and Garrard (eds) World WarII and the Soviet People p. 53 Ward argues that the Great Patriotic War as a social event, rather than political, military or economic, is largely undiscovered. 'During and even after the war, virtually the entire Soviet population was united by the drive to expel the Germans."" For example, town squares were used to grow vegetables to feed soldiers and factory workers. Anything was done to help the war effort. Also, in the towns under siege there was barely anything to eat. People ate anything edible such as wallpaper paste; people were highly motivated to survive. Food rations were later introduced to help people get by, therefore there was some government intervention. Acton argues that 'the supreme test of civilian morale and resilience was the 900 day siege of Leningrad. Although engulfed by hunger, cold, lice and death, the city refused to surrender.' House and Glanz When Titans clashed p. 290 Acton Russia p. 250 A way in which the government won the war was by using some previous experience to their advantage. After the attack on Finland in 1939, Semen Timoshenko became Defence Commissar. He implemented a new army training programme, however, the army leadership remained hopelessly disorientated and the strategic thinking remained largely unchanged, which is why the front line units were badly prepared in June 1941. The Red Army performed very badly in the Finnish War despite the fact that they well out numbered the Finnish soldiers. 'Still, individual Red Army soldiers and units displayed remarkable courage and determination. The Finns were deeply impressed with the Soviet soldier's spirit."" The Red Army in 1941 to 1942 still lacked training and equipment, so as in 1939, the soldiers fighting at the front line simply put all their energy into their efforts to fight. This shows that it was the people that won the war. Bonwetsch and Thurston (eds) The People's War p. 238 However, even in the chaos of Operation Barbarossa the system had the ability to mobilise, which is a positive attribute of the government. The government had bred a society who could cope with chaos. The 1930s Five Year Plan had created industrial chaos, this meant that the country was able to cope with the chaos of war. This was known as a constant state of emergency. This means that the war was won by the government's policies, not the people, although the people are obviously involved in these policies. The aid Russia received from Britain and America was vital in the Russian victory over Germany. The money they provided helped feed the Red Army and they provided raw material for military equipment. However, many historians argue that the Russian's victory 'must be attributed solely to the Soviet people and to the iron nerve of Stalin, Zhukov, Shaposhnikov, Vasievsky and their subordinates.' House and Glanz When Titans clashed p. 285 In addition, the role of Hitler must not be ignored when evaluating how Russia won the war. Hitler aimed to defeat an enemy whose industrial output was already comparable to Germany and whose population was twice the size. Also, Hitler's armies were not ready for a winter campaign, thus they were ill-equipped to fight in Russian winter conditions. Germany had not defeated Britain when she attacked Russia and Hitler joined Japan against America before securing reciprocal support from the Japanese against Russia, showing that Germany got caught in a war on many fronts, lessening their chance of victory. In addition, Hitler had unforced errors of racial policy, which led to flaws in Hitler's war effort against Russia. Overall, it appears that the morale of the people kept the war effort going. Although the government created the policies during the war and provided leadership, there is only so far that a government can go without the support of their people. Read discusses the movement of factories in Russia and says 'there was no way such tasks could have been achieved simply by pressing buttons at the centre. Immense hard work, dangerous and slow railway journeys (were required)."" The Russian people seemed to truly believe they could win the war and thought they would have a better future if they won. Thus refused to give up through all the hardship they suffered. Garrard and Garrard conclude that 'this after all, was a ""people's war"" not a ""Party war"".' Read, Chris The Making and Breaking of the Soviet System (Hampshire, 2001) p. 135 Garrard and Garrard (eds) World WarII and the Soviet People p. 70",True
75,"'The German invasion, launched at dawn on Sunday 22 June 1941, caught the Red Army almost fatally unprepared and the Soviet Union suffered catastrophic defeats."" How was it possible for Soviet Russia to survive and defeat Germany? Many historians argue it was the power and morale of the people that lead to the victory. For example, Chris Ward says 'to put it another way the Great Patriotic War should be characterised as a people's war."" How valid is this claim? Acton, Edward Russia: The Tsarist and Soviet Legacy (London, 1995) p. 243 Ward, Chris Stalin's Russia (London, 1999) p. 221 When Germany attacked Moscow in 1941, it appeared that Stalin immediately gave up. In 1988 Aleksei Kosygin said that 'office doors swung open, papers blew around and rustled underfoot, everywhere phones were ringing...in the absence of clear information rumours and alarms swept through the city and people began to take matters into their own hands."" Looting and rioting begun, order was collapsing. Stalin first addressed the nation after the attack on July 3 rd and order was regained. This illustrates that left to their own devises the Russian people could not defend the capital. Ward Stalin's Russia p. 191 The government created an efficient wartime economy. 'The war years witnessed the greatest triumph of Soviet planning."" Almost 1,500 factories were moved in two months and 3,500 big new factories were built from scratch during the war. Also, by 1945 Russia had more railway lines operating than at any time in their history. This required mass organisation and money. At its peak, more than half the national income was devoted to armaments. Workers had to be housed and fed in 'towns whose size tripled overnight."" Work to complete the factories continued into the freezing night, requiring incredible endurance and therefore it is down to the Russian people that relocation and production were successful. 'In World War II the Soviet economy supplied the eastern front with tens of millions of soldiers and weapons. These soldiers and weapons destroyed Hitler's dream of a German empire in Europe."" Rearmament was very fast. Munitions output doubled in 1942 and peaked in 1944. Soviet production, during the war, amounted to 100,000 tanks, 130,000 aircrafts, 800,000 field guns, half a billion artillery shells, 1.4 million machine guns, six million machine pistols and twelve million rifles. This meant the army had a much superior quantity of equipment to the Axis powers. 'What won the war for the Allies, in the end, was their ability to produce munitions in greater - much greater - quantity than Germany and Japan."" According to some historians, although the quality was similar, the sheer volume of products won the war for Russia. The volume of products created can be attributed to the hard work of the Soviet people, but also the Soviet government for organising the production line so effectively. Ward Stalin's Russia p. 175 Barber, John and Harrison, Mark The Soviet Home Front, 1941-1945: A social and economic history of the USSR in World War II (London, 1991) p. 138 Acton Russia p. 249 House, Jonathan and Glanz, David When Titans clashed: How the Red Army stopped Hitler (Kansas, 1995) p. 71 Harrison, Mark Accounting for War: Soviet production, employment and the defence burden, 1940-1945 (Cambridge, 1996) p. 172 Ward Stalin's Russia p. 197 Barber and Harrison The Soviet Home Front p. 180 Message of Goldsmith 1946 in Barber and Harrison The Soviet Home Front p. 180 As well as demands on the economy increasing, so did demands on the countryside. There was re-collectivisation and in 1942 new minimum work days and extended hours with mandatory three hours overtime were introduced. However, at the same time government relaxed many farming industry rules. People could take their own initiative, which they did, illustrating that the in some aspects the people's effort won the war. However, it could be argued that the Russian people were a resource; it was up to the government to use them efficiently. Without the government the people had no central control and thus no chance of organising themselves effectively in defence of Russia. The Soviet regime disregarded morals, was ruthless and used harsh repression to keep order during the war. If a minority were seen as siding with Germany then the whole community where the minority lived would be deported. The state would punish any soldiers who abandoned their positions. Absenteeism and lateness in defence industry factories were seen as crimes and mass overtime was demanded from offenders. Offenders could also be imprisoned from five to eight years. 'Brute force was applied, and not only to the enemy."" Another way in which the state controlled the Russian people was concealing information from the public. Harrison Accounting for War p. 172 Similarly, there obviously cannot be a state without people, the people make the government's policies happen. Thus although the government made new policies of production and had the ideas of creating new factories, the Russian people fulfilled these policies, which was the key to winning the war. The motivation of the Russian people is important to consider. Many historians believe that the cult of Stalin's personality was vital in this. Some historians consider that the Soviet Union won against Germany because of Stalin. 'Some credit for the Soviet Union's ultimate victory undoubtedly belongs to Stalin. He learnt from his mistakes....he demoted incompetent cronies from the Civil War period....he gradually mastered complex areas of strategy and logic."" Not only was he a great patriotic leader, but also his industrialisation and rearmament policies were seen as successful. However, others suggest that 'perhaps, if a different policy had been followed, the Germans would not have got as far as Stalingrad."" However, Stalin's ability to raise morale and install nationalism should not be underestimated. He was always in propaganda in officer's uniform and was seen as the protector of Russia, a symbol of ultimate power. He was vital to winning the war because he raised patriotism and was seen as a father figure to the nation. In the Great Patriotic War, socialism and communism were forgotten. The state emphasised that it was a fight for Russia as a nation. 'It is necessary to give the party organs who carried out their ideological work their due. They reacted quickly to the mood of the people."" Stalin appealed backwards to the great Princes, such as Alexander Nevsky and great leaders, such as Lenin to gain a sense of patriotism in Russia. Also, people genuinely wanted to believe that there was hope of victory; Stalin gave them this hope. 'During the war years, as the Soviet people were battered by unbelievable miseries, the name of Stalin and faith in him to some degree pulled the Soviet people together, giving them hope of victory."" The cult of Stalin's personality may help to explain how the population survived the horrific invasion of Operation Barbarossa, then endured the hardships of war and come out victorious. Popular belief in Stalin would have made people work harder, more bravely and obey orders. Support for Stalin is believed to have been greater amongst the armed forces over the civilians. Propaganda in the army was more intense. Pictures of Stalin could be seen in army camps. However, naturally there were civilians and soldiers who remained hostile to Stalin and blamed him for the war. Another way the government tried to raise morale and nationalism was through the reintroduction of the Orthodox Church. Religion provided a sense of national pride. This is illustrated in The People's War, which talks about an SD's description of a Minsk clergyman as 'the best propagandist for the German issue."" However, this clergyman changed to support Stalin when Stalin altered his strategy toward the church, meaning support for the Soviet Union was dramatically raised when linked with the church. This is again a combination of government and Soviet people securing war success. Barber and Harrison The Soviet Home Front p. 55 Harrison Accounting for War p. 13 Bonwetsch, Bernd and Thurston, Robert (eds) The People's War: Responses to World War II in the Soviet Union (Urbana, 2000) p. 82 Medvedev, Roy Let History Judge: The Origins and Consequences of Stalinism (New York, 1973) p. 455 Security Service Bonwetsch and Thurston (eds) The People's War p. 19 Patriotism was not just forced upon the nation by the government. The role of spontaneous Soviet patriotism should not be underestimated; there is a clear capacity amongst the Russian people for self-sacrifice. For example, 'soldiers staying put in their trenches while the heavy German tanks were sweeping across them, and then firing them from behind.' Ward Stalin's Russia p. 222 The effort of the individual soldier was also vital. Every soldier had a struggle for survival. Propaganda was successful in installing national pride and hatred of the Nazis, which gave people motivation to fight and people were prepared to work harder and longer hours. People believed they were fighting for a better future for Russia, they wanted to escape the Great Terror. The people were highly motivated to win. Similarly, the role of Russian women is important when considering the Russian people's role in the war. 800,000 young girls and women mobilised in Russia after Operation Barbarossa. They fought hand-to-hand combat as well as the traditional role of cooks and nurses. John Erickson argues that 'there are countless accounts of women's courage and self-sacrifice in the early defeats of Soviet forces."" Women also went down mines and worked in heavy industry, all for the war effort. 'The physical labour was demanding and exhausting."" Women's industrial training expanded and so did the discipline. For example, in Leningrad 500 housewives began work in the Kirov Factory and in Moscow 374,000 housewives begun factory work in August 1941. There was a high demand for war materials, but the total 'manpower' fell by twenty six million. So 800,000 people, including housewives and pensioners, joined the war economy effort by joining a mass voluntary movement at the start of the war. People contributed more to the war victory than the government. Often miners' wives volunteered for work in the mines. However, the Soviet Manpower Committee soon set up compulsory labour forces moving people from light to heavy industries, which means maybe it was government intervention that led to the mass economic production. Garrard, Carol and Garrard, John (eds) World WarII and the Soviet People (Hampshire, 1993) p. 61 Garrard and Garrard (eds) World WarII and the Soviet People p. 53 Garrard and Garrard (eds) World WarII and the Soviet People p. 53 Garrard and Garrard (eds) World WarII and the Soviet People p. 53 Ward argues that the Great Patriotic War as a social event, rather than political, military or economic, is largely undiscovered. 'During and even after the war, virtually the entire Soviet population was united by the drive to expel the Germans."" For example, town squares were used to grow vegetables to feed soldiers and factory workers. Anything was done to help the war effort. Also, in the towns under siege there was barely anything to eat. People ate anything edible such as wallpaper paste; people were highly motivated to survive. Food rations were later introduced to help people get by, therefore there was some government intervention. Acton argues that 'the supreme test of civilian morale and resilience was the 900 day siege of Leningrad. Although engulfed by hunger, cold, lice and death, the city refused to surrender.' House and Glanz When Titans clashed p. 290 Acton Russia p. 250 A way in which the government won the war was by using some previous experience to their advantage. After the attack on Finland in 1939, Semen Timoshenko became Defence Commissar. He implemented a new army training programme, however, the army leadership remained hopelessly disorientated and the strategic thinking remained largely unchanged, which is why the front line units were badly prepared in June 1941. The Red Army performed very badly in the Finnish War despite the fact that they well out numbered the Finnish soldiers. 'Still, individual Red Army soldiers and units displayed remarkable courage and determination. The Finns were deeply impressed with the Soviet soldier's spirit."" The Red Army in 1941 to 1942 still lacked training and equipment, so as in 1939, the soldiers fighting at the front line simply put all their energy into their efforts to fight. This shows that it was the people that won the war. Bonwetsch and Thurston (eds) The People's War p. 238 However, even in the chaos of Operation Barbarossa the system had the ability to mobilise, which is a positive attribute of the government. The government had bred a society who could cope with chaos. The 1930s Five Year Plan had created industrial chaos, this meant that the country was able to cope with the chaos of war. This was known as a constant state of emergency. This means that the war was won by the government's policies, not the people, although the people are obviously involved in these policies. The aid Russia received from Britain and America was vital in the Russian victory over Germany. The money they provided helped feed the Red Army and they provided raw material for military equipment. However, many historians argue that the Russian's victory 'must be attributed solely to the Soviet people and to the iron nerve of Stalin, Zhukov, Shaposhnikov, Vasievsky and their subordinates.' House and Glanz When Titans clashed p. 285 In addition, the role of Hitler must not be ignored when evaluating how Russia won the war. Hitler aimed to defeat an enemy whose industrial output was already comparable to Germany and whose population was twice the size. Also, Hitler's armies were not ready for a winter campaign, thus they were ill-equipped to fight in Russian winter conditions. Germany had not defeated Britain when she attacked Russia and Hitler joined Japan against America before securing reciprocal support from the Japanese against Russia, showing that Germany got caught in a war on many fronts, lessening their chance of victory. In addition, Hitler had unforced errors of racial policy, which led to flaws in Hitler's war effort against Russia. Overall, it appears that the morale of the people kept the war effort going. Although the government created the policies during the war and provided leadership, there is only so far that a government can go without the support of their people. Read discusses the movement of factories in Russia and says 'there was no way such tasks could have been achieved simply by pressing buttons at the centre. Immense hard work, dangerous and slow railway journeys (were required)."" The Russian people seemed to truly believe they could win the war and thought they would have a better future if they won. Thus refused to give up through all the hardship they suffered. Garrard and Garrard conclude that 'this after all, was a ""people's war"" not a ""Party war"".' Read, Chris The Making and Breaking of the Soviet System (Hampshire, 2001) p. 135 Garrard and Garrard (eds) World WarII and the Soviet People p. 70","The Civil War in Russia is often considered as a fight between Reds and Whites. However, many recent historians, such as Radkey, consider the Bolsheviks versus the Greens, a popular movement led by the peasants, as the main Civil War in Russia. How valid is this claim? The Whites were made up of conservatives, landowners, former generals and monarchists. However, because they had such a variety of members there was political disunity amongst them. Although White leaders themselves were strong, there was no one leader of the movement. 'In many ways, the Whites were doomed from the beginning."" Nevertheless, they had the support of foreign countries and held land where there was much grain. However, the Allies support could never have won the Whites the war as 'the Whites...appeared as undignified puppets of Britain and France, while the Bolsheviks seemed independent Russian patriots defending the country against foreign intervention."" The White Army reached a peak of 100,000, compared to the Red Army with over three and a half million members in 1920. It appears that throughout the Civil War the Reds had the upper hand over the Whites, thus the real struggle was not between the Reds and the Whites. Read argues that 'civil war against the Whites was virtually over by the end of 1919."" However, in the south the White threat was especially strong. The Whites had the support of the Cossacks, who had four and a half million members, many of whom were professional soldiers. However, the Cossacks were returning from World War I and only wished to fight to protect their homeland, therefore were not a threat to the Reds across the whole of Russia. Read, C The Making and Breaking of the Soviet System (2001, Hampshire) p. 31 Bradley, J Civil War in Russia 1917 - 1920 (1975, London) p. 178 Read, C From Tsar to Soviets (2001, Eastbourne) p. 258 Much of the Bolshevik strength lay in the land it held. The Reds held highly populated areas, including industrial Petrograd, which they could use for military equipment production and land where railways were already laid, making communication between troops easier. Conversely, the Whites were dispersed across Russia. However, the Reds relied on workers for production and railways, many of whom were members of the Greens. Therefore the Greens could pose a serious threat to the Bolsheviks if they revolted in these areas. The Greens consisted of anarchists, peasants, workers and nationalists. Brovkin says the Greens were 'the most numerous of all parties and movements."" However, the peasants made up eighty percent of the population so this is not surprising. Radkey argues that there were two Civil Wars in Russia. The first being the well-publicised Reds versus Whites and the second was a 'movement of the people itself - and particularly the peasantry - to shake off the tyranny imposed by victorious Red minority."" Reasons for this Green movement were social, political, economic and personal. The peasants had just fought in World War I, they were tired of fighting for others. They wanted to fight for their own cause now. A peasant song from the Civil War illustrates this feeling. Brovkin Behind the Front Lines of the Civil War p.415 Radkey, O The Unknown Civil War in Soviet Russia (1976, California) p. 1 'We'll drive away the Red Army, And take the flour for us. 'Figes, Orlando Peasant Russia, Civil War (1989, Oxford) p. 321 In 1917 many soldiers who previously supported the Bolsheviks, returned to their villages and were reabsorbed into the countryside and joined with the other peasants in their quest for land. Also, Red Army deserters joined the peasants to fight against the Bolsheviks. Thus, the popular movement had support from soldiers who could threaten the Bolsheviks because of their war experiences. The peasants, workers, soldiers and sailors fought in self-defence against the crisis of War Communism occurring in Russia. Peasant food was requisitioned and given to soldiers. If the peasants refused to give up their food the Cheka would brutally intervene. Martyn Latsys, the head of the Cheka, claims there were two hundred and forty five peasant rebellions in 1918 against the intervention of the Cheka. The main areas of popular movements were Nestor Makhno in the Ukraine and Aleksandr Antonov in Tambov Province. Peasants destroyed railways, bridges, ambushed Red Army units and ransacked Soviet military depots. Although War Communism helped the Reds win the Civil War, it was at the cost of the rest of the population. Five million ordinary citizens died in Russia through War Communism. The Bolsheviks had gone against one of their main promises of bread when they ceased power in 1917. 'Some historians believe it was this mistake (of requisitioning) on Lenin's part that led to the civil war against the peasants."" Lenin led a campaign called 'Peace with the Peasants' in 1919 and the Bolshevik - peasant relations looked as though they may improve. However, by 1920 the streets of Russia looked like they had been air raided. The people were starving and it was bitterly cold, they had nothing. Lenin's Secret Police Brovkin Behind the Front Lines of the Civil War p.129 Medvedev in Brovkin Behind the Front Lines of the Civil War p.129 Sometimes the Greens would join the Reds to fight the Whites or vice-versa, but this was always for their own gain. Revisionist historians hold the view that 'The majority of historians, reduce the Civil War to the struggle between the Reds and the Whites... The facts show...that there was a third force upon which fell the biggest blow - the peasant insurrectionist movement. In different periods to a differing degree this movement joined either the Whites or the Reds, but it remained a relatively independent force.' Seliunin in Brovkin Behind the Front Lines of the Civil War p.128 However, Figes argues that 'the defeat of the peasant rebellions was unavoidable' and he claims that the peasantry of central Russia generally remained loyal to the Bolsheviks. Not all the peasantry were united against the Bolsheviks, so the popular movement was not as strong as possible. However, Figes goes on to argue that by 1921, the peasants had been defeated by the Bolsheviks, but had made sufficient gains such as hospitals, a rise in the level of literacy, autonomous communes and elimination of extremes of poverty. These positive aspects surely illustrate that the Bolsheviks must have felt threatened enough by the peasants to fear more uprisings and thus make improvements in the countryside. Figes Peasant Russia, Civil War p. 354 Relations between workers and the State were also dramatically deteriorating. The populations of Moscow fell by a half and Petrograd fell by two thirds between 1917 and 1921, because Lenin wanted one man management and their had been an economic collapse leading to lower bread rations, forcing workers to leave the cities in search of bread; those workers who remained in the cities were demoralised and unreliable. The workers were supposed to be the traditional supporters of the Bolsheviks; the political system was officially defined as a proletariat dictatorship. However the Bolsheviks were moving from being a mass organisation of workers to a body of officials. By 1922 two thirds of the Bolshevik party members were administrators. The Bolsheviks needed the workers support for military and economic reasons as well as a commitment to their original political ideology, meaning workers' discontent with the Bolsheviks was an important struggle for the Reds. Where there was worker unrest the Bolsheviks censored it. However, strikes and riots did take place, more radically in provinces than the capitals. For example, the Aleksandrovskii railway workshops' strike in February 1919. The immediate cause was forty percent of their wages not being paid on time. They also demanded food rations equal to the Red Army soldiers. The Bolsheviks agreed and it was thought that peace would be resumed and the workers would go back to work. This is similar to the 'Peace with the Peasants' campaign, showing that there was a constant struggle among the Bolsheviks and the popular movement. However, the Cheka arrested the leaders of the strike and the workers demanded their release. The economic protest had now turned into a political battle. In June, the Aleksandrovskii railway workers went on strike again and were joined by numerous other railways, who were protesting against the demand that some workers leave and fight for the Red Army. However, 'the growth of the Soviet state, the broadening scope of internal repression and the need for unity against the Whites' reduced the impact of the popular movement in Soviet held areas. 'Part of it (the popular movement) had been absorbed into the party and state, another...against centralisation, and....the largest part, had simply tried to get by in worsening conditions.' Read From Tsar to Soviets p. 260 Read From Tsar to Soviets p. 258 Read From Tsar to Soviets p. 258 The 1920 harvest was a mere sixty percent of the pre-War level, which added to a rise in city and countryside discontent against the Bolsheviks. For example, the Krondstadt Mutiny took place in March 1921. Although it was crushed by the Red Army, their views for free trade of grain, democracy and free speech for socialist parties had been heard. Lenin said that Kronstadt 'was the flash which lit up reality better than anything else."" It lit up the fact that Bolshevism was not going to listen to the people of Russia, and would maintain political control in their own undemocratic way. Lenin thought that the series of peasant wars were 'far more dangerous' than other counter movements. Read From Tsar to Soviets p. 277 Figes Peasant Russia, Civil War p. 321 Propaganda in Russia constantly told the people that the Bolsheviks were in charge of Russia and life would improve. It is difficult to assess how successful the propaganda was because there were few object opinion polls, but the fact that rebellions existed shows that the Bolsheviks did not have control over the population in the way they imagined. However, propaganda may have reduced the threat that popular opinion posed. Although many rebellions posed strong threats it is unlikely that the popular movement would have defeated the Bolsheviks, because they had many spontaneous uprisings and the peasants and workers were spread across Russia with no one leader and the Bolsheviks, especially by the end of 1919, were very strong. 'The Bolsheviks must have been a tiny minority of the Russian population, which through superior organisation, ruthless determination and efficient propaganda snatched victory from an inefficient, divided and silent majority.' Bradley Civil War in Russia p. 184 After defeating the White Army, the Bolsheviks were forced to 'surrender before its own peasantry."" The New Economic Policy was introduced because of the threat mass unpopularity caused to the Bolsheviks, illustrating that the greater struggle was with popular movement because the Bolsheviks were forced to make changes to please the Greens. Although the Whites did pose a threat to the Reds, they were divided, lacked morale and had weak communication lines. The Green and Red fight lasted longer, therefore there was more of a 'struggle'. Brovkin argues that 'military and strategic factors are far less important than popular attitudes in a civil war."" The Red and White war was a far more military and strategic war, and the Green and Red a war of popular opinion. Brovkin concludes that 'the available evidence suggests...the magnitude of the Bolshevik war against the peasants on the internal front eclipsed by far the frontline civil war against the Whites.' Figes Peasant Russia, Civil War p. 321 Brovkin Behind the Front Lines of the Civil War p.91 Brovkin Behind the Front Lines of the Civil War p.127",False
76,"It has long been recognised that emotions exert powerful effects on memory, and that what we feel can strongly influence what we remember. The types of effects that emotion can have on memory differ markedly, depending on the precise conditions of encoding and retrieval (Eich, Kihlstrom, Bower, Forgas & Miedenthal, 2000). Differences in emphasis during encoding, for example, can have a profound effect on how, or indeed whether, information is remembered later (Gleitman, 1999) and similarly, the emotional state of a person at the time of retrieval may influence what they remember. Two central questions have arisen out of the study of the impact of emotion on memory; firstly, whether emotion increases or decreases the strength of memory for an event, and secondly, whether special mechanisms are required to account for the effects of emotion on memory (Eich et al, 2000). This essay will examine ways in which people's memories are affected by their feelings, and will also investigate models put forward as explanations as to why emotion has such an influence on memory. Of particular significance with respect to emotional impact at the encoding stage of memory formation are the concepts of flashbulb memory and repression. It is proposed that if an event arouses an intense emotion in someone, this may influence the likelihood that they will remember the event. The direction of such influence on encoding may be either positive, so that it creates a strong memory trace, or negative, such that powerful negative emotions may induce forgetting or repression. Flashbulb memories can be used to illustrate how emotion is often alleged to enhance memory (Eich et al, 2000). The term 'flashbulb memory' was first introduced by Brown and Kulik (1977) and is used to depict the vivid, accurate and detailed memories that people have for major news events; such as the assassination of President John F. Kennedy. Flashbulb memories concern events that are highly distinctive, unexpected and strongly emotional, and they are characterised by a personalised focus; people remember exactly what they were doing when they heard the news (Gleitman, 1999). Brown and Kulik proposed that sudden news which produces a strong emotional reaction can fix permanently in a person's memory many details of their immediate circumstances at that point. Whilst they argued that the level of detail and longevity of these memories suggests there may be a special neural mechanism for the creation of flashbulb memories (Brown and Kulik, 1977, as cited in Gleitman, 1999), this view is controversial and it is generally argued that the high retention and characteristic longevity of such memories is created by the emotional impact of the event on the individual, and increased rehearsal in subsequent conversations with others, rather than a specific encoding mechanism. Repression, as proposed by Freud, provides a direct contrast to flashbulb memories, concerning memories that, due to their unpleasant, negative emotional associations (Baddeley, 1997), are 'forgotten', or pushed out of conscious awareness, thus becoming inaccessible. Certainly, on considering the occurrence of psychogenic amnesia, there is no doubt that powerful negative emotions can induce disturbances of memory, but this does not necessarily imply that repression forms a part of normal forgetting (Baddeley, 1997), and there is little experimental evidence to support this claim, primarily because it is difficult to carry out laboratory experiments that manipulate the level of emotion due to limits of what is ethically acceptable. In everyday life, there does appear to be a bias towards recalling pleasant memories rather than preserving unpleasant ones, but this does not infer repression, since people simply tend to rehearse pleasant things that happen to them, whereas embarrassing and painful things are not pleasant to recount (Baddeley, 1997). It therefore seems unlikely that a significant role in everyday forgetting can be attributed to repression. In addition to encoding, a further effect on memory may be observed with respect to retrieval, in terms of mood-congruent retrieval, and mood-state-dependent learning. The concept of mood congruity refers to the idea that ""emotionally toned information is learned best when there is correspondence between its affective value and the learner's current mood state"" (Eysenck & Keane, 2000). Thus people are sensitised to absorb information that agrees with their prevailing emotional state (Eich et al, 2000); people in a good mood learn and remember emotionally positive material better than those in a bad mood, whereas the opposite is true for emotionally negative material. Mood congruent retrieval suggests that emotional state influences the later retrieval of affectively congruent material from memory (Eich et al, 2000). Lloyd and Lishman (1975) originally demonstrated mood congruent retrieval, testing clinically depressed and non-depressed subjects for retrieval speed of a pleasant or unpleasant memory linked to a given neutral cue word (Eich et al ,2000). It was found that depressed subjects retrieved unpleasant memories faster than pleasant ones, with the most depressed patients having the fastest retrieval times, whereas non-depressed patients were quicker at retrieving pleasant memories. A similar result was found in the non-clinical population by Teasdale and Forgarty (1979, as cited in Eich et al, 2000) who induced elated or depressed moods artificially and found that when depressed recall of positive material was slowed whilst recall of negative material was accelerated. Furthermore, Teasdale and Russell (1983, as cited in Eich et al, 2000) had subjects in a neutral mood study a mixed list of positive, neutral and negative words. A happy or sad mood was then induced in subjects, who subsequently attempted to recall the word list. It was reported that although the groups recalled neutral words at similar rates, sad subjects remembered more negative than positive words, and happy subjects vice versa. Since no specific mood was induced during original learning of the word list this effect is claimed to be solely due to mood at retrieval (Eich et al, 2000). There therefore appears to be strong evidence for the phenomenon of mood-congruent retrieval. In contrast, evidence for mood-state dependent learning and memory is somewhat more equivocal. This suggests that a person's emotional context affects their memory for events, and that if people learn something in a given mood or emotional state, they can best remember it later, at the time of recall, if they return to the same mood (Eysenck & Keane, 2000). Bower, Monteiro and Gilligan originally investigated this, using hypnosis to induce happy or sad mood states, and reporting that items experienced in a sad mood were much more likely to be subsequently retrieved when sad than items experienced in a happy mood (Baddeley, 1997). However, many of their findings have since been attributed to mood congruence (Baddeley, 1997) since they used emotionally toned material, and it has subsequently proved difficult to show mood dependency in terms of the effect of mood on learning and subsequent recall of neutral material (Baddeley, 1999). On the other hand, mood dependent memory has been more successfully demonstrated in the recall of autobiographic memories, , with Bower (1981, as cited in Eich et al, 2000) reporting that when subjects are hypnotically induced to experience happy or sad moods and asked to recall incidents from their childhood, their memories were predominantly consistent with their mood. In addition, recollections of emotional events recorded in diaries have been found to be significantly biased in the direction of the subject's current mood state (Bower, 1981, as cited in Eich et al, 2000). Bower's associative network theory is one model which explains why emotions may affect memory and thus how mood-state-dependent learning and mood congruent retrieval occur. This theory proposes that emotions are represented by nodes in a semantic network, with numerous connections to related ideas, physiological systems, to events and to muscular and expressive patterns (Eysenck & Keane, 2000). Nodes can be activated by internal or external stimuli, and activation from an activated node spreads to related nodes; thus activation of an emotion node results in activation of emotion related nodes or concepts in the semantic network. When an emotion is aroused, the emotion node spreads excitation to indicators with which it is connected; for example, a set of memories of events that had become associated with, and causally evoked, that emotion in the past (Eich et al, 2000). Retrieval occurs by activation spreading along the network from one node to another; so, for example, activation of depression facilitates remembering of an event linked to that emotion. This theory initially suggested that if a subject studies a list of words in a sad mood, those words should be better recalled later when the same emotion is induced for recall - however, this was often not found to be the case. Thus Bower's original model was later revised to include the causal belonging hypothesis. This suggests that subjects will only associate an emotion with a stimulus or situation if they see the event as causing their mood; thus laboratory studies produce only weak mood dependence because the experiment only induces temporal contiguity; thus subjects do not attribute emotional feelings to the words studied (Eich et al, 2000). In contrast, autobiographical memories show mood dependence because the emotions causally belonged originally to the events being recalled, and the event-to-emotion situation is established strongly at the initial experience (Eich et al, 2000). In addition, a further principle which explains the impact of emotion on memory is Tulving's encoding specificity principle, which states that the recollection of an event depends on the interaction between the properties of an encoded event and the retrieval information. Thus, if a mood is encoded as part of the learning experience, providing this cue and reinstating the mood during retrieval enhances memory performance (Eysenck & Keane, 2000). It is therefore clear that our memories are influenced by our feelings in numerous different ways. Memories with strong emotional associations can be either encoded strongly and in much detail, or repressed, whilst both the emotional tone of material and a person's mood state can affect what is later remembered. For this reason there is much evidence to support the idea that emotion and cognition are closely connected","The term 'meme' was first introduced by Richard Dawkins in 1976. Dawkins popularised the influential view that evolution is best understood in terms of competition between genes (Blackmore, 1999), and in his conception of memes he takes this one step further, drawing on the analogy between genetic and cultural evolution to claim that culture can be split into units called memes; ideas, habits, skills, stories or any kind of behaviour or information that is copied from person to person by imitation (Dawkins, 1976). Memes are the replicators of the cultural world; the smallest elements of culture or complex ideas that replicate themselves with reliability and fecundity (Dennett, 1993). Like genes, memes are selfish. They pursue their own interests and are concerned only with their survival and replication. Since each mind has a limited capacity for memes, there is considerable competition among memes for entry into as many minds as possible (Ball, 1984). Dawkins urges us to take the idea of meme evolution literally; it is not just analogous to biological or genetic evolution (Dennett, 1990), but a phenomenon that obeys the laws of natural selection exactly, and is achieving evolutionary change at a rate that leaves conventional genetic evolution far behind (Dawkins, 1976). One of the most comprehensive works on memes is Blackmore's (1999) The Meme Machine, which uses memes to elaborate an ambitious theory designed to account for numerous aspects of human evolution and psychology (Jahoda, 2002). Blackmore claims that imitation, as the primary mechanism by which memes replicate themselves, can be taken as the basis of a major theory of the evolution of the human mind and even of what it means to be a conscious self. Imitation is posited as the key to what set our ancestors apart from all other animals. The theory starts with one simple mechanism - the competition between memes to get into human brains and be passed on again - and from this it gives rise to explanation for such diverse phenomena as the evolution of the enormous human brain, the origins of language, human altruism, sex, religion, and even the evolution of the internet (Blackmore, 1999). Blackmore's theory is certainly well argued, and her justifications of both the meme-gene analogy and how memes drive many aspects of human development are theoretically sound. However, I believe there may be an initial flaw in Blackmore's argument in that she appears to take the existence of memes for granted and expounds her detailed argument from this premise when this fact is not necessarily clear. It may be contended that if one refuses to acknowledge the reality of memes, Blackmore's argument fails. One can question whether cultural evolution is truly analogous to genetic evolution (Dennett, 1995), whether there are really cultural units that have interests of their own, and whether imitation is indeed the driving force behind all human behaviour (Holdcroft & Lewis, 2001). Midgley (2000), for example, calls memes 'mythical entities' and an 'empty and misleading metaphor'. She argues that thought and culture cannot be broken down into distinct units, since they are moving and developing patterns in human behaviour. There is therefore no use in trying to understand them by tracing reproductive interactions among memetic units Midgley, 1999). Nonetheless, if, for now, one accepts the existence of memes and ignores such criticism, Blackmore's memetic model provides feasible and coherent explanations of a diverse range of phenomena that have traditionally proved difficult to explain in terms of biological evolution or individual genetic advantage (Edmonds, 2002). It has been argued that the development of human language, for example, does not conform to a Darwinian evolutionary explanation since it shows no genetic variation or selective advantage and cannot exist in intermediate forms (Blackmore, 1999). The same can be said of the excessively large human brain, which stands out greatly in comparisons of encephalisation with other primates. Most early theories suggest that toolmaking, technological advances and the human social environment, drove the need for a larger brain, whilst language is explained as a non-adaptive side consequence of the evolution of the brain, or as evolving as a response to selection pressure for improved communication between humans (Gould, 1997). However, Blackmore presents a memetic explanation. Memes compete to be copied, and people who are especially good at imitation gain a survival advantage by being able to copy the currently most useful memes. Since imitation is a difficult skill requiring extra brain power, the advantage is given to genes for bigger brains and better imitation. The genes were therefore forced into creating big brains, capable of spreading memes. Blackmore also proposes that the human language faculty primarily provided a selective advantage to memes, rather than genes. Successful replicators are those with high fidelity, longevity and fecundity. Digitisation of sounds into words may increase fidelity, combining words into novel combinations may improve fecundity, and every improvement leads to increased memetic competition. The memes that can get themselves spoken will be copied more than those that cannot, and people who can best copy the winning sounds have an advantage and pass on the genes that gave them that ability. Gradually, human brains would be driven by this emerging language. The development of language was thus an evolutionary process like any other, creating complex design apparently out of nowhere (Blackmore, 2003). A further problem Blackmore tackles is that of altruism, which conflicts with traditional understanding that humans selfishly pursue their own interests. Traditionally altruism has been explained either in terms of advantage to the genes; you are altruistic towards your children because they are the only direct way your genes can be carried on into future generations; you are altruistic toward fellow men because you may get aid in return, or in terms of a true morality, an independent moral conscience or a spiritual essence in humans that overcomes the selfish dictates of our genes; a view which finds little favour with scientists (Blackmore, 1999). Memetics provides a third solution. Blackmore suggests that the kind, generous, altruistic person will spread more memes. He will have more friends and spend more time talking to them; they will be influenced by him and in turn will imitate his popular behaviour. Thus his altruism will spread. In contrast, the mean and selfish person will have few chances to replicate his memes because the few people who could potentially imitate him rarely do so. Blackmore provides a number of such examples of behaviours that are mimetically adaptive but biologically unadaptive (Cook, 1995); for example, she claims the rise in the use of birth control, whilst not genetically advantageous, means women who have fewer children will have more time to spread their own memes, including memes for birth control and the pleasures of a small family. These are the women whose success inspires others, and who provide role models for other women to copy. Furthermore, whilst adoption may be viewed as a mistake from the gene's point of view, for memes the benefits of adoption are obvious. As far as memes are concerned, the time and effort expended on an adopted child are as valuable as that expended on one's own offspring. All that matters is that you can pass on your memes to people who will be influenced by you and subsequently imitate your behaviours and beliefs. Genetic relation is of no importance. In both of these examples there is a battle between memes and genes to take control over the machinery of replication (Blackmore, 1999). All of these arguments therefore appear to be perfectly logical and acceptable; however, as previously argued, I believe they are only acceptable if one first accepts the existence of memes. If one refuses to believe that such elements of culture exist or compete to get themselves replicated through imitation, there is no reason to believe Blackmore's theory. Furthermore, I believe that Blackmore goes too far in attempting to apply memes to areas such as mate selection, and in implying that memes are even responsible for our sense of a conscious self. Traditional genetic evolutionary theory suggests we choose mates based on fitness and biological advantage; men are sexually attracted to women who appear fertile, with big hips and youthful, symmetrical features, whilst women are attracted to strong, high-status men with plentiful resources. Essentially we should choose to mate with people who would, in the environment of our evolutionary past, have helped to increase our genetic legacy (Blackmore, 1999). Blackmore suggests mate choice is based not only on genetic advantage, but also memetic advantage. Once memes arose in our far past, natural selection would favour people who chose to mate with the best imitators. In an early hunter-gatherer society a man who was especially good at imitation would have been able to copy the latest hunting skills or stone tool technology and hence would have gained a biological advantage. Thus a woman who mated with him would be more likely to have children who shared that imitation ability and that advantage. Such a woman would have to look in potential mates for signs of the general ability to imitate and innovate. This argument suggests we will still want to mate with the best imitators and spreaders of memes, such as writers, artists, journalists, broadcasters, film stars and musicians (Blackmore, 1999). However, whilst it may be the case that some high profile people who are prominent in the media are often seen as desirable mates, this is not true for all such people, and I do not think it can be reasonably argued that we choose mates on the basis of whether they are a good imitator. Firstly, we arguably do not know what the signs of a good imitator are, and secondly, in being attracted to someone we tend to consider factors such as their looks and personality; we are certainly not aware of considering their potential for spreading memes. Furthermore I believe that Blackmore's conclusion regarding our concept of the conscious self is largely unbelievable. Blackmore claims that the idea of the continuous, persistent and autonomous self is an illusion; the self is actually just a collection of memes. Memes gain an advantage by becoming associated with a person's self-concept. A 'self' therefore aids their replication. The sense of 'self' has simply been created by the processes of memetic evolution acting in the relatively short period of one human lifetime. Blackmore then goes even further and says that the ways we behave, the choices we make, and the things we say are all a result of complex interactions between memes. There is no inner self which is the initiator of actions, controls the body, or is conscious; thus there is no free will. Blackmore presents an eliminative account of design, mind, and human agency which removes the possibility of voluntary choice or invention on the part of humans (Holdcroft & Lewis, 2001). Intentionality is transferred away from humans and to the memes. Whilst Blackmore has tried to argue against the fact that her argument makes humans completely passive, stating that our intelligence, capacity for making choices, and active social life are all part of the copying environment in which memes compete (Blackmore, 2002), it is hard to see that this could imply anything other than that we are caught up in ""a remorseless process in which we are mere pawns"" (Jahoda, 2002). I find this aspect of Blackmore's argument hard to accept since it contradicts our very experience of thought and conscious deliberation, and violates the assumption that in making choices we truly have free will and are not influenced or bound by either internal or external forces. Overall I would argue that The Meme Machine provides a detailed and interesting account of the concept of memes, and certainly does provide some plausible theories that appear to be explained better by memetics than any other theory we currently have available. However, I believe that in attempting to use memes to explain all aspects of human nature and behaviour, Blackmore takes memetic theory beyond logical boundaries. Furthermore, Blackmore restricts memetic transmission to imitation, claiming that imitation is what sets us apart from other animals; however Tomasello (1999) argues that whilst imitation has played a role in cultural evolution the critical difference is in fact the understanding of others as intentional beings like the self. Thus, if the basic principle of cultural learning being driven solely by imitation is rejected the foundations of The Meme Machine collapse and there is no reason why we should accept any of Blackmore's arguments. Critics also argue that just as a gene is nothing without its biological and environmental context the same is true for memes (Wheelwell, 1998; Dupre, 2000), and indeed it is difficult to conceive of memes as cultural units residing in brains that exist independently of contextual interpretation. However, proponents of memetics argue psychology will be as transformed by this discipline as biology was by Darwin (Brodie, 1996), and this perhaps is where the problem lies in getting people to accept memetic theory. In acknowledging the existence of these cultural replicators we have to radically transform our ideas regarding human nature, behaviour and motivation and reformulate our understanding of how and why culture evolved, just as Darwin caused us to rethink our beliefs about the origin of our species. Maybe we are too scared to accept we do not have consciousness, free will, or the possibility of rational choice but rather are completely at the mercy of the interaction between our memes and our genes? Whether the theory of memes is true or not, The Meme Machine provides a comprehensive and detailed insight into how memes may shape human development which, although somewhat unbelievable in parts, is certainly an interesting and enlightening read.",True
77,"The term 'meme' was first introduced by Richard Dawkins in 1976. Dawkins popularised the influential view that evolution is best understood in terms of competition between genes (Blackmore, 1999), and in his conception of memes he takes this one step further, drawing on the analogy between genetic and cultural evolution to claim that culture can be split into units called memes; ideas, habits, skills, stories or any kind of behaviour or information that is copied from person to person by imitation (Dawkins, 1976). Memes are the replicators of the cultural world; the smallest elements of culture or complex ideas that replicate themselves with reliability and fecundity (Dennett, 1993). Like genes, memes are selfish. They pursue their own interests and are concerned only with their survival and replication. Since each mind has a limited capacity for memes, there is considerable competition among memes for entry into as many minds as possible (Ball, 1984). Dawkins urges us to take the idea of meme evolution literally; it is not just analogous to biological or genetic evolution (Dennett, 1990), but a phenomenon that obeys the laws of natural selection exactly, and is achieving evolutionary change at a rate that leaves conventional genetic evolution far behind (Dawkins, 1976). One of the most comprehensive works on memes is Blackmore's (1999) The Meme Machine, which uses memes to elaborate an ambitious theory designed to account for numerous aspects of human evolution and psychology (Jahoda, 2002). Blackmore claims that imitation, as the primary mechanism by which memes replicate themselves, can be taken as the basis of a major theory of the evolution of the human mind and even of what it means to be a conscious self. Imitation is posited as the key to what set our ancestors apart from all other animals. The theory starts with one simple mechanism - the competition between memes to get into human brains and be passed on again - and from this it gives rise to explanation for such diverse phenomena as the evolution of the enormous human brain, the origins of language, human altruism, sex, religion, and even the evolution of the internet (Blackmore, 1999). Blackmore's theory is certainly well argued, and her justifications of both the meme-gene analogy and how memes drive many aspects of human development are theoretically sound. However, I believe there may be an initial flaw in Blackmore's argument in that she appears to take the existence of memes for granted and expounds her detailed argument from this premise when this fact is not necessarily clear. It may be contended that if one refuses to acknowledge the reality of memes, Blackmore's argument fails. One can question whether cultural evolution is truly analogous to genetic evolution (Dennett, 1995), whether there are really cultural units that have interests of their own, and whether imitation is indeed the driving force behind all human behaviour (Holdcroft & Lewis, 2001). Midgley (2000), for example, calls memes 'mythical entities' and an 'empty and misleading metaphor'. She argues that thought and culture cannot be broken down into distinct units, since they are moving and developing patterns in human behaviour. There is therefore no use in trying to understand them by tracing reproductive interactions among memetic units Midgley, 1999). Nonetheless, if, for now, one accepts the existence of memes and ignores such criticism, Blackmore's memetic model provides feasible and coherent explanations of a diverse range of phenomena that have traditionally proved difficult to explain in terms of biological evolution or individual genetic advantage (Edmonds, 2002). It has been argued that the development of human language, for example, does not conform to a Darwinian evolutionary explanation since it shows no genetic variation or selective advantage and cannot exist in intermediate forms (Blackmore, 1999). The same can be said of the excessively large human brain, which stands out greatly in comparisons of encephalisation with other primates. Most early theories suggest that toolmaking, technological advances and the human social environment, drove the need for a larger brain, whilst language is explained as a non-adaptive side consequence of the evolution of the brain, or as evolving as a response to selection pressure for improved communication between humans (Gould, 1997). However, Blackmore presents a memetic explanation. Memes compete to be copied, and people who are especially good at imitation gain a survival advantage by being able to copy the currently most useful memes. Since imitation is a difficult skill requiring extra brain power, the advantage is given to genes for bigger brains and better imitation. The genes were therefore forced into creating big brains, capable of spreading memes. Blackmore also proposes that the human language faculty primarily provided a selective advantage to memes, rather than genes. Successful replicators are those with high fidelity, longevity and fecundity. Digitisation of sounds into words may increase fidelity, combining words into novel combinations may improve fecundity, and every improvement leads to increased memetic competition. The memes that can get themselves spoken will be copied more than those that cannot, and people who can best copy the winning sounds have an advantage and pass on the genes that gave them that ability. Gradually, human brains would be driven by this emerging language. The development of language was thus an evolutionary process like any other, creating complex design apparently out of nowhere (Blackmore, 2003). A further problem Blackmore tackles is that of altruism, which conflicts with traditional understanding that humans selfishly pursue their own interests. Traditionally altruism has been explained either in terms of advantage to the genes; you are altruistic towards your children because they are the only direct way your genes can be carried on into future generations; you are altruistic toward fellow men because you may get aid in return, or in terms of a true morality, an independent moral conscience or a spiritual essence in humans that overcomes the selfish dictates of our genes; a view which finds little favour with scientists (Blackmore, 1999). Memetics provides a third solution. Blackmore suggests that the kind, generous, altruistic person will spread more memes. He will have more friends and spend more time talking to them; they will be influenced by him and in turn will imitate his popular behaviour. Thus his altruism will spread. In contrast, the mean and selfish person will have few chances to replicate his memes because the few people who could potentially imitate him rarely do so. Blackmore provides a number of such examples of behaviours that are mimetically adaptive but biologically unadaptive (Cook, 1995); for example, she claims the rise in the use of birth control, whilst not genetically advantageous, means women who have fewer children will have more time to spread their own memes, including memes for birth control and the pleasures of a small family. These are the women whose success inspires others, and who provide role models for other women to copy. Furthermore, whilst adoption may be viewed as a mistake from the gene's point of view, for memes the benefits of adoption are obvious. As far as memes are concerned, the time and effort expended on an adopted child are as valuable as that expended on one's own offspring. All that matters is that you can pass on your memes to people who will be influenced by you and subsequently imitate your behaviours and beliefs. Genetic relation is of no importance. In both of these examples there is a battle between memes and genes to take control over the machinery of replication (Blackmore, 1999). All of these arguments therefore appear to be perfectly logical and acceptable; however, as previously argued, I believe they are only acceptable if one first accepts the existence of memes. If one refuses to believe that such elements of culture exist or compete to get themselves replicated through imitation, there is no reason to believe Blackmore's theory. Furthermore, I believe that Blackmore goes too far in attempting to apply memes to areas such as mate selection, and in implying that memes are even responsible for our sense of a conscious self. Traditional genetic evolutionary theory suggests we choose mates based on fitness and biological advantage; men are sexually attracted to women who appear fertile, with big hips and youthful, symmetrical features, whilst women are attracted to strong, high-status men with plentiful resources. Essentially we should choose to mate with people who would, in the environment of our evolutionary past, have helped to increase our genetic legacy (Blackmore, 1999). Blackmore suggests mate choice is based not only on genetic advantage, but also memetic advantage. Once memes arose in our far past, natural selection would favour people who chose to mate with the best imitators. In an early hunter-gatherer society a man who was especially good at imitation would have been able to copy the latest hunting skills or stone tool technology and hence would have gained a biological advantage. Thus a woman who mated with him would be more likely to have children who shared that imitation ability and that advantage. Such a woman would have to look in potential mates for signs of the general ability to imitate and innovate. This argument suggests we will still want to mate with the best imitators and spreaders of memes, such as writers, artists, journalists, broadcasters, film stars and musicians (Blackmore, 1999). However, whilst it may be the case that some high profile people who are prominent in the media are often seen as desirable mates, this is not true for all such people, and I do not think it can be reasonably argued that we choose mates on the basis of whether they are a good imitator. Firstly, we arguably do not know what the signs of a good imitator are, and secondly, in being attracted to someone we tend to consider factors such as their looks and personality; we are certainly not aware of considering their potential for spreading memes. Furthermore I believe that Blackmore's conclusion regarding our concept of the conscious self is largely unbelievable. Blackmore claims that the idea of the continuous, persistent and autonomous self is an illusion; the self is actually just a collection of memes. Memes gain an advantage by becoming associated with a person's self-concept. A 'self' therefore aids their replication. The sense of 'self' has simply been created by the processes of memetic evolution acting in the relatively short period of one human lifetime. Blackmore then goes even further and says that the ways we behave, the choices we make, and the things we say are all a result of complex interactions between memes. There is no inner self which is the initiator of actions, controls the body, or is conscious; thus there is no free will. Blackmore presents an eliminative account of design, mind, and human agency which removes the possibility of voluntary choice or invention on the part of humans (Holdcroft & Lewis, 2001). Intentionality is transferred away from humans and to the memes. Whilst Blackmore has tried to argue against the fact that her argument makes humans completely passive, stating that our intelligence, capacity for making choices, and active social life are all part of the copying environment in which memes compete (Blackmore, 2002), it is hard to see that this could imply anything other than that we are caught up in ""a remorseless process in which we are mere pawns"" (Jahoda, 2002). I find this aspect of Blackmore's argument hard to accept since it contradicts our very experience of thought and conscious deliberation, and violates the assumption that in making choices we truly have free will and are not influenced or bound by either internal or external forces. Overall I would argue that The Meme Machine provides a detailed and interesting account of the concept of memes, and certainly does provide some plausible theories that appear to be explained better by memetics than any other theory we currently have available. However, I believe that in attempting to use memes to explain all aspects of human nature and behaviour, Blackmore takes memetic theory beyond logical boundaries. Furthermore, Blackmore restricts memetic transmission to imitation, claiming that imitation is what sets us apart from other animals; however Tomasello (1999) argues that whilst imitation has played a role in cultural evolution the critical difference is in fact the understanding of others as intentional beings like the self. Thus, if the basic principle of cultural learning being driven solely by imitation is rejected the foundations of The Meme Machine collapse and there is no reason why we should accept any of Blackmore's arguments. Critics also argue that just as a gene is nothing without its biological and environmental context the same is true for memes (Wheelwell, 1998; Dupre, 2000), and indeed it is difficult to conceive of memes as cultural units residing in brains that exist independently of contextual interpretation. However, proponents of memetics argue psychology will be as transformed by this discipline as biology was by Darwin (Brodie, 1996), and this perhaps is where the problem lies in getting people to accept memetic theory. In acknowledging the existence of these cultural replicators we have to radically transform our ideas regarding human nature, behaviour and motivation and reformulate our understanding of how and why culture evolved, just as Darwin caused us to rethink our beliefs about the origin of our species. Maybe we are too scared to accept we do not have consciousness, free will, or the possibility of rational choice but rather are completely at the mercy of the interaction between our memes and our genes? Whether the theory of memes is true or not, The Meme Machine provides a comprehensive and detailed insight into how memes may shape human development which, although somewhat unbelievable in parts, is certainly an interesting and enlightening read.","It has long been recognised that emotions exert powerful effects on memory, and that what we feel can strongly influence what we remember. The types of effects that emotion can have on memory differ markedly, depending on the precise conditions of encoding and retrieval (Eich, Kihlstrom, Bower, Forgas & Miedenthal, 2000). Differences in emphasis during encoding, for example, can have a profound effect on how, or indeed whether, information is remembered later (Gleitman, 1999) and similarly, the emotional state of a person at the time of retrieval may influence what they remember. Two central questions have arisen out of the study of the impact of emotion on memory; firstly, whether emotion increases or decreases the strength of memory for an event, and secondly, whether special mechanisms are required to account for the effects of emotion on memory (Eich et al, 2000). This essay will examine ways in which people's memories are affected by their feelings, and will also investigate models put forward as explanations as to why emotion has such an influence on memory. Of particular significance with respect to emotional impact at the encoding stage of memory formation are the concepts of flashbulb memory and repression. It is proposed that if an event arouses an intense emotion in someone, this may influence the likelihood that they will remember the event. The direction of such influence on encoding may be either positive, so that it creates a strong memory trace, or negative, such that powerful negative emotions may induce forgetting or repression. Flashbulb memories can be used to illustrate how emotion is often alleged to enhance memory (Eich et al, 2000). The term 'flashbulb memory' was first introduced by Brown and Kulik (1977) and is used to depict the vivid, accurate and detailed memories that people have for major news events; such as the assassination of President John F. Kennedy. Flashbulb memories concern events that are highly distinctive, unexpected and strongly emotional, and they are characterised by a personalised focus; people remember exactly what they were doing when they heard the news (Gleitman, 1999). Brown and Kulik proposed that sudden news which produces a strong emotional reaction can fix permanently in a person's memory many details of their immediate circumstances at that point. Whilst they argued that the level of detail and longevity of these memories suggests there may be a special neural mechanism for the creation of flashbulb memories (Brown and Kulik, 1977, as cited in Gleitman, 1999), this view is controversial and it is generally argued that the high retention and characteristic longevity of such memories is created by the emotional impact of the event on the individual, and increased rehearsal in subsequent conversations with others, rather than a specific encoding mechanism. Repression, as proposed by Freud, provides a direct contrast to flashbulb memories, concerning memories that, due to their unpleasant, negative emotional associations (Baddeley, 1997), are 'forgotten', or pushed out of conscious awareness, thus becoming inaccessible. Certainly, on considering the occurrence of psychogenic amnesia, there is no doubt that powerful negative emotions can induce disturbances of memory, but this does not necessarily imply that repression forms a part of normal forgetting (Baddeley, 1997), and there is little experimental evidence to support this claim, primarily because it is difficult to carry out laboratory experiments that manipulate the level of emotion due to limits of what is ethically acceptable. In everyday life, there does appear to be a bias towards recalling pleasant memories rather than preserving unpleasant ones, but this does not infer repression, since people simply tend to rehearse pleasant things that happen to them, whereas embarrassing and painful things are not pleasant to recount (Baddeley, 1997). It therefore seems unlikely that a significant role in everyday forgetting can be attributed to repression. In addition to encoding, a further effect on memory may be observed with respect to retrieval, in terms of mood-congruent retrieval, and mood-state-dependent learning. The concept of mood congruity refers to the idea that ""emotionally toned information is learned best when there is correspondence between its affective value and the learner's current mood state"" (Eysenck & Keane, 2000). Thus people are sensitised to absorb information that agrees with their prevailing emotional state (Eich et al, 2000); people in a good mood learn and remember emotionally positive material better than those in a bad mood, whereas the opposite is true for emotionally negative material. Mood congruent retrieval suggests that emotional state influences the later retrieval of affectively congruent material from memory (Eich et al, 2000). Lloyd and Lishman (1975) originally demonstrated mood congruent retrieval, testing clinically depressed and non-depressed subjects for retrieval speed of a pleasant or unpleasant memory linked to a given neutral cue word (Eich et al ,2000). It was found that depressed subjects retrieved unpleasant memories faster than pleasant ones, with the most depressed patients having the fastest retrieval times, whereas non-depressed patients were quicker at retrieving pleasant memories. A similar result was found in the non-clinical population by Teasdale and Forgarty (1979, as cited in Eich et al, 2000) who induced elated or depressed moods artificially and found that when depressed recall of positive material was slowed whilst recall of negative material was accelerated. Furthermore, Teasdale and Russell (1983, as cited in Eich et al, 2000) had subjects in a neutral mood study a mixed list of positive, neutral and negative words. A happy or sad mood was then induced in subjects, who subsequently attempted to recall the word list. It was reported that although the groups recalled neutral words at similar rates, sad subjects remembered more negative than positive words, and happy subjects vice versa. Since no specific mood was induced during original learning of the word list this effect is claimed to be solely due to mood at retrieval (Eich et al, 2000). There therefore appears to be strong evidence for the phenomenon of mood-congruent retrieval. In contrast, evidence for mood-state dependent learning and memory is somewhat more equivocal. This suggests that a person's emotional context affects their memory for events, and that if people learn something in a given mood or emotional state, they can best remember it later, at the time of recall, if they return to the same mood (Eysenck & Keane, 2000). Bower, Monteiro and Gilligan originally investigated this, using hypnosis to induce happy or sad mood states, and reporting that items experienced in a sad mood were much more likely to be subsequently retrieved when sad than items experienced in a happy mood (Baddeley, 1997). However, many of their findings have since been attributed to mood congruence (Baddeley, 1997) since they used emotionally toned material, and it has subsequently proved difficult to show mood dependency in terms of the effect of mood on learning and subsequent recall of neutral material (Baddeley, 1999). On the other hand, mood dependent memory has been more successfully demonstrated in the recall of autobiographic memories, , with Bower (1981, as cited in Eich et al, 2000) reporting that when subjects are hypnotically induced to experience happy or sad moods and asked to recall incidents from their childhood, their memories were predominantly consistent with their mood. In addition, recollections of emotional events recorded in diaries have been found to be significantly biased in the direction of the subject's current mood state (Bower, 1981, as cited in Eich et al, 2000). Bower's associative network theory is one model which explains why emotions may affect memory and thus how mood-state-dependent learning and mood congruent retrieval occur. This theory proposes that emotions are represented by nodes in a semantic network, with numerous connections to related ideas, physiological systems, to events and to muscular and expressive patterns (Eysenck & Keane, 2000). Nodes can be activated by internal or external stimuli, and activation from an activated node spreads to related nodes; thus activation of an emotion node results in activation of emotion related nodes or concepts in the semantic network. When an emotion is aroused, the emotion node spreads excitation to indicators with which it is connected; for example, a set of memories of events that had become associated with, and causally evoked, that emotion in the past (Eich et al, 2000). Retrieval occurs by activation spreading along the network from one node to another; so, for example, activation of depression facilitates remembering of an event linked to that emotion. This theory initially suggested that if a subject studies a list of words in a sad mood, those words should be better recalled later when the same emotion is induced for recall - however, this was often not found to be the case. Thus Bower's original model was later revised to include the causal belonging hypothesis. This suggests that subjects will only associate an emotion with a stimulus or situation if they see the event as causing their mood; thus laboratory studies produce only weak mood dependence because the experiment only induces temporal contiguity; thus subjects do not attribute emotional feelings to the words studied (Eich et al, 2000). In contrast, autobiographical memories show mood dependence because the emotions causally belonged originally to the events being recalled, and the event-to-emotion situation is established strongly at the initial experience (Eich et al, 2000). In addition, a further principle which explains the impact of emotion on memory is Tulving's encoding specificity principle, which states that the recollection of an event depends on the interaction between the properties of an encoded event and the retrieval information. Thus, if a mood is encoded as part of the learning experience, providing this cue and reinstating the mood during retrieval enhances memory performance (Eysenck & Keane, 2000). It is therefore clear that our memories are influenced by our feelings in numerous different ways. Memories with strong emotional associations can be either encoded strongly and in much detail, or repressed, whilst both the emotional tone of material and a person's mood state can affect what is later remembered. For this reason there is much evidence to support the idea that emotion and cognition are closely connected",False
78,"The term 'meme' was first introduced by Richard Dawkins in 1976. Dawkins popularised the influential view that evolution is best understood in terms of competition between genes (Blackmore, 1999), and in his conception of memes he takes this one step further, drawing on the analogy between genetic and cultural evolution to claim that culture can be split into units called memes; ideas, habits, skills, stories or any kind of behaviour or information that is copied from person to person by imitation (Dawkins, 1976). Memes are the replicators of the cultural world; the smallest elements of culture or complex ideas that replicate themselves with reliability and fecundity (Dennett, 1993). Like genes, memes are selfish. They pursue their own interests and are concerned only with their survival and replication. Since each mind has a limited capacity for memes, there is considerable competition among memes for entry into as many minds as possible (Ball, 1984). Dawkins urges us to take the idea of meme evolution literally; it is not just analogous to biological or genetic evolution (Dennett, 1990), but a phenomenon that obeys the laws of natural selection exactly, and is achieving evolutionary change at a rate that leaves conventional genetic evolution far behind (Dawkins, 1976). One of the most comprehensive works on memes is Blackmore's (1999) The Meme Machine, which uses memes to elaborate an ambitious theory designed to account for numerous aspects of human evolution and psychology (Jahoda, 2002). Blackmore claims that imitation, as the primary mechanism by which memes replicate themselves, can be taken as the basis of a major theory of the evolution of the human mind and even of what it means to be a conscious self. Imitation is posited as the key to what set our ancestors apart from all other animals. The theory starts with one simple mechanism - the competition between memes to get into human brains and be passed on again - and from this it gives rise to explanation for such diverse phenomena as the evolution of the enormous human brain, the origins of language, human altruism, sex, religion, and even the evolution of the internet (Blackmore, 1999). Blackmore's theory is certainly well argued, and her justifications of both the meme-gene analogy and how memes drive many aspects of human development are theoretically sound. However, I believe there may be an initial flaw in Blackmore's argument in that she appears to take the existence of memes for granted and expounds her detailed argument from this premise when this fact is not necessarily clear. It may be contended that if one refuses to acknowledge the reality of memes, Blackmore's argument fails. One can question whether cultural evolution is truly analogous to genetic evolution (Dennett, 1995), whether there are really cultural units that have interests of their own, and whether imitation is indeed the driving force behind all human behaviour (Holdcroft & Lewis, 2001). Midgley (2000), for example, calls memes 'mythical entities' and an 'empty and misleading metaphor'. She argues that thought and culture cannot be broken down into distinct units, since they are moving and developing patterns in human behaviour. There is therefore no use in trying to understand them by tracing reproductive interactions among memetic units Midgley, 1999). Nonetheless, if, for now, one accepts the existence of memes and ignores such criticism, Blackmore's memetic model provides feasible and coherent explanations of a diverse range of phenomena that have traditionally proved difficult to explain in terms of biological evolution or individual genetic advantage (Edmonds, 2002). It has been argued that the development of human language, for example, does not conform to a Darwinian evolutionary explanation since it shows no genetic variation or selective advantage and cannot exist in intermediate forms (Blackmore, 1999). The same can be said of the excessively large human brain, which stands out greatly in comparisons of encephalisation with other primates. Most early theories suggest that toolmaking, technological advances and the human social environment, drove the need for a larger brain, whilst language is explained as a non-adaptive side consequence of the evolution of the brain, or as evolving as a response to selection pressure for improved communication between humans (Gould, 1997). However, Blackmore presents a memetic explanation. Memes compete to be copied, and people who are especially good at imitation gain a survival advantage by being able to copy the currently most useful memes. Since imitation is a difficult skill requiring extra brain power, the advantage is given to genes for bigger brains and better imitation. The genes were therefore forced into creating big brains, capable of spreading memes. Blackmore also proposes that the human language faculty primarily provided a selective advantage to memes, rather than genes. Successful replicators are those with high fidelity, longevity and fecundity. Digitisation of sounds into words may increase fidelity, combining words into novel combinations may improve fecundity, and every improvement leads to increased memetic competition. The memes that can get themselves spoken will be copied more than those that cannot, and people who can best copy the winning sounds have an advantage and pass on the genes that gave them that ability. Gradually, human brains would be driven by this emerging language. The development of language was thus an evolutionary process like any other, creating complex design apparently out of nowhere (Blackmore, 2003). A further problem Blackmore tackles is that of altruism, which conflicts with traditional understanding that humans selfishly pursue their own interests. Traditionally altruism has been explained either in terms of advantage to the genes; you are altruistic towards your children because they are the only direct way your genes can be carried on into future generations; you are altruistic toward fellow men because you may get aid in return, or in terms of a true morality, an independent moral conscience or a spiritual essence in humans that overcomes the selfish dictates of our genes; a view which finds little favour with scientists (Blackmore, 1999). Memetics provides a third solution. Blackmore suggests that the kind, generous, altruistic person will spread more memes. He will have more friends and spend more time talking to them; they will be influenced by him and in turn will imitate his popular behaviour. Thus his altruism will spread. In contrast, the mean and selfish person will have few chances to replicate his memes because the few people who could potentially imitate him rarely do so. Blackmore provides a number of such examples of behaviours that are mimetically adaptive but biologically unadaptive (Cook, 1995); for example, she claims the rise in the use of birth control, whilst not genetically advantageous, means women who have fewer children will have more time to spread their own memes, including memes for birth control and the pleasures of a small family. These are the women whose success inspires others, and who provide role models for other women to copy. Furthermore, whilst adoption may be viewed as a mistake from the gene's point of view, for memes the benefits of adoption are obvious. As far as memes are concerned, the time and effort expended on an adopted child are as valuable as that expended on one's own offspring. All that matters is that you can pass on your memes to people who will be influenced by you and subsequently imitate your behaviours and beliefs. Genetic relation is of no importance. In both of these examples there is a battle between memes and genes to take control over the machinery of replication (Blackmore, 1999). All of these arguments therefore appear to be perfectly logical and acceptable; however, as previously argued, I believe they are only acceptable if one first accepts the existence of memes. If one refuses to believe that such elements of culture exist or compete to get themselves replicated through imitation, there is no reason to believe Blackmore's theory. Furthermore, I believe that Blackmore goes too far in attempting to apply memes to areas such as mate selection, and in implying that memes are even responsible for our sense of a conscious self. Traditional genetic evolutionary theory suggests we choose mates based on fitness and biological advantage; men are sexually attracted to women who appear fertile, with big hips and youthful, symmetrical features, whilst women are attracted to strong, high-status men with plentiful resources. Essentially we should choose to mate with people who would, in the environment of our evolutionary past, have helped to increase our genetic legacy (Blackmore, 1999). Blackmore suggests mate choice is based not only on genetic advantage, but also memetic advantage. Once memes arose in our far past, natural selection would favour people who chose to mate with the best imitators. In an early hunter-gatherer society a man who was especially good at imitation would have been able to copy the latest hunting skills or stone tool technology and hence would have gained a biological advantage. Thus a woman who mated with him would be more likely to have children who shared that imitation ability and that advantage. Such a woman would have to look in potential mates for signs of the general ability to imitate and innovate. This argument suggests we will still want to mate with the best imitators and spreaders of memes, such as writers, artists, journalists, broadcasters, film stars and musicians (Blackmore, 1999). However, whilst it may be the case that some high profile people who are prominent in the media are often seen as desirable mates, this is not true for all such people, and I do not think it can be reasonably argued that we choose mates on the basis of whether they are a good imitator. Firstly, we arguably do not know what the signs of a good imitator are, and secondly, in being attracted to someone we tend to consider factors such as their looks and personality; we are certainly not aware of considering their potential for spreading memes. Furthermore I believe that Blackmore's conclusion regarding our concept of the conscious self is largely unbelievable. Blackmore claims that the idea of the continuous, persistent and autonomous self is an illusion; the self is actually just a collection of memes. Memes gain an advantage by becoming associated with a person's self-concept. A 'self' therefore aids their replication. The sense of 'self' has simply been created by the processes of memetic evolution acting in the relatively short period of one human lifetime. Blackmore then goes even further and says that the ways we behave, the choices we make, and the things we say are all a result of complex interactions between memes. There is no inner self which is the initiator of actions, controls the body, or is conscious; thus there is no free will. Blackmore presents an eliminative account of design, mind, and human agency which removes the possibility of voluntary choice or invention on the part of humans (Holdcroft & Lewis, 2001). Intentionality is transferred away from humans and to the memes. Whilst Blackmore has tried to argue against the fact that her argument makes humans completely passive, stating that our intelligence, capacity for making choices, and active social life are all part of the copying environment in which memes compete (Blackmore, 2002), it is hard to see that this could imply anything other than that we are caught up in ""a remorseless process in which we are mere pawns"" (Jahoda, 2002). I find this aspect of Blackmore's argument hard to accept since it contradicts our very experience of thought and conscious deliberation, and violates the assumption that in making choices we truly have free will and are not influenced or bound by either internal or external forces. Overall I would argue that The Meme Machine provides a detailed and interesting account of the concept of memes, and certainly does provide some plausible theories that appear to be explained better by memetics than any other theory we currently have available. However, I believe that in attempting to use memes to explain all aspects of human nature and behaviour, Blackmore takes memetic theory beyond logical boundaries. Furthermore, Blackmore restricts memetic transmission to imitation, claiming that imitation is what sets us apart from other animals; however Tomasello (1999) argues that whilst imitation has played a role in cultural evolution the critical difference is in fact the understanding of others as intentional beings like the self. Thus, if the basic principle of cultural learning being driven solely by imitation is rejected the foundations of The Meme Machine collapse and there is no reason why we should accept any of Blackmore's arguments. Critics also argue that just as a gene is nothing without its biological and environmental context the same is true for memes (Wheelwell, 1998; Dupre, 2000), and indeed it is difficult to conceive of memes as cultural units residing in brains that exist independently of contextual interpretation. However, proponents of memetics argue psychology will be as transformed by this discipline as biology was by Darwin (Brodie, 1996), and this perhaps is where the problem lies in getting people to accept memetic theory. In acknowledging the existence of these cultural replicators we have to radically transform our ideas regarding human nature, behaviour and motivation and reformulate our understanding of how and why culture evolved, just as Darwin caused us to rethink our beliefs about the origin of our species. Maybe we are too scared to accept we do not have consciousness, free will, or the possibility of rational choice but rather are completely at the mercy of the interaction between our memes and our genes? Whether the theory of memes is true or not, The Meme Machine provides a comprehensive and detailed insight into how memes may shape human development which, although somewhat unbelievable in parts, is certainly an interesting and enlightening read.","High levels of avoidance coping have consistently been found to be predictive of problem drinking, which is highest among young adults. Students in particular have greatly elevated levels of drinking in comparison to the general population. One hundred undergraduates were asked about the way they cope with a high academic workload. Alcohol consumption was found to correlate positively with avoidance coping and negatively with seeking social support. Thus students who attempt to disengage and forget about their problems, and those who do not like to seek help or confide in others, are typically those with higher levels of alcohol consumption. Further research could investigate whether high levels of alcohol consumption avoidance coping significantly impact academic achievement. IntroductionWhen confronted by a stressful situation, an individual may react in any one of a variety of ways, ranging from attempting to actively address the problem and tackle it 'head-on', to merely forgetting about it and hoping it will disappear. These strategies, and many more in between, are encompassed under the broad heading 'coping', which can be defined as a person's constantly changing cognitive and behavioural efforts to manage, in the sense of mastering, reducing, or tolerating, a troubled person-environment relationship (Folkman & Lazarus, 1986). There is a growing conviction that the ways in which people cope with stress affect their psychological, physical, and social well-being (Pearlin & Schooler, 1978), and it is for this reason that there has been an explosion of research into coping in the past few decades. Coping is typically regarded as a shifting process, influenced, to some extent, by personality characteristics, but largely determined by situation variables or role demands (Aldwin & Revenson, 1987). It is widely accepted that as the dynamics of a stressful encounter change, so do the particular types of coping strategy employed by an individual (Folkman & Lazarus, 1980), and thus modern coping research is characterised by an interest in the actual coping processes that people use to manage the demands of particular stressful events. The cognitive theory of stress and coping defines stress as a particular relationship between the person and the environment (Folkman, 1984), and sees a person's reaction to a stressful event as being largely influenced by their cognitive appraisal of the situation (Parkes, 1986). Cognitive appraisal is a process through which the person assesses whether a particular encounter with the environment is relevant to his or her well being, and subsequently evaluates various coping resources that can be used in response to a stressful appraisal of harm, loss, threat or challenge, such as altering the situation, accepting it, seeking more information, or holding back from acting impulsively and in a counterproductive way (McCrae, 1984). Coping is largely seen as having two major functions: regulating stressful emotions, (emotion-focused coping), and altering the troubled person-environment relation causing the distress, (problem-focused coping). The problem-focused coping dimension involves strategies that attempt to solve, reconceptualise, or minimise the effects of a stressful situation. The emotion-focused coping dimension, on the other hand, includes strategies that involve self-preoccupation, fantasy, or other conscious activities related to affect regulation, which allow the person not to focus on the troubling situation (Parker & Endler, 1996). Problem-focused forms of coping are thus used more often in encounters that are appraised as changeable, and emotion-focused coping in encounters appraised as unchangeable (Folkman, Lazarus, Dunkel-Schetter, DeLongis & Gruen, 1986). On the whole it has also been found that problem-focused coping strategies are more adaptive than are emotion-focused strategies. People who rely more on problem-focused, direct coping strategies tend to adapt better to life stressors and experience fewer psychological symptoms, since techniques such as problem solving and seeking information can moderate the potential adverse influence of both negative life change and enduring role stressors on psychological functioning (Holahan, Moos & Schaefer, 1996). In contrast, emotion-focused coping, and in particular the use of avoidant strategies, which involve denial and withdrawal and divert attention away from the realities at hand, is generally associated with psychological stress, maladjustment and behavioural and social dysfunction (Felton & Revenson, 1984). Coyne, Aldwin & Lazarus (1981), and Billings and Moos (1984), for example, both report results that indicate that reliance on emotional discharge and avoidance coping is associated with higher levels of depression. However, the distinction between problem-focused and emotion-focused coping may not necessarily be this clear cut; not all instances of avoidant coping have negative health implications, and emotion-focused coping may have its benefits. For example, it can facilitate problem-focused coping by removing some of the distress that can hamper problem-focused efforts, and can also be used to alter the meaning of a situation, thereby enhancing the individual's sense of control over his or her distress (Carver & Scheier, 1994; Scheier, Weintraub & Carver, 1986). In addition, a problem-solving approach, which focuses attention on the problem and exacerbates negative effect, is arguably an inappropriate approach to uncontrollable stressors, where strategies which involve avoidance, distance and cognitive distortion of the situation may in fact be more adaptive (Stone, Schwartz, Neale, Shiffman, Marco, Hickcox, Paty, Porter & Cruise, 1998; Felton, Revenson & Hinrichsen, 1984). An area of growing interest which does indicate the potential negative implications of emotion-focused avoidance coping, however, is that of alcohol consumption and abuse, and it is within this area that the present study is focused. It is a widespread belief that alcohol serves to reduce tension and anxiety in an individual, with this theory being formalised in Conger's tension-reduction hypothesis (1956, cited in Erickson, 2004). The tension-reduction hypothesis is supported by physiological evidence which indicates that alcohol functions to depress central nervous activity and leads to decreased cortical arousal, resulting in a decrease in tension anxiety (Schwarz, Burkhart & Green, 1978), and by other numerous studies that have shown that alcohol reduces both physiological and psychological responsivity to stressful situations (Stasiewicz & Lisman, 1995; Hull & Young, 1983; Levenson, Sher, Grossman, Newman & Newlin, 1980). However, it remains unclear whether the effect is a function of alcohol's impact on tension per se or cognisance of the stressor (Hull, 1981), and some studies have found that alcohol actually serves to increase self-reported anxiety. Evidence for the model is therefore mixed. Nonetheless, the suggestion that alcohol reduces cognisance of a source of tension and causes impairment of perception, thought, and nearly all aspects of information processing (Steele & Josephs, 1990) led to the investigation of a possible relationship between alcohol consumption and coping style. Substance use is posited to assist coping by providing distraction from problems, and diverting attention temporarily from unpleasant self awareness (Hull, 1981), and since alcohol is widely believed to possess a range of properties that facilitate problem avoidance (e.g. helping to forget one's problems, decreasing negative mood), drinking seems especially likely to be used as a problem avoidance strategy and thus to be positively related to other avoidant coping strategies (Moos, Brennan, Fondacaro & Moos, 1990). Social learning theory proposes that people who exhibit abusive patterns of drinking differ from 'healthy' drinkers in their ability to cope with the demands of everyday life, and that deficiencies in more adaptive coping skills may predict the use of drinking as a coping mechanism (Cooper, Russell & George, 1988). People who display dispositions to deny and disengage from stressful problems (those who demonstrate avoidance coping) are thus hypothesised to be especially vulnerable to hazardous levels of alcohol use and alcohol related problems (Cantanzaro & Laurent, 2004). In accordance with this theory, a number of studies have found a positive correlation between an individual's levels of avoidance coping and the amount of alcohol they consume. Fromme & Rivet (1994), for example, report that levels of avoidant coping resources were predictive of both increased frequency of alcohol consumption and increased volume of alcohol consumed, whilst Moos and his colleagues (Moos, Finney & Chan, 1981; Moos, Finney, & Gamble, 1982) found that individuals who relied on avoidant coping strategies that served to discharge or deny emotion were more likely to drink in response to stressful events. However, in contrast to these results Armeli, Carney, Tennen, Affleck and O'Neil (2000) found no relationship between avoidant coping style and alcohol consumption. In the present study we were particularly concerned with the relationship between avoidance coping and alcohol consumption with respect to how university students cope with a high academic workload. It is widely acknowledged that university students represent a group of individuals who have unique drinking patterns (O'Hare, 1997). Drinking is a large part of university culture, and heavy or problematic drinking among students represents a major public health concern (Ham & Hope, 2003), with young adults aged 18-24 showing the highest rates of alcohol use and having the greatest percentage of problem drinkers in the population (Rabow & Duncan-Schill, 1995). However, at the same time university students are frequently confronted by stressful periods when they have a large amount of work to do, with assignments to complete, deadlines to meet and exams to prepare for. This poses a potential problem since their generally high levels of drinking suggest that university students may be particularly likely to exhibit avoidance coping in stressful situations, and thus, if alcohol fosters the use of avoidant coping to such an extent that more adaptive coping strategies such as active problem solving are excluded, it may lead to maladaptive functioning in important life domains such as academic achievement (Windle & Windle, 1996). A positive correlation between levels of avoidance coping and alcohol consumption may therefore suggest that individuals who drink more at university are more likely to deny the importance of work they have to do, perhaps leave it until the last minute, and subsequently exhibit lower grades than those who drink less and are subsequently more able to employ more successful problem-focused strategies. Our study establishes new data in the area of coping because, to our knowledge, there have been no other studies focusing on avoidance coping and alcohol consumption specifically within the student population. Furthermore, our study differs from many others in the fact that we focus purely on one area - that of academic workload - when the vast majority of coping studies ask participants to specify a particular event of their choice that they found stressful, and analyse responses and coping strategies used in terms of whether the situation was appraised as changeable or unchangeable. We decided to structure our study in this manner because a general link between alcohol consumption and the use of avoidant coping strategies has already been established, and we therefore wanted to see if this general conclusion would apply to one specific situation with which all students are familiar. We feel this issue is of particular relevance since drinking alcohol and working are arguably the two main activities that dominate the lives of many students, and our results may provide preliminary evidence indicating that excessive alcohol consumption may be detrimental to the attitudes of students towards academic work. In our study participants were provided with questionnaires comprising two sections. The first section contained 35 items selected from Folkman and Lazarus' (1985) Ways of Coping Questionnaire, commonly regarded as the standard measure for assessing coping strategies by psychologists in this field (Schwarzer & Schwarzer, 1996). Both problem-focused and emotion-focused items were included, with items from seven of the eight sub-scales identified by Folkman and Lazarus (1985). Several of the statements were rephrased in a manner more applicable to the specific study context. In the second section participants' drinking habits were assessed using the Alcohol Use Disorders Identification Test (AUDIT). Developed by Saunders, Aasland, Babor, de la Fuente and Grant (1993), the AUDIT is a 10-item self-report questionnaire measuring alcohol consumption and drinking behaviour. It is used as both a clinical and research screening instrument for hazardous alcohol consumption. The AUDIT has a maximum score of 40 with higher scores indicating more hazardous drinking, and has previously been used in a non-clinical population to discriminate heavy and light drinkers (Sharma, Albery, & Cook, 2001). In accordance with previous studies, we expected that heavy drinkers would exhibit more emotion-focused coping strategies, whilst problem-focused coping would be more prominent amongst lighter drinkers. Our experimental hypothesis was that higher levels of alcohol consumption would be correlated in particular with the greater use of avoidant coping strategies. MethodParticipantsOne hundred second year undergraduates from the University of  participated in the study. Participants were aged between 19 and 21 and were both male and female. They were obtained by means of an opportunity sample. MaterialsQuestionnaires were given out to each of the participants. The questionnaire consisted of two sections. The first section comprised 35 items extracted from the standard Ways of Coping Questionnaire (WCQ), with items from seven of the eight subscales identified in Folkman and Lazarus' 1985 study. These were: 1. Planful Problem Solving (e.g. try to analyse the problem in order to understand it better); 2. Seeking Social Support (e.g. seek sympathy and understanding from others); 3. Distancing (e.g. ignore it and turn to another activity to take my mind off things); 4. Wishful Thinking (e.g. hope a miracle will happen); 5. Self-blame (e.g. realise I brought the problem on myself); 6. Tension-reduction (e.g. jog or exercise); and 7. Self-isolation (e.g. avoid being with people in general). The 'planful problem solving' scale contains purely problem-focused coping items, the 'seeking social support' scale contains a mixture of both problem-focused and emotion-focused coping items, and the remaining five scales contain purely emotion-focused coping items. Items were selected on the basis of relevance to the issue under investigation; no items from the 'positive reappraisal' subscale were therefore selected, since we did not feel this particular coping strategy was particularly applicable. The phrasing of some items was also changed slightly to ensure a close match between the stressful experience identified and the coping statements. The response format was a five point Likert rating scale ranging from 1, ""strongly disagree"" to 5, ""strongly agree"". Since some of the scales had more items than the others, the maximum scores for each scale differed, for example the 'distancing' scale had a maximum score of 55, whilst the 'tension reduction scale' had a maximum score of 15. Participants were asked to indicate the extent of their agreement with each statement as an action they typically undertake in a situation when they are faced with a high academic workload. The second section of the questionnaire consisted of the standard, 10-item Alcohol Use Disorders Identification Test (AUDIT), a measure which aims to identify persons with hazardous or harmful alcohol consumption through asking questions relating to alcohol consumption, abnormal drinking behaviour, adverse psychological reactions to alcohol, and alcohol related problems. Design and ProcedureA within subjects design was used in which participants all filled in the same questionnaire. Participants were initially provided with a consent form which detailed the nature of the task and made it clear that they were not obliged to take part in the questionnaire or complete any questions that they were uncomfortable with. Participants were provided with the experimenters' contact details in case they later wanted to withdraw their information. The experimenters were also equipped with the telephone numbers of relevant help lines and care organisations to give to participants in the unlikely event that the alcohol related questions subjected them to undue stress. On completion of the consent form, participants were asked to think back to a time when they had a high academic workload and think about the feelings they may have and actions they may take in such a situation. They were given the phrase ""When I feel under pressure from the amount of work I have to do I..."" and the separate items from the Ways of Coping Questionnaire were then listed, with participants circling the most appropriate response to each statement. In the second section of the questionnaire participants were asked to complete AUDIT, by answering 10 questions regarding their typical drinking habits and behaviour. The resulting scores for each participant were then obtained by adding up the value of their responses to each sub-scale of the Ways of Coping Questionnaire, to obtain seven different scores, and calculating their overall AUDIT score. ResultsInitial data screening using a boxplot indicated that there were four outliers in the raw data. One had a particularly low score on the 'planful problem solving scale', one on the 'seeking social support' scale, and the other two particularly high scores on the 'self isolation' scale. The data from these subjects was subsequently removed from the study due to the potential disproportionate influence they may have had on results obtained. All analysis was carried out with these outliers removed. There was no missing data in the data set. The original Alcohol Use Disorders Identification Test recommends a cut-off score of 11 as an indicator of hazardous alcohol consumption, so those scoring 10 and under are classified as 'light drinkers', whilst those scoring 11 and over are classified as 'heavy drinkers' (Saunders et al, 1993). Descriptive statistics for the mean scores on each sub-scale of the Ways of Coping Questionnaire using this cut-off score to identify heavy and light drinkers are therefore shown in Table 1. Table 1 illustrates that heavy drinkers scored more highly on each of the sub-scales of the Ways of Coping Questionnaire than light drinkers, with the exception of 'planful problem solving' and 'seeking social support'. This was the pattern of results that was expected, and fits the experimental hypothesis, since the five scales on which heavy drinkers scored more highly all contain purely emotion-focused coping items, whilst 'planful problem solving' and 'seeking social support' contain purely problem-focused coping items, and a mixture of problem and emotion-focused items, respectively. In some cases the difference in mean scores is marginal, for example in the 'tension reduction' scale, where the mean for heavy drinkers is only 0.04 higher than the mean for light drinkers; but in others it is quite noticeable, for example the 'distancing' sub-scale, which contains purely avoidance coping items, and where the difference in mean scores between heavy and light drinkers is 3.03, providing preliminary support for the hypothesis that heavy drinkers engage in significantly more avoidance coping than light drinkers. The standard deviations are, with the exception of 'planful problem solving' and 'tension reduction', higher for heavy drinkers than light drinkers; however, for the most part the difference in standard deviations between the two groups is not large. Heavier drinkers thus simply appear to be slightly more variable in the extent to which they use particular coping strategies. A correlation matrix indicating the inter-correlation between each of the sub-scales of the Ways of Coping Questionnaire, and, most importantly, the correlation between scores on each sub-scale with the corresponding AUDIT score is shown in Table 2: Table 2 illustrates that there are a number of significant correlations in the results obtained, consisting of both inter-correlations between sub-scales of the Ways of Coping Questionnaire and correlations between scores on particular sub-scale with AUDIT scores. Of particular interest to this study are the relationships between the 'planful problem solving', 'distancing' and 'seeking social support' sub-scales, where significant inter-correlations were obtained. Firstly, there is a significant negative correlation between the 'planful problem solving' and 'distancing' sub-scales; r (94) = -0.297; p Figure 1 depicts the positive correlation between AUDIT score and distancing score. Whilst there is quite a large amount of scatter on the graph, it is nonetheless possible to see quite clearly the trend for higher scorers on AUDIT, and thus heavier drinkers, to also score more highly on the use of 'distancing', or avoidance coping, strategies. The relationship between these two variables therefore appears to be quite strong, as well as having been revealed by the Pearson correlation co-efficient test to be highly significant. Figure 2 depicts the negative correlation between AUDIT score and seeking social support score. Again, there is a considerable amount of scatter on the graph and it is arguably harder to detect the presence of a significant negative relationship between the two variables from the graph alone. This may be due to the fact that the maximum score on this sub-scale is only 15, resulting in less possible variation between participants in terms of their score and thus less potential for such a strong negative correlation to emerge. Whilst still significant therefore, this correlation does not appear to be as strong as that between AUDIT and distancing score, a fact which is confirmed by their respective co-efficient values. DiscussionThe results obtained support the hypothesis posited at the beginning of the study, which stated that those with higher levels of alcohol consumption would exhibit more avoidance coping. A significant positive correlation was obtained between participants' scores on the Alcohol Use Disorders Identification Test and their scores on the 'distancing' sub-scale of the Ways of Coping Questionnaire, the scale which is indicative of avoidance coping. In addition, a further significant result that was not originally hypothesised was obtained; that of a negative correlation between AUDIT score and score on the 'seeking social support' sub-scale. Finally, the heavy drinkers obtained higher mean scores on all of the sub-scales of 'emotion-focused coping' than the light drinkers, who in turn obtained higher mean scores on the 'problem-focused coping' sub-scales, demonstrating that heavier drinkers use more emotion-focused and less problem-focused coping strategies. The results therefore imply that, when considered in relation to academic workload in particular, a higher level of alcohol consumption is associated with a tendency to exhibit more avoidant coping strategies, and a smaller likelihood of going to others for help, whether this is emotional help in the form of seeking sympathy and understanding, or practical help in the form of asking a lecturer or tutor for guidance with work. This particular result replicates Karwacki and Bradley's (1996) finding that greater social support seeking was related to lower alcohol consumption in women who had experienced interpersonal stressors, whilst the finding that greater alcohol consumption is associated with higher levels of avoidance coping is consistent with many other research findings which indicate that avoidant coping is associated with greater affective, behavioural and social dysfunction, and with alcohol and drug use behaviours (e.g. Cooper et al, 1988; Windle & Windle, 1996; Williams & Clark, 1998). The fact that higher levels of alcohol consumption are associated with a greater prevalence of avoidant coping can, in part, be explained by the stressor-vulnerability model of alcohol use. The primary hypothesis of the stressor-vulnerability model is that individuals learn to use drinking as a coping response when they believe healthier means of coping are unavailable. Thus, individuals who have a tendency to exhibit high levels of maladaptive avoidant coping strategies and who are deficient in more active, problem-focused strategies are more likely to turn to 'drinking to cope' since they believe that healthier means of coping are unavailable to them (Cantanzaro & Laurent, 2004). These individuals are unable to confront the problem and deal with it directly by using problem-focused coping strategies; however, their high levels of avoidant coping strategies alone are not enough to take their minds off the problem, and so they will turn to alcohol as a further means of coping. This relationship is strengthened by positive expectancies about alcohol's stress-relieving effects (Hasking & Oei, 2002). However, it is admittedly difficult to determine the direction of the relationship between avoidance coping and alcohol consumption, particularly within a student population. Undoubtedly, some students may rely on avoidant coping strategies and the use of alcohol as a means of coping because they have deficits in other coping strategies; however, others - who are perfectly normal in terms of adaptive problem-focused coping strategies - may begin to use alcohol primarily because their peers are doing so, and this excessive alcohol consumption may itself lead to consequences such as the development of deficits in more adaptive coping. It is conceivable that the relationship between alcohol consumption and deficits in coping is therefore bi-directional (Britton, 2004). Increasing deficits in adaptive coping may increase reliance on the use of alcohol as a means of lowering drive level and reducing internal tension; however, even if alcohol is consumed primarily for social reasons, an increasing frequency and level of alcohol consumption may itself cause deficits in adaptive coping which previously did not exist. There is thus a theoretical paradox in trying to decide on the direction of causality in this relationship; substance use may itself be construed as a coping mechanism, but may also be a product of deficits in other coping mechanisms (McKee, Hinson, Wall & Spriel, 1996). Nonetheless, both theory and research suggest that a reliance on avoidant coping can increase risk for heavy alcohol consumption. Avoidance types of coping are predicted to increase risk because the individual is not disposed to deal with problematic situations but rather to seek the path of least resistance toward restoring affective balance (Wills & Hirky, 1996). To the extent that a person tends to cope by denying the existence of problems, seeking distraction from unpleasant self awareness, and withdrawing from engagement with normative sources of reinforcement, then the path to substance use as a coping mechanism would be more likely, primarily due to alcohol's proven ability to interfere with cognitive processes fundamental to self-awareness and to reduce negative self-evaluation following failure (Hull, Levenson, Young & Sher, 1983). Thus for individuals who typically try to cope with negative emotions through avoidance, alcohol's mood-altering properties (e.g. its analgesic effects) may offer an attractive antidote for dysphoria (Cooper, Russell, Skinner, Frone & Mudar, 1992). It is therefore conceivable that the students in our study who had higher levels of alcohol consumption exhibit this pattern of behaviour primarily due to deficits in adaptive coping strategies, a resulting high level of avoidant coping, and a subsequent desire to escape from uncomfortable, troubling, or stressful events as and when they arise. A potential explanation can also be offered for the observed negative correlation between level of alcohol consumption and the use of seeking social support as a coping strategy. Whilst there has by no means been as much research into this area as there has been into the area of avoidance coping, Karwacki and Bradley (1996) report that social support seeking may act as a possible protective factor against harmful levels of alcohol consumption or drinking-related problems. Theories of social support suggest that individuals are less likely to experience negative consequences from social stressors if they receive continued support from non-disrupted aspects of their social networks (Husson, 2003), and this is illustrated in a study by Snow, Swan, Raghavan, Connell and Klein (2003) who found that greater social support served a protective function by contributing to lower levels of stress and fostering greater use of active, problem-focused coping. Therefore, those individuals who do not like to go to others for support and are less inclined to share the negative feelings they are experiencing have less to buffer them against various environmental stressors, and subsequently at stressful times may be more likely to use avoidant coping strategies and furthermore turn to alcohol as a primary way of coping. This is highlighted by the significant negative correlation obtained in the results between the 'distancing' and 'seeking social support' sub-scales, illustrating that individuals who are high in social support seeking are low in avoidant coping strategies, and resultantly less likely to show high levels of alcohol consumption. Overall, therefore, our results support the experimental hypothesis and indicate that there is a significant positive correlation between alcohol consumption and levels of avoidance coping; specifically, heavier drinkers exhibit more avoidant coping strategies than light drinkers. This general result has been reported in much previous research; however, what is of particular interest in our study is that it holds even when all participants were asked to consider the same specific situation, that of a heavy academic workload. This suggests that the relationship observed may be stable across many stressful situations that are familiar to all people, and in particular raises questions regarding the possible detrimental effect that a heavily avoidant coping strategy may have on academic achievement. While we did not investigate this in our study, it may have been interesting to ask participants to indicate the marks they received for all pieces of assessed work in the academic year up to the point of the study, and determine whether a high level of avoidance coping and related alcohol consumption leads to a poorer level of academic functioning; primarily due to the tendency to try and forget about work and put off doing it, combined with the negative impact that alcohol can have on one's ability to concentrate and process perceptual information. Furthermore, it may also be of relevance to obtain data from other years within the university population. Our study focused on second year students for whom most work is quite important, contributing to their overall degree mark. In contrast, for many students the first year does not count, and is often the year of heaviest drinking and socialising, and thus it may be interesting to discover whether the same pattern of results is obtained for such students. It may simply be the case that overall alcohol consumption is higher among all first years, irrespective of whether they employ mainly problem-focused or avoidant coping strategies. Therefore the same pattern of results may not be obtained with first-year students. Another area into which our study could be further extended is the realm of alcohol expectancies, which is another possible predictor of problem drinking. An outcome expectancy is a person's estimate that a given behaviour will lead to certain outcomes (Bandura, 1977) and recently the role of expectancies in predicting alcohol consumption has received increasing attention. Grounded in social learning theory, expectancy argues that a person's alcohol consumption will depend on the outcomes they expect to gain from drinking, for example relaxation, improved confidence, or improved sexual functioning (Hasking & Oei, 2002). Cantanzaro and Laurent (2004) for example, found that positive alcohol expectancies interacted with avoidant coping dispositions to an increased frequency of drinking to cope, increased alcohol use and greater alcohol related problems while Cooper et al (1992) reported that avoidance coping was related to problematic alcohol use among high- but not among low-expectancy individuals in their sample. The relationship between alcohol consumption and prevalence of avoidance coping may therefore be further mediated by such alcohol expectancies. This area may be of particular interest to study in a student population, since for many, university is the first time that they will engage in frequent, heavy drinking behaviours, and thus their alcohol expectancies may be different from older adults who are accustomed to particular patterns of drinking and are well aware of the effects that alcohol can have. Finally, there are a few methodological issues that could be mentioned in the light of our study. Firstly, the fact that only one episode was analysed for each subject - that of a period of a high academic workload - meant that the extent to which the types of coping that participants employ are stable over different episodes could not be investigated. Thus the observed relationship between avoidant coping and alcohol consumption may have applied only to this one particular episode. However, the fact that many previous studies have uncovered a general relationship between alcohol consumption and avoidance coping suggests that this criticism is not particularly damaging to our study. Furthermore, the information was collected retrospectively, and all the data were self-reported, and it is common for the perceived importance of a particular episode and the way it is appraised to change over time (Parkes, 1984). Therefore, participants' responses and the extent to which they reported using particular coping strategies may not have been entirely accurate. However, for the most part we encountered few problems in carrying out the study and have confidence in the results we obtained. The main implication of our study is, we believe, that concerns about the binge drinking culture that are now so heavily expressed within the media may be well justified. The relationship between alcohol consumption and avoidance coping is a complex one, with the direction of causality currently unknown. However, these is undoubtedly a significant relationship, and this may be of particular concern within the university population, where drinking is so often excessive. Many students are placed under great stress in their second and third years of university life in particular, and this stress, combined with deficits in adaptive coping strategies and the tendency to drink large amounts, even for purely social reasons, suggests that students may enter a vicious cycle where they become so heavily dependent on avoidance coping that they cannot confront their workload and fall further and further behind. This confirms that students need to be encouraged to use more efficient, problem-focused coping strategies, and perhaps be trained on skills such as time management and dealing with a heavy workload, especially when first entering university, since there is a large transition, both in terms of work and social relationships, from the secondary school or sixth-form college environments. Students should also be educated about the effects of drinking, and encouraged to view drinking perhaps as a primarily social activity, rather than an effective means of dealing with their problems. A number of conclusions can therefore be drawn from this investigation. Firstly, in accordance with many previous studies, we can conclude that there is a significant relationship between increased level of alcohol consumption and greater use of avoidance coping strategies. Our results indicate that the general relationship between alcohol consumption and avoidance coping which has previously been reported can also be applied to more specific events, where participants are asked to think of a situation which has undoubtedly been present in all of their lives at some recent time, that of a high academic workload. The fact that this relationship was obtained even within a university population, where the vast majority of students have elevated levels of alcohol consumption when compared with the general public, implies that it may be a particularly strong one. Furthermore, we suggest that a greater level of alcohol consumption is related to a smaller likelihood of seeking support from or sharing negative feelings with others in times of stress. Those who use seeking social support as a coping strategy are subsequently more likely to use avoidance forms of coping. Our results therefore confirm the experimental hypotheses made at the beginning of the study and suggest that future research which investigates the potential relationship between students' academic achievement in relation to their levels of avoidance coping and alcohol consumption may be of particular interest.",True
79,"High levels of avoidance coping have consistently been found to be predictive of problem drinking, which is highest among young adults. Students in particular have greatly elevated levels of drinking in comparison to the general population. One hundred undergraduates were asked about the way they cope with a high academic workload. Alcohol consumption was found to correlate positively with avoidance coping and negatively with seeking social support. Thus students who attempt to disengage and forget about their problems, and those who do not like to seek help or confide in others, are typically those with higher levels of alcohol consumption. Further research could investigate whether high levels of alcohol consumption avoidance coping significantly impact academic achievement. IntroductionWhen confronted by a stressful situation, an individual may react in any one of a variety of ways, ranging from attempting to actively address the problem and tackle it 'head-on', to merely forgetting about it and hoping it will disappear. These strategies, and many more in between, are encompassed under the broad heading 'coping', which can be defined as a person's constantly changing cognitive and behavioural efforts to manage, in the sense of mastering, reducing, or tolerating, a troubled person-environment relationship (Folkman & Lazarus, 1986). There is a growing conviction that the ways in which people cope with stress affect their psychological, physical, and social well-being (Pearlin & Schooler, 1978), and it is for this reason that there has been an explosion of research into coping in the past few decades. Coping is typically regarded as a shifting process, influenced, to some extent, by personality characteristics, but largely determined by situation variables or role demands (Aldwin & Revenson, 1987). It is widely accepted that as the dynamics of a stressful encounter change, so do the particular types of coping strategy employed by an individual (Folkman & Lazarus, 1980), and thus modern coping research is characterised by an interest in the actual coping processes that people use to manage the demands of particular stressful events. The cognitive theory of stress and coping defines stress as a particular relationship between the person and the environment (Folkman, 1984), and sees a person's reaction to a stressful event as being largely influenced by their cognitive appraisal of the situation (Parkes, 1986). Cognitive appraisal is a process through which the person assesses whether a particular encounter with the environment is relevant to his or her well being, and subsequently evaluates various coping resources that can be used in response to a stressful appraisal of harm, loss, threat or challenge, such as altering the situation, accepting it, seeking more information, or holding back from acting impulsively and in a counterproductive way (McCrae, 1984). Coping is largely seen as having two major functions: regulating stressful emotions, (emotion-focused coping), and altering the troubled person-environment relation causing the distress, (problem-focused coping). The problem-focused coping dimension involves strategies that attempt to solve, reconceptualise, or minimise the effects of a stressful situation. The emotion-focused coping dimension, on the other hand, includes strategies that involve self-preoccupation, fantasy, or other conscious activities related to affect regulation, which allow the person not to focus on the troubling situation (Parker & Endler, 1996). Problem-focused forms of coping are thus used more often in encounters that are appraised as changeable, and emotion-focused coping in encounters appraised as unchangeable (Folkman, Lazarus, Dunkel-Schetter, DeLongis & Gruen, 1986). On the whole it has also been found that problem-focused coping strategies are more adaptive than are emotion-focused strategies. People who rely more on problem-focused, direct coping strategies tend to adapt better to life stressors and experience fewer psychological symptoms, since techniques such as problem solving and seeking information can moderate the potential adverse influence of both negative life change and enduring role stressors on psychological functioning (Holahan, Moos & Schaefer, 1996). In contrast, emotion-focused coping, and in particular the use of avoidant strategies, which involve denial and withdrawal and divert attention away from the realities at hand, is generally associated with psychological stress, maladjustment and behavioural and social dysfunction (Felton & Revenson, 1984). Coyne, Aldwin & Lazarus (1981), and Billings and Moos (1984), for example, both report results that indicate that reliance on emotional discharge and avoidance coping is associated with higher levels of depression. However, the distinction between problem-focused and emotion-focused coping may not necessarily be this clear cut; not all instances of avoidant coping have negative health implications, and emotion-focused coping may have its benefits. For example, it can facilitate problem-focused coping by removing some of the distress that can hamper problem-focused efforts, and can also be used to alter the meaning of a situation, thereby enhancing the individual's sense of control over his or her distress (Carver & Scheier, 1994; Scheier, Weintraub & Carver, 1986). In addition, a problem-solving approach, which focuses attention on the problem and exacerbates negative effect, is arguably an inappropriate approach to uncontrollable stressors, where strategies which involve avoidance, distance and cognitive distortion of the situation may in fact be more adaptive (Stone, Schwartz, Neale, Shiffman, Marco, Hickcox, Paty, Porter & Cruise, 1998; Felton, Revenson & Hinrichsen, 1984). An area of growing interest which does indicate the potential negative implications of emotion-focused avoidance coping, however, is that of alcohol consumption and abuse, and it is within this area that the present study is focused. It is a widespread belief that alcohol serves to reduce tension and anxiety in an individual, with this theory being formalised in Conger's tension-reduction hypothesis (1956, cited in Erickson, 2004). The tension-reduction hypothesis is supported by physiological evidence which indicates that alcohol functions to depress central nervous activity and leads to decreased cortical arousal, resulting in a decrease in tension anxiety (Schwarz, Burkhart & Green, 1978), and by other numerous studies that have shown that alcohol reduces both physiological and psychological responsivity to stressful situations (Stasiewicz & Lisman, 1995; Hull & Young, 1983; Levenson, Sher, Grossman, Newman & Newlin, 1980). However, it remains unclear whether the effect is a function of alcohol's impact on tension per se or cognisance of the stressor (Hull, 1981), and some studies have found that alcohol actually serves to increase self-reported anxiety. Evidence for the model is therefore mixed. Nonetheless, the suggestion that alcohol reduces cognisance of a source of tension and causes impairment of perception, thought, and nearly all aspects of information processing (Steele & Josephs, 1990) led to the investigation of a possible relationship between alcohol consumption and coping style. Substance use is posited to assist coping by providing distraction from problems, and diverting attention temporarily from unpleasant self awareness (Hull, 1981), and since alcohol is widely believed to possess a range of properties that facilitate problem avoidance (e.g. helping to forget one's problems, decreasing negative mood), drinking seems especially likely to be used as a problem avoidance strategy and thus to be positively related to other avoidant coping strategies (Moos, Brennan, Fondacaro & Moos, 1990). Social learning theory proposes that people who exhibit abusive patterns of drinking differ from 'healthy' drinkers in their ability to cope with the demands of everyday life, and that deficiencies in more adaptive coping skills may predict the use of drinking as a coping mechanism (Cooper, Russell & George, 1988). People who display dispositions to deny and disengage from stressful problems (those who demonstrate avoidance coping) are thus hypothesised to be especially vulnerable to hazardous levels of alcohol use and alcohol related problems (Cantanzaro & Laurent, 2004). In accordance with this theory, a number of studies have found a positive correlation between an individual's levels of avoidance coping and the amount of alcohol they consume. Fromme & Rivet (1994), for example, report that levels of avoidant coping resources were predictive of both increased frequency of alcohol consumption and increased volume of alcohol consumed, whilst Moos and his colleagues (Moos, Finney & Chan, 1981; Moos, Finney, & Gamble, 1982) found that individuals who relied on avoidant coping strategies that served to discharge or deny emotion were more likely to drink in response to stressful events. However, in contrast to these results Armeli, Carney, Tennen, Affleck and O'Neil (2000) found no relationship between avoidant coping style and alcohol consumption. In the present study we were particularly concerned with the relationship between avoidance coping and alcohol consumption with respect to how university students cope with a high academic workload. It is widely acknowledged that university students represent a group of individuals who have unique drinking patterns (O'Hare, 1997). Drinking is a large part of university culture, and heavy or problematic drinking among students represents a major public health concern (Ham & Hope, 2003), with young adults aged 18-24 showing the highest rates of alcohol use and having the greatest percentage of problem drinkers in the population (Rabow & Duncan-Schill, 1995). However, at the same time university students are frequently confronted by stressful periods when they have a large amount of work to do, with assignments to complete, deadlines to meet and exams to prepare for. This poses a potential problem since their generally high levels of drinking suggest that university students may be particularly likely to exhibit avoidance coping in stressful situations, and thus, if alcohol fosters the use of avoidant coping to such an extent that more adaptive coping strategies such as active problem solving are excluded, it may lead to maladaptive functioning in important life domains such as academic achievement (Windle & Windle, 1996). A positive correlation between levels of avoidance coping and alcohol consumption may therefore suggest that individuals who drink more at university are more likely to deny the importance of work they have to do, perhaps leave it until the last minute, and subsequently exhibit lower grades than those who drink less and are subsequently more able to employ more successful problem-focused strategies. Our study establishes new data in the area of coping because, to our knowledge, there have been no other studies focusing on avoidance coping and alcohol consumption specifically within the student population. Furthermore, our study differs from many others in the fact that we focus purely on one area - that of academic workload - when the vast majority of coping studies ask participants to specify a particular event of their choice that they found stressful, and analyse responses and coping strategies used in terms of whether the situation was appraised as changeable or unchangeable. We decided to structure our study in this manner because a general link between alcohol consumption and the use of avoidant coping strategies has already been established, and we therefore wanted to see if this general conclusion would apply to one specific situation with which all students are familiar. We feel this issue is of particular relevance since drinking alcohol and working are arguably the two main activities that dominate the lives of many students, and our results may provide preliminary evidence indicating that excessive alcohol consumption may be detrimental to the attitudes of students towards academic work. In our study participants were provided with questionnaires comprising two sections. The first section contained 35 items selected from Folkman and Lazarus' (1985) Ways of Coping Questionnaire, commonly regarded as the standard measure for assessing coping strategies by psychologists in this field (Schwarzer & Schwarzer, 1996). Both problem-focused and emotion-focused items were included, with items from seven of the eight sub-scales identified by Folkman and Lazarus (1985). Several of the statements were rephrased in a manner more applicable to the specific study context. In the second section participants' drinking habits were assessed using the Alcohol Use Disorders Identification Test (AUDIT). Developed by Saunders, Aasland, Babor, de la Fuente and Grant (1993), the AUDIT is a 10-item self-report questionnaire measuring alcohol consumption and drinking behaviour. It is used as both a clinical and research screening instrument for hazardous alcohol consumption. The AUDIT has a maximum score of 40 with higher scores indicating more hazardous drinking, and has previously been used in a non-clinical population to discriminate heavy and light drinkers (Sharma, Albery, & Cook, 2001). In accordance with previous studies, we expected that heavy drinkers would exhibit more emotion-focused coping strategies, whilst problem-focused coping would be more prominent amongst lighter drinkers. Our experimental hypothesis was that higher levels of alcohol consumption would be correlated in particular with the greater use of avoidant coping strategies. MethodParticipantsOne hundred second year undergraduates from the University of  participated in the study. Participants were aged between 19 and 21 and were both male and female. They were obtained by means of an opportunity sample. MaterialsQuestionnaires were given out to each of the participants. The questionnaire consisted of two sections. The first section comprised 35 items extracted from the standard Ways of Coping Questionnaire (WCQ), with items from seven of the eight subscales identified in Folkman and Lazarus' 1985 study. These were: 1. Planful Problem Solving (e.g. try to analyse the problem in order to understand it better); 2. Seeking Social Support (e.g. seek sympathy and understanding from others); 3. Distancing (e.g. ignore it and turn to another activity to take my mind off things); 4. Wishful Thinking (e.g. hope a miracle will happen); 5. Self-blame (e.g. realise I brought the problem on myself); 6. Tension-reduction (e.g. jog or exercise); and 7. Self-isolation (e.g. avoid being with people in general). The 'planful problem solving' scale contains purely problem-focused coping items, the 'seeking social support' scale contains a mixture of both problem-focused and emotion-focused coping items, and the remaining five scales contain purely emotion-focused coping items. Items were selected on the basis of relevance to the issue under investigation; no items from the 'positive reappraisal' subscale were therefore selected, since we did not feel this particular coping strategy was particularly applicable. The phrasing of some items was also changed slightly to ensure a close match between the stressful experience identified and the coping statements. The response format was a five point Likert rating scale ranging from 1, ""strongly disagree"" to 5, ""strongly agree"". Since some of the scales had more items than the others, the maximum scores for each scale differed, for example the 'distancing' scale had a maximum score of 55, whilst the 'tension reduction scale' had a maximum score of 15. Participants were asked to indicate the extent of their agreement with each statement as an action they typically undertake in a situation when they are faced with a high academic workload. The second section of the questionnaire consisted of the standard, 10-item Alcohol Use Disorders Identification Test (AUDIT), a measure which aims to identify persons with hazardous or harmful alcohol consumption through asking questions relating to alcohol consumption, abnormal drinking behaviour, adverse psychological reactions to alcohol, and alcohol related problems. Design and ProcedureA within subjects design was used in which participants all filled in the same questionnaire. Participants were initially provided with a consent form which detailed the nature of the task and made it clear that they were not obliged to take part in the questionnaire or complete any questions that they were uncomfortable with. Participants were provided with the experimenters' contact details in case they later wanted to withdraw their information. The experimenters were also equipped with the telephone numbers of relevant help lines and care organisations to give to participants in the unlikely event that the alcohol related questions subjected them to undue stress. On completion of the consent form, participants were asked to think back to a time when they had a high academic workload and think about the feelings they may have and actions they may take in such a situation. They were given the phrase ""When I feel under pressure from the amount of work I have to do I..."" and the separate items from the Ways of Coping Questionnaire were then listed, with participants circling the most appropriate response to each statement. In the second section of the questionnaire participants were asked to complete AUDIT, by answering 10 questions regarding their typical drinking habits and behaviour. The resulting scores for each participant were then obtained by adding up the value of their responses to each sub-scale of the Ways of Coping Questionnaire, to obtain seven different scores, and calculating their overall AUDIT score. ResultsInitial data screening using a boxplot indicated that there were four outliers in the raw data. One had a particularly low score on the 'planful problem solving scale', one on the 'seeking social support' scale, and the other two particularly high scores on the 'self isolation' scale. The data from these subjects was subsequently removed from the study due to the potential disproportionate influence they may have had on results obtained. All analysis was carried out with these outliers removed. There was no missing data in the data set. The original Alcohol Use Disorders Identification Test recommends a cut-off score of 11 as an indicator of hazardous alcohol consumption, so those scoring 10 and under are classified as 'light drinkers', whilst those scoring 11 and over are classified as 'heavy drinkers' (Saunders et al, 1993). Descriptive statistics for the mean scores on each sub-scale of the Ways of Coping Questionnaire using this cut-off score to identify heavy and light drinkers are therefore shown in Table 1. Table 1 illustrates that heavy drinkers scored more highly on each of the sub-scales of the Ways of Coping Questionnaire than light drinkers, with the exception of 'planful problem solving' and 'seeking social support'. This was the pattern of results that was expected, and fits the experimental hypothesis, since the five scales on which heavy drinkers scored more highly all contain purely emotion-focused coping items, whilst 'planful problem solving' and 'seeking social support' contain purely problem-focused coping items, and a mixture of problem and emotion-focused items, respectively. In some cases the difference in mean scores is marginal, for example in the 'tension reduction' scale, where the mean for heavy drinkers is only 0.04 higher than the mean for light drinkers; but in others it is quite noticeable, for example the 'distancing' sub-scale, which contains purely avoidance coping items, and where the difference in mean scores between heavy and light drinkers is 3.03, providing preliminary support for the hypothesis that heavy drinkers engage in significantly more avoidance coping than light drinkers. The standard deviations are, with the exception of 'planful problem solving' and 'tension reduction', higher for heavy drinkers than light drinkers; however, for the most part the difference in standard deviations between the two groups is not large. Heavier drinkers thus simply appear to be slightly more variable in the extent to which they use particular coping strategies. A correlation matrix indicating the inter-correlation between each of the sub-scales of the Ways of Coping Questionnaire, and, most importantly, the correlation between scores on each sub-scale with the corresponding AUDIT score is shown in Table 2: Table 2 illustrates that there are a number of significant correlations in the results obtained, consisting of both inter-correlations between sub-scales of the Ways of Coping Questionnaire and correlations between scores on particular sub-scale with AUDIT scores. Of particular interest to this study are the relationships between the 'planful problem solving', 'distancing' and 'seeking social support' sub-scales, where significant inter-correlations were obtained. Firstly, there is a significant negative correlation between the 'planful problem solving' and 'distancing' sub-scales; r (94) = -0.297; p Figure 1 depicts the positive correlation between AUDIT score and distancing score. Whilst there is quite a large amount of scatter on the graph, it is nonetheless possible to see quite clearly the trend for higher scorers on AUDIT, and thus heavier drinkers, to also score more highly on the use of 'distancing', or avoidance coping, strategies. The relationship between these two variables therefore appears to be quite strong, as well as having been revealed by the Pearson correlation co-efficient test to be highly significant. Figure 2 depicts the negative correlation between AUDIT score and seeking social support score. Again, there is a considerable amount of scatter on the graph and it is arguably harder to detect the presence of a significant negative relationship between the two variables from the graph alone. This may be due to the fact that the maximum score on this sub-scale is only 15, resulting in less possible variation between participants in terms of their score and thus less potential for such a strong negative correlation to emerge. Whilst still significant therefore, this correlation does not appear to be as strong as that between AUDIT and distancing score, a fact which is confirmed by their respective co-efficient values. DiscussionThe results obtained support the hypothesis posited at the beginning of the study, which stated that those with higher levels of alcohol consumption would exhibit more avoidance coping. A significant positive correlation was obtained between participants' scores on the Alcohol Use Disorders Identification Test and their scores on the 'distancing' sub-scale of the Ways of Coping Questionnaire, the scale which is indicative of avoidance coping. In addition, a further significant result that was not originally hypothesised was obtained; that of a negative correlation between AUDIT score and score on the 'seeking social support' sub-scale. Finally, the heavy drinkers obtained higher mean scores on all of the sub-scales of 'emotion-focused coping' than the light drinkers, who in turn obtained higher mean scores on the 'problem-focused coping' sub-scales, demonstrating that heavier drinkers use more emotion-focused and less problem-focused coping strategies. The results therefore imply that, when considered in relation to academic workload in particular, a higher level of alcohol consumption is associated with a tendency to exhibit more avoidant coping strategies, and a smaller likelihood of going to others for help, whether this is emotional help in the form of seeking sympathy and understanding, or practical help in the form of asking a lecturer or tutor for guidance with work. This particular result replicates Karwacki and Bradley's (1996) finding that greater social support seeking was related to lower alcohol consumption in women who had experienced interpersonal stressors, whilst the finding that greater alcohol consumption is associated with higher levels of avoidance coping is consistent with many other research findings which indicate that avoidant coping is associated with greater affective, behavioural and social dysfunction, and with alcohol and drug use behaviours (e.g. Cooper et al, 1988; Windle & Windle, 1996; Williams & Clark, 1998). The fact that higher levels of alcohol consumption are associated with a greater prevalence of avoidant coping can, in part, be explained by the stressor-vulnerability model of alcohol use. The primary hypothesis of the stressor-vulnerability model is that individuals learn to use drinking as a coping response when they believe healthier means of coping are unavailable. Thus, individuals who have a tendency to exhibit high levels of maladaptive avoidant coping strategies and who are deficient in more active, problem-focused strategies are more likely to turn to 'drinking to cope' since they believe that healthier means of coping are unavailable to them (Cantanzaro & Laurent, 2004). These individuals are unable to confront the problem and deal with it directly by using problem-focused coping strategies; however, their high levels of avoidant coping strategies alone are not enough to take their minds off the problem, and so they will turn to alcohol as a further means of coping. This relationship is strengthened by positive expectancies about alcohol's stress-relieving effects (Hasking & Oei, 2002). However, it is admittedly difficult to determine the direction of the relationship between avoidance coping and alcohol consumption, particularly within a student population. Undoubtedly, some students may rely on avoidant coping strategies and the use of alcohol as a means of coping because they have deficits in other coping strategies; however, others - who are perfectly normal in terms of adaptive problem-focused coping strategies - may begin to use alcohol primarily because their peers are doing so, and this excessive alcohol consumption may itself lead to consequences such as the development of deficits in more adaptive coping. It is conceivable that the relationship between alcohol consumption and deficits in coping is therefore bi-directional (Britton, 2004). Increasing deficits in adaptive coping may increase reliance on the use of alcohol as a means of lowering drive level and reducing internal tension; however, even if alcohol is consumed primarily for social reasons, an increasing frequency and level of alcohol consumption may itself cause deficits in adaptive coping which previously did not exist. There is thus a theoretical paradox in trying to decide on the direction of causality in this relationship; substance use may itself be construed as a coping mechanism, but may also be a product of deficits in other coping mechanisms (McKee, Hinson, Wall & Spriel, 1996). Nonetheless, both theory and research suggest that a reliance on avoidant coping can increase risk for heavy alcohol consumption. Avoidance types of coping are predicted to increase risk because the individual is not disposed to deal with problematic situations but rather to seek the path of least resistance toward restoring affective balance (Wills & Hirky, 1996). To the extent that a person tends to cope by denying the existence of problems, seeking distraction from unpleasant self awareness, and withdrawing from engagement with normative sources of reinforcement, then the path to substance use as a coping mechanism would be more likely, primarily due to alcohol's proven ability to interfere with cognitive processes fundamental to self-awareness and to reduce negative self-evaluation following failure (Hull, Levenson, Young & Sher, 1983). Thus for individuals who typically try to cope with negative emotions through avoidance, alcohol's mood-altering properties (e.g. its analgesic effects) may offer an attractive antidote for dysphoria (Cooper, Russell, Skinner, Frone & Mudar, 1992). It is therefore conceivable that the students in our study who had higher levels of alcohol consumption exhibit this pattern of behaviour primarily due to deficits in adaptive coping strategies, a resulting high level of avoidant coping, and a subsequent desire to escape from uncomfortable, troubling, or stressful events as and when they arise. A potential explanation can also be offered for the observed negative correlation between level of alcohol consumption and the use of seeking social support as a coping strategy. Whilst there has by no means been as much research into this area as there has been into the area of avoidance coping, Karwacki and Bradley (1996) report that social support seeking may act as a possible protective factor against harmful levels of alcohol consumption or drinking-related problems. Theories of social support suggest that individuals are less likely to experience negative consequences from social stressors if they receive continued support from non-disrupted aspects of their social networks (Husson, 2003), and this is illustrated in a study by Snow, Swan, Raghavan, Connell and Klein (2003) who found that greater social support served a protective function by contributing to lower levels of stress and fostering greater use of active, problem-focused coping. Therefore, those individuals who do not like to go to others for support and are less inclined to share the negative feelings they are experiencing have less to buffer them against various environmental stressors, and subsequently at stressful times may be more likely to use avoidant coping strategies and furthermore turn to alcohol as a primary way of coping. This is highlighted by the significant negative correlation obtained in the results between the 'distancing' and 'seeking social support' sub-scales, illustrating that individuals who are high in social support seeking are low in avoidant coping strategies, and resultantly less likely to show high levels of alcohol consumption. Overall, therefore, our results support the experimental hypothesis and indicate that there is a significant positive correlation between alcohol consumption and levels of avoidance coping; specifically, heavier drinkers exhibit more avoidant coping strategies than light drinkers. This general result has been reported in much previous research; however, what is of particular interest in our study is that it holds even when all participants were asked to consider the same specific situation, that of a heavy academic workload. This suggests that the relationship observed may be stable across many stressful situations that are familiar to all people, and in particular raises questions regarding the possible detrimental effect that a heavily avoidant coping strategy may have on academic achievement. While we did not investigate this in our study, it may have been interesting to ask participants to indicate the marks they received for all pieces of assessed work in the academic year up to the point of the study, and determine whether a high level of avoidance coping and related alcohol consumption leads to a poorer level of academic functioning; primarily due to the tendency to try and forget about work and put off doing it, combined with the negative impact that alcohol can have on one's ability to concentrate and process perceptual information. Furthermore, it may also be of relevance to obtain data from other years within the university population. Our study focused on second year students for whom most work is quite important, contributing to their overall degree mark. In contrast, for many students the first year does not count, and is often the year of heaviest drinking and socialising, and thus it may be interesting to discover whether the same pattern of results is obtained for such students. It may simply be the case that overall alcohol consumption is higher among all first years, irrespective of whether they employ mainly problem-focused or avoidant coping strategies. Therefore the same pattern of results may not be obtained with first-year students. Another area into which our study could be further extended is the realm of alcohol expectancies, which is another possible predictor of problem drinking. An outcome expectancy is a person's estimate that a given behaviour will lead to certain outcomes (Bandura, 1977) and recently the role of expectancies in predicting alcohol consumption has received increasing attention. Grounded in social learning theory, expectancy argues that a person's alcohol consumption will depend on the outcomes they expect to gain from drinking, for example relaxation, improved confidence, or improved sexual functioning (Hasking & Oei, 2002). Cantanzaro and Laurent (2004) for example, found that positive alcohol expectancies interacted with avoidant coping dispositions to an increased frequency of drinking to cope, increased alcohol use and greater alcohol related problems while Cooper et al (1992) reported that avoidance coping was related to problematic alcohol use among high- but not among low-expectancy individuals in their sample. The relationship between alcohol consumption and prevalence of avoidance coping may therefore be further mediated by such alcohol expectancies. This area may be of particular interest to study in a student population, since for many, university is the first time that they will engage in frequent, heavy drinking behaviours, and thus their alcohol expectancies may be different from older adults who are accustomed to particular patterns of drinking and are well aware of the effects that alcohol can have. Finally, there are a few methodological issues that could be mentioned in the light of our study. Firstly, the fact that only one episode was analysed for each subject - that of a period of a high academic workload - meant that the extent to which the types of coping that participants employ are stable over different episodes could not be investigated. Thus the observed relationship between avoidant coping and alcohol consumption may have applied only to this one particular episode. However, the fact that many previous studies have uncovered a general relationship between alcohol consumption and avoidance coping suggests that this criticism is not particularly damaging to our study. Furthermore, the information was collected retrospectively, and all the data were self-reported, and it is common for the perceived importance of a particular episode and the way it is appraised to change over time (Parkes, 1984). Therefore, participants' responses and the extent to which they reported using particular coping strategies may not have been entirely accurate. However, for the most part we encountered few problems in carrying out the study and have confidence in the results we obtained. The main implication of our study is, we believe, that concerns about the binge drinking culture that are now so heavily expressed within the media may be well justified. The relationship between alcohol consumption and avoidance coping is a complex one, with the direction of causality currently unknown. However, these is undoubtedly a significant relationship, and this may be of particular concern within the university population, where drinking is so often excessive. Many students are placed under great stress in their second and third years of university life in particular, and this stress, combined with deficits in adaptive coping strategies and the tendency to drink large amounts, even for purely social reasons, suggests that students may enter a vicious cycle where they become so heavily dependent on avoidance coping that they cannot confront their workload and fall further and further behind. This confirms that students need to be encouraged to use more efficient, problem-focused coping strategies, and perhaps be trained on skills such as time management and dealing with a heavy workload, especially when first entering university, since there is a large transition, both in terms of work and social relationships, from the secondary school or sixth-form college environments. Students should also be educated about the effects of drinking, and encouraged to view drinking perhaps as a primarily social activity, rather than an effective means of dealing with their problems. A number of conclusions can therefore be drawn from this investigation. Firstly, in accordance with many previous studies, we can conclude that there is a significant relationship between increased level of alcohol consumption and greater use of avoidance coping strategies. Our results indicate that the general relationship between alcohol consumption and avoidance coping which has previously been reported can also be applied to more specific events, where participants are asked to think of a situation which has undoubtedly been present in all of their lives at some recent time, that of a high academic workload. The fact that this relationship was obtained even within a university population, where the vast majority of students have elevated levels of alcohol consumption when compared with the general public, implies that it may be a particularly strong one. Furthermore, we suggest that a greater level of alcohol consumption is related to a smaller likelihood of seeking support from or sharing negative feelings with others in times of stress. Those who use seeking social support as a coping strategy are subsequently more likely to use avoidance forms of coping. Our results therefore confirm the experimental hypotheses made at the beginning of the study and suggest that future research which investigates the potential relationship between students' academic achievement in relation to their levels of avoidance coping and alcohol consumption may be of particular interest.","The term 'meme' was first introduced by Richard Dawkins in 1976. Dawkins popularised the influential view that evolution is best understood in terms of competition between genes (Blackmore, 1999), and in his conception of memes he takes this one step further, drawing on the analogy between genetic and cultural evolution to claim that culture can be split into units called memes; ideas, habits, skills, stories or any kind of behaviour or information that is copied from person to person by imitation (Dawkins, 1976). Memes are the replicators of the cultural world; the smallest elements of culture or complex ideas that replicate themselves with reliability and fecundity (Dennett, 1993). Like genes, memes are selfish. They pursue their own interests and are concerned only with their survival and replication. Since each mind has a limited capacity for memes, there is considerable competition among memes for entry into as many minds as possible (Ball, 1984). Dawkins urges us to take the idea of meme evolution literally; it is not just analogous to biological or genetic evolution (Dennett, 1990), but a phenomenon that obeys the laws of natural selection exactly, and is achieving evolutionary change at a rate that leaves conventional genetic evolution far behind (Dawkins, 1976). One of the most comprehensive works on memes is Blackmore's (1999) The Meme Machine, which uses memes to elaborate an ambitious theory designed to account for numerous aspects of human evolution and psychology (Jahoda, 2002). Blackmore claims that imitation, as the primary mechanism by which memes replicate themselves, can be taken as the basis of a major theory of the evolution of the human mind and even of what it means to be a conscious self. Imitation is posited as the key to what set our ancestors apart from all other animals. The theory starts with one simple mechanism - the competition between memes to get into human brains and be passed on again - and from this it gives rise to explanation for such diverse phenomena as the evolution of the enormous human brain, the origins of language, human altruism, sex, religion, and even the evolution of the internet (Blackmore, 1999). Blackmore's theory is certainly well argued, and her justifications of both the meme-gene analogy and how memes drive many aspects of human development are theoretically sound. However, I believe there may be an initial flaw in Blackmore's argument in that she appears to take the existence of memes for granted and expounds her detailed argument from this premise when this fact is not necessarily clear. It may be contended that if one refuses to acknowledge the reality of memes, Blackmore's argument fails. One can question whether cultural evolution is truly analogous to genetic evolution (Dennett, 1995), whether there are really cultural units that have interests of their own, and whether imitation is indeed the driving force behind all human behaviour (Holdcroft & Lewis, 2001). Midgley (2000), for example, calls memes 'mythical entities' and an 'empty and misleading metaphor'. She argues that thought and culture cannot be broken down into distinct units, since they are moving and developing patterns in human behaviour. There is therefore no use in trying to understand them by tracing reproductive interactions among memetic units Midgley, 1999). Nonetheless, if, for now, one accepts the existence of memes and ignores such criticism, Blackmore's memetic model provides feasible and coherent explanations of a diverse range of phenomena that have traditionally proved difficult to explain in terms of biological evolution or individual genetic advantage (Edmonds, 2002). It has been argued that the development of human language, for example, does not conform to a Darwinian evolutionary explanation since it shows no genetic variation or selective advantage and cannot exist in intermediate forms (Blackmore, 1999). The same can be said of the excessively large human brain, which stands out greatly in comparisons of encephalisation with other primates. Most early theories suggest that toolmaking, technological advances and the human social environment, drove the need for a larger brain, whilst language is explained as a non-adaptive side consequence of the evolution of the brain, or as evolving as a response to selection pressure for improved communication between humans (Gould, 1997). However, Blackmore presents a memetic explanation. Memes compete to be copied, and people who are especially good at imitation gain a survival advantage by being able to copy the currently most useful memes. Since imitation is a difficult skill requiring extra brain power, the advantage is given to genes for bigger brains and better imitation. The genes were therefore forced into creating big brains, capable of spreading memes. Blackmore also proposes that the human language faculty primarily provided a selective advantage to memes, rather than genes. Successful replicators are those with high fidelity, longevity and fecundity. Digitisation of sounds into words may increase fidelity, combining words into novel combinations may improve fecundity, and every improvement leads to increased memetic competition. The memes that can get themselves spoken will be copied more than those that cannot, and people who can best copy the winning sounds have an advantage and pass on the genes that gave them that ability. Gradually, human brains would be driven by this emerging language. The development of language was thus an evolutionary process like any other, creating complex design apparently out of nowhere (Blackmore, 2003). A further problem Blackmore tackles is that of altruism, which conflicts with traditional understanding that humans selfishly pursue their own interests. Traditionally altruism has been explained either in terms of advantage to the genes; you are altruistic towards your children because they are the only direct way your genes can be carried on into future generations; you are altruistic toward fellow men because you may get aid in return, or in terms of a true morality, an independent moral conscience or a spiritual essence in humans that overcomes the selfish dictates of our genes; a view which finds little favour with scientists (Blackmore, 1999). Memetics provides a third solution. Blackmore suggests that the kind, generous, altruistic person will spread more memes. He will have more friends and spend more time talking to them; they will be influenced by him and in turn will imitate his popular behaviour. Thus his altruism will spread. In contrast, the mean and selfish person will have few chances to replicate his memes because the few people who could potentially imitate him rarely do so. Blackmore provides a number of such examples of behaviours that are mimetically adaptive but biologically unadaptive (Cook, 1995); for example, she claims the rise in the use of birth control, whilst not genetically advantageous, means women who have fewer children will have more time to spread their own memes, including memes for birth control and the pleasures of a small family. These are the women whose success inspires others, and who provide role models for other women to copy. Furthermore, whilst adoption may be viewed as a mistake from the gene's point of view, for memes the benefits of adoption are obvious. As far as memes are concerned, the time and effort expended on an adopted child are as valuable as that expended on one's own offspring. All that matters is that you can pass on your memes to people who will be influenced by you and subsequently imitate your behaviours and beliefs. Genetic relation is of no importance. In both of these examples there is a battle between memes and genes to take control over the machinery of replication (Blackmore, 1999). All of these arguments therefore appear to be perfectly logical and acceptable; however, as previously argued, I believe they are only acceptable if one first accepts the existence of memes. If one refuses to believe that such elements of culture exist or compete to get themselves replicated through imitation, there is no reason to believe Blackmore's theory. Furthermore, I believe that Blackmore goes too far in attempting to apply memes to areas such as mate selection, and in implying that memes are even responsible for our sense of a conscious self. Traditional genetic evolutionary theory suggests we choose mates based on fitness and biological advantage; men are sexually attracted to women who appear fertile, with big hips and youthful, symmetrical features, whilst women are attracted to strong, high-status men with plentiful resources. Essentially we should choose to mate with people who would, in the environment of our evolutionary past, have helped to increase our genetic legacy (Blackmore, 1999). Blackmore suggests mate choice is based not only on genetic advantage, but also memetic advantage. Once memes arose in our far past, natural selection would favour people who chose to mate with the best imitators. In an early hunter-gatherer society a man who was especially good at imitation would have been able to copy the latest hunting skills or stone tool technology and hence would have gained a biological advantage. Thus a woman who mated with him would be more likely to have children who shared that imitation ability and that advantage. Such a woman would have to look in potential mates for signs of the general ability to imitate and innovate. This argument suggests we will still want to mate with the best imitators and spreaders of memes, such as writers, artists, journalists, broadcasters, film stars and musicians (Blackmore, 1999). However, whilst it may be the case that some high profile people who are prominent in the media are often seen as desirable mates, this is not true for all such people, and I do not think it can be reasonably argued that we choose mates on the basis of whether they are a good imitator. Firstly, we arguably do not know what the signs of a good imitator are, and secondly, in being attracted to someone we tend to consider factors such as their looks and personality; we are certainly not aware of considering their potential for spreading memes. Furthermore I believe that Blackmore's conclusion regarding our concept of the conscious self is largely unbelievable. Blackmore claims that the idea of the continuous, persistent and autonomous self is an illusion; the self is actually just a collection of memes. Memes gain an advantage by becoming associated with a person's self-concept. A 'self' therefore aids their replication. The sense of 'self' has simply been created by the processes of memetic evolution acting in the relatively short period of one human lifetime. Blackmore then goes even further and says that the ways we behave, the choices we make, and the things we say are all a result of complex interactions between memes. There is no inner self which is the initiator of actions, controls the body, or is conscious; thus there is no free will. Blackmore presents an eliminative account of design, mind, and human agency which removes the possibility of voluntary choice or invention on the part of humans (Holdcroft & Lewis, 2001). Intentionality is transferred away from humans and to the memes. Whilst Blackmore has tried to argue against the fact that her argument makes humans completely passive, stating that our intelligence, capacity for making choices, and active social life are all part of the copying environment in which memes compete (Blackmore, 2002), it is hard to see that this could imply anything other than that we are caught up in ""a remorseless process in which we are mere pawns"" (Jahoda, 2002). I find this aspect of Blackmore's argument hard to accept since it contradicts our very experience of thought and conscious deliberation, and violates the assumption that in making choices we truly have free will and are not influenced or bound by either internal or external forces. Overall I would argue that The Meme Machine provides a detailed and interesting account of the concept of memes, and certainly does provide some plausible theories that appear to be explained better by memetics than any other theory we currently have available. However, I believe that in attempting to use memes to explain all aspects of human nature and behaviour, Blackmore takes memetic theory beyond logical boundaries. Furthermore, Blackmore restricts memetic transmission to imitation, claiming that imitation is what sets us apart from other animals; however Tomasello (1999) argues that whilst imitation has played a role in cultural evolution the critical difference is in fact the understanding of others as intentional beings like the self. Thus, if the basic principle of cultural learning being driven solely by imitation is rejected the foundations of The Meme Machine collapse and there is no reason why we should accept any of Blackmore's arguments. Critics also argue that just as a gene is nothing without its biological and environmental context the same is true for memes (Wheelwell, 1998; Dupre, 2000), and indeed it is difficult to conceive of memes as cultural units residing in brains that exist independently of contextual interpretation. However, proponents of memetics argue psychology will be as transformed by this discipline as biology was by Darwin (Brodie, 1996), and this perhaps is where the problem lies in getting people to accept memetic theory. In acknowledging the existence of these cultural replicators we have to radically transform our ideas regarding human nature, behaviour and motivation and reformulate our understanding of how and why culture evolved, just as Darwin caused us to rethink our beliefs about the origin of our species. Maybe we are too scared to accept we do not have consciousness, free will, or the possibility of rational choice but rather are completely at the mercy of the interaction between our memes and our genes? Whether the theory of memes is true or not, The Meme Machine provides a comprehensive and detailed insight into how memes may shape human development which, although somewhat unbelievable in parts, is certainly an interesting and enlightening read.",False
80,"Philip G. Zimbardo. Stanley Milgram. Their names are synonymous with any form of psychological research. Their work rages with controversy as with their theories that still spark incessant debates. Both are eminent psychologist that studied aspects of human behaviour concerned with obedience to an authoritative figure. Both psychologists studied this phenomenon differently using different methods. Zimbardo created a make-shift prison and Milgram created an electric shock experiment, to ultimately deduce the same conclusion. The conceptual reason behind Zimbardo's experiment was to test the power of social situations over individuals. The focus was on the power of roles, rules, symbols, group identity, and situational validation of ordinarily ego-alien behaviors and behavioral styles. Zimbardo explored the theory of deindividuation and its influence on obedience. The psychological state of deindividuation is aroused when individuals become part of a crowd or large group. This state is characterized by diminished self-awareness and individuality. There is a reduction in an individual's self-restraint and normative regulation of behavior which leads to a newfound obedience to any form of authority. Milgram on the other hand had a different hypothesis and focus. His study was inspired by the many atrocities that had been committed in the Second World War. Many culprits put on trial at the end of the war used the chief defence that they had been following orders from somebody above. It was of popular opinion that there was something intrinsic in the German character that made them particularly cruel. That was a dispositional view that people are genetically determined and a person's genetic makeup was the influence behind their acting in a certain fashion. This view was questioned and disputed by Milgram. He inferred that most people, regardless of nationality, could perform inhuman acts upon another, under certain circumstances. Milgram reasoned that people could commit atrocities when they are given orders by an authority. Where Zimbardo's aim was to study situational behaviour, in the context of deindividualization, and its ability to affect obedience, Milgram sought to prove that when under authority, people would obey indubitably. Zimbardo's experiment commenced on a quiet Sunday morning in August. 24 males were used as subjects and they were randomly allocated to either ""guard"" or ""prisoner"". They were then to assume the roles as they best see fit. Prisoners had to wear a smock with no underwear, rubber sandals, stockings on their heads and a heavy chain on their right foot. Guards wore a khaki uniform, reflective sunglasses, a whistle around their necks and they carried a billy club. The subjects were placed in a make-shift prison. Prisoners were never called by name and were only identified by number. All these were to cause deindividuation of both prisoners and guards. Cameras and intercom systems were set up so as to be able to observe and record all the happenings. Milgram on the other hand had a very dissimilar experiment set up to Zimbardo's experiment. Milgram placed his subjects under a certain situation for only a short period of less than an hour whereas Zimbardo's subjects were placed in a make-shirt prison for a considerably longer period of 6 days. Milgram's obedience study was carried out to measure to what degree people would obey an authoritative figure in the light of a moral dilemma. The sample for the experiment was chosen from a newspaper advertisement. It must be noted that the participants were clearly informed that they were paid the money just for appearing at the laboratory, and they were free to leave at anytime without forfeiting the money. A stern looking male experimenter was used and a mild mannered man played the part of the victim. The subject was placed as the teacher in a rigged draw. The teacher reads a list of word pairs which the learner has to learn. The teacher then tests the learner and if a wrong answer is given, the electric voltage would be increased. The 1971 Stanford Prison Experiment was a classic demonstration of the power of social situations to distort personal identities and long cherished values and morality as normal healthy males internalized situated identities in their roles as prisoners and guards. This psychological condition was clearly demonstrated in a particular episode with the prisoners. Prisoners were told if they wanted to leave the prison, they would have to go through a parole hearing. When the prisoners were asked whether they would forfeit the money they had earned up to that time if they were to be paroled, most said yes. This proves that the money was not a factor in their obedience. However when the hearings were ended by the parole officers telling the prisoners to go back to their cells while their request were being considered, every prisoner obeyed, even though they could have obtained the same result by simply quitting the experiment. Why did they obey? Because they felt powerless to resist. By this stage, the prisoners were immersed too deeply in their restrictive roles to realise that the officers had turned down their request as prisoners but not as a free human being. Their sense of reality had shifted, and they no longer perceived their imprisonment as an experiment. The participants were deindividuated and recognised their status only as prisoners, not as paid experimental participant. This led to their blind compliance and unquestionable obedience to their orders. In Milgram's experiment, it was predicted that only one in a thousand would continue to the end. In fact, 26 out of 40 subjects continued until the end. It must be stated that although there were many protests from the subjects especially when the learner started to cry out in pain, none of the subjects ceased obeying the experimenter when prodded to continue. Milgram had essentially gotten 100% obedience out of his subjects. Interestingly enough, subjects in both experiments displayed physical symptoms of their discomfort. Subjects of Milgram's experiment exhibited signs of extreme nervousness and started displaying sweating, trembling, stuttering, biting lips, groan and dug their fingernails into their flesh. Some subjects burst out in nervous laughter and three subjects had seizures. This sort of behaviour is comparable to those displayed in Zimbardo's subjects. In a bid to escape the mental torture and trauma they were in, four prisoners reacted by breaking down emotionally. One prisoner even developed a psychosomatic rash all over his entire body when he learned that his parole request had been turned down. By the end of Zimbardo's study, the prisoners were disintegrated, both as a group and as individuals. There was no longer any group unity; just a bunch of isolated individuals hanging on. The guards had won total control of the prison, and they commanded the blind obedience of each prisoner. Likewise in Milgram's experiment, the experimenter seen as authority had commanded obedience of each subject. The experiment demonstrated with jarring clarity that ordinary individuals could be induced to acts of cruelty even in the absence of physical coercion. Humans need not be innately evil to behave in ways that are reprehensible and inhumane. While it was thought and believed that when confronted with a moral dilemma humans would act as their conscience dictates, Milgram's obedience experiments proved that in a concrete situation with powerful social constraints, moral sense can easily be overcame. Both Zimbardo and Milgram's studies demonstrated obedience to an authority even in moral dilemmas and personal disinclination to do so. Their controversial research stunned the world. There has since been a moral outrage about prisons and their treatment of prisoners. Their biggest contribution to the world is the knowledge that their research now brings. Knowledge is power and it empowers us to prevent and minimize such prison situations and possible suppressions of our morality in the face of authority. Milgram's warning--that when an individual ""merges...into an organizational structure, a new creature replaces autonomous man, unhindered by the limitations of individual morality, freed of human inhibition, mindful only of the sanctions of authority""--has much resonance. We did not need Milgram to tell us we have a tendency to obey orders but we did need a reminder of just how powerful this tendency is. As we have now been enlightened about our extreme readiness to obey authorities, we can attempt to better guard ourselves against unwelcome or reprehensible commands. Zimbardo's research would also benefit many prison reforms. With this knowledge of the possible repercussions and outcomes that could occur in prison, jails all over the world should be better equipped to prevent or at least minimize bad treatment of prisoners' occurrences, from happening. This knowledge has also benefited leadership courses as now people are taught to be more aware of the way they deal with their subordinates. Zimbardo's research can be mirrored to today as with the Iraqi prisoners. There is still mass torture prevailing even in this day and age. His research provided us with an explanation and a prediction of possible occurrence but has done nothing to prevent it.","What is a psychological disorder? A psychological disorder can be defined as a dysfunction within an individual that is associated with distress or impairment in functioning and a response that is not typical. The behaviourist approach in psychology understands human behaviour as a consequence of conditioning. This approach claims that all human behaviour is shaped by reinforcement from the environment and the consequences of actions. Humans become conditioned to respond to our environment as a result of leaning by association or operant. Learning by association or classical conditioning is attributed to Ivan Pavlov and his theory of classical conditioning, while operant conditioning, or learning by way of reward and unpleasant consequence, is credited to BF Skinner. Operant conditioning states that all behaviour is a result of learning, of positive reinforcement, negative reinforcement and punishment. (Skinner, B. F., 1974) Perhaps a person has been rewarded for doing exhibiting certain behaviour in the past and these acts as positive reinforcement and helps sustain the behaviour. Similarly, a person may have been told off for exhibiting a certain type of bad behaviour and these acts as punishment and a deterrent of such future behaviour. Classical conditioning is ""a form of learning in which a hitherto neutral stimulus, the conditioned stimulus is paired with an unconditioned stimulus regardless of what the animal does."" (Gleitman, H., Fridlund, A. J., Reisberg, D.J., 1999, Pg C5) The behaviourist approach is useful as it attempts to isolate past events that have produced certain behaviour. The advantages to the behaviourist approach are that it fits in with general scientific approaches and is based on a considerable amount of research evidence. This approach shows how reinforcement from the environment can shape behaviour. For example, being rewarded for displaying a certain behaviour means that this behaviour is likely to continue. Behaviourists look to establish cause and effect relationships seeing exactly how, for example, reward and punishments produce and shape behaviour. It is one of the few approaches in psychology that leads directly to successful treatments of various psychological disorders. (Mackenzie, B. D.1977) On the other hand this approach is deterministic as it suggests that people have no choice in their actions and passively experience their environment i.e. they have no free will. A major limitation is that this fails to consider cognitive processes that may explain certain types of behaviour. Humans have been known to display irrational behavior when in extreme emotional pain. In this case, there is a cognitive reason for their behavior. Additionally, in situations of danger, the body engages in a flight or fight reaction which is purely physiological. The behavior of a person in a dangerous situation is not subject to any form of conditioning but merely a specific physiological reaction. The behaviorist approach also fails to consider other factors such as those described by the psychodynamic and biological approaches. It has been proven that genetics can have an effect personality. Twin studies have shown that genetics are a major influence on personality and this should be considered when treating psychological disorders. This suggests that behaviourism is over-simplistic and too reductionist as it only considers stimulus-response. There are many different methods of treatment that encompass the behaviorist approach to psychological disorders. One of which is systematic desensitization. Systematic desensitization is a common method used to treat a patient with a phobia. This technique was developed by Joseph Wolpe and involves understanding what provokes least and most anxiety, learning relaxation techniques, then associating relaxation with the least provoking stimuli. With this method, patients are allowed to gradually overcome their phobias. They are allowed time to understand and learn an independent method of dealing with their ""irrational"" fear and feel anxious about and anxiety. This will lead to a sense of accomplishment of battling their phobia and also prevent a relapse as they have the knowledge that they are capable of controlling their fear and feel anxious about and anxiety. On the other hand, the process is slow and patients might find it difficult or unrealistic to visualize appropriate situations. Furthermore, the transition from the session to real-life experiences is not easy to accomplish as there has been a shift in reality. Another demerit of this method is that it is limit to verbal communication and as one of the main components of systematic desensitization is positive self-talk, a mute may not be able to benefit from this treatment. (Hull, C.L., 1951) A method that is similar to systematic desensitization is In Vivo Exposure. With In Vivo Exposure the patient is subjected to the actual anxiety-producing situations. The patient is slowly put in the situations starting with the least feared one with the help of their therapists. Eventually, the patient will have to physically face their fear and feel anxious about and anxiety. This is effective as there is a constant sense of reality throughout the treatment. In addition, it allows the patients to gradually get used to their fear and anxiety. A problem with this method is that unless the patient is willing to let themselves be in their feared situation, it is very difficult to implement this method. It is also a lengthy process and patients may not be willing to follow through with all the required sessions. (Atkinson, R. L., Atkinson, R. C., Smith, E. E., Bem, D. J., and Nolen-Hoeksema, S., 2000) Avery effective method of In Vivo Exposure is flooding. Flooding is a process by which the patient is exposed to their feared object or situation for an extended period with no break from it. It has been found to be very effective but it is almost impossible to find patient willing to be put through this kind of treatment. There are also ethical issues involved in whether it is ethical to put a person in a situation which can cause physical and mental harm to the patient. (Atkinson, R. L., Atkinson, R. C., Smith, E. E., Bem, D. J., and Nolen-Hoeksema, S., 2000) Modeling is another effective means of changing behaviour. It is the process by which a patient observed someone's behaviour and learning and imitating it. Perhaps one of the most evidentiary supports for this method comes from a study by Bandura, Ross and Ross in 1961. They did a study on transmission of aggression through imitation of aggressive models. It was concluded that humans learn from imitation and therefore it provides grounds for this method to be used. An advantage of this method is that as the patient is watching someone else in the feared situation, they are being exposed to it without being actually in the situation. They are not physically being hurt unlike In Vivo Exposure. A disadvantage is that the patient may be comfortable watching someone else in the situation but may not be able to handle being in the actual situation themselves. Additionally, this method is basing on people who are easily influenced into imitating. If this patient has a very strong sense of self, they might not be so susceptible to this method. Furthermore, if there is a reason for this fear and feel anxious about and anxiety, it might have cognitive reasons behind it and by not addressing it, this form of therapy might merely be suppressing it. (Gleitman, H., Fridlund, A. J., Reisberg, D.J., 1999) Avoidance learning is instrumental learning in which the response averts an aversive stimulus before it occurs. It is a type of therapy that that combines classical conditioning and operant conditioning. Classical conditioning predicts the aversive event that will follow a certain behavior and operant conditioning ""teaches"" through negative reinforcement, how to prevent the event from occurring. (Skinner, B, F., 1974) This method teaches patients how to avoid situation which they fear and feel anxious about. However, this is merely avoiding the situation and not addressing the real issue behind the fear and feel anxious about. Conversely, this method only works on selective disorders. For example it does not work on sociopaths as they have chronic under-arousal. Behaviour therapy is often used in the treatment of psychological disorders. There are many merits to this method. It teaches patients how to prevent themselves from being in situations that they fear or feel anxious about. It also arms patients with the knowledge of what to expect. Knowledge is power and having this knowledge means the patients are better equipped with their fear and anxiety. However, this method is also reductionist and deterministic as it does not consider other factors such as biological, psychodynamic and cognitive factors. These factors might play a big part in the patient's fear and feel anxious about or anxiety and by excluding such therapies, behaviour therapists might not be really addressing the problem. They might merely be teaching their patients how to repress their fear and anxiety. It is important to note however, that behaviour therapy has a very high success rate and whether the reasons behind the feared issue are truly addressed or not, the treatment is effective.",True
81,"What is a psychological disorder? A psychological disorder can be defined as a dysfunction within an individual that is associated with distress or impairment in functioning and a response that is not typical. The behaviourist approach in psychology understands human behaviour as a consequence of conditioning. This approach claims that all human behaviour is shaped by reinforcement from the environment and the consequences of actions. Humans become conditioned to respond to our environment as a result of leaning by association or operant. Learning by association or classical conditioning is attributed to Ivan Pavlov and his theory of classical conditioning, while operant conditioning, or learning by way of reward and unpleasant consequence, is credited to BF Skinner. Operant conditioning states that all behaviour is a result of learning, of positive reinforcement, negative reinforcement and punishment. (Skinner, B. F., 1974) Perhaps a person has been rewarded for doing exhibiting certain behaviour in the past and these acts as positive reinforcement and helps sustain the behaviour. Similarly, a person may have been told off for exhibiting a certain type of bad behaviour and these acts as punishment and a deterrent of such future behaviour. Classical conditioning is ""a form of learning in which a hitherto neutral stimulus, the conditioned stimulus is paired with an unconditioned stimulus regardless of what the animal does."" (Gleitman, H., Fridlund, A. J., Reisberg, D.J., 1999, Pg C5) The behaviourist approach is useful as it attempts to isolate past events that have produced certain behaviour. The advantages to the behaviourist approach are that it fits in with general scientific approaches and is based on a considerable amount of research evidence. This approach shows how reinforcement from the environment can shape behaviour. For example, being rewarded for displaying a certain behaviour means that this behaviour is likely to continue. Behaviourists look to establish cause and effect relationships seeing exactly how, for example, reward and punishments produce and shape behaviour. It is one of the few approaches in psychology that leads directly to successful treatments of various psychological disorders. (Mackenzie, B. D.1977) On the other hand this approach is deterministic as it suggests that people have no choice in their actions and passively experience their environment i.e. they have no free will. A major limitation is that this fails to consider cognitive processes that may explain certain types of behaviour. Humans have been known to display irrational behavior when in extreme emotional pain. In this case, there is a cognitive reason for their behavior. Additionally, in situations of danger, the body engages in a flight or fight reaction which is purely physiological. The behavior of a person in a dangerous situation is not subject to any form of conditioning but merely a specific physiological reaction. The behaviorist approach also fails to consider other factors such as those described by the psychodynamic and biological approaches. It has been proven that genetics can have an effect personality. Twin studies have shown that genetics are a major influence on personality and this should be considered when treating psychological disorders. This suggests that behaviourism is over-simplistic and too reductionist as it only considers stimulus-response. There are many different methods of treatment that encompass the behaviorist approach to psychological disorders. One of which is systematic desensitization. Systematic desensitization is a common method used to treat a patient with a phobia. This technique was developed by Joseph Wolpe and involves understanding what provokes least and most anxiety, learning relaxation techniques, then associating relaxation with the least provoking stimuli. With this method, patients are allowed to gradually overcome their phobias. They are allowed time to understand and learn an independent method of dealing with their ""irrational"" fear and feel anxious about and anxiety. This will lead to a sense of accomplishment of battling their phobia and also prevent a relapse as they have the knowledge that they are capable of controlling their fear and feel anxious about and anxiety. On the other hand, the process is slow and patients might find it difficult or unrealistic to visualize appropriate situations. Furthermore, the transition from the session to real-life experiences is not easy to accomplish as there has been a shift in reality. Another demerit of this method is that it is limit to verbal communication and as one of the main components of systematic desensitization is positive self-talk, a mute may not be able to benefit from this treatment. (Hull, C.L., 1951) A method that is similar to systematic desensitization is In Vivo Exposure. With In Vivo Exposure the patient is subjected to the actual anxiety-producing situations. The patient is slowly put in the situations starting with the least feared one with the help of their therapists. Eventually, the patient will have to physically face their fear and feel anxious about and anxiety. This is effective as there is a constant sense of reality throughout the treatment. In addition, it allows the patients to gradually get used to their fear and anxiety. A problem with this method is that unless the patient is willing to let themselves be in their feared situation, it is very difficult to implement this method. It is also a lengthy process and patients may not be willing to follow through with all the required sessions. (Atkinson, R. L., Atkinson, R. C., Smith, E. E., Bem, D. J., and Nolen-Hoeksema, S., 2000) Avery effective method of In Vivo Exposure is flooding. Flooding is a process by which the patient is exposed to their feared object or situation for an extended period with no break from it. It has been found to be very effective but it is almost impossible to find patient willing to be put through this kind of treatment. There are also ethical issues involved in whether it is ethical to put a person in a situation which can cause physical and mental harm to the patient. (Atkinson, R. L., Atkinson, R. C., Smith, E. E., Bem, D. J., and Nolen-Hoeksema, S., 2000) Modeling is another effective means of changing behaviour. It is the process by which a patient observed someone's behaviour and learning and imitating it. Perhaps one of the most evidentiary supports for this method comes from a study by Bandura, Ross and Ross in 1961. They did a study on transmission of aggression through imitation of aggressive models. It was concluded that humans learn from imitation and therefore it provides grounds for this method to be used. An advantage of this method is that as the patient is watching someone else in the feared situation, they are being exposed to it without being actually in the situation. They are not physically being hurt unlike In Vivo Exposure. A disadvantage is that the patient may be comfortable watching someone else in the situation but may not be able to handle being in the actual situation themselves. Additionally, this method is basing on people who are easily influenced into imitating. If this patient has a very strong sense of self, they might not be so susceptible to this method. Furthermore, if there is a reason for this fear and feel anxious about and anxiety, it might have cognitive reasons behind it and by not addressing it, this form of therapy might merely be suppressing it. (Gleitman, H., Fridlund, A. J., Reisberg, D.J., 1999) Avoidance learning is instrumental learning in which the response averts an aversive stimulus before it occurs. It is a type of therapy that that combines classical conditioning and operant conditioning. Classical conditioning predicts the aversive event that will follow a certain behavior and operant conditioning ""teaches"" through negative reinforcement, how to prevent the event from occurring. (Skinner, B, F., 1974) This method teaches patients how to avoid situation which they fear and feel anxious about. However, this is merely avoiding the situation and not addressing the real issue behind the fear and feel anxious about. Conversely, this method only works on selective disorders. For example it does not work on sociopaths as they have chronic under-arousal. Behaviour therapy is often used in the treatment of psychological disorders. There are many merits to this method. It teaches patients how to prevent themselves from being in situations that they fear or feel anxious about. It also arms patients with the knowledge of what to expect. Knowledge is power and having this knowledge means the patients are better equipped with their fear and anxiety. However, this method is also reductionist and deterministic as it does not consider other factors such as biological, psychodynamic and cognitive factors. These factors might play a big part in the patient's fear and feel anxious about or anxiety and by excluding such therapies, behaviour therapists might not be really addressing the problem. They might merely be teaching their patients how to repress their fear and anxiety. It is important to note however, that behaviour therapy has a very high success rate and whether the reasons behind the feared issue are truly addressed or not, the treatment is effective.","Philip G. Zimbardo. Stanley Milgram. Their names are synonymous with any form of psychological research. Their work rages with controversy as with their theories that still spark incessant debates. Both are eminent psychologist that studied aspects of human behaviour concerned with obedience to an authoritative figure. Both psychologists studied this phenomenon differently using different methods. Zimbardo created a make-shift prison and Milgram created an electric shock experiment, to ultimately deduce the same conclusion. The conceptual reason behind Zimbardo's experiment was to test the power of social situations over individuals. The focus was on the power of roles, rules, symbols, group identity, and situational validation of ordinarily ego-alien behaviors and behavioral styles. Zimbardo explored the theory of deindividuation and its influence on obedience. The psychological state of deindividuation is aroused when individuals become part of a crowd or large group. This state is characterized by diminished self-awareness and individuality. There is a reduction in an individual's self-restraint and normative regulation of behavior which leads to a newfound obedience to any form of authority. Milgram on the other hand had a different hypothesis and focus. His study was inspired by the many atrocities that had been committed in the Second World War. Many culprits put on trial at the end of the war used the chief defence that they had been following orders from somebody above. It was of popular opinion that there was something intrinsic in the German character that made them particularly cruel. That was a dispositional view that people are genetically determined and a person's genetic makeup was the influence behind their acting in a certain fashion. This view was questioned and disputed by Milgram. He inferred that most people, regardless of nationality, could perform inhuman acts upon another, under certain circumstances. Milgram reasoned that people could commit atrocities when they are given orders by an authority. Where Zimbardo's aim was to study situational behaviour, in the context of deindividualization, and its ability to affect obedience, Milgram sought to prove that when under authority, people would obey indubitably. Zimbardo's experiment commenced on a quiet Sunday morning in August. 24 males were used as subjects and they were randomly allocated to either ""guard"" or ""prisoner"". They were then to assume the roles as they best see fit. Prisoners had to wear a smock with no underwear, rubber sandals, stockings on their heads and a heavy chain on their right foot. Guards wore a khaki uniform, reflective sunglasses, a whistle around their necks and they carried a billy club. The subjects were placed in a make-shift prison. Prisoners were never called by name and were only identified by number. All these were to cause deindividuation of both prisoners and guards. Cameras and intercom systems were set up so as to be able to observe and record all the happenings. Milgram on the other hand had a very dissimilar experiment set up to Zimbardo's experiment. Milgram placed his subjects under a certain situation for only a short period of less than an hour whereas Zimbardo's subjects were placed in a make-shirt prison for a considerably longer period of 6 days. Milgram's obedience study was carried out to measure to what degree people would obey an authoritative figure in the light of a moral dilemma. The sample for the experiment was chosen from a newspaper advertisement. It must be noted that the participants were clearly informed that they were paid the money just for appearing at the laboratory, and they were free to leave at anytime without forfeiting the money. A stern looking male experimenter was used and a mild mannered man played the part of the victim. The subject was placed as the teacher in a rigged draw. The teacher reads a list of word pairs which the learner has to learn. The teacher then tests the learner and if a wrong answer is given, the electric voltage would be increased. The 1971 Stanford Prison Experiment was a classic demonstration of the power of social situations to distort personal identities and long cherished values and morality as normal healthy males internalized situated identities in their roles as prisoners and guards. This psychological condition was clearly demonstrated in a particular episode with the prisoners. Prisoners were told if they wanted to leave the prison, they would have to go through a parole hearing. When the prisoners were asked whether they would forfeit the money they had earned up to that time if they were to be paroled, most said yes. This proves that the money was not a factor in their obedience. However when the hearings were ended by the parole officers telling the prisoners to go back to their cells while their request were being considered, every prisoner obeyed, even though they could have obtained the same result by simply quitting the experiment. Why did they obey? Because they felt powerless to resist. By this stage, the prisoners were immersed too deeply in their restrictive roles to realise that the officers had turned down their request as prisoners but not as a free human being. Their sense of reality had shifted, and they no longer perceived their imprisonment as an experiment. The participants were deindividuated and recognised their status only as prisoners, not as paid experimental participant. This led to their blind compliance and unquestionable obedience to their orders. In Milgram's experiment, it was predicted that only one in a thousand would continue to the end. In fact, 26 out of 40 subjects continued until the end. It must be stated that although there were many protests from the subjects especially when the learner started to cry out in pain, none of the subjects ceased obeying the experimenter when prodded to continue. Milgram had essentially gotten 100% obedience out of his subjects. Interestingly enough, subjects in both experiments displayed physical symptoms of their discomfort. Subjects of Milgram's experiment exhibited signs of extreme nervousness and started displaying sweating, trembling, stuttering, biting lips, groan and dug their fingernails into their flesh. Some subjects burst out in nervous laughter and three subjects had seizures. This sort of behaviour is comparable to those displayed in Zimbardo's subjects. In a bid to escape the mental torture and trauma they were in, four prisoners reacted by breaking down emotionally. One prisoner even developed a psychosomatic rash all over his entire body when he learned that his parole request had been turned down. By the end of Zimbardo's study, the prisoners were disintegrated, both as a group and as individuals. There was no longer any group unity; just a bunch of isolated individuals hanging on. The guards had won total control of the prison, and they commanded the blind obedience of each prisoner. Likewise in Milgram's experiment, the experimenter seen as authority had commanded obedience of each subject. The experiment demonstrated with jarring clarity that ordinary individuals could be induced to acts of cruelty even in the absence of physical coercion. Humans need not be innately evil to behave in ways that are reprehensible and inhumane. While it was thought and believed that when confronted with a moral dilemma humans would act as their conscience dictates, Milgram's obedience experiments proved that in a concrete situation with powerful social constraints, moral sense can easily be overcame. Both Zimbardo and Milgram's studies demonstrated obedience to an authority even in moral dilemmas and personal disinclination to do so. Their controversial research stunned the world. There has since been a moral outrage about prisons and their treatment of prisoners. Their biggest contribution to the world is the knowledge that their research now brings. Knowledge is power and it empowers us to prevent and minimize such prison situations and possible suppressions of our morality in the face of authority. Milgram's warning--that when an individual ""merges...into an organizational structure, a new creature replaces autonomous man, unhindered by the limitations of individual morality, freed of human inhibition, mindful only of the sanctions of authority""--has much resonance. We did not need Milgram to tell us we have a tendency to obey orders but we did need a reminder of just how powerful this tendency is. As we have now been enlightened about our extreme readiness to obey authorities, we can attempt to better guard ourselves against unwelcome or reprehensible commands. Zimbardo's research would also benefit many prison reforms. With this knowledge of the possible repercussions and outcomes that could occur in prison, jails all over the world should be better equipped to prevent or at least minimize bad treatment of prisoners' occurrences, from happening. This knowledge has also benefited leadership courses as now people are taught to be more aware of the way they deal with their subordinates. Zimbardo's research can be mirrored to today as with the Iraqi prisoners. There is still mass torture prevailing even in this day and age. His research provided us with an explanation and a prediction of possible occurrence but has done nothing to prevent it.",False
82,"INTRODUCTIONIn this report, a torque sensor was designed. It was a shaft-type cantilever, which was clamped to a frame at one end, with the other end left free to be twisted under a torque. This torque was applied via a thin steel bar attached to the free end of the shaft, and a force applied to the other end of the steel bar. The shaft was a hollow one, made of steel with the dimensions as follows: length L = 200mm; outside diameter d 0 = 20mm and the internal diameter d i = 17mm. Once this test-rig was set up, it had been proposed that the rig was instrumented so that an automatic measurement of the torsion could be given. Diagrams of the test rig were shown below. METHODTo give this automatic read out of the torsion, it was decided that a torque cell should be implemented. A torque cell was a transducer and transducers were devices used for the measurement of a physical quantity by electrical means, i.e. used to convert non-electrical measurands into electrical quantities. They were used because this method often assisted subsequent operations on the data. In this case, the torque cell was converted an applied torque into an electrical output signal. A torque cell contained a mechanical element- the circular shaft, and a sensor- strain gauges. Firstly the type of strain gauges was chosen. In fact there were two types of gauges, metal or semiconductor. The metal gauges were in the form of a flat coil of wire or etched metal foil and the semiconductor gauges were a strip of semiconductor material in-between two connection leads. The element was wafer-like and had an insulating backing material so that it could be stuck like a postage stamp onto surfaces, using a suitable adhesive. Strain gauges worked on the principal that:  FORMULA  where  R = resistance,  = resistivity, L = length, and A = area of the element.So when the element was stretched, its length increased, its cross-sectional area decreased, and there was also a change in its resistivity. The result was that the resistance of the element changes. Generally, the semiconductor strain gauges were made from silicon and they had a much higher gauge factors than the metal ones which made them much more sensitive than the metal ones (Gauge factors were between 100 - 175 or -100 to -140 depending on whether the silicon was doped with 'P' or 'N' type material, compared with gauge factors of 2 for the metal ones), but it didn't mean that they were automatically better than the metal ones though. The strain gauges based on semiconductor materials were rather more expensive, more difficult to apply and had greater sensitivity to temperature changes than the metal ones. The greater sensitivity to temperature changes meant that the relative change in resistance is now non-linear and therefore more complicated. Due to these negative points, the metal foil strain gauges were chosen. Four active strain gauges are used in order to obtain maximum possible bridge output voltages, to provide temperature compensation, and to make the sensor/transducer insensitive to forces and moments other than the one being measured. These are mounted on two perpendicular 45 helices that are diametrically opposite to one another. Gauges 1 and 3, are mounted on the right hand helix, and sense a positive strain (tension), and gauges 2 and 4, mounted on the left-hand helix, sense a negative strain (compression). The two 45 helices define the principal stress and strain directions for a circular shaft subject to pure torsion. This set-up described can be seen in the following diagram The torque, T transmitted by a shaft is related to the maximum shear stress, produced on the shaft surface by the equation:  FORMULA  where J is the polar second moment of area of the shaft section and r the radius of the shaft. For a hollow circular shaft J = (d 04 - d i4)/2, where d 0 is the outside diameter of the shaft and d i is the internal diameter of the shaft. Thus the maximum shear stress is:  FORMULA  The above set-up of the strain gauges therefore would be able to sense, and thus provide us with the information of the shearing stresses on the shaft. It can be seen from the above diagram how the strain gauges will be set up. We already know that when strain gauges are put under a strain that their resistance changes. Thus, we need a circuit that will convert a change in resistance into an output voltage. A circuit commonly used for this purpose is a Wheatstone bridge circuit. (Shown below) As can be seen from the diagram previous, the output voltage, V 0 of the bridge can be determined by treating the top and bottom parts of the bridge as individual voltage dividers. Thus,  FORMULA   FORMULA  The output voltage V 0 of the bridge is:  FORMULA  Where R 1 = gauge 1 etc. The above equation indicates that the initial output voltage, V 0 = 0 if  FORMULA  This means that the bridge is balanced. The ability to do this makes it considerably easier to measure small changes in voltage output, V 0. We are using a circuit with four active bridges. Providing that they are correctly connected into the bridge, so that one opposite pair (e.g. R 1, R 2) are in tension and the other opposite pair (e.g. R 3, R 4) are in compression; then the sensitivity is four times that of a single element gauge. This bridge also compensates for changes in gauge resistance due to temperature. For metal gauges the effect of temperature is to multiply each gauge resistance by the factor 1 + T; which cancels out in the above voltage equation, as those gauges in tension will have their resistance increased by a temperature change and those in compression will have theirs decreased. As;  FORMULA  and;  FORMULA  FORMULA   FORMULA  FORMULA  Also:  FORMULA  FORMULA  Where G = Gauge factor, or strain sensitivity and = strain then;  FORMULA   FORMULA   FORMULA  This shows that there is a linear variation between the output and the variation of resistance of the strain gauges. a)It was shown earlier that there is a relationship between the maximum shear stress, on the surface of the shaft and the torque, T in the system, given by the following equation:  FORMULA  Where J is the polar second moment of area of the system and r is the outside radius of the shaft. This can be rearranged into the following form using J = (d 04 - d i4)/2 for a hollow circular shaft:  FORMULA  b)Now the relationship between the strain and the shear stress will be looked at. For a circular shaft subject to pure torsion, the direction of the maximum stresses resulting from this shear are at 45 to the shaft axis. This can be seen from looking at the diagram earlier showing the set-up of the strain gauges and the following equation:  FORMULA  These stresses at right angles to each other will give rise to strains in these directions of:  FORMULA  and  FORMULA  where E is Young's modulus for the material and Poisson's ratio, which is given by = -T/L which are the transverse and longitudinal strains. These strains can be measured by the use of the resistive strain gauges aligned as shown in the earlier diagram. c)When looking at the relationship between the resistance and the strain for each gauge, we can start with the following equation that:  FORMULA  Thus the fractional changes in resistance of each of the strain gauges is:  FORMULA  or  FORMULA  d)When looking at the relationship between the bridge output and the torque we go back to the simple equations first. To balance the circuit R 1R 4 = R 2R 3 so that V 0, which is the potential between B and D on the earlier diagram of the circuit, is zero. So when there is a change in the resistance of R 1 then  FORMULA  Similarly, the potential difference across R 4 is:  FORMULA  Thus the potential difference between B and D is:  FORMULA  This is a balanced condition again. Also:  FORMULA  When a force is applied to the shaft, each of the resistors will change their resistance. The changes in resistance in relation to the denominator terms where we have the sum of the resistances is insignificant and can be neglected. Thus:  FORMULA   FORMULA  We also know that:  FORMULA  when balanced there is no change in output voltage so:  FORMULA  And;  FORMULA  therefore;  FORMULA   FORMULA  Hence, the output voltage V 0 from the bridge is proportional to the torque T acting on the shaft. The sensitivity of the torque cell depends on the diameter of the shaft (d o), the shaft material (E and ), the gauge factor (G), and the voltage applied to the Wheatstone bridge (V S). The range of the torque cell depends on the diameter of the shaft and the proportional limit of the material in torsion. We need now though to amplify the output signal of the circuit as the strain gauges in the circuit will only be changing their resistance by a small amount, so the output voltage will therefore only change by a small amount too. Also we need to consider the fact that when we amplify the signal, we will also amplify any noise in the signal, so a filter needs to be employed to attenuate the noise in the signal before amplification. To do this we will use an active filter. This is a device that will combine an operational amplifier and an RC filter. We chose to use a non-inverting op amp as this will amplify the signal and will give a positive value for a positive torque. A diagram of a non-inverting op amp is shown below: The two resistors in the above circuit can be used to calculate the gain of the circuit (G C). The formula that is used to do this is as follows:  FORMULA   FORMULA  where G is the gain of the amplifier. This will be combined with a low-pass RC filter in order to attenuate the noise in the signal. Electrical noise can be a problem as the output voltage from the Wheatstone bridge circuit is only a few millivolts. The electrical noise occurs as a result of magnetic fields generated by current flow in wires in close proximity to the lead wires or bridge, which induce voltage (noise) in the signal loop. This combination of op amp and filter is called an active filter. Active filters are used where select frequencies can be attenuated and the signal amplified during the filtering process. Shown below is an active filter with a non-inverting op amp and a low-pass filter: Now all that is needed is to employ this into the circuit and combine it with the Wheatstone bridge circuit. Taking the output signal from the Wheatstone bridge circuit and using it as the input signal for the active filter does this, as this is the signal that needs to be amplified. The combined circuit is shown below: The circuit has now been set up with strain gauges to sense the stresses in the shaft, and these mechanical outputs have been converted into electrical ones and amplified to a sufficiently high level with the noise attenuated so the signal can be read by a voltage-measuring instrument. This system now needs to be able to be read and calibrated for data presentation. To calibrate this system, we need to precisely measure R 1, R 2, R 3, R 4, V S; the gain G of the amplifier; and the sensitivity S R of the recorder (i.e. the voltmeter). The system calibration constant C for the entire system is then given by:  FORMULA  Where, S A = the amplifier sensitivity, and S R = the recorder sensitivity (volts per division) The strain recorded with the system is given in terms of the system calibration constant as:  FORMULA  where, d S = the deflection of the recorder in divisions. SUMMARY:In Assignment 1, Part A was demonstrated the material selection for the fresh-water heat exchanger tubes. In Part B, material and process were selected for column spacers under a varying compressive load. Finally, the structural section was selected for a beam in Assignment 2. Cambridge Engineering Selector was mainly used in those cases to select the best suitable results. In conclusion, ""Higher Conductivity Copper"" was chosen for fresh-water heat exchanger tubes in Assignment 1 Part A. Secondly, ""Silicon"" was selected for column spacers and ""Fine Grinding (Automated)"" and ""Polishing (Automated)"" processes were selected to use in manufacturing in Assignment 1 Part B. Finally, ""Hot Fin. Steel (Y.S.355MPa) Circular Hollow-(194×5.0)"" was chosen for the optimum structural section. PART A - Material selection for fresh-water heat exchanger tubesIntroduction:In this part, a material was selected and used to manufacture a set of tubes in heat exchangers by using the CES Selector Materials Package. The material needed to conduct heat well, had a maximum operating temperature above the operating temperature of the device, not corrode in the fluid and have adequate ductility. Two stages (Graphical stages) were created to identify a subset of materials that met the criteria. On the other hand, some other material characteristics which might influence the choice were also considered before a suitable material was chosen. The Specification for particular heat exchanger was given below: Maximum service temperature 150°(which is equal to 423K)Elongation >20%Corrosion resistance in fresh water Very goodThermal conductivity As large as possibleMethod:First of all, a graphic stage was created by pressing the ""New graphic stage"" button on the standard toolbar in the CES window, ""Fresh water"" was selected from attributes list box on the x-axis tab and ""Ductility"" was selected on the y-axis tab. A graph was shown to indicate the ductility of materials against the corrosion resistance in fresh water. ""Box selection"" button on the Project toolbar was pressed afterwards for selecting materials within a property range. The mouse was clicked near the point (Very good, 0.2), the button was held down and the cursor was drag near to the point (Very good, 9.3(Max.)). The limits were refined by opening ""Stage properties"" window in the ""Project"" menu and the exact value was entered in the ""Selection"" tab. Total number of 155 different materials met the specification in this stage. (Figure 1.1) Failed materials were hidden by pressing ""Hide failed record"" box in ""Stage properties"" window. Secondly, another graphic stage was created in the same method as the one above. But ""Maximum service temperature"" and ""Thermal conductivity"" were selected on the x-axis and y-axis tab respectively. A graph (Figure 1.2) was shown again to indicate the maximum service temperature against the thermal conductivity for different materials. Again, ""Box selection"" button was pressed and limits were refined to 423K to 3800K (Max.) on the x-axis and refined to 200W/m.K to 422W/m.K (Max.) on the y-axis, using the same method as stage one. At the end, 21 different materials met the specification in this stage. Finally, materials that superimposed were found out by pressing the ""Result intersection"" toggle on the standard toolbar. Results could be viewed by clicking on ""Results Window"" button, in this case there were 11 materials which met both specifications; a list of results was shown in Figure 1.3. A clearer view for stage 1 and 2 after intersection were shown in Figure 1.4 and 1.5 respectively. Evaluation:In fact some characteristics were also important and might influence the choice, therefore a careful consideration was needed, e.g. lower price was important in large number of manufacturing processes, higher endurance limit was helpful for long-term usage and the relevant replacement cost could be minimized, lower specific heat lowered the amount of heat required to heat up the fluid. Although ""Silver, Commercial Purity"" (Figure 1.6) was the best material after intersection, it was too expensive. Both ""Brass"" (Figure 1.7) and ""High Conductivity Copper"" (Figure 1.8) had a lower price and a similar specific heat, ""High Conductivity Copper"" had a higher endurance limit and ""Higher Conductivity Copper"" had a higher thermal conductivity. But the heat exchanger tubes with high thermal conductivity were more important than one with a high endurance limit. So in conclusion, ""Higher Conductivity Copper"" was chosen for fresh-water heat exchanger tubes. PART B - Selection for column spacers under a varying compressive loadIntroduction:This part included selection for column spacers in a precision instrument. The instrument incorporated two stiff plates between which a fluid flowed. They were held apart by a set of seven spacers, which were to be solid bars of circular cross-section. Periodically changing compressive load was applied axially on the plates. Specification for the instrument:Spacers length 2.65±0.015 mmDiameter of plates 55 mm (0.055m)Maximum compressive load 10 5N (0.1MN)Selling price £80,000Number of selling each year 25Expected tooling cost £2500-3000Principal criteria for choice:Spacers should not fail under the load.Material should be essentially an electrical insulator.Thermal distortion should be as low as possible.Resistance to water and organic solvents should be very good and resistance to acids should be good.In theory, thermal distortion could be minimized by selecting materials with large values of the index M= λ/α, where λ is thermal conductivity and α is thermal expansion. Methods:For material selection, ""Materials"" was selected in project setting window. Actually 3 stages (a limit stage and 2 graphic stages) were created for selection of materials. A limit stage was created in stage one and the environmental resistance were adjusted to ""Very good"" for ""Fresh Water"" and ""Organic Solvents"", and ""Good"" for ""Weak Acid"" (Figure 2.1). Total number of 1007 different materials met the specification in this stage. Secondly, a graphical stage was created in stage 2, ""Compressive Strength"" and ""Resistivity"" were selected on x and y-axis respectively. Formula σ=F/A was used to find out the maximum compressive strength, where σ was compressive strength, F was compressive load applied and A was cross sectional area. In this case, σ=0.1/ [π× (0.055/2)2] MPa, which was equal to 42.0906MPa. ""Box selection"" button was pressed and limits were refined to 42.0906MPa to 3850MPa (Max.) on the x-axis and refined to 100×10 -8 ohm.m to (1e+010) ×10 -8 ohm.m (Max.) on the y-axis. At the end, 166 different materials met the specification in this stage (Figure 2.2). Again, failed materials were hidden by pressing ""Hide failed record"" box in ""Stage properties"" window. For stage 3, another graphical stage was created, ""Thermal Expansion"" was selected on the x-axis and ""Thermal Conductivity"" was selected on the y-axis, a gradient-line with a slope of 1 was added to the graph by pressing the ""Gradient-Line Selection"" button. The line was placed to a desired level after held and dragged from any point on the graph. Materials with high index M (λ/α) were indicated by clicking once on a point above line with hand cursor. There are total number of 953 materials met the specification in stage 3 (Figure 2.3). Finally the materials that met the specification in all 3 stages could be viewed on the graph by clicking on ""Result intersection"" toggle at each graphical stage. A list of results could be view by clicking on ""Results Window"" button on standard toolbar. A total number of 9 materials met all the specifications and a list of results was shown in Figure 2.4. A clearer view for stage 2 and 3 after intersection were shown in Figure 2.5 and 2.6 respectively. For process selection, ""Process"" was selected in project setting window from project menu, 2 stages were created for selection of processes. A graphical stage was created in stage 1, advanced button was pressed in graph stage wizard and an expression builder appeared. ""Shape"" folder in left hand pane was clicked, ""Prismatic"" folder was expanded in right hand pane by clicking the ""Plus sign"" next to it. Similarly the ""Axisymmetric"" folder was expanded followed by the ""Solid"" folder, ""Plain"" was selected by double clicking on the folder. On the y-axis, ""Tolerance"" was selected. Precision of 2.65±0.015 mm was given. Therefore ""Box selection"" button was pressed and the limit of tolerance was set to 1e-003mm (Min.) to 0.015mm on the ""Selection"" tab (Figure 2.7). The tooling cost was expected to be £2500-3000, so a limit stage was created in stage 2 and the limit was entered in ""Tooling cost"" boxes (Figure 2.8). ""Result intersection"" toggle was clicked in stage 2 and 8 different processes were met the specification (Figure 2.9). A short list was shown in Figure 2.10. Evaluation:Both ""Silicon"" (Figure 2.11) and ""Germanium"" (Figure 2.12) had a outstanding good result, but the compressive strength and thermal distortion of ""Silicon"" was slightly higher and lower respectively than ""Germanium"", also the price of ""Silicon"" is much lower than ""Germanium"". Therefore, ""Silicon"" was chosen to manufacture the spacers. On the other hand, ""Fine Grinding (Automated)"" (Figure 2.13) and ""Polishing (Automated)""(Figure 2.14) were selected for ""Machining"" and ""Finishing"" process respectively. A better precision and surface finish of spacers could be obtained using the ""Fine Grinding (Automated)"" even the capital cost was very high. Precisions and surface of the spacers could be improved using the ""Polishing (Automated)"" process; the capital cost was similar to the ""Fine Grinding (Automated)"" process. In conclusion, ""Silicon"" was selected for column spacers, ""Fine Grinding (Automated)"" and ""Polishing (Automated)"" processes were used for machining and polishing the spacer respectively. Assignment 2 - Shape selection: Selection of structural section for a beamIntroduction:In this section, the best standard structural section was chosen for a roof member. The interior space was intended to be divided with glass curtain walls and the roof was a pedestrian area. Because of the poor deflection tolerance of the glass, minimal deflection was the primary design consideration. Lateral torsional buckling problems were avoided by maximizing the torsional rigidity of the members and the environmental impact was also minimized. Specification for choice:Considered clear span = 10mAbsolute maximum deflection = 1/400 span = 25mm = 0.025mMaximum capacity of lifting equipment = 500lb = 226.796kgLive roof load (including safety factor) = 50kg/m ~ 500N/mMaximum deflection was given by: δmax = (5qL 4)/ (384EI) q = load in N/m, L = length and EI = flexural stiffness.In this case, δmax = (5×500×10 4)/ (384EI) Method:First of all, ""Shape"" was selected in selection table after opening the ""Project Setting"" window form ""Project"" menu. ""Structural Sections"" filters and forms were selected in shape table. Two stages were created - A graphical stage and a limit stage. In stage 1, graphical stage was created and ""Deflection"" was plotted on the x-axis. ""Deflection"" was created from expression builder window and an expression of (5×500×10 4)/ (384EI) was built. On the y-axis, the ""Optimal secondary design parameter"" (M) was plotted using the expression builder again and an expression of [Torsional Stiffness] × [Recycle Fraction] / [Energy Content] was selected and pasted. ""Box selection"" button was pressed and limits were refined to 4.01878e-005m (Min.) to 0.025m on the x-axis and refined to 100 to 9.6096e+006 (Max.) on the y-axis. Failed materials were hidden by pressing ""Hide failed record"" box in ""Stage properties"" window (Figure 3.1). On the other hand, limit stage was created and limit for ""Mass per unit length"" was filled in stage 2. The length of beam was 10m and the maximum capacity of lifting equipment was 226.796kg, so an amount of 22.6796kg/m was entered into the maximum mass per unit length box (Figure 3.2). After that the ""Result intersection"" toggle was clicked and a total number of 11 passed sections were shown in Figure 3.3. A clearer view for stage 1 after intersection was shown in Figure 3.4. Evaluation:""Hot Rolled Steel (Y.S.355MPa) Universal Beam-(305×102×25)"" (Figure 3.5) was the best section that satisfied the minimum deflection. On the other hand, ""Hot Fin. Steel (Y.S.355MPa) Circular Hollow - (194×5.0)"" (Figure 3.6) was the best section that satisfied the maximum value of M. Anyway, ""Glulam Softwood rectangular section - (495×65×16.11)"" (Figure 3.7) was the best section that satisfied both consideration. Actually, economical factor was also needed to be considered so that a new expression [Torsional Stiffness] × [Recycle Fraction] × [Price] / [Energy Content] was formed for the value of M. The range of M was adjusted to 100 to 8.12613e+006 (Max.) and adjusted to 100 to 9.6096e+006 (Max.) on the y-axis, a new graph was shown in Figure 3.8. ""Hot Fin. Steel (Y.S.355MPa) Circular Hollow - (194×5.0)"" and ""Glulam Softwood rectangular section - (495×65×16.11)"" were also appeared in the graph. By the way, in conclusion, ""Hot Fin. Steel (Y.S.355MPa) Circular Hollow-(194×5.0)"" was chosen for the beam because a high torsional stiffness and a low price could be found from this section.","SummaryIn this assignment, researches have been done on the types of engines that are usually used for racing. The four types of racing engines that was of interest were the Formula, Rally, Sports and Motorcycle racing engines. The specifications of these engines have been summarized in Appendix A and B so that one can compare their important parameters. Appendix A shows all the racing engines, of which their individual performance will be discuss in the content of this report. Appendix B is a table which accumulates all the information obtained about the four racing engine requirement and their overall comparisons are also included in the main text. In this report, we also accessed past and future developments of each of these racing engines. IntroductionIn 1894, the idea of car racing was raised once a series of petrol-fuelled cars were constructed. The first official race was held in Chicago, Illinois on 2nd November in 1895 and racing engines were improved exponentially during the 20 th century. Researches on different engines which usually used in racing were performed and discussed in this report. Engine used for general automotives was internal combustion (I.C.) engine in which the mechanical power of vehicle was produced by combustion of fuel within the combustion chamber. Internal combustion engine could have either two-strokes or four-strokes and either spark ignites or compression ignites. Gasoline, diesel and natural gas could be selected as the fuel in a SI engine. On the other hand, two major types of IC engines were identified: Rotary engine and Reciprocating engine. The major representative of rotary engines in automotive industry was the Wankel engine (Fig 2) which was the most highly developed rotary engine since 1970s; such engine was used due to the compactness and high power performance. However, the development of Wankel engine was suspended in most of the companies due to the arising environmental regulations as well as the effect of the oil crisis. The most recent car operated by Wankel engine was the Mazda RX-7 which produced in 1999. In automotive racing industry, only particular specifications were selected and employed since the efficiency could only be improved by such specifications. Different cylinder configurations such as single, in-line, v-type, w-type, u-type, opposed cylinder, opposed piston and radial could be found in IC engines. However, in-line and v-type were the most commonly used configurations in automotive racing engines. Further more, the common numbers of valves employed in each cylinder were 2 (1 intake, 1 exhaust), 4 (2 intake, 2 exhaust) and 5 (3 intake, 2 exhaust). Generally 4 valves were employed in racing engine. . Compare to normal engines, limits such as peak operating cylinder pressure were pushed up in some racing engines so that a higher performance could be obtained. Besides, the horsepower and fuel economy could be increased by maximizing the cylinder pressure. Although the cylinder pressure could be increased by increasing the compression ratio, alternative technique could also be used since cylinder pressures could be altered significantly by using camshaft selection, carburetion, nitrous and supercharging. Compression pressures could be adjusted drastically by installing supercharging, turbo-charging or intercooling system. However, apart from installing extra equipment to improve the effectiveness and efficiency of racing engines, the durability and lifetime were also a significant factor to consider since racing cars might require to operation for a long time without any failures Type of RacingsIn the report, a series of engines used in different automotive racings including formula racing, rally racing, sports racing (including endurance) and motorcycle racing were presented. In fact, the features required in racing engine were totally depended on types of racing and performances of different racing engine were adjusted in order to meet the specific purpose and provide a best fit performance. However, high acceleration, high maximum speed, high power, high torque, light weight and high efficiency of engine cycles were generally the ideal for racing engines. For instance, high power output was necessary for rally engine since such racing might consist of climbing uphill or operating under poor conditions. To conclude, particular engine performance could be adjusted in accordance with racing course conditions and requirements. Formula RacingFormula racing was a type of single-seater racing which involved a variety of special designed high performance cars. The wheels of formula cars were not covered and aerofoil wings could always be found at the front and rear of the car (Figure 3). Formula engines should have an extremely high maximum speed, extremely high acceleration, high torque, long life span, high operational temperature and extremely light weight in order to perform effectively on a special designed racing course. Rally RacingThe first World Rally Championship (WRC) was held in 1973 and highly modified cars were competed on normal roads as well as under poor conditions which included dirt, swamps (water resistance) and snowy surfaces (Figure 4). Rally engine should have an extremely high torque, extremely high horsepower, high acceleration, long life span and high operational temperature in order to maintain the car under safety condition as well as effectively operated. Sports RacingSports racing cars were modified cars and were normally operated on racing courses (Figure 5). Endurance racing is part of the sports racing where races were usually carried out over a long distance and vehicles were usually driven by a team of two to three drivers. As similar to formula engines, sports engines should have a reasonable maximum speed and torque during operation, extremely long operational time, a long life span and high operational temperature were also essential to be considered before the production process of racing engine. Motorcycle RacingFinally, motorcycle racing is a contest involving motorcycles competing with each other (Figure 6). Design of motorcycle engines varied largely in order to meet the requirements for different type of races. Normally motorcycle engine required to have light weight, long life span, high operational temperature, extremely high acceleration and maximum speed. Technical featuresAdditional features could be employed in different racing engines so that the performance could be improved further. Forced induction systems such as turbocharger and supercharger were explained as well as cooling systems and carburetor. 1. Turbocharger - An apparatus to boost the horsepower of engine without significantly increased its weight, operated by pressurizing the air flowing into the engine's cylinders in order to get more fuel to be burnt during each stroke. Advantage of compressing the air was more air could be squeezed into a cylinder, hence more fuel could be added and more power could be obtained from each explosion in each cylinder. More overall power could be produced than the same engine without the charging (Figure 7). Supercharger (Figure 8a) - The mechanism was nearly identical to the turbocharger, the major difference was the power supplies in order to run the air compressor. In supercharger, a belt was connected directly to the engine and power was transmitted via the belt. For turbocharger, power was obtained from exhaust stream since the exhaust run through a turbine which in turn the compressor was span. A graph was shown the variation of power with rpm with and without supercharger (Figure 8b). 3. Cooling systems - Either liquid-cooled or air-cooled technique was generally employed in racing engines. i) Liquid-cooling: Fluid flowed through pipes and ducts within the engine, heat was absorbed by the fluid and engine was cooled as the liquid passed through. The fluid was then passed through a heat exchanger or radiator and heat was transferred from the fluid to the air blowing through the exchanger. ii) Air-cooling: An old-fashioned cooling technique, engine block was covered in aluminum fins rather than circulating fluid through the engine so that heat could be conducted away from the cylinder. A powerful fan was used to force air over these fins and the engine was cooled by transferring the heat to air. 4. Carburetor - The right amount of gasoline with air could be mixed so that the engine could run properly. Designs of racing enginesIn an internal combustion engine with multiple (more than one) cylinders, the cylinders and piston could have V-, in-line, horizontal, w type, opposed piston and radial configuration. The most common configurations used in racing engines are the V- and in-line configurations. 1. V-configuration:V-configuration engines would have their cylinders and piston arranged in such a way that they form V shapes as shown in Figure 9 above. Engines using this configuration are named V2, V4, V6, V8, V10, V12, V16, V18, V20 and V24. The V configurations have been widely applied in racing engines as it helps to minimise the engine length and weight as compared to the in-line configuration. The cylinders can be arranged with different V-angles. The bigger the V-angle of an engine, the more stable it is as it has lower centre of gravity. However, large v-angle also gives rise to vibration problems. Therefore, despite increasing the V-angles would lowered the centre of gravity, it also increased the vibration. On the other hand, small V-angle engines will be less stable as the centres of gravities would be higher. Examples of engines with large V-angles are V2, V4, V6 and V10. 2. In-line configuration:In-line configuration is another configuration that is commonly used in racing engines. The cylinders in an in-line engine were arranged in one straight row parallel to each other. This can be applied for all multiple cylinders engine but is more commonly found in four- and six-cylinders engines. Its simple structure makes it easier in production than V-configuration engines. Due to its simple shapes it would be relatively easy for it to be fitted into the car chassis with different positions and can run rather smoothly. The disadvantage of in-line engines is that they are longer than V engines. It will be hard to install in-line engines into small cars and the cooling of engine by air can get difficult if it had been installed in inconvenient position Examples of in-line engines are straight-twin, straight-3, -4, -5, -6, -8, -10, -12 and -14. Straight-4 is the most commonly used in-line engine at the present. Sports EngineSports racings consisted of a variety of competitions between modified cars and were normally taken place on a specific racing course. Due to the enormous area sports racings have covered, only endurance racing was mentioned in this report. 1) Ford GT 2005 (Ford's MOD)The first Ford GT was built in 1963 and was became the world's best endurance racing car in mid-1960s. Ford GT achieved a result with flying colors as such vehicles placed 1-2-3 at the 24 Hours of Le Mans in 1966 as well as won the next three consecutive years. The Ford GT was reconstructed in recent years in order to celebrate the centennial of Ford Motor Company. A mid-mounted, all-aluminium 5.4L DOHC supercharged V-8 engine that produces 550 horsepower was used in this generation. Compression ratio was 8.4:1(in/mm) and Sequential multi-port Electronic Fuel Injection (SEFI) was employed in the engine with dual injectors per cylinder. The MOD V-8 featured aluminium four-valve heads, forged crankshaft, H-beam forged rods and aluminium pistons fed by an Eaton screw-type supercharger, all combined and produced more than 500 hp horsepower and 500 ft-lb of torque. In 1966 a new prototype design (GT40 MK2) was introduced with a 427 c.i. (7000cc) engine, produced a 485 BHP power at 6,200 RPM and 475 Ib-ft of torque at 4,000 RPM. In the same time as GT40 MK1 was being developed, a road used version (GT40 MK3) was designed and a 289 ci. (4700cc) engine was used which could produced 306 BHP power. Between 1966 and 1967, a new GT40 was introduced (the J-Car). However, the final product was not very satisfied as such vehicles were mainly designed from Ford's styling department. During 1968/69 seasons, the GT40 Mk1 was redeveloped and a stronger engine was employed. 400 bhp of power at 6,500 rpm and 385 lb-ft of torque was finally achieved in 1968. In 1969 the power was raised further to 425 bhp at slightly lower revolution of 6,250 rpm and 396 lb-ft of torque at 4,750 rpm, using a 302 ci. engine and other improvements. The Ford GT40 was the origin of Ford GT and was firstly produced in 1964; the engine of first prototype car (GT40 MK1) used a 4200cc Ford V8 engine, with aluminium block and heads. The engine was dry sumped with IDA Webber carburetor atop. The performance was 350 BHP of power at 7,000 rpm and 275 lb-ft of torque at 5,600 rpm. In current design, aluminium was used in most of the parts of engine in current Ford GT due to the high strength and excellent heat transfer, so that the main objectives of endurance racing (long life span, long operational time and safe operation) could be fulfilled. However, some components were manufactured by forgings instead of castings; potential hazard might exist by using such an unseasoned manufacturing process as casting was the conventional manufacturing method of automotive components. Sports Engine - Overall DevelopmentThe performance was improved by adding new components such as supercharger. The 32-valve, 330 cubic-inch, 90-degree all-aluminum V8 engine was placed behind the driver in order to lower the centre of gravity of the vehicle. Performances of 373kW of power @ 6000rpm and 678Nm of torque @ 4500rpm were generated by the engine. Forged components (such as crankshaft, connecting rods and aluminum pistons) were also utilized in order to deal with extra pressure of forced induction. Aluminium components were become familiar due to the light weight and extremely high levels of torsional rigidity. Forgings technique would also become common after the correspondence processes were further improved. Rally EngineRally engines were required to operate effectively even under poor conditions such as in swamps (water resistance), dirt and on snowy surfaces. The engine also needed to generate sufficient power in order to drive the rally car uphill. Two rally engines were shown below: 1) Subaru Impreza WRC 2004 (EJ20)The first Impreza was found in October 1992 and horizontally opposed, four-cylinder, all-alloy EJ20 engine for a displacement of 1.994 L with quad camshafts, 8.0:1 static compression, a single turbocharger and intercooler was employed in this model. The boxer engine was in accordance with a specially reinforced ""semi-closed deck"" engine block, forged aluminium alloy pistons and forged high-carbon steel which linked to rods and exhaust valves (sodium-filled). The theory behind using a 'flat four' was the engine sat lower in the car in which the car's inertia could be benefited and hence a lower centre of gravity could be obtained. In 2003, the output of this turbo charged engine was increased to 165 kW (225 PS), dampers with multiple phase valves were also used in order to obtain high performance and steering stability. Besides, the cooling performance was also enhanced by using a different shape of the inter-cooler water spray nozzle and the shape of the air baffle plate within the air scoop. Nevertheless, developments were still carried out during 2004 in order to remain effective and competitive in the WRC. A wide range of performance upgrades were conducted including using a direct water injection system and increasing the size of IHI turbocharger housing so that stable engine combustion could be obtained. Aluminium block and heads were employed in order to be operated in high temperature effectively. Area of air intake was increased and hence the cooling system including V-mount radiator could be improved. On the other hand, AVCS (Active Valve Control System) was also introduced and the inlet camshaft timing over a 35-degree range was varied in order to improve emissions, tractability and low rpm torque. The EJ20 boxer engine was also modified with a twin scroll turbo, drivability was greatly enhanced by the increased torque at low speeds, while the improved acceleration was particularly significant in rally race. 2) Mitsubishi Lancer Evolution WRC 2004 (4G63)Lancer Evolution was firstly appeared in 1973 and was equipped with the 4G63 engine. The 4G63 engine consisted of an 85.0mm (3.35"") bore and 88.0mm (3.46"") stroke, for a displacement of 1997 cc. Twin contra-rotating balance shafts were housed inside the block and Multi injection technique was employed. The engine was mainly consisted of cast iron block and an aluminium alloy cylinder head. A new turbocharger fitted with the WRC regulation 34-millimetre intake restrictor, new intake and exhaust manifolds, and new internals were used in the engine in order to obtain a higher efficiency. In 2003, refinements were performed on intake and exhaust systems, aluminium induction piping, and weight reduction measures by using a cast-magnesium cylinder head cover and hollow camshafts. The intake, exhaust, turbocharger impeller and housing, camshafts and crankshaft were all developed in 2004, in accordance with the technology already achieved in different Lancer Evolution over the past 10 years. Camshafts were design to hollow so that the rotating inertia was reduced. Moreover, the ""twin-scroll"" impeller in turbocharger was modified by doubling the amount of fins. Dual chambers were used in exhaust housing, the exhaust gases were routed from cylinder 1 and cylinder 4 to one side of the twin-scroll turbocharger and gases from cylinder 2 and cylinder 3 to the other side of turbocharger. Efficiency would be increased by improving the airflow intake. However, the WRC regulation was conflicted by this development since only a maximum of 34-millimetre intake restrictor could be used. Although downforce was a vital factor of competitive car on rally racing, good cooling was a key prerequisite on a turbocharged rally car since the ambient temperature on rally car could usually exceed 30 degrees Celsius and the temperature was often the highest on racings which carried out at the lowest average speeds. In order to achieve a high efficiency and effectiveness in rally engines, airflow beneath the rally car could be used and considered. In 2005, improvements to the rally engines would be carried out by employing a new waste gate, anti-lag valves as well as an improved engine control. On the other hand, the turbo-charger would also be investigated in order to determine the change in efficiency. The performance and tuning possibilities of such engines would also be significantly developed and become more accurate by carrying out different improvements. Rally Engine - Overall DevelopmentOver the past few years, the development of rally engines was improved from only concentrated on the modification and replacement of engine components (such as dampers, turbocharger housing, intake and exhaust systems, piping, head cover, hollow camshafts, twin scroll turbo...etc) to introducing new components and technique (such as airflow beneath the rally car, new waste gate) in order to improve further and achieve a better performance. On the other hand, exact values of some performance such as accelerations and maximum speeds were not defined since those figures could be adjusted easily by tuning. Formula Racing EngineFormula racings involve a variety of high speed and high performance cars competing against each other. At present, the most famous Formula race is Formula-1 (F1) racing. In this assignment, most research was done on Formula-1 engines. Formula-1 car engines have excellent performance as they are about ten times more powerful than normal car engines. To ensure all races are reasonably fair, FIA (Fédération Internationale de l'Automobile) have put forward some limitations to the engines of the cars that were competing in the races. For instance, the engine must be four strokes and consist of 10 circular cylinders with less than 5 valves on each cylinder. Supercharging is not allowed in Formula-1 cars. At present, the 10 cylinders of the engines are usually arranged in V configuration. They often have capacities of about 3000 cc capacity and can generate more than 800 bhp. High torque of the engines means high power output (i.e. high horsepower) as the number of engine cycles per unit time is depend on the torque. Special materials (such as Aluminium alloys, ceramic etc.) were used to manufacture the engine's components for Formula-1 for weight reduction purpose and also to reduce the chances of overheating of the engines. The cooling of the engines was very important in Formula racing cars especially in Endurance racing since the car was required to run for a long period of time (typically 6 hours, 12 hours and 24 hours) over the runway course in order to test the engine's durability. Formula-1 car engines are all air cooled. Less dense ceramic was used in making the internal components of the engines so that it was easier to accelerate and also reduce the engine's fuel consumption. The overall size of the engines should be reasonable light and small, so that they could be easily fitted into the chassis. The numerical data for the engines' torques and compression ratios (CR) are not available in all of the sources. This may because these data have to remain confidential. Following are some examples Formula-1 racing engines: Ferrari :Figures 5a and 5b above are Ferrari F2003-GA single-seater racing car and Type 052 engine respectively. Ferrari F2003-GA was Ferrari's 49 th single-seater racing car entering the FIA Formula-1 world championship. Similar to the previous generations of engines, the 3000 Ferrari (type 052) engine is load-bearing and is fitted longitudinally into the chassis. It has a V configuration and 10 cylinders arrangement with 4 valves per cylinder, i.e. 40 valves. It is a Spark Ignition (SI) engine with Magneti Marelli static electronic ignition. Fuel will be fed into the engine by Magneti Marelli digital electronic injection. Type 052 is an evolution of the former 051 engine with several improvements that increase the engine's performance and usability. Size and weight of the engine were reduced as new materials have been used for manufacturing it. The centre of gravity of the car is lowered and therefore improving the overall weight distribution. Maximum engine revolutions of type 052 engine would be about 200 rpm higher than that of the type 051. Further development of the engine would be carried on in order to improve the horsepower and its performance. The types of engines that Ferrari would develop in the future will tend to be more reliable and can integrate well with the car. Renault:Renault R24 as shown in Figure 6a was a single-seater built by Renault to compete in the Formula-1 racing in year 2004. The engine used to drive R24 was the Renault RS24 engine, which was an improved evolution of the RS23 engines that had been used in the Renault R23 (2003). Renault had been developing R24 since September in order to improve the overall engine performance. The RS23 engine launched in 2003 was a 111-degree V10 engine. It was generally reliable except that it has major problems with vibrations. Therefore RS24 had been developed from RS23 by changing some configurations and its design is based on the Supertec engines. RS24 has a 72-degree V configuration with 10 cylinders arrangements. As RS24 engine is higher, it has a centre of gravity that is 20mm higher than that of RS23. But according to the Renault F1 team, this problem has been compensated by making changes to the engine's casing. Therefore, the centre of gravity of the car R24 is not much higher than that of R23. The problems of vibrations have been improved in the development of the RS24 engine. In February 2005, Renault launched a new RS25 engine that has been developed from RS24. RS25 is basically the same as RS24, but its centre of gravity has been made lower. After the 72-degree V10 engines (RS24 and RS25), Renault was planning to develop a new 90-degree engine. The future developments for Renault engines will be more on weight reduction as well as improving its reliability. Formula Racing Engines - Overall Development:In 1950s, carburetor was widely used to feed fuel into the car engines. Shortly after, more effective direct fuel injection system was introduced by Mercedes. Renault then introduced turbocharged engines into F1. But the cooling systems of engines was not well developed, many turbocharged cars went down due to overheating. During the 80s, the developments of F1 engines were mainly on increasing their torques and the horsepower. But these increase in horsepower reduced the handling of the cars. The ground-effect of the cars were improved which increase their cornering speeds. This made it even harder to control the cars. During that period of time, many deaths of F1 driver were caused by severe accidents during races. In 1989, turbocharged engines were banned by F1 and were replaced by 3500 cc induction engines. In early 1990s, different companies were concentrating on the development of the 3500cc induction engines. The first 700 hp 3500cc induction engine was built which was much better than the previous racing engines. A 'baffle' had been introduced to enhance the cooling of the engines. F1 limited the capacity of the engines to 3000 cc in mid-90s. With the capacities limited, the engines could only be developed further by improving the mass centralization and increasing the horsepower. Horsepower can be increase by increasing the piston's bore and decreasing the size of cylinders as these will improve the volumetric efficiency of the engine. Finding the optimum cylinders configurations can improve the mass centralization (stability) of the car as well as the overall performance. Motorcycle Racing Engine:Motorcycle racing is a variety of sports involving motorcycles competing with each other. Some common examples of motorcycle racings are Isle of Man TT (road racing), MotoGP (circuit racing), Daytona (endurance racing) etc. Engines of motorcycles are gasoline engines and can be either two strokes or four strokes. They either use fuel injection or carburetors to feed the fuel-air mixture into the combustion chamber. Motorcycles used for racing are mostly four-stroke engines with multiple cylinders. The power of an engine depends on the number and sizes of its cylinders. Therefore, increasing the sizes and number of cylinders in an engine would increase its power, i.e. increase the engine's displacement. Motorcycles engines at present can have single-cylinder, two-cylinder, four-cylinder or six-cylinder and their displacements ranges from 250 cc to over 1500 cc. Most motorcycles nowadays often have two or four-cylinder engines. Engines with high displacement have higher fuel consumption and therefore generate more power. A multiple cylinder engine can have its cylinders arranged in horizontal, inline or V configurations. As mentioned beforehand, the main problems with V configuration engines were its vibrations and high centre of mass. Among the three configurations, the horizontal configuration has the best weight distribution (lower centre of gravity) and also produced the least vibrations. The following is an example of a racing motorcycle: Honda CBR1000RR:Figure 8a shows a picture of the Honda CBR1000RR motorcycle and Figures 8b and 8c show the 998cc liquid-cooled engine that drives the motorcycle. The CBR1000RR engine has a displacement of 998cc and with in-line cylinders configuration. The piston's bore and stroke sizes were made different from that of its predecessors so as to improve its compression ratio, i.e. higher volumetric efficiency. Its fuel tank is placed at the centre and fuel was fed into the engine through a dual stage fuel injection (DSFI) carburetor. Honda CBR1000RR was developed based on the previous RC211V with improved horsepower. Therefore, CBR1000RR is as reliable as RC211V but with higher maximum speed. CBR1000RR bike inherited most of RC211V's body structures such as the Aluminium frame, the suspensions etc. As the swing arm of CBR1000RR bike had been made longer, the size and structure of the engine had to be modified in order for it to fit into the bike chassis. The front to back dimensions of the engine was being shortened by arranging its main shaft, countershaft and the crankshaft in a triangular pattern with countershaft below the main shaft. The engine had been placed towards the front of the bike body to improve the overall weight distribution of the bike, i.e. more stable. In the past, Honda development group has been working on the weight centralization aspects, i.e. to improve the stability of the bike. Based on the researches that had been done, the future evolutions of the Honda racer engines would be more on increasing the engine's horsepower and torque. Motorcycle Racing Engine - Overall Development:The first ever engine used to power a motorcycle was a steam engine which was built in 1867 in the US. It was a two-cylinder engine that powered the bike by burning charcoal. In the 1880s, the first gasoline engine for motorcycle was then built which was a single-cylinder gasoline engine (reciprocating). At this early stage, the fuel feeding system of this simple engine was a spray-type carburetor. Later in 1894, a water-cooled parallel two-cylinder engine was developed. The cylinders were moving to and fro simultaneously to create motion to the rear axle by transmitting through connecting rods. An elastic band was placed on each side of the cylinders for energy storing purpose and to enhance compressions in the cylinders. A year later, a smaller four strokes engine with capacity of about 138 cc was built. It generates a power of 0.5 hp which was very small compare with the engines output power nowadays. Shortly afterwards, the AV-Twin with speed gearboxes was built. The first motorcycle endurance race was held in 1907 on the Isle of Man. In 1916, a two-cylinder engine with four valves per cylinder was developed and was used in various races as its maximum speed was over 120 mph. the development for the motorcycle racing engines had come to a halt during the two World Wars. But not long after World War 2, 125 cc bikes were built based on the previous models. From then, the development of the motorcycle engines was more on increasing the horsepower and the stability of the bike. Horsepower can be improved by changing the dimensions and configurations of the cylinders and pistons. While the stability is depend on the weight distributions of the bike. Performance Analysis and DiscussionThe performances for each racing engine were plotted in Appendix A and the comparisons of performance for different types of racing engines were plotted in Appendix B. Firstly, all the displacements of formula and rally racing were very closed to 3000cc and 2000cc respectively, however the maximum possible values for those racing vehicle should be larger since those performances were achieved under some racing regulations. On the other hand, the torque and compression ratio of formula racing engines as well as the compression ratio of rally racing engines were missing on the table due to those performance might be confidential to public. In formula racing V10 engine were normally used since the racing engine could only consist 10 cylinders in accordance with formula racing regulation. Most of the rally engines were employing inline configuration except for Subaru Impreza since a more efficient flat configuration was used. Further more, the combined table in Appendix represented the requirement of different racing engines. Ten cylinders were used in formula racing engine since the vehicle required an extremely high speed. Besides, the displacement of sports racing engine was enormous compared to the others, due to the corresponding vehicle (Ford GT) was an endurance racing vehicle, hence an extremely high value of displacement was reasonable. Conversely, the total weight of motorcycle was pretty light, therefore the fuel consumption and displacement required were not necessary to be high. The power of formula engine was the highest as the formula car needed to achieve a highest possible acceleration. On the other hand, the motorcycle power was the lowest as the weight of the vehicle was small; hence only low force and power were needed in order to drive the vehicle. Although the torque of formula engine was not available, the expected torque of formula engine should be greater than the Ford GT (500lb-ft). ConclusionFirstly, Wankel engines obtained a high power performance and were familiar in automotive industry during 1990s. However, the development was suspended due to the arising environmental regulations and the effect of the oil crisis although the future development of this type of engine was enormous. In conclusion, the performances of different racing engines were totally depended on the type of race. For instance endurance vehicle would require a high displacement whereas a motorcycle would not since endurance race needed a larger capacity of fuel. Besides, the performance could be improved by inserting extra components such as turbocharger and cooler, by modifying/replacing the current components or by either employing specific valve configuration in order to lower the centre of gravity of the vehicle. Light weight and high strength material such as aluminium alloy could also be used so that a lighter but stiffer product could be achieved. The future development of these racing engines would be tends to increase the horsepower by developing new systems to assist the current operations. Improvements of racing engine could also be achieved by developing individual components such as piston, crankshaft, camshaft, head cover...etc.",True
83,"SummaryIn this assignment, researches have been done on the types of engines that are usually used for racing. The four types of racing engines that was of interest were the Formula, Rally, Sports and Motorcycle racing engines. The specifications of these engines have been summarized in Appendix A and B so that one can compare their important parameters. Appendix A shows all the racing engines, of which their individual performance will be discuss in the content of this report. Appendix B is a table which accumulates all the information obtained about the four racing engine requirement and their overall comparisons are also included in the main text. In this report, we also accessed past and future developments of each of these racing engines. IntroductionIn 1894, the idea of car racing was raised once a series of petrol-fuelled cars were constructed. The first official race was held in Chicago, Illinois on 2nd November in 1895 and racing engines were improved exponentially during the 20 th century. Researches on different engines which usually used in racing were performed and discussed in this report. Engine used for general automotives was internal combustion (I.C.) engine in which the mechanical power of vehicle was produced by combustion of fuel within the combustion chamber. Internal combustion engine could have either two-strokes or four-strokes and either spark ignites or compression ignites. Gasoline, diesel and natural gas could be selected as the fuel in a SI engine. On the other hand, two major types of IC engines were identified: Rotary engine and Reciprocating engine. The major representative of rotary engines in automotive industry was the Wankel engine (Fig 2) which was the most highly developed rotary engine since 1970s; such engine was used due to the compactness and high power performance. However, the development of Wankel engine was suspended in most of the companies due to the arising environmental regulations as well as the effect of the oil crisis. The most recent car operated by Wankel engine was the Mazda RX-7 which produced in 1999. In automotive racing industry, only particular specifications were selected and employed since the efficiency could only be improved by such specifications. Different cylinder configurations such as single, in-line, v-type, w-type, u-type, opposed cylinder, opposed piston and radial could be found in IC engines. However, in-line and v-type were the most commonly used configurations in automotive racing engines. Further more, the common numbers of valves employed in each cylinder were 2 (1 intake, 1 exhaust), 4 (2 intake, 2 exhaust) and 5 (3 intake, 2 exhaust). Generally 4 valves were employed in racing engine. . Compare to normal engines, limits such as peak operating cylinder pressure were pushed up in some racing engines so that a higher performance could be obtained. Besides, the horsepower and fuel economy could be increased by maximizing the cylinder pressure. Although the cylinder pressure could be increased by increasing the compression ratio, alternative technique could also be used since cylinder pressures could be altered significantly by using camshaft selection, carburetion, nitrous and supercharging. Compression pressures could be adjusted drastically by installing supercharging, turbo-charging or intercooling system. However, apart from installing extra equipment to improve the effectiveness and efficiency of racing engines, the durability and lifetime were also a significant factor to consider since racing cars might require to operation for a long time without any failures Type of RacingsIn the report, a series of engines used in different automotive racings including formula racing, rally racing, sports racing (including endurance) and motorcycle racing were presented. In fact, the features required in racing engine were totally depended on types of racing and performances of different racing engine were adjusted in order to meet the specific purpose and provide a best fit performance. However, high acceleration, high maximum speed, high power, high torque, light weight and high efficiency of engine cycles were generally the ideal for racing engines. For instance, high power output was necessary for rally engine since such racing might consist of climbing uphill or operating under poor conditions. To conclude, particular engine performance could be adjusted in accordance with racing course conditions and requirements. Formula RacingFormula racing was a type of single-seater racing which involved a variety of special designed high performance cars. The wheels of formula cars were not covered and aerofoil wings could always be found at the front and rear of the car (Figure 3). Formula engines should have an extremely high maximum speed, extremely high acceleration, high torque, long life span, high operational temperature and extremely light weight in order to perform effectively on a special designed racing course. Rally RacingThe first World Rally Championship (WRC) was held in 1973 and highly modified cars were competed on normal roads as well as under poor conditions which included dirt, swamps (water resistance) and snowy surfaces (Figure 4). Rally engine should have an extremely high torque, extremely high horsepower, high acceleration, long life span and high operational temperature in order to maintain the car under safety condition as well as effectively operated. Sports RacingSports racing cars were modified cars and were normally operated on racing courses (Figure 5). Endurance racing is part of the sports racing where races were usually carried out over a long distance and vehicles were usually driven by a team of two to three drivers. As similar to formula engines, sports engines should have a reasonable maximum speed and torque during operation, extremely long operational time, a long life span and high operational temperature were also essential to be considered before the production process of racing engine. Motorcycle RacingFinally, motorcycle racing is a contest involving motorcycles competing with each other (Figure 6). Design of motorcycle engines varied largely in order to meet the requirements for different type of races. Normally motorcycle engine required to have light weight, long life span, high operational temperature, extremely high acceleration and maximum speed. Technical featuresAdditional features could be employed in different racing engines so that the performance could be improved further. Forced induction systems such as turbocharger and supercharger were explained as well as cooling systems and carburetor. 1. Turbocharger - An apparatus to boost the horsepower of engine without significantly increased its weight, operated by pressurizing the air flowing into the engine's cylinders in order to get more fuel to be burnt during each stroke. Advantage of compressing the air was more air could be squeezed into a cylinder, hence more fuel could be added and more power could be obtained from each explosion in each cylinder. More overall power could be produced than the same engine without the charging (Figure 7). Supercharger (Figure 8a) - The mechanism was nearly identical to the turbocharger, the major difference was the power supplies in order to run the air compressor. In supercharger, a belt was connected directly to the engine and power was transmitted via the belt. For turbocharger, power was obtained from exhaust stream since the exhaust run through a turbine which in turn the compressor was span. A graph was shown the variation of power with rpm with and without supercharger (Figure 8b). 3. Cooling systems - Either liquid-cooled or air-cooled technique was generally employed in racing engines. i) Liquid-cooling: Fluid flowed through pipes and ducts within the engine, heat was absorbed by the fluid and engine was cooled as the liquid passed through. The fluid was then passed through a heat exchanger or radiator and heat was transferred from the fluid to the air blowing through the exchanger. ii) Air-cooling: An old-fashioned cooling technique, engine block was covered in aluminum fins rather than circulating fluid through the engine so that heat could be conducted away from the cylinder. A powerful fan was used to force air over these fins and the engine was cooled by transferring the heat to air. 4. Carburetor - The right amount of gasoline with air could be mixed so that the engine could run properly. Designs of racing enginesIn an internal combustion engine with multiple (more than one) cylinders, the cylinders and piston could have V-, in-line, horizontal, w type, opposed piston and radial configuration. The most common configurations used in racing engines are the V- and in-line configurations. 1. V-configuration:V-configuration engines would have their cylinders and piston arranged in such a way that they form V shapes as shown in Figure 9 above. Engines using this configuration are named V2, V4, V6, V8, V10, V12, V16, V18, V20 and V24. The V configurations have been widely applied in racing engines as it helps to minimise the engine length and weight as compared to the in-line configuration. The cylinders can be arranged with different V-angles. The bigger the V-angle of an engine, the more stable it is as it has lower centre of gravity. However, large v-angle also gives rise to vibration problems. Therefore, despite increasing the V-angles would lowered the centre of gravity, it also increased the vibration. On the other hand, small V-angle engines will be less stable as the centres of gravities would be higher. Examples of engines with large V-angles are V2, V4, V6 and V10. 2. In-line configuration:In-line configuration is another configuration that is commonly used in racing engines. The cylinders in an in-line engine were arranged in one straight row parallel to each other. This can be applied for all multiple cylinders engine but is more commonly found in four- and six-cylinders engines. Its simple structure makes it easier in production than V-configuration engines. Due to its simple shapes it would be relatively easy for it to be fitted into the car chassis with different positions and can run rather smoothly. The disadvantage of in-line engines is that they are longer than V engines. It will be hard to install in-line engines into small cars and the cooling of engine by air can get difficult if it had been installed in inconvenient position Examples of in-line engines are straight-twin, straight-3, -4, -5, -6, -8, -10, -12 and -14. Straight-4 is the most commonly used in-line engine at the present. Sports EngineSports racings consisted of a variety of competitions between modified cars and were normally taken place on a specific racing course. Due to the enormous area sports racings have covered, only endurance racing was mentioned in this report. 1) Ford GT 2005 (Ford's MOD)The first Ford GT was built in 1963 and was became the world's best endurance racing car in mid-1960s. Ford GT achieved a result with flying colors as such vehicles placed 1-2-3 at the 24 Hours of Le Mans in 1966 as well as won the next three consecutive years. The Ford GT was reconstructed in recent years in order to celebrate the centennial of Ford Motor Company. A mid-mounted, all-aluminium 5.4L DOHC supercharged V-8 engine that produces 550 horsepower was used in this generation. Compression ratio was 8.4:1(in/mm) and Sequential multi-port Electronic Fuel Injection (SEFI) was employed in the engine with dual injectors per cylinder. The MOD V-8 featured aluminium four-valve heads, forged crankshaft, H-beam forged rods and aluminium pistons fed by an Eaton screw-type supercharger, all combined and produced more than 500 hp horsepower and 500 ft-lb of torque. In 1966 a new prototype design (GT40 MK2) was introduced with a 427 c.i. (7000cc) engine, produced a 485 BHP power at 6,200 RPM and 475 Ib-ft of torque at 4,000 RPM. In the same time as GT40 MK1 was being developed, a road used version (GT40 MK3) was designed and a 289 ci. (4700cc) engine was used which could produced 306 BHP power. Between 1966 and 1967, a new GT40 was introduced (the J-Car). However, the final product was not very satisfied as such vehicles were mainly designed from Ford's styling department. During 1968/69 seasons, the GT40 Mk1 was redeveloped and a stronger engine was employed. 400 bhp of power at 6,500 rpm and 385 lb-ft of torque was finally achieved in 1968. In 1969 the power was raised further to 425 bhp at slightly lower revolution of 6,250 rpm and 396 lb-ft of torque at 4,750 rpm, using a 302 ci. engine and other improvements. The Ford GT40 was the origin of Ford GT and was firstly produced in 1964; the engine of first prototype car (GT40 MK1) used a 4200cc Ford V8 engine, with aluminium block and heads. The engine was dry sumped with IDA Webber carburetor atop. The performance was 350 BHP of power at 7,000 rpm and 275 lb-ft of torque at 5,600 rpm. In current design, aluminium was used in most of the parts of engine in current Ford GT due to the high strength and excellent heat transfer, so that the main objectives of endurance racing (long life span, long operational time and safe operation) could be fulfilled. However, some components were manufactured by forgings instead of castings; potential hazard might exist by using such an unseasoned manufacturing process as casting was the conventional manufacturing method of automotive components. Sports Engine - Overall DevelopmentThe performance was improved by adding new components such as supercharger. The 32-valve, 330 cubic-inch, 90-degree all-aluminum V8 engine was placed behind the driver in order to lower the centre of gravity of the vehicle. Performances of 373kW of power @ 6000rpm and 678Nm of torque @ 4500rpm were generated by the engine. Forged components (such as crankshaft, connecting rods and aluminum pistons) were also utilized in order to deal with extra pressure of forced induction. Aluminium components were become familiar due to the light weight and extremely high levels of torsional rigidity. Forgings technique would also become common after the correspondence processes were further improved. Rally EngineRally engines were required to operate effectively even under poor conditions such as in swamps (water resistance), dirt and on snowy surfaces. The engine also needed to generate sufficient power in order to drive the rally car uphill. Two rally engines were shown below: 1) Subaru Impreza WRC 2004 (EJ20)The first Impreza was found in October 1992 and horizontally opposed, four-cylinder, all-alloy EJ20 engine for a displacement of 1.994 L with quad camshafts, 8.0:1 static compression, a single turbocharger and intercooler was employed in this model. The boxer engine was in accordance with a specially reinforced ""semi-closed deck"" engine block, forged aluminium alloy pistons and forged high-carbon steel which linked to rods and exhaust valves (sodium-filled). The theory behind using a 'flat four' was the engine sat lower in the car in which the car's inertia could be benefited and hence a lower centre of gravity could be obtained. In 2003, the output of this turbo charged engine was increased to 165 kW (225 PS), dampers with multiple phase valves were also used in order to obtain high performance and steering stability. Besides, the cooling performance was also enhanced by using a different shape of the inter-cooler water spray nozzle and the shape of the air baffle plate within the air scoop. Nevertheless, developments were still carried out during 2004 in order to remain effective and competitive in the WRC. A wide range of performance upgrades were conducted including using a direct water injection system and increasing the size of IHI turbocharger housing so that stable engine combustion could be obtained. Aluminium block and heads were employed in order to be operated in high temperature effectively. Area of air intake was increased and hence the cooling system including V-mount radiator could be improved. On the other hand, AVCS (Active Valve Control System) was also introduced and the inlet camshaft timing over a 35-degree range was varied in order to improve emissions, tractability and low rpm torque. The EJ20 boxer engine was also modified with a twin scroll turbo, drivability was greatly enhanced by the increased torque at low speeds, while the improved acceleration was particularly significant in rally race. 2) Mitsubishi Lancer Evolution WRC 2004 (4G63)Lancer Evolution was firstly appeared in 1973 and was equipped with the 4G63 engine. The 4G63 engine consisted of an 85.0mm (3.35"") bore and 88.0mm (3.46"") stroke, for a displacement of 1997 cc. Twin contra-rotating balance shafts were housed inside the block and Multi injection technique was employed. The engine was mainly consisted of cast iron block and an aluminium alloy cylinder head. A new turbocharger fitted with the WRC regulation 34-millimetre intake restrictor, new intake and exhaust manifolds, and new internals were used in the engine in order to obtain a higher efficiency. In 2003, refinements were performed on intake and exhaust systems, aluminium induction piping, and weight reduction measures by using a cast-magnesium cylinder head cover and hollow camshafts. The intake, exhaust, turbocharger impeller and housing, camshafts and crankshaft were all developed in 2004, in accordance with the technology already achieved in different Lancer Evolution over the past 10 years. Camshafts were design to hollow so that the rotating inertia was reduced. Moreover, the ""twin-scroll"" impeller in turbocharger was modified by doubling the amount of fins. Dual chambers were used in exhaust housing, the exhaust gases were routed from cylinder 1 and cylinder 4 to one side of the twin-scroll turbocharger and gases from cylinder 2 and cylinder 3 to the other side of turbocharger. Efficiency would be increased by improving the airflow intake. However, the WRC regulation was conflicted by this development since only a maximum of 34-millimetre intake restrictor could be used. Although downforce was a vital factor of competitive car on rally racing, good cooling was a key prerequisite on a turbocharged rally car since the ambient temperature on rally car could usually exceed 30 degrees Celsius and the temperature was often the highest on racings which carried out at the lowest average speeds. In order to achieve a high efficiency and effectiveness in rally engines, airflow beneath the rally car could be used and considered. In 2005, improvements to the rally engines would be carried out by employing a new waste gate, anti-lag valves as well as an improved engine control. On the other hand, the turbo-charger would also be investigated in order to determine the change in efficiency. The performance and tuning possibilities of such engines would also be significantly developed and become more accurate by carrying out different improvements. Rally Engine - Overall DevelopmentOver the past few years, the development of rally engines was improved from only concentrated on the modification and replacement of engine components (such as dampers, turbocharger housing, intake and exhaust systems, piping, head cover, hollow camshafts, twin scroll turbo...etc) to introducing new components and technique (such as airflow beneath the rally car, new waste gate) in order to improve further and achieve a better performance. On the other hand, exact values of some performance such as accelerations and maximum speeds were not defined since those figures could be adjusted easily by tuning. Formula Racing EngineFormula racings involve a variety of high speed and high performance cars competing against each other. At present, the most famous Formula race is Formula-1 (F1) racing. In this assignment, most research was done on Formula-1 engines. Formula-1 car engines have excellent performance as they are about ten times more powerful than normal car engines. To ensure all races are reasonably fair, FIA (Fédération Internationale de l'Automobile) have put forward some limitations to the engines of the cars that were competing in the races. For instance, the engine must be four strokes and consist of 10 circular cylinders with less than 5 valves on each cylinder. Supercharging is not allowed in Formula-1 cars. At present, the 10 cylinders of the engines are usually arranged in V configuration. They often have capacities of about 3000 cc capacity and can generate more than 800 bhp. High torque of the engines means high power output (i.e. high horsepower) as the number of engine cycles per unit time is depend on the torque. Special materials (such as Aluminium alloys, ceramic etc.) were used to manufacture the engine's components for Formula-1 for weight reduction purpose and also to reduce the chances of overheating of the engines. The cooling of the engines was very important in Formula racing cars especially in Endurance racing since the car was required to run for a long period of time (typically 6 hours, 12 hours and 24 hours) over the runway course in order to test the engine's durability. Formula-1 car engines are all air cooled. Less dense ceramic was used in making the internal components of the engines so that it was easier to accelerate and also reduce the engine's fuel consumption. The overall size of the engines should be reasonable light and small, so that they could be easily fitted into the chassis. The numerical data for the engines' torques and compression ratios (CR) are not available in all of the sources. This may because these data have to remain confidential. Following are some examples Formula-1 racing engines: Ferrari :Figures 5a and 5b above are Ferrari F2003-GA single-seater racing car and Type 052 engine respectively. Ferrari F2003-GA was Ferrari's 49 th single-seater racing car entering the FIA Formula-1 world championship. Similar to the previous generations of engines, the 3000 Ferrari (type 052) engine is load-bearing and is fitted longitudinally into the chassis. It has a V configuration and 10 cylinders arrangement with 4 valves per cylinder, i.e. 40 valves. It is a Spark Ignition (SI) engine with Magneti Marelli static electronic ignition. Fuel will be fed into the engine by Magneti Marelli digital electronic injection. Type 052 is an evolution of the former 051 engine with several improvements that increase the engine's performance and usability. Size and weight of the engine were reduced as new materials have been used for manufacturing it. The centre of gravity of the car is lowered and therefore improving the overall weight distribution. Maximum engine revolutions of type 052 engine would be about 200 rpm higher than that of the type 051. Further development of the engine would be carried on in order to improve the horsepower and its performance. The types of engines that Ferrari would develop in the future will tend to be more reliable and can integrate well with the car. Renault:Renault R24 as shown in Figure 6a was a single-seater built by Renault to compete in the Formula-1 racing in year 2004. The engine used to drive R24 was the Renault RS24 engine, which was an improved evolution of the RS23 engines that had been used in the Renault R23 (2003). Renault had been developing R24 since September in order to improve the overall engine performance. The RS23 engine launched in 2003 was a 111-degree V10 engine. It was generally reliable except that it has major problems with vibrations. Therefore RS24 had been developed from RS23 by changing some configurations and its design is based on the Supertec engines. RS24 has a 72-degree V configuration with 10 cylinders arrangements. As RS24 engine is higher, it has a centre of gravity that is 20mm higher than that of RS23. But according to the Renault F1 team, this problem has been compensated by making changes to the engine's casing. Therefore, the centre of gravity of the car R24 is not much higher than that of R23. The problems of vibrations have been improved in the development of the RS24 engine. In February 2005, Renault launched a new RS25 engine that has been developed from RS24. RS25 is basically the same as RS24, but its centre of gravity has been made lower. After the 72-degree V10 engines (RS24 and RS25), Renault was planning to develop a new 90-degree engine. The future developments for Renault engines will be more on weight reduction as well as improving its reliability. Formula Racing Engines - Overall Development:In 1950s, carburetor was widely used to feed fuel into the car engines. Shortly after, more effective direct fuel injection system was introduced by Mercedes. Renault then introduced turbocharged engines into F1. But the cooling systems of engines was not well developed, many turbocharged cars went down due to overheating. During the 80s, the developments of F1 engines were mainly on increasing their torques and the horsepower. But these increase in horsepower reduced the handling of the cars. The ground-effect of the cars were improved which increase their cornering speeds. This made it even harder to control the cars. During that period of time, many deaths of F1 driver were caused by severe accidents during races. In 1989, turbocharged engines were banned by F1 and were replaced by 3500 cc induction engines. In early 1990s, different companies were concentrating on the development of the 3500cc induction engines. The first 700 hp 3500cc induction engine was built which was much better than the previous racing engines. A 'baffle' had been introduced to enhance the cooling of the engines. F1 limited the capacity of the engines to 3000 cc in mid-90s. With the capacities limited, the engines could only be developed further by improving the mass centralization and increasing the horsepower. Horsepower can be increase by increasing the piston's bore and decreasing the size of cylinders as these will improve the volumetric efficiency of the engine. Finding the optimum cylinders configurations can improve the mass centralization (stability) of the car as well as the overall performance. Motorcycle Racing Engine:Motorcycle racing is a variety of sports involving motorcycles competing with each other. Some common examples of motorcycle racings are Isle of Man TT (road racing), MotoGP (circuit racing), Daytona (endurance racing) etc. Engines of motorcycles are gasoline engines and can be either two strokes or four strokes. They either use fuel injection or carburetors to feed the fuel-air mixture into the combustion chamber. Motorcycles used for racing are mostly four-stroke engines with multiple cylinders. The power of an engine depends on the number and sizes of its cylinders. Therefore, increasing the sizes and number of cylinders in an engine would increase its power, i.e. increase the engine's displacement. Motorcycles engines at present can have single-cylinder, two-cylinder, four-cylinder or six-cylinder and their displacements ranges from 250 cc to over 1500 cc. Most motorcycles nowadays often have two or four-cylinder engines. Engines with high displacement have higher fuel consumption and therefore generate more power. A multiple cylinder engine can have its cylinders arranged in horizontal, inline or V configurations. As mentioned beforehand, the main problems with V configuration engines were its vibrations and high centre of mass. Among the three configurations, the horizontal configuration has the best weight distribution (lower centre of gravity) and also produced the least vibrations. The following is an example of a racing motorcycle: Honda CBR1000RR:Figure 8a shows a picture of the Honda CBR1000RR motorcycle and Figures 8b and 8c show the 998cc liquid-cooled engine that drives the motorcycle. The CBR1000RR engine has a displacement of 998cc and with in-line cylinders configuration. The piston's bore and stroke sizes were made different from that of its predecessors so as to improve its compression ratio, i.e. higher volumetric efficiency. Its fuel tank is placed at the centre and fuel was fed into the engine through a dual stage fuel injection (DSFI) carburetor. Honda CBR1000RR was developed based on the previous RC211V with improved horsepower. Therefore, CBR1000RR is as reliable as RC211V but with higher maximum speed. CBR1000RR bike inherited most of RC211V's body structures such as the Aluminium frame, the suspensions etc. As the swing arm of CBR1000RR bike had been made longer, the size and structure of the engine had to be modified in order for it to fit into the bike chassis. The front to back dimensions of the engine was being shortened by arranging its main shaft, countershaft and the crankshaft in a triangular pattern with countershaft below the main shaft. The engine had been placed towards the front of the bike body to improve the overall weight distribution of the bike, i.e. more stable. In the past, Honda development group has been working on the weight centralization aspects, i.e. to improve the stability of the bike. Based on the researches that had been done, the future evolutions of the Honda racer engines would be more on increasing the engine's horsepower and torque. Motorcycle Racing Engine - Overall Development:The first ever engine used to power a motorcycle was a steam engine which was built in 1867 in the US. It was a two-cylinder engine that powered the bike by burning charcoal. In the 1880s, the first gasoline engine for motorcycle was then built which was a single-cylinder gasoline engine (reciprocating). At this early stage, the fuel feeding system of this simple engine was a spray-type carburetor. Later in 1894, a water-cooled parallel two-cylinder engine was developed. The cylinders were moving to and fro simultaneously to create motion to the rear axle by transmitting through connecting rods. An elastic band was placed on each side of the cylinders for energy storing purpose and to enhance compressions in the cylinders. A year later, a smaller four strokes engine with capacity of about 138 cc was built. It generates a power of 0.5 hp which was very small compare with the engines output power nowadays. Shortly afterwards, the AV-Twin with speed gearboxes was built. The first motorcycle endurance race was held in 1907 on the Isle of Man. In 1916, a two-cylinder engine with four valves per cylinder was developed and was used in various races as its maximum speed was over 120 mph. the development for the motorcycle racing engines had come to a halt during the two World Wars. But not long after World War 2, 125 cc bikes were built based on the previous models. From then, the development of the motorcycle engines was more on increasing the horsepower and the stability of the bike. Horsepower can be improved by changing the dimensions and configurations of the cylinders and pistons. While the stability is depend on the weight distributions of the bike. Performance Analysis and DiscussionThe performances for each racing engine were plotted in Appendix A and the comparisons of performance for different types of racing engines were plotted in Appendix B. Firstly, all the displacements of formula and rally racing were very closed to 3000cc and 2000cc respectively, however the maximum possible values for those racing vehicle should be larger since those performances were achieved under some racing regulations. On the other hand, the torque and compression ratio of formula racing engines as well as the compression ratio of rally racing engines were missing on the table due to those performance might be confidential to public. In formula racing V10 engine were normally used since the racing engine could only consist 10 cylinders in accordance with formula racing regulation. Most of the rally engines were employing inline configuration except for Subaru Impreza since a more efficient flat configuration was used. Further more, the combined table in Appendix represented the requirement of different racing engines. Ten cylinders were used in formula racing engine since the vehicle required an extremely high speed. Besides, the displacement of sports racing engine was enormous compared to the others, due to the corresponding vehicle (Ford GT) was an endurance racing vehicle, hence an extremely high value of displacement was reasonable. Conversely, the total weight of motorcycle was pretty light, therefore the fuel consumption and displacement required were not necessary to be high. The power of formula engine was the highest as the formula car needed to achieve a highest possible acceleration. On the other hand, the motorcycle power was the lowest as the weight of the vehicle was small; hence only low force and power were needed in order to drive the vehicle. Although the torque of formula engine was not available, the expected torque of formula engine should be greater than the Ford GT (500lb-ft). ConclusionFirstly, Wankel engines obtained a high power performance and were familiar in automotive industry during 1990s. However, the development was suspended due to the arising environmental regulations and the effect of the oil crisis although the future development of this type of engine was enormous. In conclusion, the performances of different racing engines were totally depended on the type of race. For instance endurance vehicle would require a high displacement whereas a motorcycle would not since endurance race needed a larger capacity of fuel. Besides, the performance could be improved by inserting extra components such as turbocharger and cooler, by modifying/replacing the current components or by either employing specific valve configuration in order to lower the centre of gravity of the vehicle. Light weight and high strength material such as aluminium alloy could also be used so that a lighter but stiffer product could be achieved. The future development of these racing engines would be tends to increase the horsepower by developing new systems to assist the current operations. Improvements of racing engine could also be achieved by developing individual components such as piston, crankshaft, camshaft, head cover...etc.","INTRODUCTIONIn this report, a torque sensor was designed. It was a shaft-type cantilever, which was clamped to a frame at one end, with the other end left free to be twisted under a torque. This torque was applied via a thin steel bar attached to the free end of the shaft, and a force applied to the other end of the steel bar. The shaft was a hollow one, made of steel with the dimensions as follows: length L = 200mm; outside diameter d 0 = 20mm and the internal diameter d i = 17mm. Once this test-rig was set up, it had been proposed that the rig was instrumented so that an automatic measurement of the torsion could be given. Diagrams of the test rig were shown below. METHODTo give this automatic read out of the torsion, it was decided that a torque cell should be implemented. A torque cell was a transducer and transducers were devices used for the measurement of a physical quantity by electrical means, i.e. used to convert non-electrical measurands into electrical quantities. They were used because this method often assisted subsequent operations on the data. In this case, the torque cell was converted an applied torque into an electrical output signal. A torque cell contained a mechanical element- the circular shaft, and a sensor- strain gauges. Firstly the type of strain gauges was chosen. In fact there were two types of gauges, metal or semiconductor. The metal gauges were in the form of a flat coil of wire or etched metal foil and the semiconductor gauges were a strip of semiconductor material in-between two connection leads. The element was wafer-like and had an insulating backing material so that it could be stuck like a postage stamp onto surfaces, using a suitable adhesive. Strain gauges worked on the principal that:  FORMULA  where  R = resistance,  = resistivity, L = length, and A = area of the element.So when the element was stretched, its length increased, its cross-sectional area decreased, and there was also a change in its resistivity. The result was that the resistance of the element changes. Generally, the semiconductor strain gauges were made from silicon and they had a much higher gauge factors than the metal ones which made them much more sensitive than the metal ones (Gauge factors were between 100 - 175 or -100 to -140 depending on whether the silicon was doped with 'P' or 'N' type material, compared with gauge factors of 2 for the metal ones), but it didn't mean that they were automatically better than the metal ones though. The strain gauges based on semiconductor materials were rather more expensive, more difficult to apply and had greater sensitivity to temperature changes than the metal ones. The greater sensitivity to temperature changes meant that the relative change in resistance is now non-linear and therefore more complicated. Due to these negative points, the metal foil strain gauges were chosen. Four active strain gauges are used in order to obtain maximum possible bridge output voltages, to provide temperature compensation, and to make the sensor/transducer insensitive to forces and moments other than the one being measured. These are mounted on two perpendicular 45 helices that are diametrically opposite to one another. Gauges 1 and 3, are mounted on the right hand helix, and sense a positive strain (tension), and gauges 2 and 4, mounted on the left-hand helix, sense a negative strain (compression). The two 45 helices define the principal stress and strain directions for a circular shaft subject to pure torsion. This set-up described can be seen in the following diagram The torque, T transmitted by a shaft is related to the maximum shear stress, produced on the shaft surface by the equation:  FORMULA  where J is the polar second moment of area of the shaft section and r the radius of the shaft. For a hollow circular shaft J = (d 04 - d i4)/2, where d 0 is the outside diameter of the shaft and d i is the internal diameter of the shaft. Thus the maximum shear stress is:  FORMULA  The above set-up of the strain gauges therefore would be able to sense, and thus provide us with the information of the shearing stresses on the shaft. It can be seen from the above diagram how the strain gauges will be set up. We already know that when strain gauges are put under a strain that their resistance changes. Thus, we need a circuit that will convert a change in resistance into an output voltage. A circuit commonly used for this purpose is a Wheatstone bridge circuit. (Shown below) As can be seen from the diagram previous, the output voltage, V 0 of the bridge can be determined by treating the top and bottom parts of the bridge as individual voltage dividers. Thus,  FORMULA   FORMULA  The output voltage V 0 of the bridge is:  FORMULA  Where R 1 = gauge 1 etc. The above equation indicates that the initial output voltage, V 0 = 0 if  FORMULA  This means that the bridge is balanced. The ability to do this makes it considerably easier to measure small changes in voltage output, V 0. We are using a circuit with four active bridges. Providing that they are correctly connected into the bridge, so that one opposite pair (e.g. R 1, R 2) are in tension and the other opposite pair (e.g. R 3, R 4) are in compression; then the sensitivity is four times that of a single element gauge. This bridge also compensates for changes in gauge resistance due to temperature. For metal gauges the effect of temperature is to multiply each gauge resistance by the factor 1 + T; which cancels out in the above voltage equation, as those gauges in tension will have their resistance increased by a temperature change and those in compression will have theirs decreased. As;  FORMULA  and;  FORMULA  FORMULA   FORMULA  FORMULA  Also:  FORMULA  FORMULA  Where G = Gauge factor, or strain sensitivity and = strain then;  FORMULA   FORMULA   FORMULA  This shows that there is a linear variation between the output and the variation of resistance of the strain gauges. a)It was shown earlier that there is a relationship between the maximum shear stress, on the surface of the shaft and the torque, T in the system, given by the following equation:  FORMULA  Where J is the polar second moment of area of the system and r is the outside radius of the shaft. This can be rearranged into the following form using J = (d 04 - d i4)/2 for a hollow circular shaft:  FORMULA  b)Now the relationship between the strain and the shear stress will be looked at. For a circular shaft subject to pure torsion, the direction of the maximum stresses resulting from this shear are at 45 to the shaft axis. This can be seen from looking at the diagram earlier showing the set-up of the strain gauges and the following equation:  FORMULA  These stresses at right angles to each other will give rise to strains in these directions of:  FORMULA  and  FORMULA  where E is Young's modulus for the material and Poisson's ratio, which is given by = -T/L which are the transverse and longitudinal strains. These strains can be measured by the use of the resistive strain gauges aligned as shown in the earlier diagram. c)When looking at the relationship between the resistance and the strain for each gauge, we can start with the following equation that:  FORMULA  Thus the fractional changes in resistance of each of the strain gauges is:  FORMULA  or  FORMULA  d)When looking at the relationship between the bridge output and the torque we go back to the simple equations first. To balance the circuit R 1R 4 = R 2R 3 so that V 0, which is the potential between B and D on the earlier diagram of the circuit, is zero. So when there is a change in the resistance of R 1 then  FORMULA  Similarly, the potential difference across R 4 is:  FORMULA  Thus the potential difference between B and D is:  FORMULA  This is a balanced condition again. Also:  FORMULA  When a force is applied to the shaft, each of the resistors will change their resistance. The changes in resistance in relation to the denominator terms where we have the sum of the resistances is insignificant and can be neglected. Thus:  FORMULA   FORMULA  We also know that:  FORMULA  when balanced there is no change in output voltage so:  FORMULA  And;  FORMULA  therefore;  FORMULA   FORMULA  Hence, the output voltage V 0 from the bridge is proportional to the torque T acting on the shaft. The sensitivity of the torque cell depends on the diameter of the shaft (d o), the shaft material (E and ), the gauge factor (G), and the voltage applied to the Wheatstone bridge (V S). The range of the torque cell depends on the diameter of the shaft and the proportional limit of the material in torsion. We need now though to amplify the output signal of the circuit as the strain gauges in the circuit will only be changing their resistance by a small amount, so the output voltage will therefore only change by a small amount too. Also we need to consider the fact that when we amplify the signal, we will also amplify any noise in the signal, so a filter needs to be employed to attenuate the noise in the signal before amplification. To do this we will use an active filter. This is a device that will combine an operational amplifier and an RC filter. We chose to use a non-inverting op amp as this will amplify the signal and will give a positive value for a positive torque. A diagram of a non-inverting op amp is shown below: The two resistors in the above circuit can be used to calculate the gain of the circuit (G C). The formula that is used to do this is as follows:  FORMULA   FORMULA  where G is the gain of the amplifier. This will be combined with a low-pass RC filter in order to attenuate the noise in the signal. Electrical noise can be a problem as the output voltage from the Wheatstone bridge circuit is only a few millivolts. The electrical noise occurs as a result of magnetic fields generated by current flow in wires in close proximity to the lead wires or bridge, which induce voltage (noise) in the signal loop. This combination of op amp and filter is called an active filter. Active filters are used where select frequencies can be attenuated and the signal amplified during the filtering process. Shown below is an active filter with a non-inverting op amp and a low-pass filter: Now all that is needed is to employ this into the circuit and combine it with the Wheatstone bridge circuit. Taking the output signal from the Wheatstone bridge circuit and using it as the input signal for the active filter does this, as this is the signal that needs to be amplified. The combined circuit is shown below: The circuit has now been set up with strain gauges to sense the stresses in the shaft, and these mechanical outputs have been converted into electrical ones and amplified to a sufficiently high level with the noise attenuated so the signal can be read by a voltage-measuring instrument. This system now needs to be able to be read and calibrated for data presentation. To calibrate this system, we need to precisely measure R 1, R 2, R 3, R 4, V S; the gain G of the amplifier; and the sensitivity S R of the recorder (i.e. the voltmeter). The system calibration constant C for the entire system is then given by:  FORMULA  Where, S A = the amplifier sensitivity, and S R = the recorder sensitivity (volts per division) The strain recorded with the system is given in terms of the system calibration constant as:  FORMULA  where, d S = the deflection of the recorder in divisions. SUMMARY:In Assignment 1, Part A was demonstrated the material selection for the fresh-water heat exchanger tubes. In Part B, material and process were selected for column spacers under a varying compressive load. Finally, the structural section was selected for a beam in Assignment 2. Cambridge Engineering Selector was mainly used in those cases to select the best suitable results. In conclusion, ""Higher Conductivity Copper"" was chosen for fresh-water heat exchanger tubes in Assignment 1 Part A. Secondly, ""Silicon"" was selected for column spacers and ""Fine Grinding (Automated)"" and ""Polishing (Automated)"" processes were selected to use in manufacturing in Assignment 1 Part B. Finally, ""Hot Fin. Steel (Y.S.355MPa) Circular Hollow-(194×5.0)"" was chosen for the optimum structural section. PART A - Material selection for fresh-water heat exchanger tubesIntroduction:In this part, a material was selected and used to manufacture a set of tubes in heat exchangers by using the CES Selector Materials Package. The material needed to conduct heat well, had a maximum operating temperature above the operating temperature of the device, not corrode in the fluid and have adequate ductility. Two stages (Graphical stages) were created to identify a subset of materials that met the criteria. On the other hand, some other material characteristics which might influence the choice were also considered before a suitable material was chosen. The Specification for particular heat exchanger was given below: Maximum service temperature 150°(which is equal to 423K)Elongation >20%Corrosion resistance in fresh water Very goodThermal conductivity As large as possibleMethod:First of all, a graphic stage was created by pressing the ""New graphic stage"" button on the standard toolbar in the CES window, ""Fresh water"" was selected from attributes list box on the x-axis tab and ""Ductility"" was selected on the y-axis tab. A graph was shown to indicate the ductility of materials against the corrosion resistance in fresh water. ""Box selection"" button on the Project toolbar was pressed afterwards for selecting materials within a property range. The mouse was clicked near the point (Very good, 0.2), the button was held down and the cursor was drag near to the point (Very good, 9.3(Max.)). The limits were refined by opening ""Stage properties"" window in the ""Project"" menu and the exact value was entered in the ""Selection"" tab. Total number of 155 different materials met the specification in this stage. (Figure 1.1) Failed materials were hidden by pressing ""Hide failed record"" box in ""Stage properties"" window. Secondly, another graphic stage was created in the same method as the one above. But ""Maximum service temperature"" and ""Thermal conductivity"" were selected on the x-axis and y-axis tab respectively. A graph (Figure 1.2) was shown again to indicate the maximum service temperature against the thermal conductivity for different materials. Again, ""Box selection"" button was pressed and limits were refined to 423K to 3800K (Max.) on the x-axis and refined to 200W/m.K to 422W/m.K (Max.) on the y-axis, using the same method as stage one. At the end, 21 different materials met the specification in this stage. Finally, materials that superimposed were found out by pressing the ""Result intersection"" toggle on the standard toolbar. Results could be viewed by clicking on ""Results Window"" button, in this case there were 11 materials which met both specifications; a list of results was shown in Figure 1.3. A clearer view for stage 1 and 2 after intersection were shown in Figure 1.4 and 1.5 respectively. Evaluation:In fact some characteristics were also important and might influence the choice, therefore a careful consideration was needed, e.g. lower price was important in large number of manufacturing processes, higher endurance limit was helpful for long-term usage and the relevant replacement cost could be minimized, lower specific heat lowered the amount of heat required to heat up the fluid. Although ""Silver, Commercial Purity"" (Figure 1.6) was the best material after intersection, it was too expensive. Both ""Brass"" (Figure 1.7) and ""High Conductivity Copper"" (Figure 1.8) had a lower price and a similar specific heat, ""High Conductivity Copper"" had a higher endurance limit and ""Higher Conductivity Copper"" had a higher thermal conductivity. But the heat exchanger tubes with high thermal conductivity were more important than one with a high endurance limit. So in conclusion, ""Higher Conductivity Copper"" was chosen for fresh-water heat exchanger tubes. PART B - Selection for column spacers under a varying compressive loadIntroduction:This part included selection for column spacers in a precision instrument. The instrument incorporated two stiff plates between which a fluid flowed. They were held apart by a set of seven spacers, which were to be solid bars of circular cross-section. Periodically changing compressive load was applied axially on the plates. Specification for the instrument:Spacers length 2.65±0.015 mmDiameter of plates 55 mm (0.055m)Maximum compressive load 10 5N (0.1MN)Selling price £80,000Number of selling each year 25Expected tooling cost £2500-3000Principal criteria for choice:Spacers should not fail under the load.Material should be essentially an electrical insulator.Thermal distortion should be as low as possible.Resistance to water and organic solvents should be very good and resistance to acids should be good.In theory, thermal distortion could be minimized by selecting materials with large values of the index M= λ/α, where λ is thermal conductivity and α is thermal expansion. Methods:For material selection, ""Materials"" was selected in project setting window. Actually 3 stages (a limit stage and 2 graphic stages) were created for selection of materials. A limit stage was created in stage one and the environmental resistance were adjusted to ""Very good"" for ""Fresh Water"" and ""Organic Solvents"", and ""Good"" for ""Weak Acid"" (Figure 2.1). Total number of 1007 different materials met the specification in this stage. Secondly, a graphical stage was created in stage 2, ""Compressive Strength"" and ""Resistivity"" were selected on x and y-axis respectively. Formula σ=F/A was used to find out the maximum compressive strength, where σ was compressive strength, F was compressive load applied and A was cross sectional area. In this case, σ=0.1/ [π× (0.055/2)2] MPa, which was equal to 42.0906MPa. ""Box selection"" button was pressed and limits were refined to 42.0906MPa to 3850MPa (Max.) on the x-axis and refined to 100×10 -8 ohm.m to (1e+010) ×10 -8 ohm.m (Max.) on the y-axis. At the end, 166 different materials met the specification in this stage (Figure 2.2). Again, failed materials were hidden by pressing ""Hide failed record"" box in ""Stage properties"" window. For stage 3, another graphical stage was created, ""Thermal Expansion"" was selected on the x-axis and ""Thermal Conductivity"" was selected on the y-axis, a gradient-line with a slope of 1 was added to the graph by pressing the ""Gradient-Line Selection"" button. The line was placed to a desired level after held and dragged from any point on the graph. Materials with high index M (λ/α) were indicated by clicking once on a point above line with hand cursor. There are total number of 953 materials met the specification in stage 3 (Figure 2.3). Finally the materials that met the specification in all 3 stages could be viewed on the graph by clicking on ""Result intersection"" toggle at each graphical stage. A list of results could be view by clicking on ""Results Window"" button on standard toolbar. A total number of 9 materials met all the specifications and a list of results was shown in Figure 2.4. A clearer view for stage 2 and 3 after intersection were shown in Figure 2.5 and 2.6 respectively. For process selection, ""Process"" was selected in project setting window from project menu, 2 stages were created for selection of processes. A graphical stage was created in stage 1, advanced button was pressed in graph stage wizard and an expression builder appeared. ""Shape"" folder in left hand pane was clicked, ""Prismatic"" folder was expanded in right hand pane by clicking the ""Plus sign"" next to it. Similarly the ""Axisymmetric"" folder was expanded followed by the ""Solid"" folder, ""Plain"" was selected by double clicking on the folder. On the y-axis, ""Tolerance"" was selected. Precision of 2.65±0.015 mm was given. Therefore ""Box selection"" button was pressed and the limit of tolerance was set to 1e-003mm (Min.) to 0.015mm on the ""Selection"" tab (Figure 2.7). The tooling cost was expected to be £2500-3000, so a limit stage was created in stage 2 and the limit was entered in ""Tooling cost"" boxes (Figure 2.8). ""Result intersection"" toggle was clicked in stage 2 and 8 different processes were met the specification (Figure 2.9). A short list was shown in Figure 2.10. Evaluation:Both ""Silicon"" (Figure 2.11) and ""Germanium"" (Figure 2.12) had a outstanding good result, but the compressive strength and thermal distortion of ""Silicon"" was slightly higher and lower respectively than ""Germanium"", also the price of ""Silicon"" is much lower than ""Germanium"". Therefore, ""Silicon"" was chosen to manufacture the spacers. On the other hand, ""Fine Grinding (Automated)"" (Figure 2.13) and ""Polishing (Automated)""(Figure 2.14) were selected for ""Machining"" and ""Finishing"" process respectively. A better precision and surface finish of spacers could be obtained using the ""Fine Grinding (Automated)"" even the capital cost was very high. Precisions and surface of the spacers could be improved using the ""Polishing (Automated)"" process; the capital cost was similar to the ""Fine Grinding (Automated)"" process. In conclusion, ""Silicon"" was selected for column spacers, ""Fine Grinding (Automated)"" and ""Polishing (Automated)"" processes were used for machining and polishing the spacer respectively. Assignment 2 - Shape selection: Selection of structural section for a beamIntroduction:In this section, the best standard structural section was chosen for a roof member. The interior space was intended to be divided with glass curtain walls and the roof was a pedestrian area. Because of the poor deflection tolerance of the glass, minimal deflection was the primary design consideration. Lateral torsional buckling problems were avoided by maximizing the torsional rigidity of the members and the environmental impact was also minimized. Specification for choice:Considered clear span = 10mAbsolute maximum deflection = 1/400 span = 25mm = 0.025mMaximum capacity of lifting equipment = 500lb = 226.796kgLive roof load (including safety factor) = 50kg/m ~ 500N/mMaximum deflection was given by: δmax = (5qL 4)/ (384EI) q = load in N/m, L = length and EI = flexural stiffness.In this case, δmax = (5×500×10 4)/ (384EI) Method:First of all, ""Shape"" was selected in selection table after opening the ""Project Setting"" window form ""Project"" menu. ""Structural Sections"" filters and forms were selected in shape table. Two stages were created - A graphical stage and a limit stage. In stage 1, graphical stage was created and ""Deflection"" was plotted on the x-axis. ""Deflection"" was created from expression builder window and an expression of (5×500×10 4)/ (384EI) was built. On the y-axis, the ""Optimal secondary design parameter"" (M) was plotted using the expression builder again and an expression of [Torsional Stiffness] × [Recycle Fraction] / [Energy Content] was selected and pasted. ""Box selection"" button was pressed and limits were refined to 4.01878e-005m (Min.) to 0.025m on the x-axis and refined to 100 to 9.6096e+006 (Max.) on the y-axis. Failed materials were hidden by pressing ""Hide failed record"" box in ""Stage properties"" window (Figure 3.1). On the other hand, limit stage was created and limit for ""Mass per unit length"" was filled in stage 2. The length of beam was 10m and the maximum capacity of lifting equipment was 226.796kg, so an amount of 22.6796kg/m was entered into the maximum mass per unit length box (Figure 3.2). After that the ""Result intersection"" toggle was clicked and a total number of 11 passed sections were shown in Figure 3.3. A clearer view for stage 1 after intersection was shown in Figure 3.4. Evaluation:""Hot Rolled Steel (Y.S.355MPa) Universal Beam-(305×102×25)"" (Figure 3.5) was the best section that satisfied the minimum deflection. On the other hand, ""Hot Fin. Steel (Y.S.355MPa) Circular Hollow - (194×5.0)"" (Figure 3.6) was the best section that satisfied the maximum value of M. Anyway, ""Glulam Softwood rectangular section - (495×65×16.11)"" (Figure 3.7) was the best section that satisfied both consideration. Actually, economical factor was also needed to be considered so that a new expression [Torsional Stiffness] × [Recycle Fraction] × [Price] / [Energy Content] was formed for the value of M. The range of M was adjusted to 100 to 8.12613e+006 (Max.) and adjusted to 100 to 9.6096e+006 (Max.) on the y-axis, a new graph was shown in Figure 3.8. ""Hot Fin. Steel (Y.S.355MPa) Circular Hollow - (194×5.0)"" and ""Glulam Softwood rectangular section - (495×65×16.11)"" were also appeared in the graph. By the way, in conclusion, ""Hot Fin. Steel (Y.S.355MPa) Circular Hollow-(194×5.0)"" was chosen for the beam because a high torsional stiffness and a low price could be found from this section.",False
84,"SummaryIn this assignment, researches have been done on the types of engines that are usually used for racing. The four types of racing engines that was of interest were the Formula, Rally, Sports and Motorcycle racing engines. The specifications of these engines have been summarized in Appendix A and B so that one can compare their important parameters. Appendix A shows all the racing engines, of which their individual performance will be discuss in the content of this report. Appendix B is a table which accumulates all the information obtained about the four racing engine requirement and their overall comparisons are also included in the main text. In this report, we also accessed past and future developments of each of these racing engines. IntroductionIn 1894, the idea of car racing was raised once a series of petrol-fuelled cars were constructed. The first official race was held in Chicago, Illinois on 2nd November in 1895 and racing engines were improved exponentially during the 20 th century. Researches on different engines which usually used in racing were performed and discussed in this report. Engine used for general automotives was internal combustion (I.C.) engine in which the mechanical power of vehicle was produced by combustion of fuel within the combustion chamber. Internal combustion engine could have either two-strokes or four-strokes and either spark ignites or compression ignites. Gasoline, diesel and natural gas could be selected as the fuel in a SI engine. On the other hand, two major types of IC engines were identified: Rotary engine and Reciprocating engine. The major representative of rotary engines in automotive industry was the Wankel engine (Fig 2) which was the most highly developed rotary engine since 1970s; such engine was used due to the compactness and high power performance. However, the development of Wankel engine was suspended in most of the companies due to the arising environmental regulations as well as the effect of the oil crisis. The most recent car operated by Wankel engine was the Mazda RX-7 which produced in 1999. In automotive racing industry, only particular specifications were selected and employed since the efficiency could only be improved by such specifications. Different cylinder configurations such as single, in-line, v-type, w-type, u-type, opposed cylinder, opposed piston and radial could be found in IC engines. However, in-line and v-type were the most commonly used configurations in automotive racing engines. Further more, the common numbers of valves employed in each cylinder were 2 (1 intake, 1 exhaust), 4 (2 intake, 2 exhaust) and 5 (3 intake, 2 exhaust). Generally 4 valves were employed in racing engine. . Compare to normal engines, limits such as peak operating cylinder pressure were pushed up in some racing engines so that a higher performance could be obtained. Besides, the horsepower and fuel economy could be increased by maximizing the cylinder pressure. Although the cylinder pressure could be increased by increasing the compression ratio, alternative technique could also be used since cylinder pressures could be altered significantly by using camshaft selection, carburetion, nitrous and supercharging. Compression pressures could be adjusted drastically by installing supercharging, turbo-charging or intercooling system. However, apart from installing extra equipment to improve the effectiveness and efficiency of racing engines, the durability and lifetime were also a significant factor to consider since racing cars might require to operation for a long time without any failures Type of RacingsIn the report, a series of engines used in different automotive racings including formula racing, rally racing, sports racing (including endurance) and motorcycle racing were presented. In fact, the features required in racing engine were totally depended on types of racing and performances of different racing engine were adjusted in order to meet the specific purpose and provide a best fit performance. However, high acceleration, high maximum speed, high power, high torque, light weight and high efficiency of engine cycles were generally the ideal for racing engines. For instance, high power output was necessary for rally engine since such racing might consist of climbing uphill or operating under poor conditions. To conclude, particular engine performance could be adjusted in accordance with racing course conditions and requirements. Formula RacingFormula racing was a type of single-seater racing which involved a variety of special designed high performance cars. The wheels of formula cars were not covered and aerofoil wings could always be found at the front and rear of the car (Figure 3). Formula engines should have an extremely high maximum speed, extremely high acceleration, high torque, long life span, high operational temperature and extremely light weight in order to perform effectively on a special designed racing course. Rally RacingThe first World Rally Championship (WRC) was held in 1973 and highly modified cars were competed on normal roads as well as under poor conditions which included dirt, swamps (water resistance) and snowy surfaces (Figure 4). Rally engine should have an extremely high torque, extremely high horsepower, high acceleration, long life span and high operational temperature in order to maintain the car under safety condition as well as effectively operated. Sports RacingSports racing cars were modified cars and were normally operated on racing courses (Figure 5). Endurance racing is part of the sports racing where races were usually carried out over a long distance and vehicles were usually driven by a team of two to three drivers. As similar to formula engines, sports engines should have a reasonable maximum speed and torque during operation, extremely long operational time, a long life span and high operational temperature were also essential to be considered before the production process of racing engine. Motorcycle RacingFinally, motorcycle racing is a contest involving motorcycles competing with each other (Figure 6). Design of motorcycle engines varied largely in order to meet the requirements for different type of races. Normally motorcycle engine required to have light weight, long life span, high operational temperature, extremely high acceleration and maximum speed. Technical featuresAdditional features could be employed in different racing engines so that the performance could be improved further. Forced induction systems such as turbocharger and supercharger were explained as well as cooling systems and carburetor. 1. Turbocharger - An apparatus to boost the horsepower of engine without significantly increased its weight, operated by pressurizing the air flowing into the engine's cylinders in order to get more fuel to be burnt during each stroke. Advantage of compressing the air was more air could be squeezed into a cylinder, hence more fuel could be added and more power could be obtained from each explosion in each cylinder. More overall power could be produced than the same engine without the charging (Figure 7). Supercharger (Figure 8a) - The mechanism was nearly identical to the turbocharger, the major difference was the power supplies in order to run the air compressor. In supercharger, a belt was connected directly to the engine and power was transmitted via the belt. For turbocharger, power was obtained from exhaust stream since the exhaust run through a turbine which in turn the compressor was span. A graph was shown the variation of power with rpm with and without supercharger (Figure 8b). 3. Cooling systems - Either liquid-cooled or air-cooled technique was generally employed in racing engines. i) Liquid-cooling: Fluid flowed through pipes and ducts within the engine, heat was absorbed by the fluid and engine was cooled as the liquid passed through. The fluid was then passed through a heat exchanger or radiator and heat was transferred from the fluid to the air blowing through the exchanger. ii) Air-cooling: An old-fashioned cooling technique, engine block was covered in aluminum fins rather than circulating fluid through the engine so that heat could be conducted away from the cylinder. A powerful fan was used to force air over these fins and the engine was cooled by transferring the heat to air. 4. Carburetor - The right amount of gasoline with air could be mixed so that the engine could run properly. Designs of racing enginesIn an internal combustion engine with multiple (more than one) cylinders, the cylinders and piston could have V-, in-line, horizontal, w type, opposed piston and radial configuration. The most common configurations used in racing engines are the V- and in-line configurations. 1. V-configuration:V-configuration engines would have their cylinders and piston arranged in such a way that they form V shapes as shown in Figure 9 above. Engines using this configuration are named V2, V4, V6, V8, V10, V12, V16, V18, V20 and V24. The V configurations have been widely applied in racing engines as it helps to minimise the engine length and weight as compared to the in-line configuration. The cylinders can be arranged with different V-angles. The bigger the V-angle of an engine, the more stable it is as it has lower centre of gravity. However, large v-angle also gives rise to vibration problems. Therefore, despite increasing the V-angles would lowered the centre of gravity, it also increased the vibration. On the other hand, small V-angle engines will be less stable as the centres of gravities would be higher. Examples of engines with large V-angles are V2, V4, V6 and V10. 2. In-line configuration:In-line configuration is another configuration that is commonly used in racing engines. The cylinders in an in-line engine were arranged in one straight row parallel to each other. This can be applied for all multiple cylinders engine but is more commonly found in four- and six-cylinders engines. Its simple structure makes it easier in production than V-configuration engines. Due to its simple shapes it would be relatively easy for it to be fitted into the car chassis with different positions and can run rather smoothly. The disadvantage of in-line engines is that they are longer than V engines. It will be hard to install in-line engines into small cars and the cooling of engine by air can get difficult if it had been installed in inconvenient position Examples of in-line engines are straight-twin, straight-3, -4, -5, -6, -8, -10, -12 and -14. Straight-4 is the most commonly used in-line engine at the present. Sports EngineSports racings consisted of a variety of competitions between modified cars and were normally taken place on a specific racing course. Due to the enormous area sports racings have covered, only endurance racing was mentioned in this report. 1) Ford GT 2005 (Ford's MOD)The first Ford GT was built in 1963 and was became the world's best endurance racing car in mid-1960s. Ford GT achieved a result with flying colors as such vehicles placed 1-2-3 at the 24 Hours of Le Mans in 1966 as well as won the next three consecutive years. The Ford GT was reconstructed in recent years in order to celebrate the centennial of Ford Motor Company. A mid-mounted, all-aluminium 5.4L DOHC supercharged V-8 engine that produces 550 horsepower was used in this generation. Compression ratio was 8.4:1(in/mm) and Sequential multi-port Electronic Fuel Injection (SEFI) was employed in the engine with dual injectors per cylinder. The MOD V-8 featured aluminium four-valve heads, forged crankshaft, H-beam forged rods and aluminium pistons fed by an Eaton screw-type supercharger, all combined and produced more than 500 hp horsepower and 500 ft-lb of torque. In 1966 a new prototype design (GT40 MK2) was introduced with a 427 c.i. (7000cc) engine, produced a 485 BHP power at 6,200 RPM and 475 Ib-ft of torque at 4,000 RPM. In the same time as GT40 MK1 was being developed, a road used version (GT40 MK3) was designed and a 289 ci. (4700cc) engine was used which could produced 306 BHP power. Between 1966 and 1967, a new GT40 was introduced (the J-Car). However, the final product was not very satisfied as such vehicles were mainly designed from Ford's styling department. During 1968/69 seasons, the GT40 Mk1 was redeveloped and a stronger engine was employed. 400 bhp of power at 6,500 rpm and 385 lb-ft of torque was finally achieved in 1968. In 1969 the power was raised further to 425 bhp at slightly lower revolution of 6,250 rpm and 396 lb-ft of torque at 4,750 rpm, using a 302 ci. engine and other improvements. The Ford GT40 was the origin of Ford GT and was firstly produced in 1964; the engine of first prototype car (GT40 MK1) used a 4200cc Ford V8 engine, with aluminium block and heads. The engine was dry sumped with IDA Webber carburetor atop. The performance was 350 BHP of power at 7,000 rpm and 275 lb-ft of torque at 5,600 rpm. In current design, aluminium was used in most of the parts of engine in current Ford GT due to the high strength and excellent heat transfer, so that the main objectives of endurance racing (long life span, long operational time and safe operation) could be fulfilled. However, some components were manufactured by forgings instead of castings; potential hazard might exist by using such an unseasoned manufacturing process as casting was the conventional manufacturing method of automotive components. Sports Engine - Overall DevelopmentThe performance was improved by adding new components such as supercharger. The 32-valve, 330 cubic-inch, 90-degree all-aluminum V8 engine was placed behind the driver in order to lower the centre of gravity of the vehicle. Performances of 373kW of power @ 6000rpm and 678Nm of torque @ 4500rpm were generated by the engine. Forged components (such as crankshaft, connecting rods and aluminum pistons) were also utilized in order to deal with extra pressure of forced induction. Aluminium components were become familiar due to the light weight and extremely high levels of torsional rigidity. Forgings technique would also become common after the correspondence processes were further improved. Rally EngineRally engines were required to operate effectively even under poor conditions such as in swamps (water resistance), dirt and on snowy surfaces. The engine also needed to generate sufficient power in order to drive the rally car uphill. Two rally engines were shown below: 1) Subaru Impreza WRC 2004 (EJ20)The first Impreza was found in October 1992 and horizontally opposed, four-cylinder, all-alloy EJ20 engine for a displacement of 1.994 L with quad camshafts, 8.0:1 static compression, a single turbocharger and intercooler was employed in this model. The boxer engine was in accordance with a specially reinforced ""semi-closed deck"" engine block, forged aluminium alloy pistons and forged high-carbon steel which linked to rods and exhaust valves (sodium-filled). The theory behind using a 'flat four' was the engine sat lower in the car in which the car's inertia could be benefited and hence a lower centre of gravity could be obtained. In 2003, the output of this turbo charged engine was increased to 165 kW (225 PS), dampers with multiple phase valves were also used in order to obtain high performance and steering stability. Besides, the cooling performance was also enhanced by using a different shape of the inter-cooler water spray nozzle and the shape of the air baffle plate within the air scoop. Nevertheless, developments were still carried out during 2004 in order to remain effective and competitive in the WRC. A wide range of performance upgrades were conducted including using a direct water injection system and increasing the size of IHI turbocharger housing so that stable engine combustion could be obtained. Aluminium block and heads were employed in order to be operated in high temperature effectively. Area of air intake was increased and hence the cooling system including V-mount radiator could be improved. On the other hand, AVCS (Active Valve Control System) was also introduced and the inlet camshaft timing over a 35-degree range was varied in order to improve emissions, tractability and low rpm torque. The EJ20 boxer engine was also modified with a twin scroll turbo, drivability was greatly enhanced by the increased torque at low speeds, while the improved acceleration was particularly significant in rally race. 2) Mitsubishi Lancer Evolution WRC 2004 (4G63)Lancer Evolution was firstly appeared in 1973 and was equipped with the 4G63 engine. The 4G63 engine consisted of an 85.0mm (3.35"") bore and 88.0mm (3.46"") stroke, for a displacement of 1997 cc. Twin contra-rotating balance shafts were housed inside the block and Multi injection technique was employed. The engine was mainly consisted of cast iron block and an aluminium alloy cylinder head. A new turbocharger fitted with the WRC regulation 34-millimetre intake restrictor, new intake and exhaust manifolds, and new internals were used in the engine in order to obtain a higher efficiency. In 2003, refinements were performed on intake and exhaust systems, aluminium induction piping, and weight reduction measures by using a cast-magnesium cylinder head cover and hollow camshafts. The intake, exhaust, turbocharger impeller and housing, camshafts and crankshaft were all developed in 2004, in accordance with the technology already achieved in different Lancer Evolution over the past 10 years. Camshafts were design to hollow so that the rotating inertia was reduced. Moreover, the ""twin-scroll"" impeller in turbocharger was modified by doubling the amount of fins. Dual chambers were used in exhaust housing, the exhaust gases were routed from cylinder 1 and cylinder 4 to one side of the twin-scroll turbocharger and gases from cylinder 2 and cylinder 3 to the other side of turbocharger. Efficiency would be increased by improving the airflow intake. However, the WRC regulation was conflicted by this development since only a maximum of 34-millimetre intake restrictor could be used. Although downforce was a vital factor of competitive car on rally racing, good cooling was a key prerequisite on a turbocharged rally car since the ambient temperature on rally car could usually exceed 30 degrees Celsius and the temperature was often the highest on racings which carried out at the lowest average speeds. In order to achieve a high efficiency and effectiveness in rally engines, airflow beneath the rally car could be used and considered. In 2005, improvements to the rally engines would be carried out by employing a new waste gate, anti-lag valves as well as an improved engine control. On the other hand, the turbo-charger would also be investigated in order to determine the change in efficiency. The performance and tuning possibilities of such engines would also be significantly developed and become more accurate by carrying out different improvements. Rally Engine - Overall DevelopmentOver the past few years, the development of rally engines was improved from only concentrated on the modification and replacement of engine components (such as dampers, turbocharger housing, intake and exhaust systems, piping, head cover, hollow camshafts, twin scroll turbo...etc) to introducing new components and technique (such as airflow beneath the rally car, new waste gate) in order to improve further and achieve a better performance. On the other hand, exact values of some performance such as accelerations and maximum speeds were not defined since those figures could be adjusted easily by tuning. Formula Racing EngineFormula racings involve a variety of high speed and high performance cars competing against each other. At present, the most famous Formula race is Formula-1 (F1) racing. In this assignment, most research was done on Formula-1 engines. Formula-1 car engines have excellent performance as they are about ten times more powerful than normal car engines. To ensure all races are reasonably fair, FIA (Fédération Internationale de l'Automobile) have put forward some limitations to the engines of the cars that were competing in the races. For instance, the engine must be four strokes and consist of 10 circular cylinders with less than 5 valves on each cylinder. Supercharging is not allowed in Formula-1 cars. At present, the 10 cylinders of the engines are usually arranged in V configuration. They often have capacities of about 3000 cc capacity and can generate more than 800 bhp. High torque of the engines means high power output (i.e. high horsepower) as the number of engine cycles per unit time is depend on the torque. Special materials (such as Aluminium alloys, ceramic etc.) were used to manufacture the engine's components for Formula-1 for weight reduction purpose and also to reduce the chances of overheating of the engines. The cooling of the engines was very important in Formula racing cars especially in Endurance racing since the car was required to run for a long period of time (typically 6 hours, 12 hours and 24 hours) over the runway course in order to test the engine's durability. Formula-1 car engines are all air cooled. Less dense ceramic was used in making the internal components of the engines so that it was easier to accelerate and also reduce the engine's fuel consumption. The overall size of the engines should be reasonable light and small, so that they could be easily fitted into the chassis. The numerical data for the engines' torques and compression ratios (CR) are not available in all of the sources. This may because these data have to remain confidential. Following are some examples Formula-1 racing engines: Ferrari :Figures 5a and 5b above are Ferrari F2003-GA single-seater racing car and Type 052 engine respectively. Ferrari F2003-GA was Ferrari's 49 th single-seater racing car entering the FIA Formula-1 world championship. Similar to the previous generations of engines, the 3000 Ferrari (type 052) engine is load-bearing and is fitted longitudinally into the chassis. It has a V configuration and 10 cylinders arrangement with 4 valves per cylinder, i.e. 40 valves. It is a Spark Ignition (SI) engine with Magneti Marelli static electronic ignition. Fuel will be fed into the engine by Magneti Marelli digital electronic injection. Type 052 is an evolution of the former 051 engine with several improvements that increase the engine's performance and usability. Size and weight of the engine were reduced as new materials have been used for manufacturing it. The centre of gravity of the car is lowered and therefore improving the overall weight distribution. Maximum engine revolutions of type 052 engine would be about 200 rpm higher than that of the type 051. Further development of the engine would be carried on in order to improve the horsepower and its performance. The types of engines that Ferrari would develop in the future will tend to be more reliable and can integrate well with the car. Renault:Renault R24 as shown in Figure 6a was a single-seater built by Renault to compete in the Formula-1 racing in year 2004. The engine used to drive R24 was the Renault RS24 engine, which was an improved evolution of the RS23 engines that had been used in the Renault R23 (2003). Renault had been developing R24 since September in order to improve the overall engine performance. The RS23 engine launched in 2003 was a 111-degree V10 engine. It was generally reliable except that it has major problems with vibrations. Therefore RS24 had been developed from RS23 by changing some configurations and its design is based on the Supertec engines. RS24 has a 72-degree V configuration with 10 cylinders arrangements. As RS24 engine is higher, it has a centre of gravity that is 20mm higher than that of RS23. But according to the Renault F1 team, this problem has been compensated by making changes to the engine's casing. Therefore, the centre of gravity of the car R24 is not much higher than that of R23. The problems of vibrations have been improved in the development of the RS24 engine. In February 2005, Renault launched a new RS25 engine that has been developed from RS24. RS25 is basically the same as RS24, but its centre of gravity has been made lower. After the 72-degree V10 engines (RS24 and RS25), Renault was planning to develop a new 90-degree engine. The future developments for Renault engines will be more on weight reduction as well as improving its reliability. Formula Racing Engines - Overall Development:In 1950s, carburetor was widely used to feed fuel into the car engines. Shortly after, more effective direct fuel injection system was introduced by Mercedes. Renault then introduced turbocharged engines into F1. But the cooling systems of engines was not well developed, many turbocharged cars went down due to overheating. During the 80s, the developments of F1 engines were mainly on increasing their torques and the horsepower. But these increase in horsepower reduced the handling of the cars. The ground-effect of the cars were improved which increase their cornering speeds. This made it even harder to control the cars. During that period of time, many deaths of F1 driver were caused by severe accidents during races. In 1989, turbocharged engines were banned by F1 and were replaced by 3500 cc induction engines. In early 1990s, different companies were concentrating on the development of the 3500cc induction engines. The first 700 hp 3500cc induction engine was built which was much better than the previous racing engines. A 'baffle' had been introduced to enhance the cooling of the engines. F1 limited the capacity of the engines to 3000 cc in mid-90s. With the capacities limited, the engines could only be developed further by improving the mass centralization and increasing the horsepower. Horsepower can be increase by increasing the piston's bore and decreasing the size of cylinders as these will improve the volumetric efficiency of the engine. Finding the optimum cylinders configurations can improve the mass centralization (stability) of the car as well as the overall performance. Motorcycle Racing Engine:Motorcycle racing is a variety of sports involving motorcycles competing with each other. Some common examples of motorcycle racings are Isle of Man TT (road racing), MotoGP (circuit racing), Daytona (endurance racing) etc. Engines of motorcycles are gasoline engines and can be either two strokes or four strokes. They either use fuel injection or carburetors to feed the fuel-air mixture into the combustion chamber. Motorcycles used for racing are mostly four-stroke engines with multiple cylinders. The power of an engine depends on the number and sizes of its cylinders. Therefore, increasing the sizes and number of cylinders in an engine would increase its power, i.e. increase the engine's displacement. Motorcycles engines at present can have single-cylinder, two-cylinder, four-cylinder or six-cylinder and their displacements ranges from 250 cc to over 1500 cc. Most motorcycles nowadays often have two or four-cylinder engines. Engines with high displacement have higher fuel consumption and therefore generate more power. A multiple cylinder engine can have its cylinders arranged in horizontal, inline or V configurations. As mentioned beforehand, the main problems with V configuration engines were its vibrations and high centre of mass. Among the three configurations, the horizontal configuration has the best weight distribution (lower centre of gravity) and also produced the least vibrations. The following is an example of a racing motorcycle: Honda CBR1000RR:Figure 8a shows a picture of the Honda CBR1000RR motorcycle and Figures 8b and 8c show the 998cc liquid-cooled engine that drives the motorcycle. The CBR1000RR engine has a displacement of 998cc and with in-line cylinders configuration. The piston's bore and stroke sizes were made different from that of its predecessors so as to improve its compression ratio, i.e. higher volumetric efficiency. Its fuel tank is placed at the centre and fuel was fed into the engine through a dual stage fuel injection (DSFI) carburetor. Honda CBR1000RR was developed based on the previous RC211V with improved horsepower. Therefore, CBR1000RR is as reliable as RC211V but with higher maximum speed. CBR1000RR bike inherited most of RC211V's body structures such as the Aluminium frame, the suspensions etc. As the swing arm of CBR1000RR bike had been made longer, the size and structure of the engine had to be modified in order for it to fit into the bike chassis. The front to back dimensions of the engine was being shortened by arranging its main shaft, countershaft and the crankshaft in a triangular pattern with countershaft below the main shaft. The engine had been placed towards the front of the bike body to improve the overall weight distribution of the bike, i.e. more stable. In the past, Honda development group has been working on the weight centralization aspects, i.e. to improve the stability of the bike. Based on the researches that had been done, the future evolutions of the Honda racer engines would be more on increasing the engine's horsepower and torque. Motorcycle Racing Engine - Overall Development:The first ever engine used to power a motorcycle was a steam engine which was built in 1867 in the US. It was a two-cylinder engine that powered the bike by burning charcoal. In the 1880s, the first gasoline engine for motorcycle was then built which was a single-cylinder gasoline engine (reciprocating). At this early stage, the fuel feeding system of this simple engine was a spray-type carburetor. Later in 1894, a water-cooled parallel two-cylinder engine was developed. The cylinders were moving to and fro simultaneously to create motion to the rear axle by transmitting through connecting rods. An elastic band was placed on each side of the cylinders for energy storing purpose and to enhance compressions in the cylinders. A year later, a smaller four strokes engine with capacity of about 138 cc was built. It generates a power of 0.5 hp which was very small compare with the engines output power nowadays. Shortly afterwards, the AV-Twin with speed gearboxes was built. The first motorcycle endurance race was held in 1907 on the Isle of Man. In 1916, a two-cylinder engine with four valves per cylinder was developed and was used in various races as its maximum speed was over 120 mph. the development for the motorcycle racing engines had come to a halt during the two World Wars. But not long after World War 2, 125 cc bikes were built based on the previous models. From then, the development of the motorcycle engines was more on increasing the horsepower and the stability of the bike. Horsepower can be improved by changing the dimensions and configurations of the cylinders and pistons. While the stability is depend on the weight distributions of the bike. Performance Analysis and DiscussionThe performances for each racing engine were plotted in Appendix A and the comparisons of performance for different types of racing engines were plotted in Appendix B. Firstly, all the displacements of formula and rally racing were very closed to 3000cc and 2000cc respectively, however the maximum possible values for those racing vehicle should be larger since those performances were achieved under some racing regulations. On the other hand, the torque and compression ratio of formula racing engines as well as the compression ratio of rally racing engines were missing on the table due to those performance might be confidential to public. In formula racing V10 engine were normally used since the racing engine could only consist 10 cylinders in accordance with formula racing regulation. Most of the rally engines were employing inline configuration except for Subaru Impreza since a more efficient flat configuration was used. Further more, the combined table in Appendix represented the requirement of different racing engines. Ten cylinders were used in formula racing engine since the vehicle required an extremely high speed. Besides, the displacement of sports racing engine was enormous compared to the others, due to the corresponding vehicle (Ford GT) was an endurance racing vehicle, hence an extremely high value of displacement was reasonable. Conversely, the total weight of motorcycle was pretty light, therefore the fuel consumption and displacement required were not necessary to be high. The power of formula engine was the highest as the formula car needed to achieve a highest possible acceleration. On the other hand, the motorcycle power was the lowest as the weight of the vehicle was small; hence only low force and power were needed in order to drive the vehicle. Although the torque of formula engine was not available, the expected torque of formula engine should be greater than the Ford GT (500lb-ft). ConclusionFirstly, Wankel engines obtained a high power performance and were familiar in automotive industry during 1990s. However, the development was suspended due to the arising environmental regulations and the effect of the oil crisis although the future development of this type of engine was enormous. In conclusion, the performances of different racing engines were totally depended on the type of race. For instance endurance vehicle would require a high displacement whereas a motorcycle would not since endurance race needed a larger capacity of fuel. Besides, the performance could be improved by inserting extra components such as turbocharger and cooler, by modifying/replacing the current components or by either employing specific valve configuration in order to lower the centre of gravity of the vehicle. Light weight and high strength material such as aluminium alloy could also be used so that a lighter but stiffer product could be achieved. The future development of these racing engines would be tends to increase the horsepower by developing new systems to assist the current operations. Improvements of racing engine could also be achieved by developing individual components such as piston, crankshaft, camshaft, head cover...etc.","1. Executive SummaryThe report consisted of the design and analysis of a commercial meat slicer, which connected between an AC motor with speed of 1800rpm and a grinder with an output speed of ~90rpm. A variety of calculations would be performed for different major components. Computer program would also be used in order to compare the results from computer modeling to manual calculations. A computer aid design drawing would be conducted and presented near the final stage and an appropriate size of gearbox housing would be designed. After the analysis the optimal gearbox should have a double reduction system with gear ratios of 4.472. Optimal gears would have a size of 36mm and 161mm gear diameters with 27mm thickness and the optimal shafts would have a diameter of 12mm and 25mm with a length of 100mm. Bearings would be selected based on the size of the shaft, the revolution speed and the maximum loading, diameter of 12mm and 25mm bearings with thickness of 18mm and 37.5 respectively were finally considered in order to achieve the optimal performance. Finally, the optimal size of gearbox housing was set to be FORMULA  in order to enable the operation effective and safe. Unfortunately the computer program did not operate as expected so that the comparison between computer results and manual calculations could not be conducted. However, the variation of parameters in the computer program was limited so that only a limited number of comparisons could be done even though the programs were working properly. More developments and improvements were required in gear and shaft program so that a variety of variables could be selected for analysis. 3. IntroductionThe assignment consisted of the design and analysis of a reduction gearbox which would be used for a commercial meat slicer. Appropriate components such as gears, shafts, bearings and housing would be selected and investigated in order to satisfy the requirement. A variety of calculations would firstly be performed in order to ensure the bending and surface stress conditions were within the safety limit, different dimensions would also be examined so that the optimal output could be achieved. A spreadsheet would be created in order to summarize all the possible arrangements. On the other hand, the shaft would be designed based on the optimal gear parameters, bending moment diagrams and shear force diagrams were constructed in order to perform analysis on bending. Computer program would also be used in order to determine and compare results of shaft between computer outcome and calculating results. Finally, bearings for corresponding shafts would also be selected based on their rotating speed and maximum load. A computer aid design (CAD) drawing would also be presented in order to recognize any constraints for the physical arrangement in the gearbox and hence the smallest possible gearbox housing could be constructed. RequirementsThe input of the meat slicer was a constant speed AC motor running at 1800 rev/min and delivering 1.2kW power. A gearbox was connected to the motor in order to provide speed reduction and meet the functional requirements. Rotating speed of the final shaft was expected at between 80 and 100 rev/min and the duty (utilization) each day was expected to be up to 6 hr. Severe constraint on size was required as the slicer required to go onto a work surface where space was limited. In conclusion, the initial specification could be summarized as follows:  Input speed n P = 1800rpm Input power P = 1200W Output speed n G = FORMULA  = 90 Operation time t = 6hr/day Hence the gear ratio U = FORMULA  = 20 Initial IdeasSince the reduction gear ratio was enormous (20:1), a double reduction gearbox was selected so that a higher reliability could be achieved. The gearbox was connected between the motor and grinder and bearings would be placed at both ends of shaft. Straight spur gears would be the best combination to employ as such arrangement could provide a more convenient procedure in further analysis. Conceptual Design (Initial Layout)Before the commencement of design analysis process, the initial layout of the product was illustrated (Figure 1) and the gearbox housing was represented by the grey rectangle. From the diagram, power was transmitted from the AC motor to gear 2 through the shaft and gear 2 was then connected to gear 3. Again, power was transmitted through the shaft from gear 3 to 4 and gear 4 was connected to gear 5. Finally, power was transmitted from gear 5 to the grinder through the shaft. 4. Design Analysis and EvaluationA) Gear AnalysisIn order to select the most suitable component, a range of analysis was performed and the limitation was identified so that the gearbox could be operated under safety condition. 1) First of all, the nominal velocity ratio U was determined:  FORMULA (Single reduction) However, a double reduction arrangement was employed, therefore:  FORMULA  2) As the material used in both the pinion and the gear were identical, the pinion was always weaker since more undercutting could be found from the smaller gear. In order to avoid any undercutting appear in the operation, a high pressure angle was selected (25°). To ensure no interference could be found within the gearbox, a minimum number of 12 teeth were required in the pinion (Appendix A). Hence the number of teeth could be determined for the gear:  FORMULA  If  FORMULA , then  FORMULA , hence  FORMULA (Within the required range) After that, the modulus m was assumed to be 3 and the pitch diameters were determined:  FORMULA  (Pinion)  FORMULA  (Gear) Since D P and D G were defined, the centre distance c could then be calculated:  FORMULA  3) Moreover, the pitch line speed was computed: Since  FORMULA   FORMULA  4) As power P was provided, the transmitted load F t could also be determined: 5) On the other hand, the dynamic factor K v was identified:  FORMULA  6) In order to avoid torsional distortion, the face width b was required to lie between 9m and 15m (when m=module in mm/tooth) and should be less than or equal to D P:  FORMULA  and FORMULA  if b was assumed to be 11m, FORMULA , in which  FORMULA  and FORMULA  7) Since the bending stress within the gear depended on the torque, geometry and the form factor, the expected bending stress σb in the pinion and gear could be determined:  FORMULA   FORMULA  The Lewis form factor Y could be identified (Appendix B). For the pinion, Y P=0.27677 For the gear, FORMULA  Hence,  FORMULA   FORMULA  Therefore the maximum bending stress occurred at the pinion with the value of 20.21MPa. However, in order to achieve a safety and reliable operation, the allowable bending stress S at could be calculated based on the maximum bending stress σb. 8)  FORMULA  where  S at=Allowable bending stress k a=Surface factor k b=Size factor k c=Reliability factor k d=Temperature factor k e=Modified factor for stress concentration factor k f=Miscellaneous-effects factorand  FORMULA ,1 would be taken if the corresponding factor was not concerned. Hence assumptions of factors were made for a meat slicer: k a=0.9, k b=0.8, k c=0.7, k d=0.9, k e=1, k f=0.9 As  FORMULA   FORMULA  Gray cast iron was selected as the ultimate tensile strength was approximately 160MPa, in which the bending stress could be covered sufficiently. Further analysis would be carried out in order to investigate the contact stress between gears. The contact stress (Modified Hertzian stress) σc on the surface of teeth could be identified as:  FORMULA  where FORMULA  and  FORMULA   FORMULA   FORMULA  Similarly, the allowable fatigue strength S ac was determined:  FORMULA  where  C T=Temperature factor (C T=1 for TTherefore FORMULA  Since the yield strength of gray cast iron was approximately 620MPa, the contact stress could be fully covered. The above manual calculation represented the detailed procedure of the gear analysis. A spreadsheet (Appendix D) was also created in order to simplify the calculation process and results of alternative trials could be summarized. According to the table, the fatigue strength and bending stress could be affected by variables such as life factor C L, reliability factors C R and k c, face width b and other miscellaneous factors. In general, bending stress was relatively small compared to the fatigue strength and the gear should firstly be failed by fatigue. The allowable bending stress and fatigue strength would decrease as the reliability (factor) decrease, which meant a gear with high reliability would require a material with high UTS and fatigue strength. Besides, the fatigue strength would increase as the life factor reduced. Therefore a gear with large life cycle (hence low cycle factor) could be manufactured using a high fatigue strength material. Moreover, both the bending stress and fatigue strength would become higher as the face width decreased, which represented a larger face width would have a lower bending stress and fatigue strength. On the other hand, alternative materials were compared in order to identify the optimal material to be employed in the gearbox. Since grey cast iron, case hardening carbon steel and aluminium bronze were mostly utilized, a number of trials based on those materials were performed. A larger flexibility could be achieved by using grey cast iron as the minimum module size for carbon steel and aluminium bronze were 4mm and 5mm respectively. In conclusion, an ideal gear would be manufactured by grey cast iron with 3mm module, with extremely high reliability and low life cycle factor. Since a larger face width would occupy a larger volume, low face width would be beneficial when size was significant. Although a high m or b would reduce the bending stress, both parameters were relatively small, which might due to volume was also one of the significant parameters to consider. Moreover, the UTS of the corresponding material might also be able to compensate the change of bending stress so that a small module and low face width could be selected. The ideal combination was attached at the bottom of the spreadsheet and detailed specifications were presented below (Table 1): Gear program was employed afterwards and the above parameters were inserted. Commercial cut gear class was selected and metric module definition was considered. A number of figures were attached in Appendix E in order to show the calculation results with relevant specifications using gear program. Since a double reduction process could not be performed in the program and both reduction stages were identical (with U=4.472), only the first reduction stage was analysis. However, the minimum number of pinion teeth N P could not be amended in the program and could only be 18, which might possibly due to either gear design failure or computational problems. A test was performed in order to lower the value of N P, however a gear ratio of 1 was required in order to reduce N P to 13 in the gear program. Although the face width calculation was successful, the module and the face width were different to the original specifications. Therefore the program could only be used as a supplementary element for gear analysis since a variety of parameters such as available materials and class of gears were limited. B) Shaft AnalysisThe design and analysis of shafts could be performed using bending moment diagrams and shear force diagrams. An initial design layout was drawn in order to perform further analysis in the shafts(Figure 2). In this design, three shafts would be employed in the gearbox in order to perform double reduction effect. Each shaft was assumed to be 100mm long. First of all, the power was transmitted from the motor to gear A, then to gear B on shaft 2. Since the contact force between gears was acting in two directions, the vertical and horizontal analysis of three shafts would be carried out sequentially. Initially, the arrangement of shaft 1(Figure 3) was shown and the analysis for horizontal and vertical axis of shaft 1 was also carried out. Therefore, on vertical plane X-Y (left): M(R 1V):  FORMULA   FORMULA   FORMULA   FORMULA   FORMULA  Similarly, on horizontal plane X-Z: M(R 1H):  FORMULA   FORMULA   FORMULA   FORMULA   FORMULA  The relevant shear force and bending moment diagram for both planes were shown. Therefore the maximum bending moment M B:  FORMULA  Since  FORMULA  where  n=Safety factor (assume n=2.5 in this design) S y=Yield Strength of grey cast iron=620MPa T=Maximum torque=  FORMULA  FORMULA  FORMULA  FORMULA Therefore the minimum shaft diameter should be about 12-20mm. The power was then transmitted from gear B to C via shaft 2, followed by gear D on shaft 3. The design arrangement of shaft 2 was shown(Figure 4). Again, analysis was carried out in order to investigate the vertical and horizontal axis of shaft 2. First of all, the corresponding axial components of F B were determined. FORMULA   FORMULA  Moreover, torque T at gear B was found:  FORMULA  As the torque was constant along shaft 2, the torque at gear C was equal to at gear B. FORMULA   FORMULA  The vertical (XY-plane) and horizontal (XZ-plane) analysis of shaft 2 were then performed. Since point 3 & 4 were the reaction force, they required to act towards the resultant force together. Therefore for vertical plane (XY): M(R 3V):  FORMULA   FORMULA   FORMULA  Hence:  FORMULA   FORMULA   FORMULA  Similarly, for horizontal plane(XZ): M(R 3H):  FORMULA   FORMULA   FORMULA  So:  FORMULA   FORMULA   FORMULA  Therefore the maximum bending moment M B:  FORMULA  Again:  FORMULA  where  n=Safety factor (n=2.5 in this design) S y=Yield Strength of grey cast iron=620MPa T=Maximum torque=  FORMULA  FORMULA  FORMULA  FORMULA Therefore the minimum diameter for shaft 2 should be around 20-25mm. Finally, the arrangement of shaft 3 was shown(Figure 5): Firstly, the value of F DY and F DZ were determined:  FORMULA   FORMULA  The vertical (XY-plane) and horizontal (XZ-plane) analysis was performed: Therefore, on vertical plane X-Y: M(R 5V):  FORMULA   FORMULA   FORMULA   FORMULA  Similarly, on horizontal plane X-Z: M(R 5H):  FORMULA   FORMULA   FORMULA   FORMULA  The relative shear force diagram and bending moment diagram for both planes were shown. Maximum bending moment M B:  FORMULA  Since  FORMULA  where  n=Safety factor (assume n=2.5 in this design) S y=Yield Strength of grey cast iron=620MPa T=Maximum torque=  FORMULA  FORMULA  FORMULA  FORMULA Therefore the minimum shaft diameter for shaft 3 should be between 22-30mm. A table was shown in order to summarize the specifications of three shafts: On the other hand, the face width of the larger gear was relatively small compared to the pitch diameter. Therefore a reasonable size of boss (20mm) would be placed with the large gear on shaft 2 (10mm each side) and shaft 3 in order to enhance the contact length between the shaft and gear. Since a rectangular housing would be used for the gearbox, the bearings would be positioned at both ends of the shaft and through the housing wall. C) Bearing AnalysisIn order to investigate the correct size of bearings placed at the end of shafts in the gearbox, a number of calculations were performed. Firstly, the resultant forces at all 6 bearings were determined: Bearing 1: FORMULA  Bearing 2: FORMULA  Bearing 3: FORMULA  Bearing 4: FORMULA  Bearing 5: FORMULA  Bearing 6: FORMULA  After all the resultant forces were determined, the type and size of bearings would be selected based on the maximum load on each shaft using Appendix F (i.e. the higher load would be chosen as the typical maximum load for each shaft). Bearing 1 & 2) 253N @ 1800rpm = 12mm ball rolling bearings Bearing 3 & 4) 1213N @ FORMULA  = 25mm ball rolling bearings Bearing 5 & 6) 1133N @ 90rpm = 12mm ball rolling bearings Since such corresponding points in the graph was close to the rolling bearings line, therefore they were used for the bearings at the end of the shaft. The purposed diameter of shaft 3 was 25mm, so the size of bearing 5 & 6 should be larger than 25mm. Besides, the bearing ratio BR should also be aware since the ratio should be greater than 1.5 and not less than 1 for any smooth operations. Assumed ratio=1.5, since FORMULA  Bearing 1, 2)  FORMULA  Bearing 3, 4)  FORMULA  Bearing 5, 6)  FORMULA  Finally, a summary of specifications for bearings and shafts was produced: Although the bearings were slight long compared to the available spaces on the shaft, the shaft would be extended in order to be surrounded by the bearings. Nevertheless, the shaft program was also an alternative way to simulate the outcome. However, errors occurred once all the data were entered each time (Appendix G), therefore a computational comparison could not be conducted. D) Specification & Discussion of Components and MaterialsSince roomage was also a particular factor to consider in design process, the smallest possible gear size was selected. Grey cast iron was chosen to be the production material of gear and shaft as the yield strength of grey cast iron was fairly high, so that a long lifespan could be expected. Besides, the module was also the smallest size within the range as any modules smaller than 3 would not be able to perform their duties effectively and endurably. On the other hand, bearings were selected based on the maximum load and the speed of the shaft. However, a prudent decision was made and a larger bearing was used for some shafts as the corresponding point on graph was too closed to the upper boundary. Die-cast aluminium would be used for the housing and lubricated ball bearings would be used for shaft supports. A final specification was constructed in order to summarize the parameters of each component. E) Main Design Layout Drawings & Report DiscussionThe final version of the gearbox was plotted in SolidWorks and a CAD drawing was attached (Appendix G). All the dimensions were shown in millimeters and an isometric view was shown below (Figure 5). In comparison of manual calculation with computer program, the computer program were not as convenient as expected, which might due to outdated operating system in the program or even bugs. Therefore manual calculation would be desired if the actual design parameters were given. However, the computer program will enhance the effectiveness and also provide a more convenient analytical procedure if further development was carried out Alternatively, the operation environment would also change the requirements and hence the specification. For instance the temperature factor would be increased if the operation temperature was higher than room temperature. Reliability factor would also increase if the precision of cutting is important. Developments could also be performed in order to improve the performance. Alternative materials could be used if the fatigue strength was not sufficient enough. Increasing the number of shaft sample would also improve the reliability of the analysis, which may also increase the variety of effective design. 5. ConclusionIn the design process, a number of analyses were carried out in order to achieve an optimal gearbox design for a commercial meat slicer. The manual calculations were carried out smoothly and a number of gear combinations were investigated. Unfortunately the computer program did not operate as expected so that both results could not be compared. However, the computer program only offered a limited number of variables which meant the flexibility was limited even such program run normally. Nevertheless, manual calculations enable a more flexible and reliable analysis during design process, hence the volume of the gear box could be minimized more easily. After a range of manual calculations, the gearbox could be operated as requested from 1800rpm to ~90rpm, using a double reduction gearbox with gear ratios of ~4.472. After that, the minimal gear diameters of 36mm and 161mm with 27mm thickness were selected for small and large gears respectively. Such values were then utilized for shaft analysis and hence bearing analysis near the final stage. Finally, two 25mm and one 12mm diameter shafts with 100mm length were chosen for gearbox shafts and 6 bearings (25mm and 12mm diameter with 37.5mm and 18mm thickness respectively) were selected to support the corresponding shafts. The gearbox design was then constructed in SolidWorks, CAD drawing was then produced in order to minimize the required gearbox housing. A size of FORMULA  was used for the housing and a thickness of 12mm was reserved as a safety basis.",True
85,"1. Executive SummaryThe report consisted of the design and analysis of a commercial meat slicer, which connected between an AC motor with speed of 1800rpm and a grinder with an output speed of ~90rpm. A variety of calculations would be performed for different major components. Computer program would also be used in order to compare the results from computer modeling to manual calculations. A computer aid design drawing would be conducted and presented near the final stage and an appropriate size of gearbox housing would be designed. After the analysis the optimal gearbox should have a double reduction system with gear ratios of 4.472. Optimal gears would have a size of 36mm and 161mm gear diameters with 27mm thickness and the optimal shafts would have a diameter of 12mm and 25mm with a length of 100mm. Bearings would be selected based on the size of the shaft, the revolution speed and the maximum loading, diameter of 12mm and 25mm bearings with thickness of 18mm and 37.5 respectively were finally considered in order to achieve the optimal performance. Finally, the optimal size of gearbox housing was set to be FORMULA  in order to enable the operation effective and safe. Unfortunately the computer program did not operate as expected so that the comparison between computer results and manual calculations could not be conducted. However, the variation of parameters in the computer program was limited so that only a limited number of comparisons could be done even though the programs were working properly. More developments and improvements were required in gear and shaft program so that a variety of variables could be selected for analysis. 3. IntroductionThe assignment consisted of the design and analysis of a reduction gearbox which would be used for a commercial meat slicer. Appropriate components such as gears, shafts, bearings and housing would be selected and investigated in order to satisfy the requirement. A variety of calculations would firstly be performed in order to ensure the bending and surface stress conditions were within the safety limit, different dimensions would also be examined so that the optimal output could be achieved. A spreadsheet would be created in order to summarize all the possible arrangements. On the other hand, the shaft would be designed based on the optimal gear parameters, bending moment diagrams and shear force diagrams were constructed in order to perform analysis on bending. Computer program would also be used in order to determine and compare results of shaft between computer outcome and calculating results. Finally, bearings for corresponding shafts would also be selected based on their rotating speed and maximum load. A computer aid design (CAD) drawing would also be presented in order to recognize any constraints for the physical arrangement in the gearbox and hence the smallest possible gearbox housing could be constructed. RequirementsThe input of the meat slicer was a constant speed AC motor running at 1800 rev/min and delivering 1.2kW power. A gearbox was connected to the motor in order to provide speed reduction and meet the functional requirements. Rotating speed of the final shaft was expected at between 80 and 100 rev/min and the duty (utilization) each day was expected to be up to 6 hr. Severe constraint on size was required as the slicer required to go onto a work surface where space was limited. In conclusion, the initial specification could be summarized as follows:  Input speed n P = 1800rpm Input power P = 1200W Output speed n G = FORMULA  = 90 Operation time t = 6hr/day Hence the gear ratio U = FORMULA  = 20 Initial IdeasSince the reduction gear ratio was enormous (20:1), a double reduction gearbox was selected so that a higher reliability could be achieved. The gearbox was connected between the motor and grinder and bearings would be placed at both ends of shaft. Straight spur gears would be the best combination to employ as such arrangement could provide a more convenient procedure in further analysis. Conceptual Design (Initial Layout)Before the commencement of design analysis process, the initial layout of the product was illustrated (Figure 1) and the gearbox housing was represented by the grey rectangle. From the diagram, power was transmitted from the AC motor to gear 2 through the shaft and gear 2 was then connected to gear 3. Again, power was transmitted through the shaft from gear 3 to 4 and gear 4 was connected to gear 5. Finally, power was transmitted from gear 5 to the grinder through the shaft. 4. Design Analysis and EvaluationA) Gear AnalysisIn order to select the most suitable component, a range of analysis was performed and the limitation was identified so that the gearbox could be operated under safety condition. 1) First of all, the nominal velocity ratio U was determined:  FORMULA (Single reduction) However, a double reduction arrangement was employed, therefore:  FORMULA  2) As the material used in both the pinion and the gear were identical, the pinion was always weaker since more undercutting could be found from the smaller gear. In order to avoid any undercutting appear in the operation, a high pressure angle was selected (25°). To ensure no interference could be found within the gearbox, a minimum number of 12 teeth were required in the pinion (Appendix A). Hence the number of teeth could be determined for the gear:  FORMULA  If  FORMULA , then  FORMULA , hence  FORMULA (Within the required range) After that, the modulus m was assumed to be 3 and the pitch diameters were determined:  FORMULA  (Pinion)  FORMULA  (Gear) Since D P and D G were defined, the centre distance c could then be calculated:  FORMULA  3) Moreover, the pitch line speed was computed: Since  FORMULA   FORMULA  4) As power P was provided, the transmitted load F t could also be determined: 5) On the other hand, the dynamic factor K v was identified:  FORMULA  6) In order to avoid torsional distortion, the face width b was required to lie between 9m and 15m (when m=module in mm/tooth) and should be less than or equal to D P:  FORMULA  and FORMULA  if b was assumed to be 11m, FORMULA , in which  FORMULA  and FORMULA  7) Since the bending stress within the gear depended on the torque, geometry and the form factor, the expected bending stress σb in the pinion and gear could be determined:  FORMULA   FORMULA  The Lewis form factor Y could be identified (Appendix B). For the pinion, Y P=0.27677 For the gear, FORMULA  Hence,  FORMULA   FORMULA  Therefore the maximum bending stress occurred at the pinion with the value of 20.21MPa. However, in order to achieve a safety and reliable operation, the allowable bending stress S at could be calculated based on the maximum bending stress σb. 8)  FORMULA  where  S at=Allowable bending stress k a=Surface factor k b=Size factor k c=Reliability factor k d=Temperature factor k e=Modified factor for stress concentration factor k f=Miscellaneous-effects factorand  FORMULA ,1 would be taken if the corresponding factor was not concerned. Hence assumptions of factors were made for a meat slicer: k a=0.9, k b=0.8, k c=0.7, k d=0.9, k e=1, k f=0.9 As  FORMULA   FORMULA  Gray cast iron was selected as the ultimate tensile strength was approximately 160MPa, in which the bending stress could be covered sufficiently. Further analysis would be carried out in order to investigate the contact stress between gears. The contact stress (Modified Hertzian stress) σc on the surface of teeth could be identified as:  FORMULA  where FORMULA  and  FORMULA   FORMULA   FORMULA  Similarly, the allowable fatigue strength S ac was determined:  FORMULA  where  C T=Temperature factor (C T=1 for TTherefore FORMULA  Since the yield strength of gray cast iron was approximately 620MPa, the contact stress could be fully covered. The above manual calculation represented the detailed procedure of the gear analysis. A spreadsheet (Appendix D) was also created in order to simplify the calculation process and results of alternative trials could be summarized. According to the table, the fatigue strength and bending stress could be affected by variables such as life factor C L, reliability factors C R and k c, face width b and other miscellaneous factors. In general, bending stress was relatively small compared to the fatigue strength and the gear should firstly be failed by fatigue. The allowable bending stress and fatigue strength would decrease as the reliability (factor) decrease, which meant a gear with high reliability would require a material with high UTS and fatigue strength. Besides, the fatigue strength would increase as the life factor reduced. Therefore a gear with large life cycle (hence low cycle factor) could be manufactured using a high fatigue strength material. Moreover, both the bending stress and fatigue strength would become higher as the face width decreased, which represented a larger face width would have a lower bending stress and fatigue strength. On the other hand, alternative materials were compared in order to identify the optimal material to be employed in the gearbox. Since grey cast iron, case hardening carbon steel and aluminium bronze were mostly utilized, a number of trials based on those materials were performed. A larger flexibility could be achieved by using grey cast iron as the minimum module size for carbon steel and aluminium bronze were 4mm and 5mm respectively. In conclusion, an ideal gear would be manufactured by grey cast iron with 3mm module, with extremely high reliability and low life cycle factor. Since a larger face width would occupy a larger volume, low face width would be beneficial when size was significant. Although a high m or b would reduce the bending stress, both parameters were relatively small, which might due to volume was also one of the significant parameters to consider. Moreover, the UTS of the corresponding material might also be able to compensate the change of bending stress so that a small module and low face width could be selected. The ideal combination was attached at the bottom of the spreadsheet and detailed specifications were presented below (Table 1): Gear program was employed afterwards and the above parameters were inserted. Commercial cut gear class was selected and metric module definition was considered. A number of figures were attached in Appendix E in order to show the calculation results with relevant specifications using gear program. Since a double reduction process could not be performed in the program and both reduction stages were identical (with U=4.472), only the first reduction stage was analysis. However, the minimum number of pinion teeth N P could not be amended in the program and could only be 18, which might possibly due to either gear design failure or computational problems. A test was performed in order to lower the value of N P, however a gear ratio of 1 was required in order to reduce N P to 13 in the gear program. Although the face width calculation was successful, the module and the face width were different to the original specifications. Therefore the program could only be used as a supplementary element for gear analysis since a variety of parameters such as available materials and class of gears were limited. B) Shaft AnalysisThe design and analysis of shafts could be performed using bending moment diagrams and shear force diagrams. An initial design layout was drawn in order to perform further analysis in the shafts(Figure 2). In this design, three shafts would be employed in the gearbox in order to perform double reduction effect. Each shaft was assumed to be 100mm long. First of all, the power was transmitted from the motor to gear A, then to gear B on shaft 2. Since the contact force between gears was acting in two directions, the vertical and horizontal analysis of three shafts would be carried out sequentially. Initially, the arrangement of shaft 1(Figure 3) was shown and the analysis for horizontal and vertical axis of shaft 1 was also carried out. Therefore, on vertical plane X-Y (left): M(R 1V):  FORMULA   FORMULA   FORMULA   FORMULA   FORMULA  Similarly, on horizontal plane X-Z: M(R 1H):  FORMULA   FORMULA   FORMULA   FORMULA   FORMULA  The relevant shear force and bending moment diagram for both planes were shown. Therefore the maximum bending moment M B:  FORMULA  Since  FORMULA  where  n=Safety factor (assume n=2.5 in this design) S y=Yield Strength of grey cast iron=620MPa T=Maximum torque=  FORMULA  FORMULA  FORMULA  FORMULA Therefore the minimum shaft diameter should be about 12-20mm. The power was then transmitted from gear B to C via shaft 2, followed by gear D on shaft 3. The design arrangement of shaft 2 was shown(Figure 4). Again, analysis was carried out in order to investigate the vertical and horizontal axis of shaft 2. First of all, the corresponding axial components of F B were determined. FORMULA   FORMULA  Moreover, torque T at gear B was found:  FORMULA  As the torque was constant along shaft 2, the torque at gear C was equal to at gear B. FORMULA   FORMULA  The vertical (XY-plane) and horizontal (XZ-plane) analysis of shaft 2 were then performed. Since point 3 & 4 were the reaction force, they required to act towards the resultant force together. Therefore for vertical plane (XY): M(R 3V):  FORMULA   FORMULA   FORMULA  Hence:  FORMULA   FORMULA   FORMULA  Similarly, for horizontal plane(XZ): M(R 3H):  FORMULA   FORMULA   FORMULA  So:  FORMULA   FORMULA   FORMULA  Therefore the maximum bending moment M B:  FORMULA  Again:  FORMULA  where  n=Safety factor (n=2.5 in this design) S y=Yield Strength of grey cast iron=620MPa T=Maximum torque=  FORMULA  FORMULA  FORMULA  FORMULA Therefore the minimum diameter for shaft 2 should be around 20-25mm. Finally, the arrangement of shaft 3 was shown(Figure 5): Firstly, the value of F DY and F DZ were determined:  FORMULA   FORMULA  The vertical (XY-plane) and horizontal (XZ-plane) analysis was performed: Therefore, on vertical plane X-Y: M(R 5V):  FORMULA   FORMULA   FORMULA   FORMULA  Similarly, on horizontal plane X-Z: M(R 5H):  FORMULA   FORMULA   FORMULA   FORMULA  The relative shear force diagram and bending moment diagram for both planes were shown. Maximum bending moment M B:  FORMULA  Since  FORMULA  where  n=Safety factor (assume n=2.5 in this design) S y=Yield Strength of grey cast iron=620MPa T=Maximum torque=  FORMULA  FORMULA  FORMULA  FORMULA Therefore the minimum shaft diameter for shaft 3 should be between 22-30mm. A table was shown in order to summarize the specifications of three shafts: On the other hand, the face width of the larger gear was relatively small compared to the pitch diameter. Therefore a reasonable size of boss (20mm) would be placed with the large gear on shaft 2 (10mm each side) and shaft 3 in order to enhance the contact length between the shaft and gear. Since a rectangular housing would be used for the gearbox, the bearings would be positioned at both ends of the shaft and through the housing wall. C) Bearing AnalysisIn order to investigate the correct size of bearings placed at the end of shafts in the gearbox, a number of calculations were performed. Firstly, the resultant forces at all 6 bearings were determined: Bearing 1: FORMULA  Bearing 2: FORMULA  Bearing 3: FORMULA  Bearing 4: FORMULA  Bearing 5: FORMULA  Bearing 6: FORMULA  After all the resultant forces were determined, the type and size of bearings would be selected based on the maximum load on each shaft using Appendix F (i.e. the higher load would be chosen as the typical maximum load for each shaft). Bearing 1 & 2) 253N @ 1800rpm = 12mm ball rolling bearings Bearing 3 & 4) 1213N @ FORMULA  = 25mm ball rolling bearings Bearing 5 & 6) 1133N @ 90rpm = 12mm ball rolling bearings Since such corresponding points in the graph was close to the rolling bearings line, therefore they were used for the bearings at the end of the shaft. The purposed diameter of shaft 3 was 25mm, so the size of bearing 5 & 6 should be larger than 25mm. Besides, the bearing ratio BR should also be aware since the ratio should be greater than 1.5 and not less than 1 for any smooth operations. Assumed ratio=1.5, since FORMULA  Bearing 1, 2)  FORMULA  Bearing 3, 4)  FORMULA  Bearing 5, 6)  FORMULA  Finally, a summary of specifications for bearings and shafts was produced: Although the bearings were slight long compared to the available spaces on the shaft, the shaft would be extended in order to be surrounded by the bearings. Nevertheless, the shaft program was also an alternative way to simulate the outcome. However, errors occurred once all the data were entered each time (Appendix G), therefore a computational comparison could not be conducted. D) Specification & Discussion of Components and MaterialsSince roomage was also a particular factor to consider in design process, the smallest possible gear size was selected. Grey cast iron was chosen to be the production material of gear and shaft as the yield strength of grey cast iron was fairly high, so that a long lifespan could be expected. Besides, the module was also the smallest size within the range as any modules smaller than 3 would not be able to perform their duties effectively and endurably. On the other hand, bearings were selected based on the maximum load and the speed of the shaft. However, a prudent decision was made and a larger bearing was used for some shafts as the corresponding point on graph was too closed to the upper boundary. Die-cast aluminium would be used for the housing and lubricated ball bearings would be used for shaft supports. A final specification was constructed in order to summarize the parameters of each component. E) Main Design Layout Drawings & Report DiscussionThe final version of the gearbox was plotted in SolidWorks and a CAD drawing was attached (Appendix G). All the dimensions were shown in millimeters and an isometric view was shown below (Figure 5). In comparison of manual calculation with computer program, the computer program were not as convenient as expected, which might due to outdated operating system in the program or even bugs. Therefore manual calculation would be desired if the actual design parameters were given. However, the computer program will enhance the effectiveness and also provide a more convenient analytical procedure if further development was carried out Alternatively, the operation environment would also change the requirements and hence the specification. For instance the temperature factor would be increased if the operation temperature was higher than room temperature. Reliability factor would also increase if the precision of cutting is important. Developments could also be performed in order to improve the performance. Alternative materials could be used if the fatigue strength was not sufficient enough. Increasing the number of shaft sample would also improve the reliability of the analysis, which may also increase the variety of effective design. 5. ConclusionIn the design process, a number of analyses were carried out in order to achieve an optimal gearbox design for a commercial meat slicer. The manual calculations were carried out smoothly and a number of gear combinations were investigated. Unfortunately the computer program did not operate as expected so that both results could not be compared. However, the computer program only offered a limited number of variables which meant the flexibility was limited even such program run normally. Nevertheless, manual calculations enable a more flexible and reliable analysis during design process, hence the volume of the gear box could be minimized more easily. After a range of manual calculations, the gearbox could be operated as requested from 1800rpm to ~90rpm, using a double reduction gearbox with gear ratios of ~4.472. After that, the minimal gear diameters of 36mm and 161mm with 27mm thickness were selected for small and large gears respectively. Such values were then utilized for shaft analysis and hence bearing analysis near the final stage. Finally, two 25mm and one 12mm diameter shafts with 100mm length were chosen for gearbox shafts and 6 bearings (25mm and 12mm diameter with 37.5mm and 18mm thickness respectively) were selected to support the corresponding shafts. The gearbox design was then constructed in SolidWorks, CAD drawing was then produced in order to minimize the required gearbox housing. A size of FORMULA  was used for the housing and a thickness of 12mm was reserved as a safety basis.","SummaryIn this assignment, researches have been done on the types of engines that are usually used for racing. The four types of racing engines that was of interest were the Formula, Rally, Sports and Motorcycle racing engines. The specifications of these engines have been summarized in Appendix A and B so that one can compare their important parameters. Appendix A shows all the racing engines, of which their individual performance will be discuss in the content of this report. Appendix B is a table which accumulates all the information obtained about the four racing engine requirement and their overall comparisons are also included in the main text. In this report, we also accessed past and future developments of each of these racing engines. IntroductionIn 1894, the idea of car racing was raised once a series of petrol-fuelled cars were constructed. The first official race was held in Chicago, Illinois on 2nd November in 1895 and racing engines were improved exponentially during the 20 th century. Researches on different engines which usually used in racing were performed and discussed in this report. Engine used for general automotives was internal combustion (I.C.) engine in which the mechanical power of vehicle was produced by combustion of fuel within the combustion chamber. Internal combustion engine could have either two-strokes or four-strokes and either spark ignites or compression ignites. Gasoline, diesel and natural gas could be selected as the fuel in a SI engine. On the other hand, two major types of IC engines were identified: Rotary engine and Reciprocating engine. The major representative of rotary engines in automotive industry was the Wankel engine (Fig 2) which was the most highly developed rotary engine since 1970s; such engine was used due to the compactness and high power performance. However, the development of Wankel engine was suspended in most of the companies due to the arising environmental regulations as well as the effect of the oil crisis. The most recent car operated by Wankel engine was the Mazda RX-7 which produced in 1999. In automotive racing industry, only particular specifications were selected and employed since the efficiency could only be improved by such specifications. Different cylinder configurations such as single, in-line, v-type, w-type, u-type, opposed cylinder, opposed piston and radial could be found in IC engines. However, in-line and v-type were the most commonly used configurations in automotive racing engines. Further more, the common numbers of valves employed in each cylinder were 2 (1 intake, 1 exhaust), 4 (2 intake, 2 exhaust) and 5 (3 intake, 2 exhaust). Generally 4 valves were employed in racing engine. . Compare to normal engines, limits such as peak operating cylinder pressure were pushed up in some racing engines so that a higher performance could be obtained. Besides, the horsepower and fuel economy could be increased by maximizing the cylinder pressure. Although the cylinder pressure could be increased by increasing the compression ratio, alternative technique could also be used since cylinder pressures could be altered significantly by using camshaft selection, carburetion, nitrous and supercharging. Compression pressures could be adjusted drastically by installing supercharging, turbo-charging or intercooling system. However, apart from installing extra equipment to improve the effectiveness and efficiency of racing engines, the durability and lifetime were also a significant factor to consider since racing cars might require to operation for a long time without any failures Type of RacingsIn the report, a series of engines used in different automotive racings including formula racing, rally racing, sports racing (including endurance) and motorcycle racing were presented. In fact, the features required in racing engine were totally depended on types of racing and performances of different racing engine were adjusted in order to meet the specific purpose and provide a best fit performance. However, high acceleration, high maximum speed, high power, high torque, light weight and high efficiency of engine cycles were generally the ideal for racing engines. For instance, high power output was necessary for rally engine since such racing might consist of climbing uphill or operating under poor conditions. To conclude, particular engine performance could be adjusted in accordance with racing course conditions and requirements. Formula RacingFormula racing was a type of single-seater racing which involved a variety of special designed high performance cars. The wheels of formula cars were not covered and aerofoil wings could always be found at the front and rear of the car (Figure 3). Formula engines should have an extremely high maximum speed, extremely high acceleration, high torque, long life span, high operational temperature and extremely light weight in order to perform effectively on a special designed racing course. Rally RacingThe first World Rally Championship (WRC) was held in 1973 and highly modified cars were competed on normal roads as well as under poor conditions which included dirt, swamps (water resistance) and snowy surfaces (Figure 4). Rally engine should have an extremely high torque, extremely high horsepower, high acceleration, long life span and high operational temperature in order to maintain the car under safety condition as well as effectively operated. Sports RacingSports racing cars were modified cars and were normally operated on racing courses (Figure 5). Endurance racing is part of the sports racing where races were usually carried out over a long distance and vehicles were usually driven by a team of two to three drivers. As similar to formula engines, sports engines should have a reasonable maximum speed and torque during operation, extremely long operational time, a long life span and high operational temperature were also essential to be considered before the production process of racing engine. Motorcycle RacingFinally, motorcycle racing is a contest involving motorcycles competing with each other (Figure 6). Design of motorcycle engines varied largely in order to meet the requirements for different type of races. Normally motorcycle engine required to have light weight, long life span, high operational temperature, extremely high acceleration and maximum speed. Technical featuresAdditional features could be employed in different racing engines so that the performance could be improved further. Forced induction systems such as turbocharger and supercharger were explained as well as cooling systems and carburetor. 1. Turbocharger - An apparatus to boost the horsepower of engine without significantly increased its weight, operated by pressurizing the air flowing into the engine's cylinders in order to get more fuel to be burnt during each stroke. Advantage of compressing the air was more air could be squeezed into a cylinder, hence more fuel could be added and more power could be obtained from each explosion in each cylinder. More overall power could be produced than the same engine without the charging (Figure 7). Supercharger (Figure 8a) - The mechanism was nearly identical to the turbocharger, the major difference was the power supplies in order to run the air compressor. In supercharger, a belt was connected directly to the engine and power was transmitted via the belt. For turbocharger, power was obtained from exhaust stream since the exhaust run through a turbine which in turn the compressor was span. A graph was shown the variation of power with rpm with and without supercharger (Figure 8b). 3. Cooling systems - Either liquid-cooled or air-cooled technique was generally employed in racing engines. i) Liquid-cooling: Fluid flowed through pipes and ducts within the engine, heat was absorbed by the fluid and engine was cooled as the liquid passed through. The fluid was then passed through a heat exchanger or radiator and heat was transferred from the fluid to the air blowing through the exchanger. ii) Air-cooling: An old-fashioned cooling technique, engine block was covered in aluminum fins rather than circulating fluid through the engine so that heat could be conducted away from the cylinder. A powerful fan was used to force air over these fins and the engine was cooled by transferring the heat to air. 4. Carburetor - The right amount of gasoline with air could be mixed so that the engine could run properly. Designs of racing enginesIn an internal combustion engine with multiple (more than one) cylinders, the cylinders and piston could have V-, in-line, horizontal, w type, opposed piston and radial configuration. The most common configurations used in racing engines are the V- and in-line configurations. 1. V-configuration:V-configuration engines would have their cylinders and piston arranged in such a way that they form V shapes as shown in Figure 9 above. Engines using this configuration are named V2, V4, V6, V8, V10, V12, V16, V18, V20 and V24. The V configurations have been widely applied in racing engines as it helps to minimise the engine length and weight as compared to the in-line configuration. The cylinders can be arranged with different V-angles. The bigger the V-angle of an engine, the more stable it is as it has lower centre of gravity. However, large v-angle also gives rise to vibration problems. Therefore, despite increasing the V-angles would lowered the centre of gravity, it also increased the vibration. On the other hand, small V-angle engines will be less stable as the centres of gravities would be higher. Examples of engines with large V-angles are V2, V4, V6 and V10. 2. In-line configuration:In-line configuration is another configuration that is commonly used in racing engines. The cylinders in an in-line engine were arranged in one straight row parallel to each other. This can be applied for all multiple cylinders engine but is more commonly found in four- and six-cylinders engines. Its simple structure makes it easier in production than V-configuration engines. Due to its simple shapes it would be relatively easy for it to be fitted into the car chassis with different positions and can run rather smoothly. The disadvantage of in-line engines is that they are longer than V engines. It will be hard to install in-line engines into small cars and the cooling of engine by air can get difficult if it had been installed in inconvenient position Examples of in-line engines are straight-twin, straight-3, -4, -5, -6, -8, -10, -12 and -14. Straight-4 is the most commonly used in-line engine at the present. Sports EngineSports racings consisted of a variety of competitions between modified cars and were normally taken place on a specific racing course. Due to the enormous area sports racings have covered, only endurance racing was mentioned in this report. 1) Ford GT 2005 (Ford's MOD)The first Ford GT was built in 1963 and was became the world's best endurance racing car in mid-1960s. Ford GT achieved a result with flying colors as such vehicles placed 1-2-3 at the 24 Hours of Le Mans in 1966 as well as won the next three consecutive years. The Ford GT was reconstructed in recent years in order to celebrate the centennial of Ford Motor Company. A mid-mounted, all-aluminium 5.4L DOHC supercharged V-8 engine that produces 550 horsepower was used in this generation. Compression ratio was 8.4:1(in/mm) and Sequential multi-port Electronic Fuel Injection (SEFI) was employed in the engine with dual injectors per cylinder. The MOD V-8 featured aluminium four-valve heads, forged crankshaft, H-beam forged rods and aluminium pistons fed by an Eaton screw-type supercharger, all combined and produced more than 500 hp horsepower and 500 ft-lb of torque. In 1966 a new prototype design (GT40 MK2) was introduced with a 427 c.i. (7000cc) engine, produced a 485 BHP power at 6,200 RPM and 475 Ib-ft of torque at 4,000 RPM. In the same time as GT40 MK1 was being developed, a road used version (GT40 MK3) was designed and a 289 ci. (4700cc) engine was used which could produced 306 BHP power. Between 1966 and 1967, a new GT40 was introduced (the J-Car). However, the final product was not very satisfied as such vehicles were mainly designed from Ford's styling department. During 1968/69 seasons, the GT40 Mk1 was redeveloped and a stronger engine was employed. 400 bhp of power at 6,500 rpm and 385 lb-ft of torque was finally achieved in 1968. In 1969 the power was raised further to 425 bhp at slightly lower revolution of 6,250 rpm and 396 lb-ft of torque at 4,750 rpm, using a 302 ci. engine and other improvements. The Ford GT40 was the origin of Ford GT and was firstly produced in 1964; the engine of first prototype car (GT40 MK1) used a 4200cc Ford V8 engine, with aluminium block and heads. The engine was dry sumped with IDA Webber carburetor atop. The performance was 350 BHP of power at 7,000 rpm and 275 lb-ft of torque at 5,600 rpm. In current design, aluminium was used in most of the parts of engine in current Ford GT due to the high strength and excellent heat transfer, so that the main objectives of endurance racing (long life span, long operational time and safe operation) could be fulfilled. However, some components were manufactured by forgings instead of castings; potential hazard might exist by using such an unseasoned manufacturing process as casting was the conventional manufacturing method of automotive components. Sports Engine - Overall DevelopmentThe performance was improved by adding new components such as supercharger. The 32-valve, 330 cubic-inch, 90-degree all-aluminum V8 engine was placed behind the driver in order to lower the centre of gravity of the vehicle. Performances of 373kW of power @ 6000rpm and 678Nm of torque @ 4500rpm were generated by the engine. Forged components (such as crankshaft, connecting rods and aluminum pistons) were also utilized in order to deal with extra pressure of forced induction. Aluminium components were become familiar due to the light weight and extremely high levels of torsional rigidity. Forgings technique would also become common after the correspondence processes were further improved. Rally EngineRally engines were required to operate effectively even under poor conditions such as in swamps (water resistance), dirt and on snowy surfaces. The engine also needed to generate sufficient power in order to drive the rally car uphill. Two rally engines were shown below: 1) Subaru Impreza WRC 2004 (EJ20)The first Impreza was found in October 1992 and horizontally opposed, four-cylinder, all-alloy EJ20 engine for a displacement of 1.994 L with quad camshafts, 8.0:1 static compression, a single turbocharger and intercooler was employed in this model. The boxer engine was in accordance with a specially reinforced ""semi-closed deck"" engine block, forged aluminium alloy pistons and forged high-carbon steel which linked to rods and exhaust valves (sodium-filled). The theory behind using a 'flat four' was the engine sat lower in the car in which the car's inertia could be benefited and hence a lower centre of gravity could be obtained. In 2003, the output of this turbo charged engine was increased to 165 kW (225 PS), dampers with multiple phase valves were also used in order to obtain high performance and steering stability. Besides, the cooling performance was also enhanced by using a different shape of the inter-cooler water spray nozzle and the shape of the air baffle plate within the air scoop. Nevertheless, developments were still carried out during 2004 in order to remain effective and competitive in the WRC. A wide range of performance upgrades were conducted including using a direct water injection system and increasing the size of IHI turbocharger housing so that stable engine combustion could be obtained. Aluminium block and heads were employed in order to be operated in high temperature effectively. Area of air intake was increased and hence the cooling system including V-mount radiator could be improved. On the other hand, AVCS (Active Valve Control System) was also introduced and the inlet camshaft timing over a 35-degree range was varied in order to improve emissions, tractability and low rpm torque. The EJ20 boxer engine was also modified with a twin scroll turbo, drivability was greatly enhanced by the increased torque at low speeds, while the improved acceleration was particularly significant in rally race. 2) Mitsubishi Lancer Evolution WRC 2004 (4G63)Lancer Evolution was firstly appeared in 1973 and was equipped with the 4G63 engine. The 4G63 engine consisted of an 85.0mm (3.35"") bore and 88.0mm (3.46"") stroke, for a displacement of 1997 cc. Twin contra-rotating balance shafts were housed inside the block and Multi injection technique was employed. The engine was mainly consisted of cast iron block and an aluminium alloy cylinder head. A new turbocharger fitted with the WRC regulation 34-millimetre intake restrictor, new intake and exhaust manifolds, and new internals were used in the engine in order to obtain a higher efficiency. In 2003, refinements were performed on intake and exhaust systems, aluminium induction piping, and weight reduction measures by using a cast-magnesium cylinder head cover and hollow camshafts. The intake, exhaust, turbocharger impeller and housing, camshafts and crankshaft were all developed in 2004, in accordance with the technology already achieved in different Lancer Evolution over the past 10 years. Camshafts were design to hollow so that the rotating inertia was reduced. Moreover, the ""twin-scroll"" impeller in turbocharger was modified by doubling the amount of fins. Dual chambers were used in exhaust housing, the exhaust gases were routed from cylinder 1 and cylinder 4 to one side of the twin-scroll turbocharger and gases from cylinder 2 and cylinder 3 to the other side of turbocharger. Efficiency would be increased by improving the airflow intake. However, the WRC regulation was conflicted by this development since only a maximum of 34-millimetre intake restrictor could be used. Although downforce was a vital factor of competitive car on rally racing, good cooling was a key prerequisite on a turbocharged rally car since the ambient temperature on rally car could usually exceed 30 degrees Celsius and the temperature was often the highest on racings which carried out at the lowest average speeds. In order to achieve a high efficiency and effectiveness in rally engines, airflow beneath the rally car could be used and considered. In 2005, improvements to the rally engines would be carried out by employing a new waste gate, anti-lag valves as well as an improved engine control. On the other hand, the turbo-charger would also be investigated in order to determine the change in efficiency. The performance and tuning possibilities of such engines would also be significantly developed and become more accurate by carrying out different improvements. Rally Engine - Overall DevelopmentOver the past few years, the development of rally engines was improved from only concentrated on the modification and replacement of engine components (such as dampers, turbocharger housing, intake and exhaust systems, piping, head cover, hollow camshafts, twin scroll turbo...etc) to introducing new components and technique (such as airflow beneath the rally car, new waste gate) in order to improve further and achieve a better performance. On the other hand, exact values of some performance such as accelerations and maximum speeds were not defined since those figures could be adjusted easily by tuning. Formula Racing EngineFormula racings involve a variety of high speed and high performance cars competing against each other. At present, the most famous Formula race is Formula-1 (F1) racing. In this assignment, most research was done on Formula-1 engines. Formula-1 car engines have excellent performance as they are about ten times more powerful than normal car engines. To ensure all races are reasonably fair, FIA (Fédération Internationale de l'Automobile) have put forward some limitations to the engines of the cars that were competing in the races. For instance, the engine must be four strokes and consist of 10 circular cylinders with less than 5 valves on each cylinder. Supercharging is not allowed in Formula-1 cars. At present, the 10 cylinders of the engines are usually arranged in V configuration. They often have capacities of about 3000 cc capacity and can generate more than 800 bhp. High torque of the engines means high power output (i.e. high horsepower) as the number of engine cycles per unit time is depend on the torque. Special materials (such as Aluminium alloys, ceramic etc.) were used to manufacture the engine's components for Formula-1 for weight reduction purpose and also to reduce the chances of overheating of the engines. The cooling of the engines was very important in Formula racing cars especially in Endurance racing since the car was required to run for a long period of time (typically 6 hours, 12 hours and 24 hours) over the runway course in order to test the engine's durability. Formula-1 car engines are all air cooled. Less dense ceramic was used in making the internal components of the engines so that it was easier to accelerate and also reduce the engine's fuel consumption. The overall size of the engines should be reasonable light and small, so that they could be easily fitted into the chassis. The numerical data for the engines' torques and compression ratios (CR) are not available in all of the sources. This may because these data have to remain confidential. Following are some examples Formula-1 racing engines: Ferrari :Figures 5a and 5b above are Ferrari F2003-GA single-seater racing car and Type 052 engine respectively. Ferrari F2003-GA was Ferrari's 49 th single-seater racing car entering the FIA Formula-1 world championship. Similar to the previous generations of engines, the 3000 Ferrari (type 052) engine is load-bearing and is fitted longitudinally into the chassis. It has a V configuration and 10 cylinders arrangement with 4 valves per cylinder, i.e. 40 valves. It is a Spark Ignition (SI) engine with Magneti Marelli static electronic ignition. Fuel will be fed into the engine by Magneti Marelli digital electronic injection. Type 052 is an evolution of the former 051 engine with several improvements that increase the engine's performance and usability. Size and weight of the engine were reduced as new materials have been used for manufacturing it. The centre of gravity of the car is lowered and therefore improving the overall weight distribution. Maximum engine revolutions of type 052 engine would be about 200 rpm higher than that of the type 051. Further development of the engine would be carried on in order to improve the horsepower and its performance. The types of engines that Ferrari would develop in the future will tend to be more reliable and can integrate well with the car. Renault:Renault R24 as shown in Figure 6a was a single-seater built by Renault to compete in the Formula-1 racing in year 2004. The engine used to drive R24 was the Renault RS24 engine, which was an improved evolution of the RS23 engines that had been used in the Renault R23 (2003). Renault had been developing R24 since September in order to improve the overall engine performance. The RS23 engine launched in 2003 was a 111-degree V10 engine. It was generally reliable except that it has major problems with vibrations. Therefore RS24 had been developed from RS23 by changing some configurations and its design is based on the Supertec engines. RS24 has a 72-degree V configuration with 10 cylinders arrangements. As RS24 engine is higher, it has a centre of gravity that is 20mm higher than that of RS23. But according to the Renault F1 team, this problem has been compensated by making changes to the engine's casing. Therefore, the centre of gravity of the car R24 is not much higher than that of R23. The problems of vibrations have been improved in the development of the RS24 engine. In February 2005, Renault launched a new RS25 engine that has been developed from RS24. RS25 is basically the same as RS24, but its centre of gravity has been made lower. After the 72-degree V10 engines (RS24 and RS25), Renault was planning to develop a new 90-degree engine. The future developments for Renault engines will be more on weight reduction as well as improving its reliability. Formula Racing Engines - Overall Development:In 1950s, carburetor was widely used to feed fuel into the car engines. Shortly after, more effective direct fuel injection system was introduced by Mercedes. Renault then introduced turbocharged engines into F1. But the cooling systems of engines was not well developed, many turbocharged cars went down due to overheating. During the 80s, the developments of F1 engines were mainly on increasing their torques and the horsepower. But these increase in horsepower reduced the handling of the cars. The ground-effect of the cars were improved which increase their cornering speeds. This made it even harder to control the cars. During that period of time, many deaths of F1 driver were caused by severe accidents during races. In 1989, turbocharged engines were banned by F1 and were replaced by 3500 cc induction engines. In early 1990s, different companies were concentrating on the development of the 3500cc induction engines. The first 700 hp 3500cc induction engine was built which was much better than the previous racing engines. A 'baffle' had been introduced to enhance the cooling of the engines. F1 limited the capacity of the engines to 3000 cc in mid-90s. With the capacities limited, the engines could only be developed further by improving the mass centralization and increasing the horsepower. Horsepower can be increase by increasing the piston's bore and decreasing the size of cylinders as these will improve the volumetric efficiency of the engine. Finding the optimum cylinders configurations can improve the mass centralization (stability) of the car as well as the overall performance. Motorcycle Racing Engine:Motorcycle racing is a variety of sports involving motorcycles competing with each other. Some common examples of motorcycle racings are Isle of Man TT (road racing), MotoGP (circuit racing), Daytona (endurance racing) etc. Engines of motorcycles are gasoline engines and can be either two strokes or four strokes. They either use fuel injection or carburetors to feed the fuel-air mixture into the combustion chamber. Motorcycles used for racing are mostly four-stroke engines with multiple cylinders. The power of an engine depends on the number and sizes of its cylinders. Therefore, increasing the sizes and number of cylinders in an engine would increase its power, i.e. increase the engine's displacement. Motorcycles engines at present can have single-cylinder, two-cylinder, four-cylinder or six-cylinder and their displacements ranges from 250 cc to over 1500 cc. Most motorcycles nowadays often have two or four-cylinder engines. Engines with high displacement have higher fuel consumption and therefore generate more power. A multiple cylinder engine can have its cylinders arranged in horizontal, inline or V configurations. As mentioned beforehand, the main problems with V configuration engines were its vibrations and high centre of mass. Among the three configurations, the horizontal configuration has the best weight distribution (lower centre of gravity) and also produced the least vibrations. The following is an example of a racing motorcycle: Honda CBR1000RR:Figure 8a shows a picture of the Honda CBR1000RR motorcycle and Figures 8b and 8c show the 998cc liquid-cooled engine that drives the motorcycle. The CBR1000RR engine has a displacement of 998cc and with in-line cylinders configuration. The piston's bore and stroke sizes were made different from that of its predecessors so as to improve its compression ratio, i.e. higher volumetric efficiency. Its fuel tank is placed at the centre and fuel was fed into the engine through a dual stage fuel injection (DSFI) carburetor. Honda CBR1000RR was developed based on the previous RC211V with improved horsepower. Therefore, CBR1000RR is as reliable as RC211V but with higher maximum speed. CBR1000RR bike inherited most of RC211V's body structures such as the Aluminium frame, the suspensions etc. As the swing arm of CBR1000RR bike had been made longer, the size and structure of the engine had to be modified in order for it to fit into the bike chassis. The front to back dimensions of the engine was being shortened by arranging its main shaft, countershaft and the crankshaft in a triangular pattern with countershaft below the main shaft. The engine had been placed towards the front of the bike body to improve the overall weight distribution of the bike, i.e. more stable. In the past, Honda development group has been working on the weight centralization aspects, i.e. to improve the stability of the bike. Based on the researches that had been done, the future evolutions of the Honda racer engines would be more on increasing the engine's horsepower and torque. Motorcycle Racing Engine - Overall Development:The first ever engine used to power a motorcycle was a steam engine which was built in 1867 in the US. It was a two-cylinder engine that powered the bike by burning charcoal. In the 1880s, the first gasoline engine for motorcycle was then built which was a single-cylinder gasoline engine (reciprocating). At this early stage, the fuel feeding system of this simple engine was a spray-type carburetor. Later in 1894, a water-cooled parallel two-cylinder engine was developed. The cylinders were moving to and fro simultaneously to create motion to the rear axle by transmitting through connecting rods. An elastic band was placed on each side of the cylinders for energy storing purpose and to enhance compressions in the cylinders. A year later, a smaller four strokes engine with capacity of about 138 cc was built. It generates a power of 0.5 hp which was very small compare with the engines output power nowadays. Shortly afterwards, the AV-Twin with speed gearboxes was built. The first motorcycle endurance race was held in 1907 on the Isle of Man. In 1916, a two-cylinder engine with four valves per cylinder was developed and was used in various races as its maximum speed was over 120 mph. the development for the motorcycle racing engines had come to a halt during the two World Wars. But not long after World War 2, 125 cc bikes were built based on the previous models. From then, the development of the motorcycle engines was more on increasing the horsepower and the stability of the bike. Horsepower can be improved by changing the dimensions and configurations of the cylinders and pistons. While the stability is depend on the weight distributions of the bike. Performance Analysis and DiscussionThe performances for each racing engine were plotted in Appendix A and the comparisons of performance for different types of racing engines were plotted in Appendix B. Firstly, all the displacements of formula and rally racing were very closed to 3000cc and 2000cc respectively, however the maximum possible values for those racing vehicle should be larger since those performances were achieved under some racing regulations. On the other hand, the torque and compression ratio of formula racing engines as well as the compression ratio of rally racing engines were missing on the table due to those performance might be confidential to public. In formula racing V10 engine were normally used since the racing engine could only consist 10 cylinders in accordance with formula racing regulation. Most of the rally engines were employing inline configuration except for Subaru Impreza since a more efficient flat configuration was used. Further more, the combined table in Appendix represented the requirement of different racing engines. Ten cylinders were used in formula racing engine since the vehicle required an extremely high speed. Besides, the displacement of sports racing engine was enormous compared to the others, due to the corresponding vehicle (Ford GT) was an endurance racing vehicle, hence an extremely high value of displacement was reasonable. Conversely, the total weight of motorcycle was pretty light, therefore the fuel consumption and displacement required were not necessary to be high. The power of formula engine was the highest as the formula car needed to achieve a highest possible acceleration. On the other hand, the motorcycle power was the lowest as the weight of the vehicle was small; hence only low force and power were needed in order to drive the vehicle. Although the torque of formula engine was not available, the expected torque of formula engine should be greater than the Ford GT (500lb-ft). ConclusionFirstly, Wankel engines obtained a high power performance and were familiar in automotive industry during 1990s. However, the development was suspended due to the arising environmental regulations and the effect of the oil crisis although the future development of this type of engine was enormous. In conclusion, the performances of different racing engines were totally depended on the type of race. For instance endurance vehicle would require a high displacement whereas a motorcycle would not since endurance race needed a larger capacity of fuel. Besides, the performance could be improved by inserting extra components such as turbocharger and cooler, by modifying/replacing the current components or by either employing specific valve configuration in order to lower the centre of gravity of the vehicle. Light weight and high strength material such as aluminium alloy could also be used so that a lighter but stiffer product could be achieved. The future development of these racing engines would be tends to increase the horsepower by developing new systems to assist the current operations. Improvements of racing engine could also be achieved by developing individual components such as piston, crankshaft, camshaft, head cover...etc.",False
86,"The luxury debates have been closely entwined with the rise of modernity, and while 'luxury' has ceased to be a central issue, in the supposed 'post-modern' world, I would argue that the issues that were pertinent to the writers of the eighteenth and late-nineteenth century are still with us today; and our knowledge of this past debate can only serve to sharpen our acumen, and make us more aware of the role of 'conspicuous consumption' in our society in shaping the discourse about women; the self; taste and civility; and progress (economic and social) which was closely related to the notion of the well-being of the nation. It is my intention to examine the implications of the eighteenth and late nineteenth century luxury debates by employing these categories as a means of injecting a bit of order (albeit artificial) into the vast amount of literature written on this subject. Despite the tendencies of our relativist age to avoid crouching the debate in the language of morality, it is important to note that morality remains very much an integral part of the luxury debates, despite the shift from the vice/virtue dichotomy to one of dependence/freedom. Whatever their stands on luxury, the writers who participated in the luxury debates were keenly aware that 'luxury' was not a self-contained concept, but was rather a useful tool in understanding and critiquing the wider notion of the nature and progress of society. Coined by Thorstein Veblen in The Theory of the Leisure Class, (New York, 2001) However we may choose to judge women's association with luxury, the two remain closely yoked, and it seems almost impossible to tear them asunder. It is significant that the participants of the luxury debates have chosen to work with, instead of challenge, the assumption of a 'Female Luxury'. Even Mandeville, who disputed the accepted universality of Christian morality, and who was perhaps most favourable to the position of women (as major contributors to the economy due to their demand for luxury) chose to relate an anecdote featuring a male Mercer and a young lady customer, undoubtedly as an excerpt to illustrate the general; also, in his argument that luxury (or fashion) had done more to promote the development of nations than the Reformation, he picked ""the silly and capricious invention of hoop'd and quilted petticoats"" as a means of making a stark contrast. That ""luxury was identified with effeminacy and weakness"" is perhaps best seen in Rousseau's first discourse where he argues that the result of the pursuit of the arts and ""...the study of the sciences is much more apt to soften and effeminate men's courage than to strengthen and animate it"" Elsewhere in Emile, Emile's education teaches him to be an honest man whereas Sophie is taught deception and arts of manipulation and coquetry by her mother. F.B.Kaye (ed. ), Mandeville: The Fable of the Bees Vol I, (Indianapolis, 1988), p. 356 Ibid, p. 330 Ibid, p. 349-353 Ibid, p. 356. Maxine Berg and Elizabeth Eger, ""The Rise and Fall of the Luxury Debates"", in Maxine Berg and Elizabeth Eger (ed. ), Luxury in the eighteenth century: debates, desires and delectable goods, (Hampshire, 2003), p. 18 Victor Gourevitch (ed. ), Rousseau: The Discourses and other early political writings, (Cambridge, 2003), p. 21 This link between women, artifice, enticement and luxury might account in part for the 'Female Luxury', which Sombart, at the turn of the twentieth century harps on. Sombart attributed an important role to women of leisure in the development of luxury, and commented: ""luxury was fostered by illustrious mistresses and promoted for their sake"". In his central theory, which states that: ""all personal luxury springs from purely sensuous pleasure"", the identification of female as mistresses (and therefore sex objects) establishes the link between the rapacious sexual appetite of these women and their equally insatiable demand for luxuries, which, interestingly enough seems to conjure up the image of a particular 'Lady Allurea Luxury' (which as an 18 th century creation was the product of and engendered prevailing prejudices). The notion of women as 'objects' or adjuncts is also suggested in Veblen's illustration of the corset. While it ""lowers the subjects' vitality, rendering her permanently unfit for work...the loss suffered on that score is offset by the gain in reputability which comes of her visibly increased expensiveness and infirmity."" Though these women may gain some 'reputability' for being women of leisure, it is apparent that the men on whom these women are dependent reap the bulk of this 'reputability'. Hence, while women seem to consume more than men, it is the case because women have a duty, as adjuncts, not only to consume on behalf of the household, but also to display 'conspicuous consumption' in a way that would serve to elevate the status of their husbands/master. Werner Sombart, Luxury and Capitalism, (Michigan, 1967), p. 78 Ibid, p. 60 Veblen, The Theory of the Leisure Class, p. 126. It is perhaps appropriate to involve 'the Self' in the implications of the luxury debates of the eighteenth and nineteenth century, as the debates of the eighteenth century inaugurated the growth of the 'cult of the individual' that is still apparent in the present Western society. Luxury became a means by which an image of the 'Self' could be cultivated, which on one hand would allow others to view the 'Self' in a certain way; and on the other hand, permitted the 'Self' to nurse fantasies, which were stimulated by the acquisition of luxury objects. Hundert, through his study of Mandeville, highlighted the importance of ""the difference between being and appearing...for commercial moderns"", and how city people ""wear clothes above (their) rank...and consequently have the pleasure of being esteemed by a vast majority, not as what they are, but what they appear to be"". Veblen picks up on this point, and comments on the role that conspicuous consumption has in forging an image for the 'Self', especially in the midst of strangers. In succumbing to an appearance of public 'decency', we demonstrate how much we value others' good opinions of ourselves, and ironically, how this opinion shapes our understanding of the private 'Self'. The dictum ""a cheap coat makes a cheap man"" clearly illustrates the importance of conspicuous consumption in validating one's self worth. No doubt the freedom-loving Rousseau lamented this dependence not only on foreign luxuries, but more significantly, the dependence of oneself on the opinions of others. It is thought provoking, even for the modern readers that he concludes with: ""what good is it to seek our happiness in someone else's opinion if we can find it within ourselves."" Edward Hundert, ""Mandeville, Rousseau and the Political Economy of Fantasy"", in Maxine Berg and Elizabeth Eger (ed. ), Luxury in the eighteenth century: debates, desires and delectable goods, (Hampshire, 2003) p. 29 Veblen, The Theory of the Leisure Class, p. 65 Ibid, p. 114 Gourevitch (ed. ), Rousseau, p.27 The shifting perception of the 'Self' was part of a change in wider society that the changing definitions of luxury reflected. The Industrial Revolution, the increased exploration and exploitation of the colonies through trade led to the creation of an industrial, commercial and professional middle class, which sought to elevate its social standing as its pecuniary resources were increased. The old dichotomy between 'luxury' and 'necessity' became antiquated and was replaced by categories that were more indicative of the social climate-new luxuries (associated with commerce, utility, taste and comfort) were adopted in place of the excesses of old aristocratic luxury. As Dena Goodman points out ""by associating it (luxury) with commerce, the superfluous could now be embraced as a civilizing force in modern society without carrying the taint of barbarous luxury."" It was in this climate of changing values and meanings that 'taste' became the chief indicator of value, and a mechanism by which the surplus could be employed for the benefit of refinement and civility. The inclusiveness of taste as opposed to the exclusiveness of luxury driven by ostentation allowed for a realm of self-expression through fashion, which ""became a function of knowledge rather than wealth, and gave power to those who carried the authority of taste."" However, Veblen would have us believe that taste is still a function of wealth and that ""by habitually identifying beauty with reputability, it comes about that a beautiful article which is not expensive is accounted not beautiful."" And if it is as Veblen claims, that ""the canons of taste have been coloured by the canons of pecuniary reputability"", we once again sink back into conspicuous waste and excesses of 'old luxury', and taste becomes once again the prerogative of the rich. Berg and Eger, ""The Rise and Fall of he Luxury Debates"", p. 9 Dena Goodman, ""Furnishing Discourses: Readings of a Writing Desk in Eighteenth-Century France"", in Maxine Berg and Elizabeth Eger (ed. ), Luxury in the eighteenth century: debates, desires and delectable goods, (Hampshire, 2003) p. 76 Ibid, p. 77 Veblen, The Theory of the Leisure Class, p. 97 The implications of luxury did not merely pertain to the self and society, but were of national importance, as it affected the material and nonmaterial well being of the nation. The suspicions generated about 'luxury' are perhaps in part the product of its association with 'foreignness', and the mercantilist fear that the demand for luxury imports will drain the metropolis of its gold and silver, leading to its economic downfall. The advocates of the Scottish Enlightenment, David Hume and Adam Smith were keen to challenge this antediluvian attitude and promote luxury as a ""progressive social force"" through which free trade in goods would also allow for a transfer of technologies from other advanced civilization to Britain, through the process of emulation and 'imitative industries'. As Hume argues persuasively ""...in most nations foreign trade has preceded a refinement in home manufactures, and given birth to domestic luxury...and their delicacy and industry being once awakened, carry them on to further improvements in every branch of domestic as well as foreign trade."" A classic example would be the stimulus that the Wedgwood pottery industry received from the East-West trade in porcelain, which created the market for fine dinner services that Wedgwood was able to capitalize on in the mid eighteenth century. It is often the argument employed by the advocates of luxury (notably Mandeville) that the demand for luxury would ""lead to the expansion of commerce and the wider employment of the poor"" As Sombart elaborates in his theory that luxury gave birth to Capitalism, luxury was an indirect tool for social redistribution, through the rich spending themselves into debt in pursuit of frivolities and the subsequent employment of the poor to produce luxury items for consumption. Berg and Eger, ""The Rise and Fall of he Luxury Debates"", p. 9 Ibid, p. 11 Ibid, p. 12 Ibid, p. 10 Sombart, Luxury and Capitalism, p. 171 Ibid, p. 114 This economic argument by no means displaced the element of morality in the luxury debates. Though Mandeville accepted traditional associations of luxury with vice, his central thesis ""that Private Vices by the dextrous management of a skilful Politician may be turned into Publick Benefits"" is indeed refreshing, and justified the state's act of harnessing the demand for luxury as an engine for economic growth. Diametrically opposed to this line of argument, Rousseau instead espouses the view that civilization corrupts because civic virtue is compromised as a result of the development of the arts and sciences. Rousseau sarcastically commented: ""the ancient politicians forever spoke of morals and virtue; ours speak only of commerce and money."" Morality was an intrinsic part of national well being, and the attack on luxury and its association with moral corruption was often raised in the light of national wars. In particular, ""French critics associated luxury with France's humiliation by England during the Seven Years' War""; in England, the war ""also provided a platform for a denunciation of French luxury..."" and ""journalism during the war years rehabilitated classical condemnations of luxury in defence of Britishness."" Gourevitch (ed. ), Rousseau, p.18 Berg and Eger, ""The Rise and Fall of he Luxury Debates"", p. 14 It should be obvious from the above that ""luxury was not so much a key term in a debate as it was the marker for certain anxiety-'a convenient code for all of society's perceived problems.'"" Through participation in this debate, one was commenting not merely on luxury, but more importantly, was discussing the implications of luxury for a host of issues, revolving around Women, the Self, sociability and taste, and most importantly national well being. The luxury debates reinforced notions of 'Female luxury' and the close associations of women with consumption, without a deeper understanding of how men have helped shaped the ways in which women consume. It also served to highlight the modern tendency to use goods as the ""primary vehicle for the construction of the self that has the opinion of others as an indelible part of its contents."" The changing definitions of luxury and its subsequent link with taste also illustrate the changing expressions of sociability, which reflected the change in the pecuniary structure of society. Despite this and the argument that luxury facilitates social progress, there is a realization that while the luxury debates are becoming more inclusive, the gap between the haves and the have-nots has not narrowed, but have instead widened during Veblen's and Sombart's time. This has obvious bearing on the well being of a nation, and the social distribution of income is still very much on the present agenda. Lastly, despite the modern preference for economic arguments, the link between morality and luxury has been paradoxically strengthened, and remains as a bastion from which the critics of luxury draw their ammunition. The luxury debates therefore became an avenue where the problems of society were raised, and correspondingly, where the shape of an ideal modern society could be discussed. Goodman, ""Furnishing Discourses: Readings of a Writing Desk in Eighteenth-Century France"", p. 73 Hundert, ""Mandeville, Rousseau and the Political Economy of Fantasy"", p. 35","Hitler's popularity amongst the majority of the German people have been variously analysed by historians, and though the percentages are debatable, it is generally recognized that his popularity was of fundamental importance to the functioning of the third Reich. As Ian Kershaw suggested 'The adulation of Hitler by millions of Germans who might otherwise have been only marginally committed to Nazism meant that the person of the Führer, as the focal point of basic consensus, formed a crucial integratory force in the Nazi system of rule."" Despite the concession that Hitler, or rather the 'Hitler myth' played an important role in the third Reich, it does not follow that the influence was necessarily advantageous for Nazi Germany as a dictatorship. The nature of this question is quite deceptive, and the search for a direct answer can be elusive, due to the fact that the line between strength and weakness may be obscured. Hence, what may initially appear to have strengthened the Third Reich may in the long term prove to be the ultimate de-stabilising factor, threatening the very foundations that the Reich was built upon. To reach a conclusion regarding this question, it is imperative to examine the effects of Hitler's 'charismatic rule' on the following: the German people, the issue of legitimacy, the state bureaucracy, the party, and the resistance movement. Ian Kershaw, The 'Hitler Myth', (Oxford, 1989), p.1. From the start, the German people were never adverse towards the concept of dictatorship. Their distrust and contempt for the system of liberal democracy that Weimar represented was pervasive throughout society. There was a 'national' yearning for a Caesar figure to pull Germany out of the economic doldrums, and to lead Germany to reclaiming her former glory. Hence, Hitler's 'charismatic rule' was seen as a welcome and much needed respite to the inefficient Weimar Republic, and did in fact appeal to the romantic within each person- the harking back to the Second Reich of Bismarck. This acceptance of the rule of a 'strong man' made possible the tolerance of Party dictatorship of the Nazis, and the subsequent new state bureaucracy. Also, Hitler's style of leadership, the propaganda machinery of floodlights and military processions created the façade of participation, that made the regime seem like an 'enlightened dictatorship', thus reconciling the people with the loss of their political vote. Judging from the plebiscites held in 1933, 34, 36 and 38, '...though no doubt the product of intense propaganda and coercion, nevertheless reflected genuine widespread approval and admiration for Hitler's accomplishments, and persuaded the masses to fall in line."" The extension of the personal loyalty of the population from Hitler to the state would strengthen the regime, and lessen the impact of the transition from a liberal democracy to a dictatorship. ibid. p. 258. It is imperative to examine Hitler's 'charismatic rule' as a claim to the legitimacy of the Third Reich. Max Weber in 'Economy and Society' expounds on the ideal type of 'charismatic authority' by which the leader is seen as an exemplary character of superhuman powers, and derives his backing from the personal loyalty of his subjects. This description would seem to fit Hitler's style of leadership in the Third Reich. To a large extent, it gave legitimacy, which was lacking during the Weimar republic, to the Third Reich, and allowed it to function with the support of the people. This would appear to have strengthened the Third Reich as a dictatorship because it portrayed Hitler not as a usurper of power, but merely fulfilling the will of the people (reliance on personal loyalty). However, 'if those to whom he feels sent do not recognize him, his claim collapses; if they recognize it, he is their master as long as he proves himself."" The conditional status of his power can be de-stabilising in that Hitler is under constant pressure to bring in success. Failure, or a series of failure (Operation 'Barbarossa' against Russia) would mean that the mandate was passing out of the hands of Hitler. Routinisation (the lapse back into normality) was also a threat to a regime that innately relied on dynamism to rule. Hence, the 'charismatic' form of leadership was 'an extraordinary, unstable and therefore transient form of rule, tending to arise in unusual or crisis conditions, and directed not at the solutions of everyday problems of government, but at the over-coming of supra-dimensional crises and emergencies."" With the above question in mind, the inherent instability of the regime is definitely a point to consider, when evaluating the impact of 'charismatic authority' on the dictatorship. With the prospect of normalcy approaching as Germany emerged from the Great Depression, the Fuhrer had to act in his own interest: the entry into a long-drawn war would cement Hitler's status as the 'messiah' of the German population, and allow for the reign over the 'thousand-year Reich'. This apparent inevitability of war if the Nazi's were to stay in power was risk-taking (and in some aspects, miscalculation) on the part of Hitler and his cronies, because the fate of the party was now tied up with the outcome of the war. Max Weber, Economy and Society, ed. G Roth and C. Wittich, (Berkeley, 1978), p. 241. Kershaw, The 'Hitler Myth'. p.8. The emphasis on personal loyalty to Hitler in the system meant that there was in reality no bureaucracy to speak of, but rather, 'the administrative structure of Nazi Germany formed a complex mosaic of party and state agencies with ill-defined and over-lapping jurisdictions, sometimes complementing each other, more often mutually antagonistic, all striving to obtain a monopoly of power in their own domain."" This polycractic and inefficient structure, whereby the duplication of jobs, confusion amongst the various ministries, and internal rivalry to gain the backing of the Fuhrer, can be said to have undermined the political system in Germany. As Carr pointed out 'Confusion about the direction of policy was not restricted to the immediate circle around the Fuhrer. The whole structure of government has been aptly dubbed ""authoritarian anarchy""."" Also, as Ian Kershaw observed 'the demise of collective centralised government (the Cabinet never met again after 1938) promoted the disintegration of government into a proliferation of departments with ministries working largely independently of each other."" Should the confusion be seen as the necessary and inevitable outcome of Hitler's 'charismatic rule' hence weakening the system from within; or can it be, that this was a deliberate policy of Hitler to encourage '...confusion and uncertainty as a means of consolidating his power', thus the implication that a strong and omnipotent dictator would unite the will of Germany within himself, making the Third Reich stronger as a personal (rather than party) dictatorship, because the power struggle at the bottom would not be able to penetrate the office held by the Fuhrer. William Carr, Hitler: A study in personality and politics, (Bath, 1978), p. 41. Ibid. Ian Kershaw, The Nazi Dictatorship, (Cornwall, 2000), p. 75. Carr, Hitler, p. 43. Where did Hitler feature in this chaotic system of rule? It has been said that '...the symbolic Fuhrer authority was more important than the direct governing will of the person Hitler', and to a large extent it is true, because the presence of the Fuhrer was largely absent in the day-to-day running of the government. For his part, Hitler encouraged Social Darwinism within the party and state, allowing competing interests to fight it out amongst themselves, through which, a victor emerges, or a consensus is reached, which would then have the backing of the Fuhrer. This laissez-faire attitude seemed to have compromised the standing of the Fuhrer on the affairs of the state, which is reinforced by records telling of Hitler's laid-back attitude to work. However, to conclude that Hitler was a weak dictator and that this in turn affected the standing of the Third Reich as a dictatorship would be unjustified. Though Hitler took a very long time to act, 'but when he did, his personal orders, or Fuhrererlasse, cut quickly through the red-tape and ensured compliance with his decisions at least on major matters of policy and within the limits of available resources."" Despite the apparent success of this policy of Social Darwinism, it must be noted that this system 'led inexorably to an accelerating decline into aggression, lawlessness and criminal brutality' that could have undermined the stability of the regime. The system was essentially a very cracked pot, held together only by the 'glue', which was Hitler's authority. Ibid. Carr, Hitler, p. 45. Kershaw, The Nazi Dictatorship, p. 75. The over-reliance on a figure is perhaps one of the inherent weaknesses, as well as strengths of a dictatorship. The centrality of the figure of Hitler in rallying the party together cannot be over-emphasised. Hitler's personal magnetism provided the foundations of charismatic authority within his own entourage and bonded the party leadership to him. It was as Kershaw described ""personal conflicts within the Nazi elite which otherwise would have torn the system apart, were resolved only in Hitler's charismatic authority-in his indisputable position as the base of Nazism's popular legitimacy and the embodiment of Nazism's 'idea'."" It is interesting to note that even though vicious rivalry and power struggles were prevalent within the Nazi party, none of it was aimed at toppling Hitler. And contrary to Stalin's fear of being overthrown, which manifested itself in the purges (Gulags), there was only one purge that Hitler agreed to, and even so, reluctantly-the purging of Ernst Rohm. It must be emphasised that Nazi dictatorship of the Third Reich would not have been possible without Hitler, because the mandate of the people resided in him, and not in the party. The man remained ever popular, despite the party's decline in popularity, owing to some unfavourable economic policies. This would however be interpreted as a strength to some extent, because the party could borrow Hitler's name to gain the much needed power for rule, but yet at the same time, the over-reliance on one man makes it virtually impossible to appoint an heir to replace Hitler as Fuhrer. Hence, when Hitler shot himself in the bunker, the Third Reich died with him. Kershaw, The 'Hitler Myth'. p. 262. Finally, the impact of Hitler's 'charismatic rule' on resistance (or potential resistance) must be considered. Perhaps it would be logical to suggest that because of his immense popularity, those who resisted him were few. Few it seem, could contend with Hitler's deification and cult-like status, to the point that Hitler seemed to a large portion of the population as Germany's 'messiah'. However, this 'Hitler-worship' could have created a backlash against Hitler amongst the devout Christian/Catholic groups. More importantly, the lack of resistance from the traditional elites was in part, the result of Hitler's charismatic rule, which reduced the Army officers from 'power' elites to merely 'functional' elites, thus ensuring that countering and overthrowing Nazi rule would be extremely difficult to do accomplish, even if the Army had the will to do so. Therefore, it was due to the charisma and will of Hitler, that the traditional elites were unable to use him as a puppet for their own purposes. However, charisma alone was inadequate to secure the support of the German population for the Third Reich. Like all other dictatorships, the element of terror was crucial in taming the obstinate 'enemies of the state.' The 'Hitler myth' and terror were therefore two sides of the same coin, 'ensuring political control and mobilisation behind the regime. It is no coincidence that terroristic repression escalated visibly in the final phrase of the waning regime as the binding force of Hitler's popularity weakened and collapsed.' Kershaw, The 'Hitler myth', p. 258. Hitler's 'charismatic rule' worked to the benefit of the regime initially, when the conditions were favourable, and the success that was much desired was obtained. Entry into World War II was inevitable for the survival of the system that inherently thrived on dynamism and periods of emergency, but it was a double-edged sword, in that failure of the war effort would surely bring the Nazi dictatorship to its knees, and it did. As the war started going badly, 'the tide of Hitler's popularity first waned rather slowly, then ebbed sharply-a decline accelerating after Stalingrad, when Hitler's personal responsibility for the catastrophe was widely recognised."" Even the tool of propaganda could not erase the fallibility of the Fuhrer in the eyes of the people, instead, it made the population even more distrustful of the system that they had come to doubt. Therefore, the writer would conclude that ultimately, Hitler's 'charismatic rule' weakened the Third Reich as a dictatorship, due to the innately destructive elements in this concept of legitimacy. Ibid, p.200.",True
87,"Hitler's popularity amongst the majority of the German people have been variously analysed by historians, and though the percentages are debatable, it is generally recognized that his popularity was of fundamental importance to the functioning of the third Reich. As Ian Kershaw suggested 'The adulation of Hitler by millions of Germans who might otherwise have been only marginally committed to Nazism meant that the person of the Führer, as the focal point of basic consensus, formed a crucial integratory force in the Nazi system of rule."" Despite the concession that Hitler, or rather the 'Hitler myth' played an important role in the third Reich, it does not follow that the influence was necessarily advantageous for Nazi Germany as a dictatorship. The nature of this question is quite deceptive, and the search for a direct answer can be elusive, due to the fact that the line between strength and weakness may be obscured. Hence, what may initially appear to have strengthened the Third Reich may in the long term prove to be the ultimate de-stabilising factor, threatening the very foundations that the Reich was built upon. To reach a conclusion regarding this question, it is imperative to examine the effects of Hitler's 'charismatic rule' on the following: the German people, the issue of legitimacy, the state bureaucracy, the party, and the resistance movement. Ian Kershaw, The 'Hitler Myth', (Oxford, 1989), p.1. From the start, the German people were never adverse towards the concept of dictatorship. Their distrust and contempt for the system of liberal democracy that Weimar represented was pervasive throughout society. There was a 'national' yearning for a Caesar figure to pull Germany out of the economic doldrums, and to lead Germany to reclaiming her former glory. Hence, Hitler's 'charismatic rule' was seen as a welcome and much needed respite to the inefficient Weimar Republic, and did in fact appeal to the romantic within each person- the harking back to the Second Reich of Bismarck. This acceptance of the rule of a 'strong man' made possible the tolerance of Party dictatorship of the Nazis, and the subsequent new state bureaucracy. Also, Hitler's style of leadership, the propaganda machinery of floodlights and military processions created the façade of participation, that made the regime seem like an 'enlightened dictatorship', thus reconciling the people with the loss of their political vote. Judging from the plebiscites held in 1933, 34, 36 and 38, '...though no doubt the product of intense propaganda and coercion, nevertheless reflected genuine widespread approval and admiration for Hitler's accomplishments, and persuaded the masses to fall in line."" The extension of the personal loyalty of the population from Hitler to the state would strengthen the regime, and lessen the impact of the transition from a liberal democracy to a dictatorship. ibid. p. 258. It is imperative to examine Hitler's 'charismatic rule' as a claim to the legitimacy of the Third Reich. Max Weber in 'Economy and Society' expounds on the ideal type of 'charismatic authority' by which the leader is seen as an exemplary character of superhuman powers, and derives his backing from the personal loyalty of his subjects. This description would seem to fit Hitler's style of leadership in the Third Reich. To a large extent, it gave legitimacy, which was lacking during the Weimar republic, to the Third Reich, and allowed it to function with the support of the people. This would appear to have strengthened the Third Reich as a dictatorship because it portrayed Hitler not as a usurper of power, but merely fulfilling the will of the people (reliance on personal loyalty). However, 'if those to whom he feels sent do not recognize him, his claim collapses; if they recognize it, he is their master as long as he proves himself."" The conditional status of his power can be de-stabilising in that Hitler is under constant pressure to bring in success. Failure, or a series of failure (Operation 'Barbarossa' against Russia) would mean that the mandate was passing out of the hands of Hitler. Routinisation (the lapse back into normality) was also a threat to a regime that innately relied on dynamism to rule. Hence, the 'charismatic' form of leadership was 'an extraordinary, unstable and therefore transient form of rule, tending to arise in unusual or crisis conditions, and directed not at the solutions of everyday problems of government, but at the over-coming of supra-dimensional crises and emergencies."" With the above question in mind, the inherent instability of the regime is definitely a point to consider, when evaluating the impact of 'charismatic authority' on the dictatorship. With the prospect of normalcy approaching as Germany emerged from the Great Depression, the Fuhrer had to act in his own interest: the entry into a long-drawn war would cement Hitler's status as the 'messiah' of the German population, and allow for the reign over the 'thousand-year Reich'. This apparent inevitability of war if the Nazi's were to stay in power was risk-taking (and in some aspects, miscalculation) on the part of Hitler and his cronies, because the fate of the party was now tied up with the outcome of the war. Max Weber, Economy and Society, ed. G Roth and C. Wittich, (Berkeley, 1978), p. 241. Kershaw, The 'Hitler Myth'. p.8. The emphasis on personal loyalty to Hitler in the system meant that there was in reality no bureaucracy to speak of, but rather, 'the administrative structure of Nazi Germany formed a complex mosaic of party and state agencies with ill-defined and over-lapping jurisdictions, sometimes complementing each other, more often mutually antagonistic, all striving to obtain a monopoly of power in their own domain."" This polycractic and inefficient structure, whereby the duplication of jobs, confusion amongst the various ministries, and internal rivalry to gain the backing of the Fuhrer, can be said to have undermined the political system in Germany. As Carr pointed out 'Confusion about the direction of policy was not restricted to the immediate circle around the Fuhrer. The whole structure of government has been aptly dubbed ""authoritarian anarchy""."" Also, as Ian Kershaw observed 'the demise of collective centralised government (the Cabinet never met again after 1938) promoted the disintegration of government into a proliferation of departments with ministries working largely independently of each other."" Should the confusion be seen as the necessary and inevitable outcome of Hitler's 'charismatic rule' hence weakening the system from within; or can it be, that this was a deliberate policy of Hitler to encourage '...confusion and uncertainty as a means of consolidating his power', thus the implication that a strong and omnipotent dictator would unite the will of Germany within himself, making the Third Reich stronger as a personal (rather than party) dictatorship, because the power struggle at the bottom would not be able to penetrate the office held by the Fuhrer. William Carr, Hitler: A study in personality and politics, (Bath, 1978), p. 41. Ibid. Ian Kershaw, The Nazi Dictatorship, (Cornwall, 2000), p. 75. Carr, Hitler, p. 43. Where did Hitler feature in this chaotic system of rule? It has been said that '...the symbolic Fuhrer authority was more important than the direct governing will of the person Hitler', and to a large extent it is true, because the presence of the Fuhrer was largely absent in the day-to-day running of the government. For his part, Hitler encouraged Social Darwinism within the party and state, allowing competing interests to fight it out amongst themselves, through which, a victor emerges, or a consensus is reached, which would then have the backing of the Fuhrer. This laissez-faire attitude seemed to have compromised the standing of the Fuhrer on the affairs of the state, which is reinforced by records telling of Hitler's laid-back attitude to work. However, to conclude that Hitler was a weak dictator and that this in turn affected the standing of the Third Reich as a dictatorship would be unjustified. Though Hitler took a very long time to act, 'but when he did, his personal orders, or Fuhrererlasse, cut quickly through the red-tape and ensured compliance with his decisions at least on major matters of policy and within the limits of available resources."" Despite the apparent success of this policy of Social Darwinism, it must be noted that this system 'led inexorably to an accelerating decline into aggression, lawlessness and criminal brutality' that could have undermined the stability of the regime. The system was essentially a very cracked pot, held together only by the 'glue', which was Hitler's authority. Ibid. Carr, Hitler, p. 45. Kershaw, The Nazi Dictatorship, p. 75. The over-reliance on a figure is perhaps one of the inherent weaknesses, as well as strengths of a dictatorship. The centrality of the figure of Hitler in rallying the party together cannot be over-emphasised. Hitler's personal magnetism provided the foundations of charismatic authority within his own entourage and bonded the party leadership to him. It was as Kershaw described ""personal conflicts within the Nazi elite which otherwise would have torn the system apart, were resolved only in Hitler's charismatic authority-in his indisputable position as the base of Nazism's popular legitimacy and the embodiment of Nazism's 'idea'."" It is interesting to note that even though vicious rivalry and power struggles were prevalent within the Nazi party, none of it was aimed at toppling Hitler. And contrary to Stalin's fear of being overthrown, which manifested itself in the purges (Gulags), there was only one purge that Hitler agreed to, and even so, reluctantly-the purging of Ernst Rohm. It must be emphasised that Nazi dictatorship of the Third Reich would not have been possible without Hitler, because the mandate of the people resided in him, and not in the party. The man remained ever popular, despite the party's decline in popularity, owing to some unfavourable economic policies. This would however be interpreted as a strength to some extent, because the party could borrow Hitler's name to gain the much needed power for rule, but yet at the same time, the over-reliance on one man makes it virtually impossible to appoint an heir to replace Hitler as Fuhrer. Hence, when Hitler shot himself in the bunker, the Third Reich died with him. Kershaw, The 'Hitler Myth'. p. 262. Finally, the impact of Hitler's 'charismatic rule' on resistance (or potential resistance) must be considered. Perhaps it would be logical to suggest that because of his immense popularity, those who resisted him were few. Few it seem, could contend with Hitler's deification and cult-like status, to the point that Hitler seemed to a large portion of the population as Germany's 'messiah'. However, this 'Hitler-worship' could have created a backlash against Hitler amongst the devout Christian/Catholic groups. More importantly, the lack of resistance from the traditional elites was in part, the result of Hitler's charismatic rule, which reduced the Army officers from 'power' elites to merely 'functional' elites, thus ensuring that countering and overthrowing Nazi rule would be extremely difficult to do accomplish, even if the Army had the will to do so. Therefore, it was due to the charisma and will of Hitler, that the traditional elites were unable to use him as a puppet for their own purposes. However, charisma alone was inadequate to secure the support of the German population for the Third Reich. Like all other dictatorships, the element of terror was crucial in taming the obstinate 'enemies of the state.' The 'Hitler myth' and terror were therefore two sides of the same coin, 'ensuring political control and mobilisation behind the regime. It is no coincidence that terroristic repression escalated visibly in the final phrase of the waning regime as the binding force of Hitler's popularity weakened and collapsed.' Kershaw, The 'Hitler myth', p. 258. Hitler's 'charismatic rule' worked to the benefit of the regime initially, when the conditions were favourable, and the success that was much desired was obtained. Entry into World War II was inevitable for the survival of the system that inherently thrived on dynamism and periods of emergency, but it was a double-edged sword, in that failure of the war effort would surely bring the Nazi dictatorship to its knees, and it did. As the war started going badly, 'the tide of Hitler's popularity first waned rather slowly, then ebbed sharply-a decline accelerating after Stalingrad, when Hitler's personal responsibility for the catastrophe was widely recognised."" Even the tool of propaganda could not erase the fallibility of the Fuhrer in the eyes of the people, instead, it made the population even more distrustful of the system that they had come to doubt. Therefore, the writer would conclude that ultimately, Hitler's 'charismatic rule' weakened the Third Reich as a dictatorship, due to the innately destructive elements in this concept of legitimacy. Ibid, p.200.","The luxury debates have been closely entwined with the rise of modernity, and while 'luxury' has ceased to be a central issue, in the supposed 'post-modern' world, I would argue that the issues that were pertinent to the writers of the eighteenth and late-nineteenth century are still with us today; and our knowledge of this past debate can only serve to sharpen our acumen, and make us more aware of the role of 'conspicuous consumption' in our society in shaping the discourse about women; the self; taste and civility; and progress (economic and social) which was closely related to the notion of the well-being of the nation. It is my intention to examine the implications of the eighteenth and late nineteenth century luxury debates by employing these categories as a means of injecting a bit of order (albeit artificial) into the vast amount of literature written on this subject. Despite the tendencies of our relativist age to avoid crouching the debate in the language of morality, it is important to note that morality remains very much an integral part of the luxury debates, despite the shift from the vice/virtue dichotomy to one of dependence/freedom. Whatever their stands on luxury, the writers who participated in the luxury debates were keenly aware that 'luxury' was not a self-contained concept, but was rather a useful tool in understanding and critiquing the wider notion of the nature and progress of society. Coined by Thorstein Veblen in The Theory of the Leisure Class, (New York, 2001) However we may choose to judge women's association with luxury, the two remain closely yoked, and it seems almost impossible to tear them asunder. It is significant that the participants of the luxury debates have chosen to work with, instead of challenge, the assumption of a 'Female Luxury'. Even Mandeville, who disputed the accepted universality of Christian morality, and who was perhaps most favourable to the position of women (as major contributors to the economy due to their demand for luxury) chose to relate an anecdote featuring a male Mercer and a young lady customer, undoubtedly as an excerpt to illustrate the general; also, in his argument that luxury (or fashion) had done more to promote the development of nations than the Reformation, he picked ""the silly and capricious invention of hoop'd and quilted petticoats"" as a means of making a stark contrast. That ""luxury was identified with effeminacy and weakness"" is perhaps best seen in Rousseau's first discourse where he argues that the result of the pursuit of the arts and ""...the study of the sciences is much more apt to soften and effeminate men's courage than to strengthen and animate it"" Elsewhere in Emile, Emile's education teaches him to be an honest man whereas Sophie is taught deception and arts of manipulation and coquetry by her mother. F.B.Kaye (ed. ), Mandeville: The Fable of the Bees Vol I, (Indianapolis, 1988), p. 356 Ibid, p. 330 Ibid, p. 349-353 Ibid, p. 356. Maxine Berg and Elizabeth Eger, ""The Rise and Fall of the Luxury Debates"", in Maxine Berg and Elizabeth Eger (ed. ), Luxury in the eighteenth century: debates, desires and delectable goods, (Hampshire, 2003), p. 18 Victor Gourevitch (ed. ), Rousseau: The Discourses and other early political writings, (Cambridge, 2003), p. 21 This link between women, artifice, enticement and luxury might account in part for the 'Female Luxury', which Sombart, at the turn of the twentieth century harps on. Sombart attributed an important role to women of leisure in the development of luxury, and commented: ""luxury was fostered by illustrious mistresses and promoted for their sake"". In his central theory, which states that: ""all personal luxury springs from purely sensuous pleasure"", the identification of female as mistresses (and therefore sex objects) establishes the link between the rapacious sexual appetite of these women and their equally insatiable demand for luxuries, which, interestingly enough seems to conjure up the image of a particular 'Lady Allurea Luxury' (which as an 18 th century creation was the product of and engendered prevailing prejudices). The notion of women as 'objects' or adjuncts is also suggested in Veblen's illustration of the corset. While it ""lowers the subjects' vitality, rendering her permanently unfit for work...the loss suffered on that score is offset by the gain in reputability which comes of her visibly increased expensiveness and infirmity."" Though these women may gain some 'reputability' for being women of leisure, it is apparent that the men on whom these women are dependent reap the bulk of this 'reputability'. Hence, while women seem to consume more than men, it is the case because women have a duty, as adjuncts, not only to consume on behalf of the household, but also to display 'conspicuous consumption' in a way that would serve to elevate the status of their husbands/master. Werner Sombart, Luxury and Capitalism, (Michigan, 1967), p. 78 Ibid, p. 60 Veblen, The Theory of the Leisure Class, p. 126. It is perhaps appropriate to involve 'the Self' in the implications of the luxury debates of the eighteenth and nineteenth century, as the debates of the eighteenth century inaugurated the growth of the 'cult of the individual' that is still apparent in the present Western society. Luxury became a means by which an image of the 'Self' could be cultivated, which on one hand would allow others to view the 'Self' in a certain way; and on the other hand, permitted the 'Self' to nurse fantasies, which were stimulated by the acquisition of luxury objects. Hundert, through his study of Mandeville, highlighted the importance of ""the difference between being and appearing...for commercial moderns"", and how city people ""wear clothes above (their) rank...and consequently have the pleasure of being esteemed by a vast majority, not as what they are, but what they appear to be"". Veblen picks up on this point, and comments on the role that conspicuous consumption has in forging an image for the 'Self', especially in the midst of strangers. In succumbing to an appearance of public 'decency', we demonstrate how much we value others' good opinions of ourselves, and ironically, how this opinion shapes our understanding of the private 'Self'. The dictum ""a cheap coat makes a cheap man"" clearly illustrates the importance of conspicuous consumption in validating one's self worth. No doubt the freedom-loving Rousseau lamented this dependence not only on foreign luxuries, but more significantly, the dependence of oneself on the opinions of others. It is thought provoking, even for the modern readers that he concludes with: ""what good is it to seek our happiness in someone else's opinion if we can find it within ourselves."" Edward Hundert, ""Mandeville, Rousseau and the Political Economy of Fantasy"", in Maxine Berg and Elizabeth Eger (ed. ), Luxury in the eighteenth century: debates, desires and delectable goods, (Hampshire, 2003) p. 29 Veblen, The Theory of the Leisure Class, p. 65 Ibid, p. 114 Gourevitch (ed. ), Rousseau, p.27 The shifting perception of the 'Self' was part of a change in wider society that the changing definitions of luxury reflected. The Industrial Revolution, the increased exploration and exploitation of the colonies through trade led to the creation of an industrial, commercial and professional middle class, which sought to elevate its social standing as its pecuniary resources were increased. The old dichotomy between 'luxury' and 'necessity' became antiquated and was replaced by categories that were more indicative of the social climate-new luxuries (associated with commerce, utility, taste and comfort) were adopted in place of the excesses of old aristocratic luxury. As Dena Goodman points out ""by associating it (luxury) with commerce, the superfluous could now be embraced as a civilizing force in modern society without carrying the taint of barbarous luxury."" It was in this climate of changing values and meanings that 'taste' became the chief indicator of value, and a mechanism by which the surplus could be employed for the benefit of refinement and civility. The inclusiveness of taste as opposed to the exclusiveness of luxury driven by ostentation allowed for a realm of self-expression through fashion, which ""became a function of knowledge rather than wealth, and gave power to those who carried the authority of taste."" However, Veblen would have us believe that taste is still a function of wealth and that ""by habitually identifying beauty with reputability, it comes about that a beautiful article which is not expensive is accounted not beautiful."" And if it is as Veblen claims, that ""the canons of taste have been coloured by the canons of pecuniary reputability"", we once again sink back into conspicuous waste and excesses of 'old luxury', and taste becomes once again the prerogative of the rich. Berg and Eger, ""The Rise and Fall of he Luxury Debates"", p. 9 Dena Goodman, ""Furnishing Discourses: Readings of a Writing Desk in Eighteenth-Century France"", in Maxine Berg and Elizabeth Eger (ed. ), Luxury in the eighteenth century: debates, desires and delectable goods, (Hampshire, 2003) p. 76 Ibid, p. 77 Veblen, The Theory of the Leisure Class, p. 97 The implications of luxury did not merely pertain to the self and society, but were of national importance, as it affected the material and nonmaterial well being of the nation. The suspicions generated about 'luxury' are perhaps in part the product of its association with 'foreignness', and the mercantilist fear that the demand for luxury imports will drain the metropolis of its gold and silver, leading to its economic downfall. The advocates of the Scottish Enlightenment, David Hume and Adam Smith were keen to challenge this antediluvian attitude and promote luxury as a ""progressive social force"" through which free trade in goods would also allow for a transfer of technologies from other advanced civilization to Britain, through the process of emulation and 'imitative industries'. As Hume argues persuasively ""...in most nations foreign trade has preceded a refinement in home manufactures, and given birth to domestic luxury...and their delicacy and industry being once awakened, carry them on to further improvements in every branch of domestic as well as foreign trade."" A classic example would be the stimulus that the Wedgwood pottery industry received from the East-West trade in porcelain, which created the market for fine dinner services that Wedgwood was able to capitalize on in the mid eighteenth century. It is often the argument employed by the advocates of luxury (notably Mandeville) that the demand for luxury would ""lead to the expansion of commerce and the wider employment of the poor"" As Sombart elaborates in his theory that luxury gave birth to Capitalism, luxury was an indirect tool for social redistribution, through the rich spending themselves into debt in pursuit of frivolities and the subsequent employment of the poor to produce luxury items for consumption. Berg and Eger, ""The Rise and Fall of he Luxury Debates"", p. 9 Ibid, p. 11 Ibid, p. 12 Ibid, p. 10 Sombart, Luxury and Capitalism, p. 171 Ibid, p. 114 This economic argument by no means displaced the element of morality in the luxury debates. Though Mandeville accepted traditional associations of luxury with vice, his central thesis ""that Private Vices by the dextrous management of a skilful Politician may be turned into Publick Benefits"" is indeed refreshing, and justified the state's act of harnessing the demand for luxury as an engine for economic growth. Diametrically opposed to this line of argument, Rousseau instead espouses the view that civilization corrupts because civic virtue is compromised as a result of the development of the arts and sciences. Rousseau sarcastically commented: ""the ancient politicians forever spoke of morals and virtue; ours speak only of commerce and money."" Morality was an intrinsic part of national well being, and the attack on luxury and its association with moral corruption was often raised in the light of national wars. In particular, ""French critics associated luxury with France's humiliation by England during the Seven Years' War""; in England, the war ""also provided a platform for a denunciation of French luxury..."" and ""journalism during the war years rehabilitated classical condemnations of luxury in defence of Britishness."" Gourevitch (ed. ), Rousseau, p.18 Berg and Eger, ""The Rise and Fall of he Luxury Debates"", p. 14 It should be obvious from the above that ""luxury was not so much a key term in a debate as it was the marker for certain anxiety-'a convenient code for all of society's perceived problems.'"" Through participation in this debate, one was commenting not merely on luxury, but more importantly, was discussing the implications of luxury for a host of issues, revolving around Women, the Self, sociability and taste, and most importantly national well being. The luxury debates reinforced notions of 'Female luxury' and the close associations of women with consumption, without a deeper understanding of how men have helped shaped the ways in which women consume. It also served to highlight the modern tendency to use goods as the ""primary vehicle for the construction of the self that has the opinion of others as an indelible part of its contents."" The changing definitions of luxury and its subsequent link with taste also illustrate the changing expressions of sociability, which reflected the change in the pecuniary structure of society. Despite this and the argument that luxury facilitates social progress, there is a realization that while the luxury debates are becoming more inclusive, the gap between the haves and the have-nots has not narrowed, but have instead widened during Veblen's and Sombart's time. This has obvious bearing on the well being of a nation, and the social distribution of income is still very much on the present agenda. Lastly, despite the modern preference for economic arguments, the link between morality and luxury has been paradoxically strengthened, and remains as a bastion from which the critics of luxury draw their ammunition. The luxury debates therefore became an avenue where the problems of society were raised, and correspondingly, where the shape of an ideal modern society could be discussed. Goodman, ""Furnishing Discourses: Readings of a Writing Desk in Eighteenth-Century France"", p. 73 Hundert, ""Mandeville, Rousseau and the Political Economy of Fantasy"", p. 35",False
88,"Hitler's popularity amongst the majority of the German people have been variously analysed by historians, and though the percentages are debatable, it is generally recognized that his popularity was of fundamental importance to the functioning of the third Reich. As Ian Kershaw suggested 'The adulation of Hitler by millions of Germans who might otherwise have been only marginally committed to Nazism meant that the person of the Führer, as the focal point of basic consensus, formed a crucial integratory force in the Nazi system of rule."" Despite the concession that Hitler, or rather the 'Hitler myth' played an important role in the third Reich, it does not follow that the influence was necessarily advantageous for Nazi Germany as a dictatorship. The nature of this question is quite deceptive, and the search for a direct answer can be elusive, due to the fact that the line between strength and weakness may be obscured. Hence, what may initially appear to have strengthened the Third Reich may in the long term prove to be the ultimate de-stabilising factor, threatening the very foundations that the Reich was built upon. To reach a conclusion regarding this question, it is imperative to examine the effects of Hitler's 'charismatic rule' on the following: the German people, the issue of legitimacy, the state bureaucracy, the party, and the resistance movement. Ian Kershaw, The 'Hitler Myth', (Oxford, 1989), p.1. From the start, the German people were never adverse towards the concept of dictatorship. Their distrust and contempt for the system of liberal democracy that Weimar represented was pervasive throughout society. There was a 'national' yearning for a Caesar figure to pull Germany out of the economic doldrums, and to lead Germany to reclaiming her former glory. Hence, Hitler's 'charismatic rule' was seen as a welcome and much needed respite to the inefficient Weimar Republic, and did in fact appeal to the romantic within each person- the harking back to the Second Reich of Bismarck. This acceptance of the rule of a 'strong man' made possible the tolerance of Party dictatorship of the Nazis, and the subsequent new state bureaucracy. Also, Hitler's style of leadership, the propaganda machinery of floodlights and military processions created the façade of participation, that made the regime seem like an 'enlightened dictatorship', thus reconciling the people with the loss of their political vote. Judging from the plebiscites held in 1933, 34, 36 and 38, '...though no doubt the product of intense propaganda and coercion, nevertheless reflected genuine widespread approval and admiration for Hitler's accomplishments, and persuaded the masses to fall in line."" The extension of the personal loyalty of the population from Hitler to the state would strengthen the regime, and lessen the impact of the transition from a liberal democracy to a dictatorship. ibid. p. 258. It is imperative to examine Hitler's 'charismatic rule' as a claim to the legitimacy of the Third Reich. Max Weber in 'Economy and Society' expounds on the ideal type of 'charismatic authority' by which the leader is seen as an exemplary character of superhuman powers, and derives his backing from the personal loyalty of his subjects. This description would seem to fit Hitler's style of leadership in the Third Reich. To a large extent, it gave legitimacy, which was lacking during the Weimar republic, to the Third Reich, and allowed it to function with the support of the people. This would appear to have strengthened the Third Reich as a dictatorship because it portrayed Hitler not as a usurper of power, but merely fulfilling the will of the people (reliance on personal loyalty). However, 'if those to whom he feels sent do not recognize him, his claim collapses; if they recognize it, he is their master as long as he proves himself."" The conditional status of his power can be de-stabilising in that Hitler is under constant pressure to bring in success. Failure, or a series of failure (Operation 'Barbarossa' against Russia) would mean that the mandate was passing out of the hands of Hitler. Routinisation (the lapse back into normality) was also a threat to a regime that innately relied on dynamism to rule. Hence, the 'charismatic' form of leadership was 'an extraordinary, unstable and therefore transient form of rule, tending to arise in unusual or crisis conditions, and directed not at the solutions of everyday problems of government, but at the over-coming of supra-dimensional crises and emergencies."" With the above question in mind, the inherent instability of the regime is definitely a point to consider, when evaluating the impact of 'charismatic authority' on the dictatorship. With the prospect of normalcy approaching as Germany emerged from the Great Depression, the Fuhrer had to act in his own interest: the entry into a long-drawn war would cement Hitler's status as the 'messiah' of the German population, and allow for the reign over the 'thousand-year Reich'. This apparent inevitability of war if the Nazi's were to stay in power was risk-taking (and in some aspects, miscalculation) on the part of Hitler and his cronies, because the fate of the party was now tied up with the outcome of the war. Max Weber, Economy and Society, ed. G Roth and C. Wittich, (Berkeley, 1978), p. 241. Kershaw, The 'Hitler Myth'. p.8. The emphasis on personal loyalty to Hitler in the system meant that there was in reality no bureaucracy to speak of, but rather, 'the administrative structure of Nazi Germany formed a complex mosaic of party and state agencies with ill-defined and over-lapping jurisdictions, sometimes complementing each other, more often mutually antagonistic, all striving to obtain a monopoly of power in their own domain."" This polycractic and inefficient structure, whereby the duplication of jobs, confusion amongst the various ministries, and internal rivalry to gain the backing of the Fuhrer, can be said to have undermined the political system in Germany. As Carr pointed out 'Confusion about the direction of policy was not restricted to the immediate circle around the Fuhrer. The whole structure of government has been aptly dubbed ""authoritarian anarchy""."" Also, as Ian Kershaw observed 'the demise of collective centralised government (the Cabinet never met again after 1938) promoted the disintegration of government into a proliferation of departments with ministries working largely independently of each other."" Should the confusion be seen as the necessary and inevitable outcome of Hitler's 'charismatic rule' hence weakening the system from within; or can it be, that this was a deliberate policy of Hitler to encourage '...confusion and uncertainty as a means of consolidating his power', thus the implication that a strong and omnipotent dictator would unite the will of Germany within himself, making the Third Reich stronger as a personal (rather than party) dictatorship, because the power struggle at the bottom would not be able to penetrate the office held by the Fuhrer. William Carr, Hitler: A study in personality and politics, (Bath, 1978), p. 41. Ibid. Ian Kershaw, The Nazi Dictatorship, (Cornwall, 2000), p. 75. Carr, Hitler, p. 43. Where did Hitler feature in this chaotic system of rule? It has been said that '...the symbolic Fuhrer authority was more important than the direct governing will of the person Hitler', and to a large extent it is true, because the presence of the Fuhrer was largely absent in the day-to-day running of the government. For his part, Hitler encouraged Social Darwinism within the party and state, allowing competing interests to fight it out amongst themselves, through which, a victor emerges, or a consensus is reached, which would then have the backing of the Fuhrer. This laissez-faire attitude seemed to have compromised the standing of the Fuhrer on the affairs of the state, which is reinforced by records telling of Hitler's laid-back attitude to work. However, to conclude that Hitler was a weak dictator and that this in turn affected the standing of the Third Reich as a dictatorship would be unjustified. Though Hitler took a very long time to act, 'but when he did, his personal orders, or Fuhrererlasse, cut quickly through the red-tape and ensured compliance with his decisions at least on major matters of policy and within the limits of available resources."" Despite the apparent success of this policy of Social Darwinism, it must be noted that this system 'led inexorably to an accelerating decline into aggression, lawlessness and criminal brutality' that could have undermined the stability of the regime. The system was essentially a very cracked pot, held together only by the 'glue', which was Hitler's authority. Ibid. Carr, Hitler, p. 45. Kershaw, The Nazi Dictatorship, p. 75. The over-reliance on a figure is perhaps one of the inherent weaknesses, as well as strengths of a dictatorship. The centrality of the figure of Hitler in rallying the party together cannot be over-emphasised. Hitler's personal magnetism provided the foundations of charismatic authority within his own entourage and bonded the party leadership to him. It was as Kershaw described ""personal conflicts within the Nazi elite which otherwise would have torn the system apart, were resolved only in Hitler's charismatic authority-in his indisputable position as the base of Nazism's popular legitimacy and the embodiment of Nazism's 'idea'."" It is interesting to note that even though vicious rivalry and power struggles were prevalent within the Nazi party, none of it was aimed at toppling Hitler. And contrary to Stalin's fear of being overthrown, which manifested itself in the purges (Gulags), there was only one purge that Hitler agreed to, and even so, reluctantly-the purging of Ernst Rohm. It must be emphasised that Nazi dictatorship of the Third Reich would not have been possible without Hitler, because the mandate of the people resided in him, and not in the party. The man remained ever popular, despite the party's decline in popularity, owing to some unfavourable economic policies. This would however be interpreted as a strength to some extent, because the party could borrow Hitler's name to gain the much needed power for rule, but yet at the same time, the over-reliance on one man makes it virtually impossible to appoint an heir to replace Hitler as Fuhrer. Hence, when Hitler shot himself in the bunker, the Third Reich died with him. Kershaw, The 'Hitler Myth'. p. 262. Finally, the impact of Hitler's 'charismatic rule' on resistance (or potential resistance) must be considered. Perhaps it would be logical to suggest that because of his immense popularity, those who resisted him were few. Few it seem, could contend with Hitler's deification and cult-like status, to the point that Hitler seemed to a large portion of the population as Germany's 'messiah'. However, this 'Hitler-worship' could have created a backlash against Hitler amongst the devout Christian/Catholic groups. More importantly, the lack of resistance from the traditional elites was in part, the result of Hitler's charismatic rule, which reduced the Army officers from 'power' elites to merely 'functional' elites, thus ensuring that countering and overthrowing Nazi rule would be extremely difficult to do accomplish, even if the Army had the will to do so. Therefore, it was due to the charisma and will of Hitler, that the traditional elites were unable to use him as a puppet for their own purposes. However, charisma alone was inadequate to secure the support of the German population for the Third Reich. Like all other dictatorships, the element of terror was crucial in taming the obstinate 'enemies of the state.' The 'Hitler myth' and terror were therefore two sides of the same coin, 'ensuring political control and mobilisation behind the regime. It is no coincidence that terroristic repression escalated visibly in the final phrase of the waning regime as the binding force of Hitler's popularity weakened and collapsed.' Kershaw, The 'Hitler myth', p. 258. Hitler's 'charismatic rule' worked to the benefit of the regime initially, when the conditions were favourable, and the success that was much desired was obtained. Entry into World War II was inevitable for the survival of the system that inherently thrived on dynamism and periods of emergency, but it was a double-edged sword, in that failure of the war effort would surely bring the Nazi dictatorship to its knees, and it did. As the war started going badly, 'the tide of Hitler's popularity first waned rather slowly, then ebbed sharply-a decline accelerating after Stalingrad, when Hitler's personal responsibility for the catastrophe was widely recognised."" Even the tool of propaganda could not erase the fallibility of the Fuhrer in the eyes of the people, instead, it made the population even more distrustful of the system that they had come to doubt. Therefore, the writer would conclude that ultimately, Hitler's 'charismatic rule' weakened the Third Reich as a dictatorship, due to the innately destructive elements in this concept of legitimacy. Ibid, p.200.","The debate on the memory of the Holocaust is perhaps the latest issue in the vast ocean of literature concerning other aspects of the Nazi holocaust, and this trend can be traced to the 1980s and 1990s when there was a sudden proliferation of Holocaust memorials, in the form of books, films, paintings and in particular, monuments and museums. The academic value of this enterprise must be judged, and the historian Norman Finkelstein astutely points out that 'currently all the rage in the ivory tower, memory is surely the most impoverished concept to come down the academic pike in a long time."" Yet despite this, 'the intensity of the debates about memory and the Holocaust has never abated."" The task at hand is perhaps even more disdainful because it is expressed in a normative statement, and seemingly, it beckons to the writer to take part in a doomed enterprise. The role of the impartial judge is one that is hard to play, and though it is acknowledged that the historian's task is not to judge, it is not entirely possible for the historian to distance herself from this issue, because she is necessarily the product of her time. As such, the writer does not profess impartiality, but attempts to bring in the professionalism of her discipline to tackle this issue by first calling into question the assumption that the Holocaust should be memorialised; the social and political functions of these memorials; and finally, by examining the various memorials, aim to show that the forms that memorials take often reflect their position in time and location, and more importantly, is a reflection of the society that has created them, hence the futility of this question, because the form that a memorial takes is not dictated by a single universal principle, but obeys a number of intertwined forces. Norman Finkelstein, The Holocaust Industry: Reflections on the Exploitation of Jewish suffering, (New York, 2001), p.5. Caroline Wiedmer, The Claims of Memory: Representations of the Holocaust in Contemporary Germany and France, (New York, 1999), p. 4. It is imperative to inquire into the assumption of the above question by first examining the basis by which the Holocaust should be memorialised. Should the Holocaust be memorialised on the basis of its uniqueness, as some historians have been fast to claim? If the Holocaust was truly so unique, does it not preclude the same event from ever happening again? So why is there this supposed need to commemorate for remembrance? What does one aim to achieve by commemorating it? The above questions become even more pressing with the emergence of the Holocaust Industry. Is it not ironic that the proliferation of monuments comes only in the later half of the twentieth century when the generation of survivors had largely perished? Perhaps it is because of the passing of private, living memory of the Holocaust that a vicarious and surrogate public, fixed interpretation of the past is promoted. 'Collective memory, it would seem, begins where the individual memories of a group cease, along with its members, to exist."" Or maybe it is precisely because so many decades have passed from the event that people now dare to look back at their past, and that 'the further events of World War Two recede into time, the more prominent its memorials become."" The flourishing of memorial politics in France, Germany, Israel and America do perhaps lend this question a certain direction. In places such as Germany and France where the Jews were persecuted, the upsurge of a need to know the past was perhaps a reaction against the code of silence and the 'absolute negation of the black years (les années noires)."" Building memorials was therefore seen as safeguarding the memory of the Holocaust against the cover-up of the State. Memorialisation however, is a double-edged sword, and the late historian Martin Broszat cautioned that monuments may not remember events as to bury them beneath layers of national myths and explanations. Hence, as cultural reifications, monuments coarsen historical understanding as much as they generate it. Caroline Wiedmer, The Claims of Memory, p.57. James E. Young, The Texture of Memory: Holocaust Memorials and Meaning, (Michigan, 1993), p.1. Caroline Wiedmer , The Claims of Memory, p. 33 As the title of the question suggests, what is being commemorated is not the historical event (Nazi holocaust), but rather, ideological interpretations of it. Films (such as Holocaust the television series, Schindler's List, The Pianist, Life is Beautiful etc.) in particular fuel such interpretations of the holocaust, and by trying to recreate its likeness, is in a sense trivializing it. There is not one nazi holocaust, but many versions of the real thing. Despite the attempted homage to the holocaust, the necessary dramatization of the real event in order to make it more palatable for Mass Culture is perhaps a sacrilege to the memory of the holocaust, because it distorts how one perceives it. However, this is not to deny the fact that 'such cinematic dramatizations of the holocaust have helped to shape memory of the Holocaust in the popular consciousness' allowing such films to become defenders of memory. The films raised awareness amongst the generations that had no connection to the Holocaust, and as without knowledge there is no memory, memorialisation of the Holocaust is a necessary evil. Judith E. Doneson, Holocaust Revisited: A Catalyst for Memory or Trivialisation, The Annals of The American Academy of Political and Social Science, Vol. 548, November 1996, (California, 1996), p.70. Determining the functions of the Holocaust memorials is integral to the answering of the question. How the Holocaust is memorialised, and the form it takes is largely dependant on the perceived functions of the memorials. Memorials can be built as a result of a government's need to explain a nation's past to itself (in the case of Germany and France); to educate the next generation and to inculcate a sense of shared experience and destiny; as expiations of guilt and self-aggrandizement; and even to attract tourists. An extreme case in which a memorial was built for political ends was when President Jimmy Carter proposed the building of a national memorial in Washington D.C. in order to placate Jewish supporters angered by his sale of F-15 fighter planes to Saudi Arabia. Much of the USA's enthusiasm for memorial building was in part due to the declaration of solidarity with the influential American Jews, as well as in support of her ally Israel. Similarly, the initial code of silence surrounding the Holocaust immediately after the war was due to the more urgent need for reconciliation with West Germany, in the face of the Communist threat. Hence, as James E. Young concluded 'All such memorial decisions are made in political time, contingent on political realities.' James E. Young, The Texture of Memory, p. 285. Memorials are also essential to the foundations of political legitimacy in that 'by themselves, monuments are of little value, mere stones in the landscape. But as a part of a nation's rites or the objects of a people's national pilgrimage, they are invested with national soul and memory.' This is particularly true in the case of Israel, whose political foundations rest firmly on the back of the Holocaust, to the extent that according to the historian Peter Baldwin, 'the singularity of the Jewish suffering adds to the moral and emotional claims that Israel can make ...on other nations."" Having defined themselves as a people through commemorative recitations of their past, 'the Jews now depend on Memory for their very existence as a nation.' Norman Finkelstein, The Holocaust Industry, p. 48. James E. Young, The Texture of Memory, p. 211. The social functions that are fulfilled by memorial building can be quite diverse. On the one hand, it is useful for consensus building: 'in the absence of shared belief or common interests, Art in public spaces may force an otherwise fragmented populace to frame diverse values and ideals in common spaces. By creating common spaces for memory, monuments propagate the illusion of common memory."" On the other hand, memorials relieve the population of the burden to remember, 'under the illusion that our memorial edifices will always be there to remind us, we take leave of them and return only at our own convenience."" Despite the various facets of remembrance, there is a general consensus that monuments are meant for public consumption, and hence, a part of the answer to the question of how the Holocaust should be memorialised would rest on public opinion and expectations. James E. Young, The Texture of Memory, p.6. Ibid, p.5. The main debates on the form a Holocaust memorial should take will be discussed here, under the main themes of Abstract vs. Realist, Jews vs. Non Jews, Secular vs. Religious and the anti-monument. It would be worth noting that the backing of the State, as well as public opinion, is necessary for the memorial to serve its purpose. In every case, Holocaust memorials reflect not only national and communal remembrance, but also the memorial designer's own time and place. For contemporary artists (working in an era of abstract expressionism, conceptual art, post-modern and de-constructionist design), though there is a recognition that they are accountable to both art and memory, Albert Elsen reminds us that for them 'the needs of art, not the public memory come first."" However, this is in direct opposition to many survivors, who 'believe that the searing reality of their experiences demand as literal a memorial expression as possible.' Hence the dilemma of the artist, an example of whom is Nathan Rapoport, whose Warsaw Ghetto Monument 1948,is probably the most widely known, celebrated and controversial. Conversely, Sol LeWitt's unconventional Memorial to the Missing Jews, 1989, in Germany had to bow to popular opinion to have it removed from the Platz der Republic, in Hamburg. Ibid, p.8 It must be remembered that Jews, as well as non-Jews perished during the Nazi holocaust, and it should be the latter's entitlement to share in the remembrance and mourning of the former. In a particularly volatile situation in 1988 Germany, Leah Ross, a journalist demanded that a memorial be set up for Jews alone. This provoked an outcry from Romani Rose of the Central Council of the Sinti and Roma that this 'represented a hierarchization of victims, which is insulting for the victims of the genocide and for the survivors of the Sinti and Roma majority."" It would make sense in Israel to dedicate the memorial exclusively to Jews, but given the racial composition in Germany, it would be impertinent to do so. Caroline Wiedmer, The Claims of Memory, p. 144. Due to the intertwined relationship of Jewish suffering (the Shoah) and religion, commemoration of the former inevitably involves the latter. As James E. Young suggests, 'Israel's over-arching national ideology and religion may be memory."" Victims of the Shoah are portrayed as martyrs who died for their faith, fighters who are remembered for their part in the state's founding. Contrast this to the secular state of France, who, during one of the national competitions for the Vél d'Hiv Monument, picked the literal figurative design of Walter Spitzer over that of Shelomo Selinger because the ministry felt that the latter's proposal was 'Too Jewish.' As such, it is highly apparent that how the Holocaust should be memorialised depends largely on cultural and national factors. James E. Young, The Texture of Memory, p. 210. An interesting development would be that of the counter-monument. This is particularly so with regard to the new generation of German artists, who 'have a deep distrust of monumental forms in light of their systematic exploitation by the Nazis, and a profound desire to distinguish their generation from that of the killers through memory."" These artists explore both the necessity of memory, and their incapacity to recall events they never experienced directly, hence in capturing the memory of events, remember therefore only their own relationship to it. The Gerzes' counter-monument is perhaps the best example of this. Theirs is a self-abnegating monument, a 'vanishing monument' which, instead of graciously accepting the burden of memory, throws it back at the town's feet, forcing them to remember for themselves. This audaciously simple idea challenges the idea of monumentality and the permanence of collective memory, and provokes the population to examine themselves as part of the piece's performance. Ibid, p.27. The writer's main emphasis on public monuments reflects her bias in thinking that such forms of public art are more effective in inculcating a common sense of destiny, and fulfilling the social and political functions attached to them, more so than films, books and painting can. With the boom of the Holocaust industry however, films have become an essential medium by which large populations can learn more about the Holocaust. The writer has avoided a direct answer to the question up to this point, but has attempted to illustrate the many debates and considerations that have arise from the question. In conclusion, the Holocaust should be memorialised according to its time and location, asserting its relevance in the lives of the denizens there. Also, it is worth noting that 'new generations visit memorials under new circumstances and invest them with new meanings', and since there is no one universally accepted way of memorialising the Holocaust, the normative connotation posed by the question above is self-defeating, and ought not to be answered by a historian. The history of empires is a long one: beginning with the Greek, Roman and Persian empires, which pitted race against race; the empires of Spain, Portugal, the Netherlands and Britain in the sixteenth and seventeenth century, which saw the rivalry between monarchies; and finally, the 'New' Imperialism of the mid-nineteenth century, which saw a renewed interest in and frantic scramble for, new colonies, between nation-states. 'By early twentieth century, Britain, France, Germany, Italy, Belgium, the Netherlands, Denmark, Spain, and Portugal together held sway over almost 84 percent of the earth's surface."" Imperialism had become not only fashionable, but also defined the era from the mid-nineteenth onwards to World War I. Despite many attempts to attach priority to a certain aspect of imperialism as being the driving force behind the phenomenon, it is perhaps more helpful to understand it as a product of a complex web of motives and concerns. Also, imperialism must be examined in the context of its time (post-enlightenment ideas, rapid industrialisation, the formation of scientific theories which supported the superiority of one race over another, nationalism, and the economic depression of 1873), without which, the phenomenon either would not have occurred, or might have taken on a different form. Alice L Conklin and Ian Christopher Fletcher, European Imperialism (ed.) (1830-1930), (Boston 1999) p. 1. Imperialism, according to Hobsbawm was 'the child of an era of competition between rival industrial-capitalist national economies which was new and which was intensified by the pressure to secure and safe-guard markets in a period of business uncertainty."" Lenin also suggested that 'Imperialism emerged as the development and direct continuation of the fundamental characteristics of capitalism in general."" Despite the acknowledged importance of economics, there is a tendency for Marxist historians to over-emphasise its role. 'The importance of economic interests and calculation in the expansion of European influence and control over the extra-European world is undeniable."" It is one thing to concede the importance of economic interests, and another to assert that imperialism was primarily an economic phenomenon. Eric Hobsbawm, Age of Empires 1875-1914, (New York, 1989) p.64. Conklin and Fletcher, European Imperialism (ed. ), V.I Lenin on 'Imperialism, the highest stage of Capitalism', p. 38. Andrew Porter, European Imperialism (1860-1914), (Malaysia, 1994), p. 38. 'New' imperialism was only possible in the light of rapid industrialisation, which peaked during the mid-nineteenth century. Industrialisation required raw materials such as tin and iron to make steel, which could be obtained cheaply in the colonies; the movement from rural to urban areas, the subsequent shift from agriculture to industries, and the surge in population put severe strain on food production in the metropolis, which would be alleviated if the colonies took on the function of being the 'granaries of Europe'. Also, with the efficiencies of machinery, colonies could provide the markets needed to absorb the increased production. 'Moreover, cheaper markets were required to offset the general fall in prices from the 1870s to the 1890s (economic depression); with most industrial powers adopting protectionist tariffs, colonies could serve both needs."" Finally, there is the theory of 'surplus capital' as expounded by Hobson 'aggressive imperialism...is a source of great gain to the investor who cannot find at home the profitable use he seeks for his capital, and insists that his government should help him to profitable and secure investments abroad...'  Porter, European Imperialism, p. 39. Conklin and Fletcher, European Imperialism (ed. ), J.A. Hobson on 'The New Imperialism', p. 15. It would seem that all of the above mentioned was best associated with British imperialism more than any other. At a glance, Britain's colonies were the most economically viable, with particular regards to India and South Africa. Using the data from the geographical distribution of British long-term capital investment overseas (as of 1913), out of the 1,780 million pounds invested in her empire, 378.8 million pounds was invested in India and Ceylon, while 370.2 million pounds went to South Africa. Moreover, the existence of chartered companies, such as George Goldie's Royal Niger Company, Sir William Mackinnon's British East Africa Company, Cecil Rhodes' British South Africa Company and the British East India Company (EIC) in Asia would seek to reinforce the pattern of British colonial rule: the ideal situation being that of an informal empire, which would allow effective economic exploitation without the additional costs of setting up a governing administrative body and the moral responsibility attached to a formal colony. Porter, European Imperialism, p. 41, table 1. There are however, cracks in the theory of over-riding economic concerns in British imperialism. Firstly, it must be said that possessions that paid off handsomely, such as India and South Africa, were extremely rare and were lacking amongst the possessions of the other colonial powers. Hence 'with few exceptions, colonial trade remained throughout this period an insignificant proportion of metropolitan commerce...'. Instead, most of Europe's long-term overseas investment went significantly to areas outside of the formal colonial empires: French capital went into Russian bonds, German to Austria-Hungary and British to Latin America (in particular Argentina) and the other white-settlement former colonies (Canada and Australia). Moreover, very few colonies were actually profit-making entities, with the Germans colonising large areas of the African desert, and French indo-china being incomparable to the economically viable British colonies in the region. Porter, European Imperialism, p. 40. The role of ideology in imperialism cannot be overlooked, though many have dismissed it as a cloak for economic exploitation, there is still much to be said about Social Darwinism, the ""White Man's Burden"" and the moral responsibility attached to it. In the light of the Enlightenment ideas of the link between change and progress, and Darwin's theory of evolution, to remain stagnant was to be weak. This change in ideas also marked a change in the perceptions of the colonial masters, who increasingly stressed the difference between the cultures of the metropolis and the colonies. Also, the advent of Science in promoting such racist ideas succeeded in explaining irrational beliefs in rational terms '...evolutionary explanation of racial differentiation and the mechanisms of natural selection seemed to explain and justify cultural differences in terms of inherent racial capacity and a natural hierarchy..."" This sense of superior culture brought with it the moral responsibility, that 'societies which had travelled ahead...had some duty to offer guidance, instruction and even to rule."" This was manifested in the ways the colonial masters tried to 'educate' the natives about proper 'Western' hygiene, setting up public schools to educate the natives in the colonial language, proper dressing (wearing shoes), western consumerism, and very importantly, through the spread of religion (Christian or Catholic faith). Another feature of the 'White Man's Burden' would be that of humanitarian concerns (whether rhetoric or reality), such as the abolishment of slavery in all French and British colonies, as the parallel movement alongside economic exploitation. Ibid, p.24. Phillip Darby, Three faces of Imperialism (British and American Approach to Asia and Africa 1870-1970), (Avon, 1987), p. 31. Though all the colonial powers claimed to be civilising their colonies, it was 'only in republican France (that) this claim (was) elevated to the realm of official imperial doctrine."" Nineteenth century France was the great missionary power of the Roman Catholic world, and it can be argued that the colonisation of Vietnam was partly the result of the French desire to protect their missionaries in Vietnam from official persecution. However, the French were unique in believing in the universality of French values, and the perfectibility of human kind, and this perception was not evident amongst the other European colonial masters, who generally, as a rule avoided 'over-education' of the natives. Alice Conklin , A Mission to Civilise, the republican idea of empire in France and West Africa 1895-1930, (California 1997).p. 1. To look at imperialism from a cynical point of view, it must be said that where economic and humanitarian interests were conflicting, the former would always triumph. Empirical observations regarding the sale of opium to Britain's Southeast Asian colonies, and that of 'trader gin' (a noxious but potent narcotic) in West Africa are telling of the priority of economic over humanitarian concerns. Abolishment of slavery was possible because it benefited both parties. Also, the presence of missionaries/explorers such as David Livingston could not compete with that of the international community of businessmen and investors. A very poignant example would be that of Belgium Congo, where King Leopold colonised it on the basis of 'humanitarian reasons', only to use the local population as slave labourers. Though it is extremely difficult to argue that cultural influences were the direct causes of European imperialism, it nevertheless 'assisted in creating the general circumstances within which specific instances of imperial domination...became for contemporaries, not only imaginable but acceptable."" The consequences of cultural transfers are still seen in our modern world, where the colonies largely taken on the parliamentary model of their former colonial masters. This change has its roots in the era of colonisation where the indigenous political and often social hierarchy was replaced by the political institutions and protocol of the metropolis. The adoption of the colonial language as the working language, the western architecture of new buildings and the assimilation into the cash-economy, are all examples of the transfer of culture. Similarly, there were many ways in which the natives have inspired European art and lifestyle. Porter, European Imperialism, p. 27. An examination of purely economic and cultural aspects of imperialism has failed to truly grasp the essence of this significant event in history. The economic and cultural aspects were more of consequences rather than the causes of imperialism, and should be put in their correct places, under the broader themes of Power, Rivalry (stemming from Nationalism), Fear and Regeneration, which are interrelated concepts. Even so, the economic and cultural consequences were in fact additional benefits to be gained from imperialism, and not the actual aims that the imperialists wanted to achieve. The acquiring of colonies was seen by many countries as the means by which they could attain the status of world power. This is particularly so for new countries like Italy and Germany, which saw their participation in imperialism as heightening their world standing. For an established nation like Britain however, imperialism was a defensive reaction, derived from a sense of insecurity and threats from newly industrialising powers like Germany, America and Russia, and the subsequent assertion of her power. As for France, she saw imperialism as the opportunity for the expansion of her army and navy; the role successful imperialism would play in exorcising her defeat in the Franco-Prussian war of 1871, and rebuild her status as a world power. Germany's fear of encirclement also played a part in the formulation of 'Weltpolitik'. It was not only the external fear that countries were trying to undermine each other in the race for colonies, but more importantly, there was the fear of internal degeneration, and that imperialism was the cure to the diagnosis of national ills, revitalising the country, restoring its self confidence, and reinforcing its power abroad. Imperialism was often the exclusive prerogative of a small group of elites, but when the issue of national pride was at stake, and rivalry ensued, the public could often be aroused into supporting imperialist policies (clearly seen in the example of France.) Germany's aggressive 'Weltpolitik' (World policy) to secure her 'place in the sun' was an extension of the Anglo-German rivalry towards the end of the nineteenth century. Perhaps it can be suggested that, the above arguments regarding economic and cultural aspects were merely the manifestation of the broader concept of European rivalry. Linked to this concept is the theme of nationalism. The situation in Europe was balancing on a knife-edge after the unification of Italy and Germany, and the outward movement of expanding into a vast territory of unconquered plains was profitable not only as a safe outlet for the consumptive force of nationalism, but also to maintain the balance of power in Europe (Africa as the periphery of Europe). Finally, there is the suggestion put forward by some historians that it was the domestic situation in Europe itself that led to imperialism. Social Imperialism, whereby '...expansionist policies are consciously devised to unite the nation and to defuse tensions at home, while simultaneously avoiding significant domestic reforms."" In addition, the German 'Sammslungpolitik' (the gathering together of the elements that would naturally support the state) against the domestic threat from the Social Democrats, which coincided with their enthusiasm for imperialism, would seek to reinforce this theory. Porter, European Imperialism, p.32. In conclusion, it is inadequate to understand imperialism primarily either as an economic or cultural phenomenon, simply because both were secondary concerns in the impetus to colonise. Economic determinism is the prerogative of Marxist historians, and it should be kept this way; for the vast majority of the other historians who have attempted to write about imperialism have placed due recognition to the other factors that have contributed to the motives of imperialism, which provide a more balanced view of this phenomenon. Moreover, the question specifically refers to European, and not British imperialism, hence, the assertion of economic interests as the primary force in imperialism would seem highly Anglo-centric in the light of the arguments put forth in this essay. It is perhaps more accurate to see the economic and cultural aspects as being the consequences of imperialism, and since 'primarily' suggests some sense of causation, it would be myopic to insist upon a direct answer to this question.",True
89,"The debate on the memory of the Holocaust is perhaps the latest issue in the vast ocean of literature concerning other aspects of the Nazi holocaust, and this trend can be traced to the 1980s and 1990s when there was a sudden proliferation of Holocaust memorials, in the form of books, films, paintings and in particular, monuments and museums. The academic value of this enterprise must be judged, and the historian Norman Finkelstein astutely points out that 'currently all the rage in the ivory tower, memory is surely the most impoverished concept to come down the academic pike in a long time."" Yet despite this, 'the intensity of the debates about memory and the Holocaust has never abated."" The task at hand is perhaps even more disdainful because it is expressed in a normative statement, and seemingly, it beckons to the writer to take part in a doomed enterprise. The role of the impartial judge is one that is hard to play, and though it is acknowledged that the historian's task is not to judge, it is not entirely possible for the historian to distance herself from this issue, because she is necessarily the product of her time. As such, the writer does not profess impartiality, but attempts to bring in the professionalism of her discipline to tackle this issue by first calling into question the assumption that the Holocaust should be memorialised; the social and political functions of these memorials; and finally, by examining the various memorials, aim to show that the forms that memorials take often reflect their position in time and location, and more importantly, is a reflection of the society that has created them, hence the futility of this question, because the form that a memorial takes is not dictated by a single universal principle, but obeys a number of intertwined forces. Norman Finkelstein, The Holocaust Industry: Reflections on the Exploitation of Jewish suffering, (New York, 2001), p.5. Caroline Wiedmer, The Claims of Memory: Representations of the Holocaust in Contemporary Germany and France, (New York, 1999), p. 4. It is imperative to inquire into the assumption of the above question by first examining the basis by which the Holocaust should be memorialised. Should the Holocaust be memorialised on the basis of its uniqueness, as some historians have been fast to claim? If the Holocaust was truly so unique, does it not preclude the same event from ever happening again? So why is there this supposed need to commemorate for remembrance? What does one aim to achieve by commemorating it? The above questions become even more pressing with the emergence of the Holocaust Industry. Is it not ironic that the proliferation of monuments comes only in the later half of the twentieth century when the generation of survivors had largely perished? Perhaps it is because of the passing of private, living memory of the Holocaust that a vicarious and surrogate public, fixed interpretation of the past is promoted. 'Collective memory, it would seem, begins where the individual memories of a group cease, along with its members, to exist."" Or maybe it is precisely because so many decades have passed from the event that people now dare to look back at their past, and that 'the further events of World War Two recede into time, the more prominent its memorials become."" The flourishing of memorial politics in France, Germany, Israel and America do perhaps lend this question a certain direction. In places such as Germany and France where the Jews were persecuted, the upsurge of a need to know the past was perhaps a reaction against the code of silence and the 'absolute negation of the black years (les années noires)."" Building memorials was therefore seen as safeguarding the memory of the Holocaust against the cover-up of the State. Memorialisation however, is a double-edged sword, and the late historian Martin Broszat cautioned that monuments may not remember events as to bury them beneath layers of national myths and explanations. Hence, as cultural reifications, monuments coarsen historical understanding as much as they generate it. Caroline Wiedmer, The Claims of Memory, p.57. James E. Young, The Texture of Memory: Holocaust Memorials and Meaning, (Michigan, 1993), p.1. Caroline Wiedmer , The Claims of Memory, p. 33 As the title of the question suggests, what is being commemorated is not the historical event (Nazi holocaust), but rather, ideological interpretations of it. Films (such as Holocaust the television series, Schindler's List, The Pianist, Life is Beautiful etc.) in particular fuel such interpretations of the holocaust, and by trying to recreate its likeness, is in a sense trivializing it. There is not one nazi holocaust, but many versions of the real thing. Despite the attempted homage to the holocaust, the necessary dramatization of the real event in order to make it more palatable for Mass Culture is perhaps a sacrilege to the memory of the holocaust, because it distorts how one perceives it. However, this is not to deny the fact that 'such cinematic dramatizations of the holocaust have helped to shape memory of the Holocaust in the popular consciousness' allowing such films to become defenders of memory. The films raised awareness amongst the generations that had no connection to the Holocaust, and as without knowledge there is no memory, memorialisation of the Holocaust is a necessary evil. Judith E. Doneson, Holocaust Revisited: A Catalyst for Memory or Trivialisation, The Annals of The American Academy of Political and Social Science, Vol. 548, November 1996, (California, 1996), p.70. Determining the functions of the Holocaust memorials is integral to the answering of the question. How the Holocaust is memorialised, and the form it takes is largely dependant on the perceived functions of the memorials. Memorials can be built as a result of a government's need to explain a nation's past to itself (in the case of Germany and France); to educate the next generation and to inculcate a sense of shared experience and destiny; as expiations of guilt and self-aggrandizement; and even to attract tourists. An extreme case in which a memorial was built for political ends was when President Jimmy Carter proposed the building of a national memorial in Washington D.C. in order to placate Jewish supporters angered by his sale of F-15 fighter planes to Saudi Arabia. Much of the USA's enthusiasm for memorial building was in part due to the declaration of solidarity with the influential American Jews, as well as in support of her ally Israel. Similarly, the initial code of silence surrounding the Holocaust immediately after the war was due to the more urgent need for reconciliation with West Germany, in the face of the Communist threat. Hence, as James E. Young concluded 'All such memorial decisions are made in political time, contingent on political realities.' James E. Young, The Texture of Memory, p. 285. Memorials are also essential to the foundations of political legitimacy in that 'by themselves, monuments are of little value, mere stones in the landscape. But as a part of a nation's rites or the objects of a people's national pilgrimage, they are invested with national soul and memory.' This is particularly true in the case of Israel, whose political foundations rest firmly on the back of the Holocaust, to the extent that according to the historian Peter Baldwin, 'the singularity of the Jewish suffering adds to the moral and emotional claims that Israel can make ...on other nations."" Having defined themselves as a people through commemorative recitations of their past, 'the Jews now depend on Memory for their very existence as a nation.' Norman Finkelstein, The Holocaust Industry, p. 48. James E. Young, The Texture of Memory, p. 211. The social functions that are fulfilled by memorial building can be quite diverse. On the one hand, it is useful for consensus building: 'in the absence of shared belief or common interests, Art in public spaces may force an otherwise fragmented populace to frame diverse values and ideals in common spaces. By creating common spaces for memory, monuments propagate the illusion of common memory."" On the other hand, memorials relieve the population of the burden to remember, 'under the illusion that our memorial edifices will always be there to remind us, we take leave of them and return only at our own convenience."" Despite the various facets of remembrance, there is a general consensus that monuments are meant for public consumption, and hence, a part of the answer to the question of how the Holocaust should be memorialised would rest on public opinion and expectations. James E. Young, The Texture of Memory, p.6. Ibid, p.5. The main debates on the form a Holocaust memorial should take will be discussed here, under the main themes of Abstract vs. Realist, Jews vs. Non Jews, Secular vs. Religious and the anti-monument. It would be worth noting that the backing of the State, as well as public opinion, is necessary for the memorial to serve its purpose. In every case, Holocaust memorials reflect not only national and communal remembrance, but also the memorial designer's own time and place. For contemporary artists (working in an era of abstract expressionism, conceptual art, post-modern and de-constructionist design), though there is a recognition that they are accountable to both art and memory, Albert Elsen reminds us that for them 'the needs of art, not the public memory come first."" However, this is in direct opposition to many survivors, who 'believe that the searing reality of their experiences demand as literal a memorial expression as possible.' Hence the dilemma of the artist, an example of whom is Nathan Rapoport, whose Warsaw Ghetto Monument 1948,is probably the most widely known, celebrated and controversial. Conversely, Sol LeWitt's unconventional Memorial to the Missing Jews, 1989, in Germany had to bow to popular opinion to have it removed from the Platz der Republic, in Hamburg. Ibid, p.8 It must be remembered that Jews, as well as non-Jews perished during the Nazi holocaust, and it should be the latter's entitlement to share in the remembrance and mourning of the former. In a particularly volatile situation in 1988 Germany, Leah Ross, a journalist demanded that a memorial be set up for Jews alone. This provoked an outcry from Romani Rose of the Central Council of the Sinti and Roma that this 'represented a hierarchization of victims, which is insulting for the victims of the genocide and for the survivors of the Sinti and Roma majority."" It would make sense in Israel to dedicate the memorial exclusively to Jews, but given the racial composition in Germany, it would be impertinent to do so. Caroline Wiedmer, The Claims of Memory, p. 144. Due to the intertwined relationship of Jewish suffering (the Shoah) and religion, commemoration of the former inevitably involves the latter. As James E. Young suggests, 'Israel's over-arching national ideology and religion may be memory."" Victims of the Shoah are portrayed as martyrs who died for their faith, fighters who are remembered for their part in the state's founding. Contrast this to the secular state of France, who, during one of the national competitions for the Vél d'Hiv Monument, picked the literal figurative design of Walter Spitzer over that of Shelomo Selinger because the ministry felt that the latter's proposal was 'Too Jewish.' As such, it is highly apparent that how the Holocaust should be memorialised depends largely on cultural and national factors. James E. Young, The Texture of Memory, p. 210. An interesting development would be that of the counter-monument. This is particularly so with regard to the new generation of German artists, who 'have a deep distrust of monumental forms in light of their systematic exploitation by the Nazis, and a profound desire to distinguish their generation from that of the killers through memory."" These artists explore both the necessity of memory, and their incapacity to recall events they never experienced directly, hence in capturing the memory of events, remember therefore only their own relationship to it. The Gerzes' counter-monument is perhaps the best example of this. Theirs is a self-abnegating monument, a 'vanishing monument' which, instead of graciously accepting the burden of memory, throws it back at the town's feet, forcing them to remember for themselves. This audaciously simple idea challenges the idea of monumentality and the permanence of collective memory, and provokes the population to examine themselves as part of the piece's performance. Ibid, p.27. The writer's main emphasis on public monuments reflects her bias in thinking that such forms of public art are more effective in inculcating a common sense of destiny, and fulfilling the social and political functions attached to them, more so than films, books and painting can. With the boom of the Holocaust industry however, films have become an essential medium by which large populations can learn more about the Holocaust. The writer has avoided a direct answer to the question up to this point, but has attempted to illustrate the many debates and considerations that have arise from the question. In conclusion, the Holocaust should be memorialised according to its time and location, asserting its relevance in the lives of the denizens there. Also, it is worth noting that 'new generations visit memorials under new circumstances and invest them with new meanings', and since there is no one universally accepted way of memorialising the Holocaust, the normative connotation posed by the question above is self-defeating, and ought not to be answered by a historian. The history of empires is a long one: beginning with the Greek, Roman and Persian empires, which pitted race against race; the empires of Spain, Portugal, the Netherlands and Britain in the sixteenth and seventeenth century, which saw the rivalry between monarchies; and finally, the 'New' Imperialism of the mid-nineteenth century, which saw a renewed interest in and frantic scramble for, new colonies, between nation-states. 'By early twentieth century, Britain, France, Germany, Italy, Belgium, the Netherlands, Denmark, Spain, and Portugal together held sway over almost 84 percent of the earth's surface."" Imperialism had become not only fashionable, but also defined the era from the mid-nineteenth onwards to World War I. Despite many attempts to attach priority to a certain aspect of imperialism as being the driving force behind the phenomenon, it is perhaps more helpful to understand it as a product of a complex web of motives and concerns. Also, imperialism must be examined in the context of its time (post-enlightenment ideas, rapid industrialisation, the formation of scientific theories which supported the superiority of one race over another, nationalism, and the economic depression of 1873), without which, the phenomenon either would not have occurred, or might have taken on a different form. Alice L Conklin and Ian Christopher Fletcher, European Imperialism (ed.) (1830-1930), (Boston 1999) p. 1. Imperialism, according to Hobsbawm was 'the child of an era of competition between rival industrial-capitalist national economies which was new and which was intensified by the pressure to secure and safe-guard markets in a period of business uncertainty."" Lenin also suggested that 'Imperialism emerged as the development and direct continuation of the fundamental characteristics of capitalism in general."" Despite the acknowledged importance of economics, there is a tendency for Marxist historians to over-emphasise its role. 'The importance of economic interests and calculation in the expansion of European influence and control over the extra-European world is undeniable."" It is one thing to concede the importance of economic interests, and another to assert that imperialism was primarily an economic phenomenon. Eric Hobsbawm, Age of Empires 1875-1914, (New York, 1989) p.64. Conklin and Fletcher, European Imperialism (ed. ), V.I Lenin on 'Imperialism, the highest stage of Capitalism', p. 38. Andrew Porter, European Imperialism (1860-1914), (Malaysia, 1994), p. 38. 'New' imperialism was only possible in the light of rapid industrialisation, which peaked during the mid-nineteenth century. Industrialisation required raw materials such as tin and iron to make steel, which could be obtained cheaply in the colonies; the movement from rural to urban areas, the subsequent shift from agriculture to industries, and the surge in population put severe strain on food production in the metropolis, which would be alleviated if the colonies took on the function of being the 'granaries of Europe'. Also, with the efficiencies of machinery, colonies could provide the markets needed to absorb the increased production. 'Moreover, cheaper markets were required to offset the general fall in prices from the 1870s to the 1890s (economic depression); with most industrial powers adopting protectionist tariffs, colonies could serve both needs."" Finally, there is the theory of 'surplus capital' as expounded by Hobson 'aggressive imperialism...is a source of great gain to the investor who cannot find at home the profitable use he seeks for his capital, and insists that his government should help him to profitable and secure investments abroad...'  Porter, European Imperialism, p. 39. Conklin and Fletcher, European Imperialism (ed. ), J.A. Hobson on 'The New Imperialism', p. 15. It would seem that all of the above mentioned was best associated with British imperialism more than any other. At a glance, Britain's colonies were the most economically viable, with particular regards to India and South Africa. Using the data from the geographical distribution of British long-term capital investment overseas (as of 1913), out of the 1,780 million pounds invested in her empire, 378.8 million pounds was invested in India and Ceylon, while 370.2 million pounds went to South Africa. Moreover, the existence of chartered companies, such as George Goldie's Royal Niger Company, Sir William Mackinnon's British East Africa Company, Cecil Rhodes' British South Africa Company and the British East India Company (EIC) in Asia would seek to reinforce the pattern of British colonial rule: the ideal situation being that of an informal empire, which would allow effective economic exploitation without the additional costs of setting up a governing administrative body and the moral responsibility attached to a formal colony. Porter, European Imperialism, p. 41, table 1. There are however, cracks in the theory of over-riding economic concerns in British imperialism. Firstly, it must be said that possessions that paid off handsomely, such as India and South Africa, were extremely rare and were lacking amongst the possessions of the other colonial powers. Hence 'with few exceptions, colonial trade remained throughout this period an insignificant proportion of metropolitan commerce...'. Instead, most of Europe's long-term overseas investment went significantly to areas outside of the formal colonial empires: French capital went into Russian bonds, German to Austria-Hungary and British to Latin America (in particular Argentina) and the other white-settlement former colonies (Canada and Australia). Moreover, very few colonies were actually profit-making entities, with the Germans colonising large areas of the African desert, and French indo-china being incomparable to the economically viable British colonies in the region. Porter, European Imperialism, p. 40. The role of ideology in imperialism cannot be overlooked, though many have dismissed it as a cloak for economic exploitation, there is still much to be said about Social Darwinism, the ""White Man's Burden"" and the moral responsibility attached to it. In the light of the Enlightenment ideas of the link between change and progress, and Darwin's theory of evolution, to remain stagnant was to be weak. This change in ideas also marked a change in the perceptions of the colonial masters, who increasingly stressed the difference between the cultures of the metropolis and the colonies. Also, the advent of Science in promoting such racist ideas succeeded in explaining irrational beliefs in rational terms '...evolutionary explanation of racial differentiation and the mechanisms of natural selection seemed to explain and justify cultural differences in terms of inherent racial capacity and a natural hierarchy..."" This sense of superior culture brought with it the moral responsibility, that 'societies which had travelled ahead...had some duty to offer guidance, instruction and even to rule."" This was manifested in the ways the colonial masters tried to 'educate' the natives about proper 'Western' hygiene, setting up public schools to educate the natives in the colonial language, proper dressing (wearing shoes), western consumerism, and very importantly, through the spread of religion (Christian or Catholic faith). Another feature of the 'White Man's Burden' would be that of humanitarian concerns (whether rhetoric or reality), such as the abolishment of slavery in all French and British colonies, as the parallel movement alongside economic exploitation. Ibid, p.24. Phillip Darby, Three faces of Imperialism (British and American Approach to Asia and Africa 1870-1970), (Avon, 1987), p. 31. Though all the colonial powers claimed to be civilising their colonies, it was 'only in republican France (that) this claim (was) elevated to the realm of official imperial doctrine."" Nineteenth century France was the great missionary power of the Roman Catholic world, and it can be argued that the colonisation of Vietnam was partly the result of the French desire to protect their missionaries in Vietnam from official persecution. However, the French were unique in believing in the universality of French values, and the perfectibility of human kind, and this perception was not evident amongst the other European colonial masters, who generally, as a rule avoided 'over-education' of the natives. Alice Conklin , A Mission to Civilise, the republican idea of empire in France and West Africa 1895-1930, (California 1997).p. 1. To look at imperialism from a cynical point of view, it must be said that where economic and humanitarian interests were conflicting, the former would always triumph. Empirical observations regarding the sale of opium to Britain's Southeast Asian colonies, and that of 'trader gin' (a noxious but potent narcotic) in West Africa are telling of the priority of economic over humanitarian concerns. Abolishment of slavery was possible because it benefited both parties. Also, the presence of missionaries/explorers such as David Livingston could not compete with that of the international community of businessmen and investors. A very poignant example would be that of Belgium Congo, where King Leopold colonised it on the basis of 'humanitarian reasons', only to use the local population as slave labourers. Though it is extremely difficult to argue that cultural influences were the direct causes of European imperialism, it nevertheless 'assisted in creating the general circumstances within which specific instances of imperial domination...became for contemporaries, not only imaginable but acceptable."" The consequences of cultural transfers are still seen in our modern world, where the colonies largely taken on the parliamentary model of their former colonial masters. This change has its roots in the era of colonisation where the indigenous political and often social hierarchy was replaced by the political institutions and protocol of the metropolis. The adoption of the colonial language as the working language, the western architecture of new buildings and the assimilation into the cash-economy, are all examples of the transfer of culture. Similarly, there were many ways in which the natives have inspired European art and lifestyle. Porter, European Imperialism, p. 27. An examination of purely economic and cultural aspects of imperialism has failed to truly grasp the essence of this significant event in history. The economic and cultural aspects were more of consequences rather than the causes of imperialism, and should be put in their correct places, under the broader themes of Power, Rivalry (stemming from Nationalism), Fear and Regeneration, which are interrelated concepts. Even so, the economic and cultural consequences were in fact additional benefits to be gained from imperialism, and not the actual aims that the imperialists wanted to achieve. The acquiring of colonies was seen by many countries as the means by which they could attain the status of world power. This is particularly so for new countries like Italy and Germany, which saw their participation in imperialism as heightening their world standing. For an established nation like Britain however, imperialism was a defensive reaction, derived from a sense of insecurity and threats from newly industrialising powers like Germany, America and Russia, and the subsequent assertion of her power. As for France, she saw imperialism as the opportunity for the expansion of her army and navy; the role successful imperialism would play in exorcising her defeat in the Franco-Prussian war of 1871, and rebuild her status as a world power. Germany's fear of encirclement also played a part in the formulation of 'Weltpolitik'. It was not only the external fear that countries were trying to undermine each other in the race for colonies, but more importantly, there was the fear of internal degeneration, and that imperialism was the cure to the diagnosis of national ills, revitalising the country, restoring its self confidence, and reinforcing its power abroad. Imperialism was often the exclusive prerogative of a small group of elites, but when the issue of national pride was at stake, and rivalry ensued, the public could often be aroused into supporting imperialist policies (clearly seen in the example of France.) Germany's aggressive 'Weltpolitik' (World policy) to secure her 'place in the sun' was an extension of the Anglo-German rivalry towards the end of the nineteenth century. Perhaps it can be suggested that, the above arguments regarding economic and cultural aspects were merely the manifestation of the broader concept of European rivalry. Linked to this concept is the theme of nationalism. The situation in Europe was balancing on a knife-edge after the unification of Italy and Germany, and the outward movement of expanding into a vast territory of unconquered plains was profitable not only as a safe outlet for the consumptive force of nationalism, but also to maintain the balance of power in Europe (Africa as the periphery of Europe). Finally, there is the suggestion put forward by some historians that it was the domestic situation in Europe itself that led to imperialism. Social Imperialism, whereby '...expansionist policies are consciously devised to unite the nation and to defuse tensions at home, while simultaneously avoiding significant domestic reforms."" In addition, the German 'Sammslungpolitik' (the gathering together of the elements that would naturally support the state) against the domestic threat from the Social Democrats, which coincided with their enthusiasm for imperialism, would seek to reinforce this theory. Porter, European Imperialism, p.32. In conclusion, it is inadequate to understand imperialism primarily either as an economic or cultural phenomenon, simply because both were secondary concerns in the impetus to colonise. Economic determinism is the prerogative of Marxist historians, and it should be kept this way; for the vast majority of the other historians who have attempted to write about imperialism have placed due recognition to the other factors that have contributed to the motives of imperialism, which provide a more balanced view of this phenomenon. Moreover, the question specifically refers to European, and not British imperialism, hence, the assertion of economic interests as the primary force in imperialism would seem highly Anglo-centric in the light of the arguments put forth in this essay. It is perhaps more accurate to see the economic and cultural aspects as being the consequences of imperialism, and since 'primarily' suggests some sense of causation, it would be myopic to insist upon a direct answer to this question.","Hitler's popularity amongst the majority of the German people have been variously analysed by historians, and though the percentages are debatable, it is generally recognized that his popularity was of fundamental importance to the functioning of the third Reich. As Ian Kershaw suggested 'The adulation of Hitler by millions of Germans who might otherwise have been only marginally committed to Nazism meant that the person of the Führer, as the focal point of basic consensus, formed a crucial integratory force in the Nazi system of rule."" Despite the concession that Hitler, or rather the 'Hitler myth' played an important role in the third Reich, it does not follow that the influence was necessarily advantageous for Nazi Germany as a dictatorship. The nature of this question is quite deceptive, and the search for a direct answer can be elusive, due to the fact that the line between strength and weakness may be obscured. Hence, what may initially appear to have strengthened the Third Reich may in the long term prove to be the ultimate de-stabilising factor, threatening the very foundations that the Reich was built upon. To reach a conclusion regarding this question, it is imperative to examine the effects of Hitler's 'charismatic rule' on the following: the German people, the issue of legitimacy, the state bureaucracy, the party, and the resistance movement. Ian Kershaw, The 'Hitler Myth', (Oxford, 1989), p.1. From the start, the German people were never adverse towards the concept of dictatorship. Their distrust and contempt for the system of liberal democracy that Weimar represented was pervasive throughout society. There was a 'national' yearning for a Caesar figure to pull Germany out of the economic doldrums, and to lead Germany to reclaiming her former glory. Hence, Hitler's 'charismatic rule' was seen as a welcome and much needed respite to the inefficient Weimar Republic, and did in fact appeal to the romantic within each person- the harking back to the Second Reich of Bismarck. This acceptance of the rule of a 'strong man' made possible the tolerance of Party dictatorship of the Nazis, and the subsequent new state bureaucracy. Also, Hitler's style of leadership, the propaganda machinery of floodlights and military processions created the façade of participation, that made the regime seem like an 'enlightened dictatorship', thus reconciling the people with the loss of their political vote. Judging from the plebiscites held in 1933, 34, 36 and 38, '...though no doubt the product of intense propaganda and coercion, nevertheless reflected genuine widespread approval and admiration for Hitler's accomplishments, and persuaded the masses to fall in line."" The extension of the personal loyalty of the population from Hitler to the state would strengthen the regime, and lessen the impact of the transition from a liberal democracy to a dictatorship. ibid. p. 258. It is imperative to examine Hitler's 'charismatic rule' as a claim to the legitimacy of the Third Reich. Max Weber in 'Economy and Society' expounds on the ideal type of 'charismatic authority' by which the leader is seen as an exemplary character of superhuman powers, and derives his backing from the personal loyalty of his subjects. This description would seem to fit Hitler's style of leadership in the Third Reich. To a large extent, it gave legitimacy, which was lacking during the Weimar republic, to the Third Reich, and allowed it to function with the support of the people. This would appear to have strengthened the Third Reich as a dictatorship because it portrayed Hitler not as a usurper of power, but merely fulfilling the will of the people (reliance on personal loyalty). However, 'if those to whom he feels sent do not recognize him, his claim collapses; if they recognize it, he is their master as long as he proves himself."" The conditional status of his power can be de-stabilising in that Hitler is under constant pressure to bring in success. Failure, or a series of failure (Operation 'Barbarossa' against Russia) would mean that the mandate was passing out of the hands of Hitler. Routinisation (the lapse back into normality) was also a threat to a regime that innately relied on dynamism to rule. Hence, the 'charismatic' form of leadership was 'an extraordinary, unstable and therefore transient form of rule, tending to arise in unusual or crisis conditions, and directed not at the solutions of everyday problems of government, but at the over-coming of supra-dimensional crises and emergencies."" With the above question in mind, the inherent instability of the regime is definitely a point to consider, when evaluating the impact of 'charismatic authority' on the dictatorship. With the prospect of normalcy approaching as Germany emerged from the Great Depression, the Fuhrer had to act in his own interest: the entry into a long-drawn war would cement Hitler's status as the 'messiah' of the German population, and allow for the reign over the 'thousand-year Reich'. This apparent inevitability of war if the Nazi's were to stay in power was risk-taking (and in some aspects, miscalculation) on the part of Hitler and his cronies, because the fate of the party was now tied up with the outcome of the war. Max Weber, Economy and Society, ed. G Roth and C. Wittich, (Berkeley, 1978), p. 241. Kershaw, The 'Hitler Myth'. p.8. The emphasis on personal loyalty to Hitler in the system meant that there was in reality no bureaucracy to speak of, but rather, 'the administrative structure of Nazi Germany formed a complex mosaic of party and state agencies with ill-defined and over-lapping jurisdictions, sometimes complementing each other, more often mutually antagonistic, all striving to obtain a monopoly of power in their own domain."" This polycractic and inefficient structure, whereby the duplication of jobs, confusion amongst the various ministries, and internal rivalry to gain the backing of the Fuhrer, can be said to have undermined the political system in Germany. As Carr pointed out 'Confusion about the direction of policy was not restricted to the immediate circle around the Fuhrer. The whole structure of government has been aptly dubbed ""authoritarian anarchy""."" Also, as Ian Kershaw observed 'the demise of collective centralised government (the Cabinet never met again after 1938) promoted the disintegration of government into a proliferation of departments with ministries working largely independently of each other."" Should the confusion be seen as the necessary and inevitable outcome of Hitler's 'charismatic rule' hence weakening the system from within; or can it be, that this was a deliberate policy of Hitler to encourage '...confusion and uncertainty as a means of consolidating his power', thus the implication that a strong and omnipotent dictator would unite the will of Germany within himself, making the Third Reich stronger as a personal (rather than party) dictatorship, because the power struggle at the bottom would not be able to penetrate the office held by the Fuhrer. William Carr, Hitler: A study in personality and politics, (Bath, 1978), p. 41. Ibid. Ian Kershaw, The Nazi Dictatorship, (Cornwall, 2000), p. 75. Carr, Hitler, p. 43. Where did Hitler feature in this chaotic system of rule? It has been said that '...the symbolic Fuhrer authority was more important than the direct governing will of the person Hitler', and to a large extent it is true, because the presence of the Fuhrer was largely absent in the day-to-day running of the government. For his part, Hitler encouraged Social Darwinism within the party and state, allowing competing interests to fight it out amongst themselves, through which, a victor emerges, or a consensus is reached, which would then have the backing of the Fuhrer. This laissez-faire attitude seemed to have compromised the standing of the Fuhrer on the affairs of the state, which is reinforced by records telling of Hitler's laid-back attitude to work. However, to conclude that Hitler was a weak dictator and that this in turn affected the standing of the Third Reich as a dictatorship would be unjustified. Though Hitler took a very long time to act, 'but when he did, his personal orders, or Fuhrererlasse, cut quickly through the red-tape and ensured compliance with his decisions at least on major matters of policy and within the limits of available resources."" Despite the apparent success of this policy of Social Darwinism, it must be noted that this system 'led inexorably to an accelerating decline into aggression, lawlessness and criminal brutality' that could have undermined the stability of the regime. The system was essentially a very cracked pot, held together only by the 'glue', which was Hitler's authority. Ibid. Carr, Hitler, p. 45. Kershaw, The Nazi Dictatorship, p. 75. The over-reliance on a figure is perhaps one of the inherent weaknesses, as well as strengths of a dictatorship. The centrality of the figure of Hitler in rallying the party together cannot be over-emphasised. Hitler's personal magnetism provided the foundations of charismatic authority within his own entourage and bonded the party leadership to him. It was as Kershaw described ""personal conflicts within the Nazi elite which otherwise would have torn the system apart, were resolved only in Hitler's charismatic authority-in his indisputable position as the base of Nazism's popular legitimacy and the embodiment of Nazism's 'idea'."" It is interesting to note that even though vicious rivalry and power struggles were prevalent within the Nazi party, none of it was aimed at toppling Hitler. And contrary to Stalin's fear of being overthrown, which manifested itself in the purges (Gulags), there was only one purge that Hitler agreed to, and even so, reluctantly-the purging of Ernst Rohm. It must be emphasised that Nazi dictatorship of the Third Reich would not have been possible without Hitler, because the mandate of the people resided in him, and not in the party. The man remained ever popular, despite the party's decline in popularity, owing to some unfavourable economic policies. This would however be interpreted as a strength to some extent, because the party could borrow Hitler's name to gain the much needed power for rule, but yet at the same time, the over-reliance on one man makes it virtually impossible to appoint an heir to replace Hitler as Fuhrer. Hence, when Hitler shot himself in the bunker, the Third Reich died with him. Kershaw, The 'Hitler Myth'. p. 262. Finally, the impact of Hitler's 'charismatic rule' on resistance (or potential resistance) must be considered. Perhaps it would be logical to suggest that because of his immense popularity, those who resisted him were few. Few it seem, could contend with Hitler's deification and cult-like status, to the point that Hitler seemed to a large portion of the population as Germany's 'messiah'. However, this 'Hitler-worship' could have created a backlash against Hitler amongst the devout Christian/Catholic groups. More importantly, the lack of resistance from the traditional elites was in part, the result of Hitler's charismatic rule, which reduced the Army officers from 'power' elites to merely 'functional' elites, thus ensuring that countering and overthrowing Nazi rule would be extremely difficult to do accomplish, even if the Army had the will to do so. Therefore, it was due to the charisma and will of Hitler, that the traditional elites were unable to use him as a puppet for their own purposes. However, charisma alone was inadequate to secure the support of the German population for the Third Reich. Like all other dictatorships, the element of terror was crucial in taming the obstinate 'enemies of the state.' The 'Hitler myth' and terror were therefore two sides of the same coin, 'ensuring political control and mobilisation behind the regime. It is no coincidence that terroristic repression escalated visibly in the final phrase of the waning regime as the binding force of Hitler's popularity weakened and collapsed.' Kershaw, The 'Hitler myth', p. 258. Hitler's 'charismatic rule' worked to the benefit of the regime initially, when the conditions were favourable, and the success that was much desired was obtained. Entry into World War II was inevitable for the survival of the system that inherently thrived on dynamism and periods of emergency, but it was a double-edged sword, in that failure of the war effort would surely bring the Nazi dictatorship to its knees, and it did. As the war started going badly, 'the tide of Hitler's popularity first waned rather slowly, then ebbed sharply-a decline accelerating after Stalingrad, when Hitler's personal responsibility for the catastrophe was widely recognised."" Even the tool of propaganda could not erase the fallibility of the Fuhrer in the eyes of the people, instead, it made the population even more distrustful of the system that they had come to doubt. Therefore, the writer would conclude that ultimately, Hitler's 'charismatic rule' weakened the Third Reich as a dictatorship, due to the innately destructive elements in this concept of legitimacy. Ibid, p.200.",False
90,"The debate on the memory of the Holocaust is perhaps the latest issue in the vast ocean of literature concerning other aspects of the Nazi holocaust, and this trend can be traced to the 1980s and 1990s when there was a sudden proliferation of Holocaust memorials, in the form of books, films, paintings and in particular, monuments and museums. The academic value of this enterprise must be judged, and the historian Norman Finkelstein astutely points out that 'currently all the rage in the ivory tower, memory is surely the most impoverished concept to come down the academic pike in a long time."" Yet despite this, 'the intensity of the debates about memory and the Holocaust has never abated."" The task at hand is perhaps even more disdainful because it is expressed in a normative statement, and seemingly, it beckons to the writer to take part in a doomed enterprise. The role of the impartial judge is one that is hard to play, and though it is acknowledged that the historian's task is not to judge, it is not entirely possible for the historian to distance herself from this issue, because she is necessarily the product of her time. As such, the writer does not profess impartiality, but attempts to bring in the professionalism of her discipline to tackle this issue by first calling into question the assumption that the Holocaust should be memorialised; the social and political functions of these memorials; and finally, by examining the various memorials, aim to show that the forms that memorials take often reflect their position in time and location, and more importantly, is a reflection of the society that has created them, hence the futility of this question, because the form that a memorial takes is not dictated by a single universal principle, but obeys a number of intertwined forces. Norman Finkelstein, The Holocaust Industry: Reflections on the Exploitation of Jewish suffering, (New York, 2001), p.5. Caroline Wiedmer, The Claims of Memory: Representations of the Holocaust in Contemporary Germany and France, (New York, 1999), p. 4. It is imperative to inquire into the assumption of the above question by first examining the basis by which the Holocaust should be memorialised. Should the Holocaust be memorialised on the basis of its uniqueness, as some historians have been fast to claim? If the Holocaust was truly so unique, does it not preclude the same event from ever happening again? So why is there this supposed need to commemorate for remembrance? What does one aim to achieve by commemorating it? The above questions become even more pressing with the emergence of the Holocaust Industry. Is it not ironic that the proliferation of monuments comes only in the later half of the twentieth century when the generation of survivors had largely perished? Perhaps it is because of the passing of private, living memory of the Holocaust that a vicarious and surrogate public, fixed interpretation of the past is promoted. 'Collective memory, it would seem, begins where the individual memories of a group cease, along with its members, to exist."" Or maybe it is precisely because so many decades have passed from the event that people now dare to look back at their past, and that 'the further events of World War Two recede into time, the more prominent its memorials become."" The flourishing of memorial politics in France, Germany, Israel and America do perhaps lend this question a certain direction. In places such as Germany and France where the Jews were persecuted, the upsurge of a need to know the past was perhaps a reaction against the code of silence and the 'absolute negation of the black years (les années noires)."" Building memorials was therefore seen as safeguarding the memory of the Holocaust against the cover-up of the State. Memorialisation however, is a double-edged sword, and the late historian Martin Broszat cautioned that monuments may not remember events as to bury them beneath layers of national myths and explanations. Hence, as cultural reifications, monuments coarsen historical understanding as much as they generate it. Caroline Wiedmer, The Claims of Memory, p.57. James E. Young, The Texture of Memory: Holocaust Memorials and Meaning, (Michigan, 1993), p.1. Caroline Wiedmer , The Claims of Memory, p. 33 As the title of the question suggests, what is being commemorated is not the historical event (Nazi holocaust), but rather, ideological interpretations of it. Films (such as Holocaust the television series, Schindler's List, The Pianist, Life is Beautiful etc.) in particular fuel such interpretations of the holocaust, and by trying to recreate its likeness, is in a sense trivializing it. There is not one nazi holocaust, but many versions of the real thing. Despite the attempted homage to the holocaust, the necessary dramatization of the real event in order to make it more palatable for Mass Culture is perhaps a sacrilege to the memory of the holocaust, because it distorts how one perceives it. However, this is not to deny the fact that 'such cinematic dramatizations of the holocaust have helped to shape memory of the Holocaust in the popular consciousness' allowing such films to become defenders of memory. The films raised awareness amongst the generations that had no connection to the Holocaust, and as without knowledge there is no memory, memorialisation of the Holocaust is a necessary evil. Judith E. Doneson, Holocaust Revisited: A Catalyst for Memory or Trivialisation, The Annals of The American Academy of Political and Social Science, Vol. 548, November 1996, (California, 1996), p.70. Determining the functions of the Holocaust memorials is integral to the answering of the question. How the Holocaust is memorialised, and the form it takes is largely dependant on the perceived functions of the memorials. Memorials can be built as a result of a government's need to explain a nation's past to itself (in the case of Germany and France); to educate the next generation and to inculcate a sense of shared experience and destiny; as expiations of guilt and self-aggrandizement; and even to attract tourists. An extreme case in which a memorial was built for political ends was when President Jimmy Carter proposed the building of a national memorial in Washington D.C. in order to placate Jewish supporters angered by his sale of F-15 fighter planes to Saudi Arabia. Much of the USA's enthusiasm for memorial building was in part due to the declaration of solidarity with the influential American Jews, as well as in support of her ally Israel. Similarly, the initial code of silence surrounding the Holocaust immediately after the war was due to the more urgent need for reconciliation with West Germany, in the face of the Communist threat. Hence, as James E. Young concluded 'All such memorial decisions are made in political time, contingent on political realities.' James E. Young, The Texture of Memory, p. 285. Memorials are also essential to the foundations of political legitimacy in that 'by themselves, monuments are of little value, mere stones in the landscape. But as a part of a nation's rites or the objects of a people's national pilgrimage, they are invested with national soul and memory.' This is particularly true in the case of Israel, whose political foundations rest firmly on the back of the Holocaust, to the extent that according to the historian Peter Baldwin, 'the singularity of the Jewish suffering adds to the moral and emotional claims that Israel can make ...on other nations."" Having defined themselves as a people through commemorative recitations of their past, 'the Jews now depend on Memory for their very existence as a nation.' Norman Finkelstein, The Holocaust Industry, p. 48. James E. Young, The Texture of Memory, p. 211. The social functions that are fulfilled by memorial building can be quite diverse. On the one hand, it is useful for consensus building: 'in the absence of shared belief or common interests, Art in public spaces may force an otherwise fragmented populace to frame diverse values and ideals in common spaces. By creating common spaces for memory, monuments propagate the illusion of common memory."" On the other hand, memorials relieve the population of the burden to remember, 'under the illusion that our memorial edifices will always be there to remind us, we take leave of them and return only at our own convenience."" Despite the various facets of remembrance, there is a general consensus that monuments are meant for public consumption, and hence, a part of the answer to the question of how the Holocaust should be memorialised would rest on public opinion and expectations. James E. Young, The Texture of Memory, p.6. Ibid, p.5. The main debates on the form a Holocaust memorial should take will be discussed here, under the main themes of Abstract vs. Realist, Jews vs. Non Jews, Secular vs. Religious and the anti-monument. It would be worth noting that the backing of the State, as well as public opinion, is necessary for the memorial to serve its purpose. In every case, Holocaust memorials reflect not only national and communal remembrance, but also the memorial designer's own time and place. For contemporary artists (working in an era of abstract expressionism, conceptual art, post-modern and de-constructionist design), though there is a recognition that they are accountable to both art and memory, Albert Elsen reminds us that for them 'the needs of art, not the public memory come first."" However, this is in direct opposition to many survivors, who 'believe that the searing reality of their experiences demand as literal a memorial expression as possible.' Hence the dilemma of the artist, an example of whom is Nathan Rapoport, whose Warsaw Ghetto Monument 1948,is probably the most widely known, celebrated and controversial. Conversely, Sol LeWitt's unconventional Memorial to the Missing Jews, 1989, in Germany had to bow to popular opinion to have it removed from the Platz der Republic, in Hamburg. Ibid, p.8 It must be remembered that Jews, as well as non-Jews perished during the Nazi holocaust, and it should be the latter's entitlement to share in the remembrance and mourning of the former. In a particularly volatile situation in 1988 Germany, Leah Ross, a journalist demanded that a memorial be set up for Jews alone. This provoked an outcry from Romani Rose of the Central Council of the Sinti and Roma that this 'represented a hierarchization of victims, which is insulting for the victims of the genocide and for the survivors of the Sinti and Roma majority."" It would make sense in Israel to dedicate the memorial exclusively to Jews, but given the racial composition in Germany, it would be impertinent to do so. Caroline Wiedmer, The Claims of Memory, p. 144. Due to the intertwined relationship of Jewish suffering (the Shoah) and religion, commemoration of the former inevitably involves the latter. As James E. Young suggests, 'Israel's over-arching national ideology and religion may be memory."" Victims of the Shoah are portrayed as martyrs who died for their faith, fighters who are remembered for their part in the state's founding. Contrast this to the secular state of France, who, during one of the national competitions for the Vél d'Hiv Monument, picked the literal figurative design of Walter Spitzer over that of Shelomo Selinger because the ministry felt that the latter's proposal was 'Too Jewish.' As such, it is highly apparent that how the Holocaust should be memorialised depends largely on cultural and national factors. James E. Young, The Texture of Memory, p. 210. An interesting development would be that of the counter-monument. This is particularly so with regard to the new generation of German artists, who 'have a deep distrust of monumental forms in light of their systematic exploitation by the Nazis, and a profound desire to distinguish their generation from that of the killers through memory."" These artists explore both the necessity of memory, and their incapacity to recall events they never experienced directly, hence in capturing the memory of events, remember therefore only their own relationship to it. The Gerzes' counter-monument is perhaps the best example of this. Theirs is a self-abnegating monument, a 'vanishing monument' which, instead of graciously accepting the burden of memory, throws it back at the town's feet, forcing them to remember for themselves. This audaciously simple idea challenges the idea of monumentality and the permanence of collective memory, and provokes the population to examine themselves as part of the piece's performance. Ibid, p.27. The writer's main emphasis on public monuments reflects her bias in thinking that such forms of public art are more effective in inculcating a common sense of destiny, and fulfilling the social and political functions attached to them, more so than films, books and painting can. With the boom of the Holocaust industry however, films have become an essential medium by which large populations can learn more about the Holocaust. The writer has avoided a direct answer to the question up to this point, but has attempted to illustrate the many debates and considerations that have arise from the question. In conclusion, the Holocaust should be memorialised according to its time and location, asserting its relevance in the lives of the denizens there. Also, it is worth noting that 'new generations visit memorials under new circumstances and invest them with new meanings', and since there is no one universally accepted way of memorialising the Holocaust, the normative connotation posed by the question above is self-defeating, and ought not to be answered by a historian. The history of empires is a long one: beginning with the Greek, Roman and Persian empires, which pitted race against race; the empires of Spain, Portugal, the Netherlands and Britain in the sixteenth and seventeenth century, which saw the rivalry between monarchies; and finally, the 'New' Imperialism of the mid-nineteenth century, which saw a renewed interest in and frantic scramble for, new colonies, between nation-states. 'By early twentieth century, Britain, France, Germany, Italy, Belgium, the Netherlands, Denmark, Spain, and Portugal together held sway over almost 84 percent of the earth's surface."" Imperialism had become not only fashionable, but also defined the era from the mid-nineteenth onwards to World War I. Despite many attempts to attach priority to a certain aspect of imperialism as being the driving force behind the phenomenon, it is perhaps more helpful to understand it as a product of a complex web of motives and concerns. Also, imperialism must be examined in the context of its time (post-enlightenment ideas, rapid industrialisation, the formation of scientific theories which supported the superiority of one race over another, nationalism, and the economic depression of 1873), without which, the phenomenon either would not have occurred, or might have taken on a different form. Alice L Conklin and Ian Christopher Fletcher, European Imperialism (ed.) (1830-1930), (Boston 1999) p. 1. Imperialism, according to Hobsbawm was 'the child of an era of competition between rival industrial-capitalist national economies which was new and which was intensified by the pressure to secure and safe-guard markets in a period of business uncertainty."" Lenin also suggested that 'Imperialism emerged as the development and direct continuation of the fundamental characteristics of capitalism in general."" Despite the acknowledged importance of economics, there is a tendency for Marxist historians to over-emphasise its role. 'The importance of economic interests and calculation in the expansion of European influence and control over the extra-European world is undeniable."" It is one thing to concede the importance of economic interests, and another to assert that imperialism was primarily an economic phenomenon. Eric Hobsbawm, Age of Empires 1875-1914, (New York, 1989) p.64. Conklin and Fletcher, European Imperialism (ed. ), V.I Lenin on 'Imperialism, the highest stage of Capitalism', p. 38. Andrew Porter, European Imperialism (1860-1914), (Malaysia, 1994), p. 38. 'New' imperialism was only possible in the light of rapid industrialisation, which peaked during the mid-nineteenth century. Industrialisation required raw materials such as tin and iron to make steel, which could be obtained cheaply in the colonies; the movement from rural to urban areas, the subsequent shift from agriculture to industries, and the surge in population put severe strain on food production in the metropolis, which would be alleviated if the colonies took on the function of being the 'granaries of Europe'. Also, with the efficiencies of machinery, colonies could provide the markets needed to absorb the increased production. 'Moreover, cheaper markets were required to offset the general fall in prices from the 1870s to the 1890s (economic depression); with most industrial powers adopting protectionist tariffs, colonies could serve both needs."" Finally, there is the theory of 'surplus capital' as expounded by Hobson 'aggressive imperialism...is a source of great gain to the investor who cannot find at home the profitable use he seeks for his capital, and insists that his government should help him to profitable and secure investments abroad...'  Porter, European Imperialism, p. 39. Conklin and Fletcher, European Imperialism (ed. ), J.A. Hobson on 'The New Imperialism', p. 15. It would seem that all of the above mentioned was best associated with British imperialism more than any other. At a glance, Britain's colonies were the most economically viable, with particular regards to India and South Africa. Using the data from the geographical distribution of British long-term capital investment overseas (as of 1913), out of the 1,780 million pounds invested in her empire, 378.8 million pounds was invested in India and Ceylon, while 370.2 million pounds went to South Africa. Moreover, the existence of chartered companies, such as George Goldie's Royal Niger Company, Sir William Mackinnon's British East Africa Company, Cecil Rhodes' British South Africa Company and the British East India Company (EIC) in Asia would seek to reinforce the pattern of British colonial rule: the ideal situation being that of an informal empire, which would allow effective economic exploitation without the additional costs of setting up a governing administrative body and the moral responsibility attached to a formal colony. Porter, European Imperialism, p. 41, table 1. There are however, cracks in the theory of over-riding economic concerns in British imperialism. Firstly, it must be said that possessions that paid off handsomely, such as India and South Africa, were extremely rare and were lacking amongst the possessions of the other colonial powers. Hence 'with few exceptions, colonial trade remained throughout this period an insignificant proportion of metropolitan commerce...'. Instead, most of Europe's long-term overseas investment went significantly to areas outside of the formal colonial empires: French capital went into Russian bonds, German to Austria-Hungary and British to Latin America (in particular Argentina) and the other white-settlement former colonies (Canada and Australia). Moreover, very few colonies were actually profit-making entities, with the Germans colonising large areas of the African desert, and French indo-china being incomparable to the economically viable British colonies in the region. Porter, European Imperialism, p. 40. The role of ideology in imperialism cannot be overlooked, though many have dismissed it as a cloak for economic exploitation, there is still much to be said about Social Darwinism, the ""White Man's Burden"" and the moral responsibility attached to it. In the light of the Enlightenment ideas of the link between change and progress, and Darwin's theory of evolution, to remain stagnant was to be weak. This change in ideas also marked a change in the perceptions of the colonial masters, who increasingly stressed the difference between the cultures of the metropolis and the colonies. Also, the advent of Science in promoting such racist ideas succeeded in explaining irrational beliefs in rational terms '...evolutionary explanation of racial differentiation and the mechanisms of natural selection seemed to explain and justify cultural differences in terms of inherent racial capacity and a natural hierarchy..."" This sense of superior culture brought with it the moral responsibility, that 'societies which had travelled ahead...had some duty to offer guidance, instruction and even to rule."" This was manifested in the ways the colonial masters tried to 'educate' the natives about proper 'Western' hygiene, setting up public schools to educate the natives in the colonial language, proper dressing (wearing shoes), western consumerism, and very importantly, through the spread of religion (Christian or Catholic faith). Another feature of the 'White Man's Burden' would be that of humanitarian concerns (whether rhetoric or reality), such as the abolishment of slavery in all French and British colonies, as the parallel movement alongside economic exploitation. Ibid, p.24. Phillip Darby, Three faces of Imperialism (British and American Approach to Asia and Africa 1870-1970), (Avon, 1987), p. 31. Though all the colonial powers claimed to be civilising their colonies, it was 'only in republican France (that) this claim (was) elevated to the realm of official imperial doctrine."" Nineteenth century France was the great missionary power of the Roman Catholic world, and it can be argued that the colonisation of Vietnam was partly the result of the French desire to protect their missionaries in Vietnam from official persecution. However, the French were unique in believing in the universality of French values, and the perfectibility of human kind, and this perception was not evident amongst the other European colonial masters, who generally, as a rule avoided 'over-education' of the natives. Alice Conklin , A Mission to Civilise, the republican idea of empire in France and West Africa 1895-1930, (California 1997).p. 1. To look at imperialism from a cynical point of view, it must be said that where economic and humanitarian interests were conflicting, the former would always triumph. Empirical observations regarding the sale of opium to Britain's Southeast Asian colonies, and that of 'trader gin' (a noxious but potent narcotic) in West Africa are telling of the priority of economic over humanitarian concerns. Abolishment of slavery was possible because it benefited both parties. Also, the presence of missionaries/explorers such as David Livingston could not compete with that of the international community of businessmen and investors. A very poignant example would be that of Belgium Congo, where King Leopold colonised it on the basis of 'humanitarian reasons', only to use the local population as slave labourers. Though it is extremely difficult to argue that cultural influences were the direct causes of European imperialism, it nevertheless 'assisted in creating the general circumstances within which specific instances of imperial domination...became for contemporaries, not only imaginable but acceptable."" The consequences of cultural transfers are still seen in our modern world, where the colonies largely taken on the parliamentary model of their former colonial masters. This change has its roots in the era of colonisation where the indigenous political and often social hierarchy was replaced by the political institutions and protocol of the metropolis. The adoption of the colonial language as the working language, the western architecture of new buildings and the assimilation into the cash-economy, are all examples of the transfer of culture. Similarly, there were many ways in which the natives have inspired European art and lifestyle. Porter, European Imperialism, p. 27. An examination of purely economic and cultural aspects of imperialism has failed to truly grasp the essence of this significant event in history. The economic and cultural aspects were more of consequences rather than the causes of imperialism, and should be put in their correct places, under the broader themes of Power, Rivalry (stemming from Nationalism), Fear and Regeneration, which are interrelated concepts. Even so, the economic and cultural consequences were in fact additional benefits to be gained from imperialism, and not the actual aims that the imperialists wanted to achieve. The acquiring of colonies was seen by many countries as the means by which they could attain the status of world power. This is particularly so for new countries like Italy and Germany, which saw their participation in imperialism as heightening their world standing. For an established nation like Britain however, imperialism was a defensive reaction, derived from a sense of insecurity and threats from newly industrialising powers like Germany, America and Russia, and the subsequent assertion of her power. As for France, she saw imperialism as the opportunity for the expansion of her army and navy; the role successful imperialism would play in exorcising her defeat in the Franco-Prussian war of 1871, and rebuild her status as a world power. Germany's fear of encirclement also played a part in the formulation of 'Weltpolitik'. It was not only the external fear that countries were trying to undermine each other in the race for colonies, but more importantly, there was the fear of internal degeneration, and that imperialism was the cure to the diagnosis of national ills, revitalising the country, restoring its self confidence, and reinforcing its power abroad. Imperialism was often the exclusive prerogative of a small group of elites, but when the issue of national pride was at stake, and rivalry ensued, the public could often be aroused into supporting imperialist policies (clearly seen in the example of France.) Germany's aggressive 'Weltpolitik' (World policy) to secure her 'place in the sun' was an extension of the Anglo-German rivalry towards the end of the nineteenth century. Perhaps it can be suggested that, the above arguments regarding economic and cultural aspects were merely the manifestation of the broader concept of European rivalry. Linked to this concept is the theme of nationalism. The situation in Europe was balancing on a knife-edge after the unification of Italy and Germany, and the outward movement of expanding into a vast territory of unconquered plains was profitable not only as a safe outlet for the consumptive force of nationalism, but also to maintain the balance of power in Europe (Africa as the periphery of Europe). Finally, there is the suggestion put forward by some historians that it was the domestic situation in Europe itself that led to imperialism. Social Imperialism, whereby '...expansionist policies are consciously devised to unite the nation and to defuse tensions at home, while simultaneously avoiding significant domestic reforms."" In addition, the German 'Sammslungpolitik' (the gathering together of the elements that would naturally support the state) against the domestic threat from the Social Democrats, which coincided with their enthusiasm for imperialism, would seek to reinforce this theory. Porter, European Imperialism, p.32. In conclusion, it is inadequate to understand imperialism primarily either as an economic or cultural phenomenon, simply because both were secondary concerns in the impetus to colonise. Economic determinism is the prerogative of Marxist historians, and it should be kept this way; for the vast majority of the other historians who have attempted to write about imperialism have placed due recognition to the other factors that have contributed to the motives of imperialism, which provide a more balanced view of this phenomenon. Moreover, the question specifically refers to European, and not British imperialism, hence, the assertion of economic interests as the primary force in imperialism would seem highly Anglo-centric in the light of the arguments put forth in this essay. It is perhaps more accurate to see the economic and cultural aspects as being the consequences of imperialism, and since 'primarily' suggests some sense of causation, it would be myopic to insist upon a direct answer to this question.","Traditional accounts on the causes of the Reformation necessarily included castigation of clerical abuses, such as concubinage, pluralism and absenteeism; the ignorance and poor training of priests and monks; and perhaps even the supposed failure of papal leadership as manifestations of the decay and decline of the Church on the eve of the Reformation. This view, and the more recent revisionist view of the topic in question has informed the main body of debate in scholarly circles, but before embarking on the essay, there are a few points that deserve attention. Firstly, the question suggests that what was 'wrong' with the Church could have led, in some sort of linear equation, to the birth of the Reformation. Following in the footsteps of Peter Marshall, ""the intention is not to sift the evidence for signs of impending Reformation"", and teleological history should be avoided due to the inaccurate interpretations and conclusions which may emerge from that approach. Secondly, it is ambiguous as to what constitutes 'the Church'. According to Robert Swanson, there are two main definitions: ""the restricted, institutional definition, which segregated the 'ecclesiastical' from the 'secular'..."" and ""the body of the faithful which made up the mystical body of the church which was also the body of Christ."" To limit the scope of this essay, the first of Swanson's two definitions will be taken up, noting however, that this requires a generalization of the circumstances in the various national churches. This essay will avoid a direct, simplistic answer to the question but will instead attempt to put the criticisms of the Church into perspective and suggest that there was a certain measure of continuity, rather than exclusivity to the eve of the Reformation; and to locate the essay within the confines of the changing climate of opinion that may have contributed to the perceived problems of the Church. The recurrent themes of Expectations and Reform will be examined in the following context: the Clergy, the Papacy, Doctrines and Practices, and will revolve around the issue of the function of the Church. Peter Marshall, Reformation England 1480-1642, (Cornwall, 2003), p. 2 Robert N. Swanson, Religion and Devotion in Europe 1215-1515, (Cambridge, 2003), p. 7 The failure of the Clergy to live up to lay expectations of moral superiority (derived from being the agent through whom man communicated with God) and to fulfill the functions they were ordained for opened up the Church to criticisms, from without and within. 'On the eve of the Reformation, laity protested against pervasive clerical fiscalism, absenteeism, mal-administration, and concubinage..."" and were joined in their castigations, by some members of the Clergy, and more notably, the humanists. John Colet, the humanist dean of St Paul's even went as far as to blame ""all the ills of the Church on the secular lifestyles and 'covetousness' of the Clergy"" and urged ""reformation of the Church's estate"". According to Duggan, even ""Catholic and Protestant historians alike largely concur on the (sexual) 'immorality' of many Clergy on the eve of the Reformation."" However, a reconsideration of the reaction to the abuses of the Clergy is necessary. Despite the widespread practice of concubinage, the population of Europe was not overly concerned unless this moral failing on the part of the Clergy impinged on their primary role of administering the sacraments. As Duggan articulates succinctly: ""whether the clergy were 'immoral' or not, the more germane question is whether they ministered adequately to the needs of their flock."" Steve Ozment, Age of Reform: An intellectual and Religious History of Late Medieval and Reformation Europe, (Newhaven, 1980) p, 211. Marshall, Reformation England, p.11 Lawrence G. Duggan, The Unresponsiveness of the Late Medieval Church: A Reconsideration, Sixteen Century Journal, Vol. 9, No. 1 (April 1978), p. 23 Perhaps, it would be plausible to suggest that the more pressing considerations on what was 'wrong' with the Clergy would be ""pluralism and absenteeism, which are frequently construed as abuses and signs of decay and pastoral neglect"" The centrality of the Church in providing ""contact between human beings and the supernatural order of their universe, to explain, direct, and console"" is crucial to the understanding of late medieval religion; and the perceived failure of the Church in providing the above services would shake the foundations on which it was built. The Church was faced with two main dilemmas: firstly, it had inadequate jurisdiction over the appointments to benefices, which increasingly became the prerogative of the rulers. Also, clergymen seeking to further their knowledge and competence often enrolled in the universities, with the obvious consequences of being away from their benefices. Despite pluralism being widespread, it did not necessarily indicate neglect, as absentee beneficiaries were required by canon law to provide a vicar to cater to the needs of the local populace. Ibid, p. 23 Euan Cameron, The European Reformation, (New York, 1991), p. 9 What proved more destructive than the above-mentioned abuses ""were the words and ideas of clerics...born of an impatience with the imperfect established church and a moral vision of purified Christianity."" The more prominent members of this group include John Wycliffe, Erasmus and Luther. It is noteworthy that criticisms of the Church went as far back as to the late 14 th Century (the Lollards) and were not exclusive to the eve of the Reformation. The irony lies in the fact that the Church had effectively 'dug its own grave' by educating these men-empowering them with knowledge which allowed for the formulation of reformist ideas and expectations of change, which clashed, at every level, with the vested interests of the clerical class. Duggan, The Unresponsiveness of the Late Medieval Church: A Reconsideration, p. 25 The fact that the Church was becoming more accessible to the Laity may have opened up the former to criticisms from their congregation. The increased interaction between the Clergy and the Laity may have led to the exposure of church flaws which may seem less apparent previously; and the printing of the provincial decrees and sermons denouncing clerical abuses, by the church, in the vernacular ""might have convinced laymen not only of the widespread efforts of reform but also of the widespread problems of the church ...raising in some circles expectations that could not be realized."" Ibid, p. 14 The Papacy was also not immune to criticisms of 'worldliness', and the Renaissance popes provided the best illustration of this by participating in the struggle for temporal power, concubinage, and as patron of the Arts. Since the thirteenth century, reformers have 'found themselves at cross-purpose with a papacy whose pastoral role had become increasingly confused with its political and economic ambitions."" However, far from being a power-hungry and oppressive apparatus, it can be said that the Papacy had undermined its own power through the setting up of Universities, encouragement of printing, and even patronage of the humanists. The Papacy has often been accused of being lofty and unresponsive, but in reality, was the main impetus behind the improvement of society, largely through the channeling of Church resources into building and maintaining educational institutions that served both the state and the society at large. Most significantly, they 'took under their aegis the devotio moderna the principles of which strikingly foreshadowed the worth and dignity fully accorded laymen by the reformers of the sixteenth century.' Ozment, Age of Reform, p.208 Duggan, The Unresponsiveness of the Late Medieval Church: A Reconsideration, p.16 It is imperative to examine the doctrines and practices (in particular, the deep belief in Purgatory, the sacrament of Penance, and its derivatives such as relics, pilgrimages and most notoriously, indulgences) of the Church, which generated oppositions from the laity and clergy. Interestingly, criticisms of Church practices and doctrines existed in the late fourteenth century, in the form of Lollardy (led by the Oxford theologian John Wycliffe) called for a clear divide between spiritual and temporal power; obedience to the church authorities as far as their behaviour merited; the Bible as the only source of religious authority; condemned the doctrine of transubstantiation; and attacked the veneration of saints (and relics). The humanists also shared the view that the Church had diverged away from the one true religion, embraced misconstrued beliefs and had become too intertwined with the secular world. Convinced that grace and forgiveness could only come from God alone, Luther also harshly condemned the sale of Indulgences in his ninety-five theses. However, the cynicism surrounding the practice of Indulgences were not unique to the eve of the Reformation, and was a pervasive element in literary traditions, as seen in Chaucer's Pardoner's Tale which was written in 1400. It is interesting to note however, that the supply of indulgences was in response to the demands of the laity, and was perhaps a manifestation of the paranoia of securing one's own salvation. What is perhaps significant is that the demanding penitential system and mechanical ritualized religion offered by the Church only provided temporary relief for the laity, and had led instead to the exacerbation of fear and anxiety as reflected in the obsession with the constant absolving of sins. Marshall, Reformation England, p. 16 According to Steve Ozment, one of the chief failings of the Church lay in 'the absence of a distinctive concept of lay religious life and the consequent imposition upon the laity of traditional clerical ideals'; and the proliferation of confraternities, in addition to the earlier movement of the devotio moderna are perhaps indicative of the failures of the church to cater for lay piety. The advocacy of clerical piety as the zenith of religious piety had placed unrealistic expectations on the laity and precluded their search for consolation through the Church. The incongruity of this enterprise was not lost on the increasingly literate laity, who rejected the ""prominent clerical ideals of obedience and sexual impurity"" in favour of ""simplicity, directness and respectful treatment in all spheres of their lives."" Moreover, as mentioned above, the behaviour of the clergy did not correspond to the standards that church traditions had imposed on them, and policies such as enforced celibacy increased the incidence of breaches in moral standards that served to reinforce, amongst the laity, the futility of this endeavour. Ozment, Age of Reform, p. 219 Ibid, p. 220 Some historians have also suggest that the increasing desire amongst the laity for a personal and direct relationship with God, and the subsequent failure of the Church to cater to this aspect led many into the realm of mysticism. Though some have argued that mysticism could be regarded as a form of subversion, it is perhaps more appropriate to think of it as a supplementary to, rather than as a substitute for the institution of the Church. Mysticism of both the fourteenth and sixteenth century coexisted with the Church and may be seen as testament to the religiosity of the Age. There is a possibility that the multiple responsibilities of the Church in society may have led to its resources becoming over-stretched, hence, compromising on the primary role of the Church to minister the sacraments. Swanson puts forth the idea that most of the anti-clericals ""wanted churchmen to behave just as churchmen, rather than officious, interfering, lordly bureaucrats."" However, the Church had for centuries combined both its temporal and spiritual responsibilities without much difficulty, and it appears rather peculiar that the church would find it hard to cope with its dual roles on the eve of the Reformation. Moreover, the increased accessibility of university education provided the necessary training for aspiring civil servants, and this would have taken the burden off the Church as a source of administrative officials, implying perhaps that the Church could concentrate more on its primary, spiritual function. Although there were many problems (both discipline and doctrinal) that the Church faced, it is apparent that these problems had been in existence for the past one or two centuries, and that despite the criticisms, the aim of the reformers was not to subvert the church, but had in mind the desire to change the church into a more responsive, relevant and effective apparatus. Throughout the essay, the writer has tried to suggest that it was the rising expectations of a growing educated class (laity and clergy) and the inability of the church to keep up the pace of reform and to satisfy the needs of the laity - the disparity between expectations and reality- that led to disillusionment. Similarly, the high expectations that the Church set with regards to piety eventually led to the alienation of the people from the Church. Despite the deep-rooted flaws of the church, the attempts at reforming it suggest ""men were seeking salvation and wished to find it with the help of the church."" Hence, in trying to account for the Reformation, it is inadequate to merely examine what was 'wrong' with the church, but requires instead, a broader understanding of the climate and considerations (political, economic and social) of the Age. B.Moeller 'Religious life in Germany on the eve of the Reformation', in G. Strauss (ed.) Pre-Reformation Germany, (London, 1972) p. 27 Social unrests were not unique to the 19 th Century, and the Qing state was by no means inexperienced in putting down such attempts to destabilize the State. Despite the consolidation of power during the reign of the Emperor Kangsi to Qianlong, there remained however, certain underlying grievances that were never fully eradicated, which led to the persistence of the themes such as 'Over-throw the Qing, Restore the Ming', ethnic and religious tensions. The end of the Qianlong era marked the conclusion of the golden age of Qing rule and saw the gradual decline of central power vis-à-vis the provincial power and the emerging western participation in Chinese politics. Though westerners have had a presence in China dating back to the Yuan dynasty, the Opium Wars and the corresponding treaty system marked a watershed for China-West relations, resulting in the change of status quo, of perceptions; and manifested itself in the increasingly intrusive presence of the West, which exposed the many weaknesses of the Qing state, making it more vulnerable to attacks from within. It is in this context that the above question should be addressed, and the writer will examine firstly the internal conditions and entrenched corruption which made it conducive for the outbreak of social unrests, followed by the implications of change brought about by the West, such as the desire to trade, introduction of new technology, ideas, religion, and the pursuit of Imperialism which clashed with Chinese assumptions of the monopoly of civilization, and in turn produced a virulent anti-foreign nationalism which threatened the social order during the period in question. Hence the writer will argue that the social unrests at the beginning of the 19 th Century was due more to the internal rather than external factors; but increasingly, the presence of the West is crucial to the emergence of violent xenophobic nationalism, which account for the social unrests in the later part of the 19 th Century. One of the persistent problems that the Qing failed to eradicate were the deep-rooted ethnic tensions which lent itself to many rebellions at the first half of the 19 th Century. The entrenched sentiments that the Qing were a foreign power residing in China led to the creation of many secret societies, such as the White Lotus Society and the Heaven and Earth Society (Tian-li-jiao), whose rallying cry of 'Overthrow the Qing and Restore the Ming' saw the outbreak of the White Lotus Revolt in 1796, Szechuan and Hubei, which lasted until 1904, as well as the attempt made by Li Wen Cheng to attack the imperial palace in 1813. The prejudices that the Han Chinese held against the Muslims and the latter's desire for autonomy from the Qing, led in the 1820s to the Muslim rebellion led by Jehangir in Turkestan, and subsequently, in the two Muslim rebellions, in Yunnan (1855-1873) and in Gansu and Shaanxi (1862-1873). The Taiping rebellion has its roots in ethnic tensions as well, with Hong Xiuquan and a considerable number of his followers coming from the Hakka (ke jia ren) sub-group, and these 'guest-settlers' were frowned upon by the largely Han population in Guangxi. The anti-Manchu sentiments of the Nian rebellion is perhaps most clearly indicated by their leader Zhang's honorific title of 'Great Han Prince with the Heavenly Mandate'. Jonathan D. Spence, The Search for Modern China (2nd Edition), (New York, 1999), p. 184. Ethnic tensions alone do not cause rebellions, but mixed with deplorable economic and social conditions can be a recipe for disaster. Between 1741-1841, the population increased by three-fold, but the increase in arable land could not keep pace with it, which led to food and land shortages, with the latter commodity increasingly becoming concentrated in the hands of the wealthy. The yearly occurrence of floods and droughts in the provinces along the valleys of Yellow and Yangtze rivers during the end of Daoguang's reign (1821-1850) and the lack of state initiatives in alleviating the situation may have led the population to believe that the mandate of Heaven was passing out of the hands of the Qing dynasty. Also, the Opium trade, conducted by the British led to the great outflow of silver, resulting in the devaluation of copper coins, the inflation in the prices of other commodities, and the corresponding increase in the peasants tax burden which was paid in silver. All these combined events accentuated the miseries of the common people and made for fertile ground for the outbreak of social unrests. Potential rebellion leaders could find amongst the population large numbers of poor peasants and artisans who were willing to throw their weight behind anything that promised better conditions than the existing one. The attractiveness of the Taiping laid in its Christian-Communism ideology and the proclamation that 'nowhere will inequality exist, and no one not be well fed and clothed."" The triggers for the Muslim rebellion in Yunnan 'were the heavy land taxes and extra levies imposed by Peking on the Yunnanese Muslims' who in addition to that, were being ousted out of their sliver and gold mines by the Chinese. Li Chien-nung, The Political History of China, 1840-1928, translated and edited by Teng Ssu-yu and Jeremy Ingalls, (California, 1967) p. 49. Jonathan D. Spence, The Search for Modern China (2nd Edition), (New York, 1999). p. 176. Ibid, p.187. The inability of the State to alleviate the situation plaguing the population, and the ineffectiveness of the Army in putting down the rebellions are perhaps telling of the decay within the Qing central administration, which permeated to the lower level officials. The appalling inertia of the State led Zeng Guofan to condemn the ruinous tendency of officials 'to gloss over, to make up and to steal days of ease."" The two chief ministers under Daoguang's reign, Tsao Chen Yung and Mu-chang-a discouraged officials from talking about national policy, and those who dared petition to the throne were threatened with punishments. It is perhaps not surprising then, to find the character of Chen Tsu-ch'en, the governor of Guangxi who did not bother with administrative affairs, bit prayed instead to Buddha to dispel catastrophes. 'Such was the immediate political background-general lethargy and irresponsibility-which paved the way for Taiping's rapid development."" The corruption and lack of discipline within the Army was so rampant, that by the time of the White Lotus Rebellion, local militias had to be organized to compensate for the ineffectual standing army. Similarly, it was Zeng's Hunan army, Li Hongzhang's Huai army and Zuo Zhongtang's army, that suppressed the Taiping, the Nian and the Muslim rebellions. Also, inner court intrigues dominated the post Xian-feng era, which saw the rise of the Empress Dowager Cixi and her cronies, and in turn compromised the power of the Emperor to act as the unifying force of China. The diversion of resources meant for military and emergency purposes to fund the decadence of the court left China in a weakened state, creating ample opportunities for the outbreak of rebellions. Li Chien-nung, The Political History of China, 1840-1928, (California, 1967), p. 49. Ibid, p.50. The Chinese Pandora box was sitting there, waiting to be unlocked, and when the West came to pry China open, they unleashed torrents of tensions, discontentment and grievances, which when mixed with Western military technology, religion and ideas, became more malignant, and threatened to topple the Qing state. The rise of Capitalism and the nation-state in the West led to the corresponding desire to gain economic advantage and prestige via the acquiring of colonies, and largely accounts for the increased presence of the West, and the advent of the Age of Imperialism, in China and else where (seen also in the 'Scramble for Africa') leading to the creation of anti-foreign nationalism, which is a major source of disturbances during the later half of the 19 th Century. The role of the West became increasingly important because the weak central Qing government were defenseless against the military superiority of the West, as seen in the Opium War. Correspondingly, the treaties that were concluded reflected this imbalance, and. 'under the treaties, China's sovereignty was increasingly impaired."" 'The formative decades of the treaty system in the 1840s and 1850s must therefore be seen as the opening phase in an intricate and portentous growth of foreign influence on Chinese life..."" and the creation of new institutions for Sino-foreign contact. China had to cede Hong Kong permanently to the British, open up Amoy, Fuzhou, Ningpo, Canton and Shanghai for trade, allow the British to fix customs duties, permit extraterritoriality and the establishment of equal basis for official correspondence, all of which were seen as attacks on the sovereignty of China, and the inability of the Qing to protect Chinese territory led to the creation of anti-Manchu as well as anti-foreign sentiments. In particular, the tradition of anti-foreign resistance can be traced to the San-yuan-li incident in Canton 1841, in which '...specific incitements (raping and defiling of temples in San-yuan-li), together with pervasive popular anxieties about the war, helped to transform relatively benign stereotypes about foreigners into xenophobic racism."" Also, the period from 1841-1850 saw the upsurge of social disorders in Canton, and may have precipitated the Taiping Rebellion. 'In general after the Opium War, the prestige of the Manchu government was destroyed and the invitation to rebellion was ever-present...from 1841-1850 there was not a single year free from local uprisings."" In retaliation to the Qing obstinate refusal to honour the treaties, the allied British and French forces stormed Peking, burned the Yuan-Ming-Yuan, and dealt Qing China a big blow to its prestige when Emperor Xian-feng had to flee to Jehol. This demonstrated clearly that the Manchus were no longer able to be guardians of China, and created a power vacuum within China, and made her vulnerable to attacks from within and without. John K. Fairbank, The Creation of the Treaty System, in Dennis Twitchett and John K, Fairbank (ed. ), The Cambridge History of China, Vol. 10 Late Ching 1800-1911, Part I, (New York, 1995), p. 214 Ibid, p.215. Frederic Wakeman Jr, The Canton Trade and the Opium War, in Dennis Twitchett and John K, Fairbank (ed. ), The Cambridge History of China, Vol. 10 Late Ching 1800-1911, Part I, (New York, 1995), p. 202. Frederic Wakeman Jr, Strangers at the Gate: Social Disorder in South China 1839-1861, (Los Angeles, 1966), p. 117-31. Li Chien-nung, The Political History of China, 1840-1928, translated and edited by Teng Ssu-yu and Jeremy Ingalls, (California, 1967) p. 52. Chinese defeat to the Allied British and French forces in 1960 led to the conclusion of a treaty, which granted full official toleration to the missionary, and also opened up the hinterland to the spread of Christianity. 'One result of this new set of circumstances was a considerable broadening and intensification of the tradition of Chinese hostility toward Christianity."" Though the Christian attack on ancestor worship and idolatry offended all Chinese, it is interesting to note that anti-Christian demonstrations were often provoked by the exhortations of the gentry and official class; who saw themselves as defenders of Confucian order and civilisation in a period of disorder, and whom regarded Christianity as the anti-thesis of Confucianism, and as the imperialistic tool of the foreigners. The presence of missionaries who abused their positions by interfering in lawsuits, and in shielding their converts from Chinese law, as well as the flaunting of treaty rights in demanding large amount of indemnity as payment for damages did not endear them to the local populace. The problem was particularly acute with regards to the French Catholic missionaries, and 'the Tientsin Massacre (1870) represented the culmination of a decade of Sino-foreign friction revolving around Christian missionary activities.' Paul A. Cohen, China and Christianity: The Missionary movement and the Growth of Chinese Anti-foreignism 1860-1870, (Massachusetts, 1963), p. 64. Ibid, p. 233. Deep-rooted Chinese ethnocentrism, the resentment against unwanted Western intrusion and the tendency of any society that has been seriously disturbed by internal disorders to seek an external scapegoat accounts largely for the Boxer Rebellion, which occurred when the 19 th Century drew to a close. It is perhaps telling that there was a song that went: 'Learn to be a Boxer, study the Red Lantern. Kill all the foreign devils and make the churches burn."" The churches were the most visible facades of western imperialism and hence were the targets of anti-foreign movements, and for large segment of the population, 'the missionary was the only concrete manifestation of the foreign intrusion and, as such, the only flesh and blood object against which opposition to this intrusion could be directed.' Ibid, p. 269. Cheng Pei-Kai, Michael Lestz and Jonathan D. Spence, The Search for Modern China: A Documentary Collection, (New York, 1999), p. 186. Paul A. Cohen, China and Christianity: The Missionary movement and the Growth of Chinese Anti-foreignism 1860-1870, (Massachusetts, 1963), p. 269. The role of Western influence in inspiring rebellions, and in particular the Taiping, must be considered. Though the Taiping claimed initially to be led by the Christian ideology, in reality, source of it ideas were from the rites of Chou and the Works of Mencius, interwoven with the tenets of Christianity. Also, though Hung Jen Kan had a reform proposal in which western influence can be detected, the Taiping eventually reverted back to the old Chinese corruption, habits of conservatism and of fixed ideas. Hence, though the Taipings were initially trying to identify themselves with their foreign brethren, and sought to employ Western military technology, their adoption of Western ways was very superficial, and hence the West cannot be said to have contributed to the Taiping cause in any significant way. Li Chien-nung, The Political History of China, 1840-1928, translated and edited by Teng Ssu-yu and Jeremy Ingalls, (California, 1967) p. 64 Teng Ssu-yu and John K. Fairbank, China's response to the West: A Documentary Study 1839-1923, (Massachusetts, 1994), p. 57-59. However, the West were particularly influential in helping the Qing to put down the social unrests, such as the employment of the Ever-Victorious Army and Gordon's artillery in supplementing the regular forces and local militias in putting down the Taiping. It must be said that generally, the West were adverse to social unrests, especially in the treaty ports where trade was concentrated, because social instability often interfered with trade and economic prosperity. The open intervention pursued by the West in suppressing the Taiping was to their interests: for the defense of Shanghai, and as a useful bargaining chip with the Qing government when seeking concessions. Also, in the Muslim rebellion in the Kansu, England secretly supported Yakub Beg, the leader of the rebellion, while Russia opposed the expansion of Yakub Beg's power, and under the pretense of maintaining peace along the border, occupied Ili in 1871, subdued the Mohammedan chief and moved troops into Urumchi. Hence when the West moved in to suppress social unrests, they did so only if it was within their interests to do so, should not be regarded as a philanthropic gesture. Li Chien-nung, The Political History of China, 1840-1928, translated and edited by Teng Ssu-yu and Jeremy Ingalls, (California, 1967) p. 110. Perhaps the most glaring exclusion in this essay is the role of Japan in the social unrests of the 19 th Century, but it would be convenient to evaluate Japan on the basis of a foreign power with imperialistic designs on China. The Sino-Japanese War of 1894-5 shook the Chinese self-confidence, and marked an end to the era of 'Self-Strengthening'. Though there was no particular anti-Japanese event, anti-Japanese sentiments took root during this era, and set the stage for more explicit and violent anti-Japanese movements that continue to present time. In conclusion, the social unrests of the early 19 th Century were caused more by internal than external problems, such as underlying ethnic tensions, the prevailing socio-economic conditions, the entrenched corruption and degeneration of the court and the army. However, The Opium War and the Treaty System that was imposed marked a watershed in the Qing-West relations, allowing the West to expose the weaknesses of the Qing State, making it more vulnerable to rebellions, and the rise of Western Imperialism in the quest for territorial gains and in the spread of Christianity, which led to the creation of anti-foreign nationalism, which dominated the face of social unrests in the late 19 th Century. Also, where their interests were concerned, the West would intervene to suppress social unrests in order to safeguard their commercial interests. Hence the parallel streams of development: the decline of the central power of the Qing State, and the growing western presence and penetration gave the West increasing importance in the creation and suppression of social unrests of the 19 th Century.",True
91,"Traditional accounts on the causes of the Reformation necessarily included castigation of clerical abuses, such as concubinage, pluralism and absenteeism; the ignorance and poor training of priests and monks; and perhaps even the supposed failure of papal leadership as manifestations of the decay and decline of the Church on the eve of the Reformation. This view, and the more recent revisionist view of the topic in question has informed the main body of debate in scholarly circles, but before embarking on the essay, there are a few points that deserve attention. Firstly, the question suggests that what was 'wrong' with the Church could have led, in some sort of linear equation, to the birth of the Reformation. Following in the footsteps of Peter Marshall, ""the intention is not to sift the evidence for signs of impending Reformation"", and teleological history should be avoided due to the inaccurate interpretations and conclusions which may emerge from that approach. Secondly, it is ambiguous as to what constitutes 'the Church'. According to Robert Swanson, there are two main definitions: ""the restricted, institutional definition, which segregated the 'ecclesiastical' from the 'secular'..."" and ""the body of the faithful which made up the mystical body of the church which was also the body of Christ."" To limit the scope of this essay, the first of Swanson's two definitions will be taken up, noting however, that this requires a generalization of the circumstances in the various national churches. This essay will avoid a direct, simplistic answer to the question but will instead attempt to put the criticisms of the Church into perspective and suggest that there was a certain measure of continuity, rather than exclusivity to the eve of the Reformation; and to locate the essay within the confines of the changing climate of opinion that may have contributed to the perceived problems of the Church. The recurrent themes of Expectations and Reform will be examined in the following context: the Clergy, the Papacy, Doctrines and Practices, and will revolve around the issue of the function of the Church. Peter Marshall, Reformation England 1480-1642, (Cornwall, 2003), p. 2 Robert N. Swanson, Religion and Devotion in Europe 1215-1515, (Cambridge, 2003), p. 7 The failure of the Clergy to live up to lay expectations of moral superiority (derived from being the agent through whom man communicated with God) and to fulfill the functions they were ordained for opened up the Church to criticisms, from without and within. 'On the eve of the Reformation, laity protested against pervasive clerical fiscalism, absenteeism, mal-administration, and concubinage..."" and were joined in their castigations, by some members of the Clergy, and more notably, the humanists. John Colet, the humanist dean of St Paul's even went as far as to blame ""all the ills of the Church on the secular lifestyles and 'covetousness' of the Clergy"" and urged ""reformation of the Church's estate"". According to Duggan, even ""Catholic and Protestant historians alike largely concur on the (sexual) 'immorality' of many Clergy on the eve of the Reformation."" However, a reconsideration of the reaction to the abuses of the Clergy is necessary. Despite the widespread practice of concubinage, the population of Europe was not overly concerned unless this moral failing on the part of the Clergy impinged on their primary role of administering the sacraments. As Duggan articulates succinctly: ""whether the clergy were 'immoral' or not, the more germane question is whether they ministered adequately to the needs of their flock."" Steve Ozment, Age of Reform: An intellectual and Religious History of Late Medieval and Reformation Europe, (Newhaven, 1980) p, 211. Marshall, Reformation England, p.11 Lawrence G. Duggan, The Unresponsiveness of the Late Medieval Church: A Reconsideration, Sixteen Century Journal, Vol. 9, No. 1 (April 1978), p. 23 Perhaps, it would be plausible to suggest that the more pressing considerations on what was 'wrong' with the Clergy would be ""pluralism and absenteeism, which are frequently construed as abuses and signs of decay and pastoral neglect"" The centrality of the Church in providing ""contact between human beings and the supernatural order of their universe, to explain, direct, and console"" is crucial to the understanding of late medieval religion; and the perceived failure of the Church in providing the above services would shake the foundations on which it was built. The Church was faced with two main dilemmas: firstly, it had inadequate jurisdiction over the appointments to benefices, which increasingly became the prerogative of the rulers. Also, clergymen seeking to further their knowledge and competence often enrolled in the universities, with the obvious consequences of being away from their benefices. Despite pluralism being widespread, it did not necessarily indicate neglect, as absentee beneficiaries were required by canon law to provide a vicar to cater to the needs of the local populace. Ibid, p. 23 Euan Cameron, The European Reformation, (New York, 1991), p. 9 What proved more destructive than the above-mentioned abuses ""were the words and ideas of clerics...born of an impatience with the imperfect established church and a moral vision of purified Christianity."" The more prominent members of this group include John Wycliffe, Erasmus and Luther. It is noteworthy that criticisms of the Church went as far back as to the late 14 th Century (the Lollards) and were not exclusive to the eve of the Reformation. The irony lies in the fact that the Church had effectively 'dug its own grave' by educating these men-empowering them with knowledge which allowed for the formulation of reformist ideas and expectations of change, which clashed, at every level, with the vested interests of the clerical class. Duggan, The Unresponsiveness of the Late Medieval Church: A Reconsideration, p. 25 The fact that the Church was becoming more accessible to the Laity may have opened up the former to criticisms from their congregation. The increased interaction between the Clergy and the Laity may have led to the exposure of church flaws which may seem less apparent previously; and the printing of the provincial decrees and sermons denouncing clerical abuses, by the church, in the vernacular ""might have convinced laymen not only of the widespread efforts of reform but also of the widespread problems of the church ...raising in some circles expectations that could not be realized."" Ibid, p. 14 The Papacy was also not immune to criticisms of 'worldliness', and the Renaissance popes provided the best illustration of this by participating in the struggle for temporal power, concubinage, and as patron of the Arts. Since the thirteenth century, reformers have 'found themselves at cross-purpose with a papacy whose pastoral role had become increasingly confused with its political and economic ambitions."" However, far from being a power-hungry and oppressive apparatus, it can be said that the Papacy had undermined its own power through the setting up of Universities, encouragement of printing, and even patronage of the humanists. The Papacy has often been accused of being lofty and unresponsive, but in reality, was the main impetus behind the improvement of society, largely through the channeling of Church resources into building and maintaining educational institutions that served both the state and the society at large. Most significantly, they 'took under their aegis the devotio moderna the principles of which strikingly foreshadowed the worth and dignity fully accorded laymen by the reformers of the sixteenth century.' Ozment, Age of Reform, p.208 Duggan, The Unresponsiveness of the Late Medieval Church: A Reconsideration, p.16 It is imperative to examine the doctrines and practices (in particular, the deep belief in Purgatory, the sacrament of Penance, and its derivatives such as relics, pilgrimages and most notoriously, indulgences) of the Church, which generated oppositions from the laity and clergy. Interestingly, criticisms of Church practices and doctrines existed in the late fourteenth century, in the form of Lollardy (led by the Oxford theologian John Wycliffe) called for a clear divide between spiritual and temporal power; obedience to the church authorities as far as their behaviour merited; the Bible as the only source of religious authority; condemned the doctrine of transubstantiation; and attacked the veneration of saints (and relics). The humanists also shared the view that the Church had diverged away from the one true religion, embraced misconstrued beliefs and had become too intertwined with the secular world. Convinced that grace and forgiveness could only come from God alone, Luther also harshly condemned the sale of Indulgences in his ninety-five theses. However, the cynicism surrounding the practice of Indulgences were not unique to the eve of the Reformation, and was a pervasive element in literary traditions, as seen in Chaucer's Pardoner's Tale which was written in 1400. It is interesting to note however, that the supply of indulgences was in response to the demands of the laity, and was perhaps a manifestation of the paranoia of securing one's own salvation. What is perhaps significant is that the demanding penitential system and mechanical ritualized religion offered by the Church only provided temporary relief for the laity, and had led instead to the exacerbation of fear and anxiety as reflected in the obsession with the constant absolving of sins. Marshall, Reformation England, p. 16 According to Steve Ozment, one of the chief failings of the Church lay in 'the absence of a distinctive concept of lay religious life and the consequent imposition upon the laity of traditional clerical ideals'; and the proliferation of confraternities, in addition to the earlier movement of the devotio moderna are perhaps indicative of the failures of the church to cater for lay piety. The advocacy of clerical piety as the zenith of religious piety had placed unrealistic expectations on the laity and precluded their search for consolation through the Church. The incongruity of this enterprise was not lost on the increasingly literate laity, who rejected the ""prominent clerical ideals of obedience and sexual impurity"" in favour of ""simplicity, directness and respectful treatment in all spheres of their lives."" Moreover, as mentioned above, the behaviour of the clergy did not correspond to the standards that church traditions had imposed on them, and policies such as enforced celibacy increased the incidence of breaches in moral standards that served to reinforce, amongst the laity, the futility of this endeavour. Ozment, Age of Reform, p. 219 Ibid, p. 220 Some historians have also suggest that the increasing desire amongst the laity for a personal and direct relationship with God, and the subsequent failure of the Church to cater to this aspect led many into the realm of mysticism. Though some have argued that mysticism could be regarded as a form of subversion, it is perhaps more appropriate to think of it as a supplementary to, rather than as a substitute for the institution of the Church. Mysticism of both the fourteenth and sixteenth century coexisted with the Church and may be seen as testament to the religiosity of the Age. There is a possibility that the multiple responsibilities of the Church in society may have led to its resources becoming over-stretched, hence, compromising on the primary role of the Church to minister the sacraments. Swanson puts forth the idea that most of the anti-clericals ""wanted churchmen to behave just as churchmen, rather than officious, interfering, lordly bureaucrats."" However, the Church had for centuries combined both its temporal and spiritual responsibilities without much difficulty, and it appears rather peculiar that the church would find it hard to cope with its dual roles on the eve of the Reformation. Moreover, the increased accessibility of university education provided the necessary training for aspiring civil servants, and this would have taken the burden off the Church as a source of administrative officials, implying perhaps that the Church could concentrate more on its primary, spiritual function. Although there were many problems (both discipline and doctrinal) that the Church faced, it is apparent that these problems had been in existence for the past one or two centuries, and that despite the criticisms, the aim of the reformers was not to subvert the church, but had in mind the desire to change the church into a more responsive, relevant and effective apparatus. Throughout the essay, the writer has tried to suggest that it was the rising expectations of a growing educated class (laity and clergy) and the inability of the church to keep up the pace of reform and to satisfy the needs of the laity - the disparity between expectations and reality- that led to disillusionment. Similarly, the high expectations that the Church set with regards to piety eventually led to the alienation of the people from the Church. Despite the deep-rooted flaws of the church, the attempts at reforming it suggest ""men were seeking salvation and wished to find it with the help of the church."" Hence, in trying to account for the Reformation, it is inadequate to merely examine what was 'wrong' with the church, but requires instead, a broader understanding of the climate and considerations (political, economic and social) of the Age. B.Moeller 'Religious life in Germany on the eve of the Reformation', in G. Strauss (ed.) Pre-Reformation Germany, (London, 1972) p. 27 Social unrests were not unique to the 19 th Century, and the Qing state was by no means inexperienced in putting down such attempts to destabilize the State. Despite the consolidation of power during the reign of the Emperor Kangsi to Qianlong, there remained however, certain underlying grievances that were never fully eradicated, which led to the persistence of the themes such as 'Over-throw the Qing, Restore the Ming', ethnic and religious tensions. The end of the Qianlong era marked the conclusion of the golden age of Qing rule and saw the gradual decline of central power vis-à-vis the provincial power and the emerging western participation in Chinese politics. Though westerners have had a presence in China dating back to the Yuan dynasty, the Opium Wars and the corresponding treaty system marked a watershed for China-West relations, resulting in the change of status quo, of perceptions; and manifested itself in the increasingly intrusive presence of the West, which exposed the many weaknesses of the Qing state, making it more vulnerable to attacks from within. It is in this context that the above question should be addressed, and the writer will examine firstly the internal conditions and entrenched corruption which made it conducive for the outbreak of social unrests, followed by the implications of change brought about by the West, such as the desire to trade, introduction of new technology, ideas, religion, and the pursuit of Imperialism which clashed with Chinese assumptions of the monopoly of civilization, and in turn produced a virulent anti-foreign nationalism which threatened the social order during the period in question. Hence the writer will argue that the social unrests at the beginning of the 19 th Century was due more to the internal rather than external factors; but increasingly, the presence of the West is crucial to the emergence of violent xenophobic nationalism, which account for the social unrests in the later part of the 19 th Century. One of the persistent problems that the Qing failed to eradicate were the deep-rooted ethnic tensions which lent itself to many rebellions at the first half of the 19 th Century. The entrenched sentiments that the Qing were a foreign power residing in China led to the creation of many secret societies, such as the White Lotus Society and the Heaven and Earth Society (Tian-li-jiao), whose rallying cry of 'Overthrow the Qing and Restore the Ming' saw the outbreak of the White Lotus Revolt in 1796, Szechuan and Hubei, which lasted until 1904, as well as the attempt made by Li Wen Cheng to attack the imperial palace in 1813. The prejudices that the Han Chinese held against the Muslims and the latter's desire for autonomy from the Qing, led in the 1820s to the Muslim rebellion led by Jehangir in Turkestan, and subsequently, in the two Muslim rebellions, in Yunnan (1855-1873) and in Gansu and Shaanxi (1862-1873). The Taiping rebellion has its roots in ethnic tensions as well, with Hong Xiuquan and a considerable number of his followers coming from the Hakka (ke jia ren) sub-group, and these 'guest-settlers' were frowned upon by the largely Han population in Guangxi. The anti-Manchu sentiments of the Nian rebellion is perhaps most clearly indicated by their leader Zhang's honorific title of 'Great Han Prince with the Heavenly Mandate'. Jonathan D. Spence, The Search for Modern China (2nd Edition), (New York, 1999), p. 184. Ethnic tensions alone do not cause rebellions, but mixed with deplorable economic and social conditions can be a recipe for disaster. Between 1741-1841, the population increased by three-fold, but the increase in arable land could not keep pace with it, which led to food and land shortages, with the latter commodity increasingly becoming concentrated in the hands of the wealthy. The yearly occurrence of floods and droughts in the provinces along the valleys of Yellow and Yangtze rivers during the end of Daoguang's reign (1821-1850) and the lack of state initiatives in alleviating the situation may have led the population to believe that the mandate of Heaven was passing out of the hands of the Qing dynasty. Also, the Opium trade, conducted by the British led to the great outflow of silver, resulting in the devaluation of copper coins, the inflation in the prices of other commodities, and the corresponding increase in the peasants tax burden which was paid in silver. All these combined events accentuated the miseries of the common people and made for fertile ground for the outbreak of social unrests. Potential rebellion leaders could find amongst the population large numbers of poor peasants and artisans who were willing to throw their weight behind anything that promised better conditions than the existing one. The attractiveness of the Taiping laid in its Christian-Communism ideology and the proclamation that 'nowhere will inequality exist, and no one not be well fed and clothed."" The triggers for the Muslim rebellion in Yunnan 'were the heavy land taxes and extra levies imposed by Peking on the Yunnanese Muslims' who in addition to that, were being ousted out of their sliver and gold mines by the Chinese. Li Chien-nung, The Political History of China, 1840-1928, translated and edited by Teng Ssu-yu and Jeremy Ingalls, (California, 1967) p. 49. Jonathan D. Spence, The Search for Modern China (2nd Edition), (New York, 1999). p. 176. Ibid, p.187. The inability of the State to alleviate the situation plaguing the population, and the ineffectiveness of the Army in putting down the rebellions are perhaps telling of the decay within the Qing central administration, which permeated to the lower level officials. The appalling inertia of the State led Zeng Guofan to condemn the ruinous tendency of officials 'to gloss over, to make up and to steal days of ease."" The two chief ministers under Daoguang's reign, Tsao Chen Yung and Mu-chang-a discouraged officials from talking about national policy, and those who dared petition to the throne were threatened with punishments. It is perhaps not surprising then, to find the character of Chen Tsu-ch'en, the governor of Guangxi who did not bother with administrative affairs, bit prayed instead to Buddha to dispel catastrophes. 'Such was the immediate political background-general lethargy and irresponsibility-which paved the way for Taiping's rapid development."" The corruption and lack of discipline within the Army was so rampant, that by the time of the White Lotus Rebellion, local militias had to be organized to compensate for the ineffectual standing army. Similarly, it was Zeng's Hunan army, Li Hongzhang's Huai army and Zuo Zhongtang's army, that suppressed the Taiping, the Nian and the Muslim rebellions. Also, inner court intrigues dominated the post Xian-feng era, which saw the rise of the Empress Dowager Cixi and her cronies, and in turn compromised the power of the Emperor to act as the unifying force of China. The diversion of resources meant for military and emergency purposes to fund the decadence of the court left China in a weakened state, creating ample opportunities for the outbreak of rebellions. Li Chien-nung, The Political History of China, 1840-1928, (California, 1967), p. 49. Ibid, p.50. The Chinese Pandora box was sitting there, waiting to be unlocked, and when the West came to pry China open, they unleashed torrents of tensions, discontentment and grievances, which when mixed with Western military technology, religion and ideas, became more malignant, and threatened to topple the Qing state. The rise of Capitalism and the nation-state in the West led to the corresponding desire to gain economic advantage and prestige via the acquiring of colonies, and largely accounts for the increased presence of the West, and the advent of the Age of Imperialism, in China and else where (seen also in the 'Scramble for Africa') leading to the creation of anti-foreign nationalism, which is a major source of disturbances during the later half of the 19 th Century. The role of the West became increasingly important because the weak central Qing government were defenseless against the military superiority of the West, as seen in the Opium War. Correspondingly, the treaties that were concluded reflected this imbalance, and. 'under the treaties, China's sovereignty was increasingly impaired."" 'The formative decades of the treaty system in the 1840s and 1850s must therefore be seen as the opening phase in an intricate and portentous growth of foreign influence on Chinese life..."" and the creation of new institutions for Sino-foreign contact. China had to cede Hong Kong permanently to the British, open up Amoy, Fuzhou, Ningpo, Canton and Shanghai for trade, allow the British to fix customs duties, permit extraterritoriality and the establishment of equal basis for official correspondence, all of which were seen as attacks on the sovereignty of China, and the inability of the Qing to protect Chinese territory led to the creation of anti-Manchu as well as anti-foreign sentiments. In particular, the tradition of anti-foreign resistance can be traced to the San-yuan-li incident in Canton 1841, in which '...specific incitements (raping and defiling of temples in San-yuan-li), together with pervasive popular anxieties about the war, helped to transform relatively benign stereotypes about foreigners into xenophobic racism."" Also, the period from 1841-1850 saw the upsurge of social disorders in Canton, and may have precipitated the Taiping Rebellion. 'In general after the Opium War, the prestige of the Manchu government was destroyed and the invitation to rebellion was ever-present...from 1841-1850 there was not a single year free from local uprisings."" In retaliation to the Qing obstinate refusal to honour the treaties, the allied British and French forces stormed Peking, burned the Yuan-Ming-Yuan, and dealt Qing China a big blow to its prestige when Emperor Xian-feng had to flee to Jehol. This demonstrated clearly that the Manchus were no longer able to be guardians of China, and created a power vacuum within China, and made her vulnerable to attacks from within and without. John K. Fairbank, The Creation of the Treaty System, in Dennis Twitchett and John K, Fairbank (ed. ), The Cambridge History of China, Vol. 10 Late Ching 1800-1911, Part I, (New York, 1995), p. 214 Ibid, p.215. Frederic Wakeman Jr, The Canton Trade and the Opium War, in Dennis Twitchett and John K, Fairbank (ed. ), The Cambridge History of China, Vol. 10 Late Ching 1800-1911, Part I, (New York, 1995), p. 202. Frederic Wakeman Jr, Strangers at the Gate: Social Disorder in South China 1839-1861, (Los Angeles, 1966), p. 117-31. Li Chien-nung, The Political History of China, 1840-1928, translated and edited by Teng Ssu-yu and Jeremy Ingalls, (California, 1967) p. 52. Chinese defeat to the Allied British and French forces in 1960 led to the conclusion of a treaty, which granted full official toleration to the missionary, and also opened up the hinterland to the spread of Christianity. 'One result of this new set of circumstances was a considerable broadening and intensification of the tradition of Chinese hostility toward Christianity."" Though the Christian attack on ancestor worship and idolatry offended all Chinese, it is interesting to note that anti-Christian demonstrations were often provoked by the exhortations of the gentry and official class; who saw themselves as defenders of Confucian order and civilisation in a period of disorder, and whom regarded Christianity as the anti-thesis of Confucianism, and as the imperialistic tool of the foreigners. The presence of missionaries who abused their positions by interfering in lawsuits, and in shielding their converts from Chinese law, as well as the flaunting of treaty rights in demanding large amount of indemnity as payment for damages did not endear them to the local populace. The problem was particularly acute with regards to the French Catholic missionaries, and 'the Tientsin Massacre (1870) represented the culmination of a decade of Sino-foreign friction revolving around Christian missionary activities.' Paul A. Cohen, China and Christianity: The Missionary movement and the Growth of Chinese Anti-foreignism 1860-1870, (Massachusetts, 1963), p. 64. Ibid, p. 233. Deep-rooted Chinese ethnocentrism, the resentment against unwanted Western intrusion and the tendency of any society that has been seriously disturbed by internal disorders to seek an external scapegoat accounts largely for the Boxer Rebellion, which occurred when the 19 th Century drew to a close. It is perhaps telling that there was a song that went: 'Learn to be a Boxer, study the Red Lantern. Kill all the foreign devils and make the churches burn."" The churches were the most visible facades of western imperialism and hence were the targets of anti-foreign movements, and for large segment of the population, 'the missionary was the only concrete manifestation of the foreign intrusion and, as such, the only flesh and blood object against which opposition to this intrusion could be directed.' Ibid, p. 269. Cheng Pei-Kai, Michael Lestz and Jonathan D. Spence, The Search for Modern China: A Documentary Collection, (New York, 1999), p. 186. Paul A. Cohen, China and Christianity: The Missionary movement and the Growth of Chinese Anti-foreignism 1860-1870, (Massachusetts, 1963), p. 269. The role of Western influence in inspiring rebellions, and in particular the Taiping, must be considered. Though the Taiping claimed initially to be led by the Christian ideology, in reality, source of it ideas were from the rites of Chou and the Works of Mencius, interwoven with the tenets of Christianity. Also, though Hung Jen Kan had a reform proposal in which western influence can be detected, the Taiping eventually reverted back to the old Chinese corruption, habits of conservatism and of fixed ideas. Hence, though the Taipings were initially trying to identify themselves with their foreign brethren, and sought to employ Western military technology, their adoption of Western ways was very superficial, and hence the West cannot be said to have contributed to the Taiping cause in any significant way. Li Chien-nung, The Political History of China, 1840-1928, translated and edited by Teng Ssu-yu and Jeremy Ingalls, (California, 1967) p. 64 Teng Ssu-yu and John K. Fairbank, China's response to the West: A Documentary Study 1839-1923, (Massachusetts, 1994), p. 57-59. However, the West were particularly influential in helping the Qing to put down the social unrests, such as the employment of the Ever-Victorious Army and Gordon's artillery in supplementing the regular forces and local militias in putting down the Taiping. It must be said that generally, the West were adverse to social unrests, especially in the treaty ports where trade was concentrated, because social instability often interfered with trade and economic prosperity. The open intervention pursued by the West in suppressing the Taiping was to their interests: for the defense of Shanghai, and as a useful bargaining chip with the Qing government when seeking concessions. Also, in the Muslim rebellion in the Kansu, England secretly supported Yakub Beg, the leader of the rebellion, while Russia opposed the expansion of Yakub Beg's power, and under the pretense of maintaining peace along the border, occupied Ili in 1871, subdued the Mohammedan chief and moved troops into Urumchi. Hence when the West moved in to suppress social unrests, they did so only if it was within their interests to do so, should not be regarded as a philanthropic gesture. Li Chien-nung, The Political History of China, 1840-1928, translated and edited by Teng Ssu-yu and Jeremy Ingalls, (California, 1967) p. 110. Perhaps the most glaring exclusion in this essay is the role of Japan in the social unrests of the 19 th Century, but it would be convenient to evaluate Japan on the basis of a foreign power with imperialistic designs on China. The Sino-Japanese War of 1894-5 shook the Chinese self-confidence, and marked an end to the era of 'Self-Strengthening'. Though there was no particular anti-Japanese event, anti-Japanese sentiments took root during this era, and set the stage for more explicit and violent anti-Japanese movements that continue to present time. In conclusion, the social unrests of the early 19 th Century were caused more by internal than external problems, such as underlying ethnic tensions, the prevailing socio-economic conditions, the entrenched corruption and degeneration of the court and the army. However, The Opium War and the Treaty System that was imposed marked a watershed in the Qing-West relations, allowing the West to expose the weaknesses of the Qing State, making it more vulnerable to rebellions, and the rise of Western Imperialism in the quest for territorial gains and in the spread of Christianity, which led to the creation of anti-foreign nationalism, which dominated the face of social unrests in the late 19 th Century. Also, where their interests were concerned, the West would intervene to suppress social unrests in order to safeguard their commercial interests. Hence the parallel streams of development: the decline of the central power of the Qing State, and the growing western presence and penetration gave the West increasing importance in the creation and suppression of social unrests of the 19 th Century.","The debate on the memory of the Holocaust is perhaps the latest issue in the vast ocean of literature concerning other aspects of the Nazi holocaust, and this trend can be traced to the 1980s and 1990s when there was a sudden proliferation of Holocaust memorials, in the form of books, films, paintings and in particular, monuments and museums. The academic value of this enterprise must be judged, and the historian Norman Finkelstein astutely points out that 'currently all the rage in the ivory tower, memory is surely the most impoverished concept to come down the academic pike in a long time."" Yet despite this, 'the intensity of the debates about memory and the Holocaust has never abated."" The task at hand is perhaps even more disdainful because it is expressed in a normative statement, and seemingly, it beckons to the writer to take part in a doomed enterprise. The role of the impartial judge is one that is hard to play, and though it is acknowledged that the historian's task is not to judge, it is not entirely possible for the historian to distance herself from this issue, because she is necessarily the product of her time. As such, the writer does not profess impartiality, but attempts to bring in the professionalism of her discipline to tackle this issue by first calling into question the assumption that the Holocaust should be memorialised; the social and political functions of these memorials; and finally, by examining the various memorials, aim to show that the forms that memorials take often reflect their position in time and location, and more importantly, is a reflection of the society that has created them, hence the futility of this question, because the form that a memorial takes is not dictated by a single universal principle, but obeys a number of intertwined forces. Norman Finkelstein, The Holocaust Industry: Reflections on the Exploitation of Jewish suffering, (New York, 2001), p.5. Caroline Wiedmer, The Claims of Memory: Representations of the Holocaust in Contemporary Germany and France, (New York, 1999), p. 4. It is imperative to inquire into the assumption of the above question by first examining the basis by which the Holocaust should be memorialised. Should the Holocaust be memorialised on the basis of its uniqueness, as some historians have been fast to claim? If the Holocaust was truly so unique, does it not preclude the same event from ever happening again? So why is there this supposed need to commemorate for remembrance? What does one aim to achieve by commemorating it? The above questions become even more pressing with the emergence of the Holocaust Industry. Is it not ironic that the proliferation of monuments comes only in the later half of the twentieth century when the generation of survivors had largely perished? Perhaps it is because of the passing of private, living memory of the Holocaust that a vicarious and surrogate public, fixed interpretation of the past is promoted. 'Collective memory, it would seem, begins where the individual memories of a group cease, along with its members, to exist."" Or maybe it is precisely because so many decades have passed from the event that people now dare to look back at their past, and that 'the further events of World War Two recede into time, the more prominent its memorials become."" The flourishing of memorial politics in France, Germany, Israel and America do perhaps lend this question a certain direction. In places such as Germany and France where the Jews were persecuted, the upsurge of a need to know the past was perhaps a reaction against the code of silence and the 'absolute negation of the black years (les années noires)."" Building memorials was therefore seen as safeguarding the memory of the Holocaust against the cover-up of the State. Memorialisation however, is a double-edged sword, and the late historian Martin Broszat cautioned that monuments may not remember events as to bury them beneath layers of national myths and explanations. Hence, as cultural reifications, monuments coarsen historical understanding as much as they generate it. Caroline Wiedmer, The Claims of Memory, p.57. James E. Young, The Texture of Memory: Holocaust Memorials and Meaning, (Michigan, 1993), p.1. Caroline Wiedmer , The Claims of Memory, p. 33 As the title of the question suggests, what is being commemorated is not the historical event (Nazi holocaust), but rather, ideological interpretations of it. Films (such as Holocaust the television series, Schindler's List, The Pianist, Life is Beautiful etc.) in particular fuel such interpretations of the holocaust, and by trying to recreate its likeness, is in a sense trivializing it. There is not one nazi holocaust, but many versions of the real thing. Despite the attempted homage to the holocaust, the necessary dramatization of the real event in order to make it more palatable for Mass Culture is perhaps a sacrilege to the memory of the holocaust, because it distorts how one perceives it. However, this is not to deny the fact that 'such cinematic dramatizations of the holocaust have helped to shape memory of the Holocaust in the popular consciousness' allowing such films to become defenders of memory. The films raised awareness amongst the generations that had no connection to the Holocaust, and as without knowledge there is no memory, memorialisation of the Holocaust is a necessary evil. Judith E. Doneson, Holocaust Revisited: A Catalyst for Memory or Trivialisation, The Annals of The American Academy of Political and Social Science, Vol. 548, November 1996, (California, 1996), p.70. Determining the functions of the Holocaust memorials is integral to the answering of the question. How the Holocaust is memorialised, and the form it takes is largely dependant on the perceived functions of the memorials. Memorials can be built as a result of a government's need to explain a nation's past to itself (in the case of Germany and France); to educate the next generation and to inculcate a sense of shared experience and destiny; as expiations of guilt and self-aggrandizement; and even to attract tourists. An extreme case in which a memorial was built for political ends was when President Jimmy Carter proposed the building of a national memorial in Washington D.C. in order to placate Jewish supporters angered by his sale of F-15 fighter planes to Saudi Arabia. Much of the USA's enthusiasm for memorial building was in part due to the declaration of solidarity with the influential American Jews, as well as in support of her ally Israel. Similarly, the initial code of silence surrounding the Holocaust immediately after the war was due to the more urgent need for reconciliation with West Germany, in the face of the Communist threat. Hence, as James E. Young concluded 'All such memorial decisions are made in political time, contingent on political realities.' James E. Young, The Texture of Memory, p. 285. Memorials are also essential to the foundations of political legitimacy in that 'by themselves, monuments are of little value, mere stones in the landscape. But as a part of a nation's rites or the objects of a people's national pilgrimage, they are invested with national soul and memory.' This is particularly true in the case of Israel, whose political foundations rest firmly on the back of the Holocaust, to the extent that according to the historian Peter Baldwin, 'the singularity of the Jewish suffering adds to the moral and emotional claims that Israel can make ...on other nations."" Having defined themselves as a people through commemorative recitations of their past, 'the Jews now depend on Memory for their very existence as a nation.' Norman Finkelstein, The Holocaust Industry, p. 48. James E. Young, The Texture of Memory, p. 211. The social functions that are fulfilled by memorial building can be quite diverse. On the one hand, it is useful for consensus building: 'in the absence of shared belief or common interests, Art in public spaces may force an otherwise fragmented populace to frame diverse values and ideals in common spaces. By creating common spaces for memory, monuments propagate the illusion of common memory."" On the other hand, memorials relieve the population of the burden to remember, 'under the illusion that our memorial edifices will always be there to remind us, we take leave of them and return only at our own convenience."" Despite the various facets of remembrance, there is a general consensus that monuments are meant for public consumption, and hence, a part of the answer to the question of how the Holocaust should be memorialised would rest on public opinion and expectations. James E. Young, The Texture of Memory, p.6. Ibid, p.5. The main debates on the form a Holocaust memorial should take will be discussed here, under the main themes of Abstract vs. Realist, Jews vs. Non Jews, Secular vs. Religious and the anti-monument. It would be worth noting that the backing of the State, as well as public opinion, is necessary for the memorial to serve its purpose. In every case, Holocaust memorials reflect not only national and communal remembrance, but also the memorial designer's own time and place. For contemporary artists (working in an era of abstract expressionism, conceptual art, post-modern and de-constructionist design), though there is a recognition that they are accountable to both art and memory, Albert Elsen reminds us that for them 'the needs of art, not the public memory come first."" However, this is in direct opposition to many survivors, who 'believe that the searing reality of their experiences demand as literal a memorial expression as possible.' Hence the dilemma of the artist, an example of whom is Nathan Rapoport, whose Warsaw Ghetto Monument 1948,is probably the most widely known, celebrated and controversial. Conversely, Sol LeWitt's unconventional Memorial to the Missing Jews, 1989, in Germany had to bow to popular opinion to have it removed from the Platz der Republic, in Hamburg. Ibid, p.8 It must be remembered that Jews, as well as non-Jews perished during the Nazi holocaust, and it should be the latter's entitlement to share in the remembrance and mourning of the former. In a particularly volatile situation in 1988 Germany, Leah Ross, a journalist demanded that a memorial be set up for Jews alone. This provoked an outcry from Romani Rose of the Central Council of the Sinti and Roma that this 'represented a hierarchization of victims, which is insulting for the victims of the genocide and for the survivors of the Sinti and Roma majority."" It would make sense in Israel to dedicate the memorial exclusively to Jews, but given the racial composition in Germany, it would be impertinent to do so. Caroline Wiedmer, The Claims of Memory, p. 144. Due to the intertwined relationship of Jewish suffering (the Shoah) and religion, commemoration of the former inevitably involves the latter. As James E. Young suggests, 'Israel's over-arching national ideology and religion may be memory."" Victims of the Shoah are portrayed as martyrs who died for their faith, fighters who are remembered for their part in the state's founding. Contrast this to the secular state of France, who, during one of the national competitions for the Vél d'Hiv Monument, picked the literal figurative design of Walter Spitzer over that of Shelomo Selinger because the ministry felt that the latter's proposal was 'Too Jewish.' As such, it is highly apparent that how the Holocaust should be memorialised depends largely on cultural and national factors. James E. Young, The Texture of Memory, p. 210. An interesting development would be that of the counter-monument. This is particularly so with regard to the new generation of German artists, who 'have a deep distrust of monumental forms in light of their systematic exploitation by the Nazis, and a profound desire to distinguish their generation from that of the killers through memory."" These artists explore both the necessity of memory, and their incapacity to recall events they never experienced directly, hence in capturing the memory of events, remember therefore only their own relationship to it. The Gerzes' counter-monument is perhaps the best example of this. Theirs is a self-abnegating monument, a 'vanishing monument' which, instead of graciously accepting the burden of memory, throws it back at the town's feet, forcing them to remember for themselves. This audaciously simple idea challenges the idea of monumentality and the permanence of collective memory, and provokes the population to examine themselves as part of the piece's performance. Ibid, p.27. The writer's main emphasis on public monuments reflects her bias in thinking that such forms of public art are more effective in inculcating a common sense of destiny, and fulfilling the social and political functions attached to them, more so than films, books and painting can. With the boom of the Holocaust industry however, films have become an essential medium by which large populations can learn more about the Holocaust. The writer has avoided a direct answer to the question up to this point, but has attempted to illustrate the many debates and considerations that have arise from the question. In conclusion, the Holocaust should be memorialised according to its time and location, asserting its relevance in the lives of the denizens there. Also, it is worth noting that 'new generations visit memorials under new circumstances and invest them with new meanings', and since there is no one universally accepted way of memorialising the Holocaust, the normative connotation posed by the question above is self-defeating, and ought not to be answered by a historian. The history of empires is a long one: beginning with the Greek, Roman and Persian empires, which pitted race against race; the empires of Spain, Portugal, the Netherlands and Britain in the sixteenth and seventeenth century, which saw the rivalry between monarchies; and finally, the 'New' Imperialism of the mid-nineteenth century, which saw a renewed interest in and frantic scramble for, new colonies, between nation-states. 'By early twentieth century, Britain, France, Germany, Italy, Belgium, the Netherlands, Denmark, Spain, and Portugal together held sway over almost 84 percent of the earth's surface."" Imperialism had become not only fashionable, but also defined the era from the mid-nineteenth onwards to World War I. Despite many attempts to attach priority to a certain aspect of imperialism as being the driving force behind the phenomenon, it is perhaps more helpful to understand it as a product of a complex web of motives and concerns. Also, imperialism must be examined in the context of its time (post-enlightenment ideas, rapid industrialisation, the formation of scientific theories which supported the superiority of one race over another, nationalism, and the economic depression of 1873), without which, the phenomenon either would not have occurred, or might have taken on a different form. Alice L Conklin and Ian Christopher Fletcher, European Imperialism (ed.) (1830-1930), (Boston 1999) p. 1. Imperialism, according to Hobsbawm was 'the child of an era of competition between rival industrial-capitalist national economies which was new and which was intensified by the pressure to secure and safe-guard markets in a period of business uncertainty."" Lenin also suggested that 'Imperialism emerged as the development and direct continuation of the fundamental characteristics of capitalism in general."" Despite the acknowledged importance of economics, there is a tendency for Marxist historians to over-emphasise its role. 'The importance of economic interests and calculation in the expansion of European influence and control over the extra-European world is undeniable."" It is one thing to concede the importance of economic interests, and another to assert that imperialism was primarily an economic phenomenon. Eric Hobsbawm, Age of Empires 1875-1914, (New York, 1989) p.64. Conklin and Fletcher, European Imperialism (ed. ), V.I Lenin on 'Imperialism, the highest stage of Capitalism', p. 38. Andrew Porter, European Imperialism (1860-1914), (Malaysia, 1994), p. 38. 'New' imperialism was only possible in the light of rapid industrialisation, which peaked during the mid-nineteenth century. Industrialisation required raw materials such as tin and iron to make steel, which could be obtained cheaply in the colonies; the movement from rural to urban areas, the subsequent shift from agriculture to industries, and the surge in population put severe strain on food production in the metropolis, which would be alleviated if the colonies took on the function of being the 'granaries of Europe'. Also, with the efficiencies of machinery, colonies could provide the markets needed to absorb the increased production. 'Moreover, cheaper markets were required to offset the general fall in prices from the 1870s to the 1890s (economic depression); with most industrial powers adopting protectionist tariffs, colonies could serve both needs."" Finally, there is the theory of 'surplus capital' as expounded by Hobson 'aggressive imperialism...is a source of great gain to the investor who cannot find at home the profitable use he seeks for his capital, and insists that his government should help him to profitable and secure investments abroad...'  Porter, European Imperialism, p. 39. Conklin and Fletcher, European Imperialism (ed. ), J.A. Hobson on 'The New Imperialism', p. 15. It would seem that all of the above mentioned was best associated with British imperialism more than any other. At a glance, Britain's colonies were the most economically viable, with particular regards to India and South Africa. Using the data from the geographical distribution of British long-term capital investment overseas (as of 1913), out of the 1,780 million pounds invested in her empire, 378.8 million pounds was invested in India and Ceylon, while 370.2 million pounds went to South Africa. Moreover, the existence of chartered companies, such as George Goldie's Royal Niger Company, Sir William Mackinnon's British East Africa Company, Cecil Rhodes' British South Africa Company and the British East India Company (EIC) in Asia would seek to reinforce the pattern of British colonial rule: the ideal situation being that of an informal empire, which would allow effective economic exploitation without the additional costs of setting up a governing administrative body and the moral responsibility attached to a formal colony. Porter, European Imperialism, p. 41, table 1. There are however, cracks in the theory of over-riding economic concerns in British imperialism. Firstly, it must be said that possessions that paid off handsomely, such as India and South Africa, were extremely rare and were lacking amongst the possessions of the other colonial powers. Hence 'with few exceptions, colonial trade remained throughout this period an insignificant proportion of metropolitan commerce...'. Instead, most of Europe's long-term overseas investment went significantly to areas outside of the formal colonial empires: French capital went into Russian bonds, German to Austria-Hungary and British to Latin America (in particular Argentina) and the other white-settlement former colonies (Canada and Australia). Moreover, very few colonies were actually profit-making entities, with the Germans colonising large areas of the African desert, and French indo-china being incomparable to the economically viable British colonies in the region. Porter, European Imperialism, p. 40. The role of ideology in imperialism cannot be overlooked, though many have dismissed it as a cloak for economic exploitation, there is still much to be said about Social Darwinism, the ""White Man's Burden"" and the moral responsibility attached to it. In the light of the Enlightenment ideas of the link between change and progress, and Darwin's theory of evolution, to remain stagnant was to be weak. This change in ideas also marked a change in the perceptions of the colonial masters, who increasingly stressed the difference between the cultures of the metropolis and the colonies. Also, the advent of Science in promoting such racist ideas succeeded in explaining irrational beliefs in rational terms '...evolutionary explanation of racial differentiation and the mechanisms of natural selection seemed to explain and justify cultural differences in terms of inherent racial capacity and a natural hierarchy..."" This sense of superior culture brought with it the moral responsibility, that 'societies which had travelled ahead...had some duty to offer guidance, instruction and even to rule."" This was manifested in the ways the colonial masters tried to 'educate' the natives about proper 'Western' hygiene, setting up public schools to educate the natives in the colonial language, proper dressing (wearing shoes), western consumerism, and very importantly, through the spread of religion (Christian or Catholic faith). Another feature of the 'White Man's Burden' would be that of humanitarian concerns (whether rhetoric or reality), such as the abolishment of slavery in all French and British colonies, as the parallel movement alongside economic exploitation. Ibid, p.24. Phillip Darby, Three faces of Imperialism (British and American Approach to Asia and Africa 1870-1970), (Avon, 1987), p. 31. Though all the colonial powers claimed to be civilising their colonies, it was 'only in republican France (that) this claim (was) elevated to the realm of official imperial doctrine."" Nineteenth century France was the great missionary power of the Roman Catholic world, and it can be argued that the colonisation of Vietnam was partly the result of the French desire to protect their missionaries in Vietnam from official persecution. However, the French were unique in believing in the universality of French values, and the perfectibility of human kind, and this perception was not evident amongst the other European colonial masters, who generally, as a rule avoided 'over-education' of the natives. Alice Conklin , A Mission to Civilise, the republican idea of empire in France and West Africa 1895-1930, (California 1997).p. 1. To look at imperialism from a cynical point of view, it must be said that where economic and humanitarian interests were conflicting, the former would always triumph. Empirical observations regarding the sale of opium to Britain's Southeast Asian colonies, and that of 'trader gin' (a noxious but potent narcotic) in West Africa are telling of the priority of economic over humanitarian concerns. Abolishment of slavery was possible because it benefited both parties. Also, the presence of missionaries/explorers such as David Livingston could not compete with that of the international community of businessmen and investors. A very poignant example would be that of Belgium Congo, where King Leopold colonised it on the basis of 'humanitarian reasons', only to use the local population as slave labourers. Though it is extremely difficult to argue that cultural influences were the direct causes of European imperialism, it nevertheless 'assisted in creating the general circumstances within which specific instances of imperial domination...became for contemporaries, not only imaginable but acceptable."" The consequences of cultural transfers are still seen in our modern world, where the colonies largely taken on the parliamentary model of their former colonial masters. This change has its roots in the era of colonisation where the indigenous political and often social hierarchy was replaced by the political institutions and protocol of the metropolis. The adoption of the colonial language as the working language, the western architecture of new buildings and the assimilation into the cash-economy, are all examples of the transfer of culture. Similarly, there were many ways in which the natives have inspired European art and lifestyle. Porter, European Imperialism, p. 27. An examination of purely economic and cultural aspects of imperialism has failed to truly grasp the essence of this significant event in history. The economic and cultural aspects were more of consequences rather than the causes of imperialism, and should be put in their correct places, under the broader themes of Power, Rivalry (stemming from Nationalism), Fear and Regeneration, which are interrelated concepts. Even so, the economic and cultural consequences were in fact additional benefits to be gained from imperialism, and not the actual aims that the imperialists wanted to achieve. The acquiring of colonies was seen by many countries as the means by which they could attain the status of world power. This is particularly so for new countries like Italy and Germany, which saw their participation in imperialism as heightening their world standing. For an established nation like Britain however, imperialism was a defensive reaction, derived from a sense of insecurity and threats from newly industrialising powers like Germany, America and Russia, and the subsequent assertion of her power. As for France, she saw imperialism as the opportunity for the expansion of her army and navy; the role successful imperialism would play in exorcising her defeat in the Franco-Prussian war of 1871, and rebuild her status as a world power. Germany's fear of encirclement also played a part in the formulation of 'Weltpolitik'. It was not only the external fear that countries were trying to undermine each other in the race for colonies, but more importantly, there was the fear of internal degeneration, and that imperialism was the cure to the diagnosis of national ills, revitalising the country, restoring its self confidence, and reinforcing its power abroad. Imperialism was often the exclusive prerogative of a small group of elites, but when the issue of national pride was at stake, and rivalry ensued, the public could often be aroused into supporting imperialist policies (clearly seen in the example of France.) Germany's aggressive 'Weltpolitik' (World policy) to secure her 'place in the sun' was an extension of the Anglo-German rivalry towards the end of the nineteenth century. Perhaps it can be suggested that, the above arguments regarding economic and cultural aspects were merely the manifestation of the broader concept of European rivalry. Linked to this concept is the theme of nationalism. The situation in Europe was balancing on a knife-edge after the unification of Italy and Germany, and the outward movement of expanding into a vast territory of unconquered plains was profitable not only as a safe outlet for the consumptive force of nationalism, but also to maintain the balance of power in Europe (Africa as the periphery of Europe). Finally, there is the suggestion put forward by some historians that it was the domestic situation in Europe itself that led to imperialism. Social Imperialism, whereby '...expansionist policies are consciously devised to unite the nation and to defuse tensions at home, while simultaneously avoiding significant domestic reforms."" In addition, the German 'Sammslungpolitik' (the gathering together of the elements that would naturally support the state) against the domestic threat from the Social Democrats, which coincided with their enthusiasm for imperialism, would seek to reinforce this theory. Porter, European Imperialism, p.32. In conclusion, it is inadequate to understand imperialism primarily either as an economic or cultural phenomenon, simply because both were secondary concerns in the impetus to colonise. Economic determinism is the prerogative of Marxist historians, and it should be kept this way; for the vast majority of the other historians who have attempted to write about imperialism have placed due recognition to the other factors that have contributed to the motives of imperialism, which provide a more balanced view of this phenomenon. Moreover, the question specifically refers to European, and not British imperialism, hence, the assertion of economic interests as the primary force in imperialism would seem highly Anglo-centric in the light of the arguments put forth in this essay. It is perhaps more accurate to see the economic and cultural aspects as being the consequences of imperialism, and since 'primarily' suggests some sense of causation, it would be myopic to insist upon a direct answer to this question.",False
92,"Traditional accounts on the causes of the Reformation necessarily included castigation of clerical abuses, such as concubinage, pluralism and absenteeism; the ignorance and poor training of priests and monks; and perhaps even the supposed failure of papal leadership as manifestations of the decay and decline of the Church on the eve of the Reformation. This view, and the more recent revisionist view of the topic in question has informed the main body of debate in scholarly circles, but before embarking on the essay, there are a few points that deserve attention. Firstly, the question suggests that what was 'wrong' with the Church could have led, in some sort of linear equation, to the birth of the Reformation. Following in the footsteps of Peter Marshall, ""the intention is not to sift the evidence for signs of impending Reformation"", and teleological history should be avoided due to the inaccurate interpretations and conclusions which may emerge from that approach. Secondly, it is ambiguous as to what constitutes 'the Church'. According to Robert Swanson, there are two main definitions: ""the restricted, institutional definition, which segregated the 'ecclesiastical' from the 'secular'..."" and ""the body of the faithful which made up the mystical body of the church which was also the body of Christ."" To limit the scope of this essay, the first of Swanson's two definitions will be taken up, noting however, that this requires a generalization of the circumstances in the various national churches. This essay will avoid a direct, simplistic answer to the question but will instead attempt to put the criticisms of the Church into perspective and suggest that there was a certain measure of continuity, rather than exclusivity to the eve of the Reformation; and to locate the essay within the confines of the changing climate of opinion that may have contributed to the perceived problems of the Church. The recurrent themes of Expectations and Reform will be examined in the following context: the Clergy, the Papacy, Doctrines and Practices, and will revolve around the issue of the function of the Church. Peter Marshall, Reformation England 1480-1642, (Cornwall, 2003), p. 2 Robert N. Swanson, Religion and Devotion in Europe 1215-1515, (Cambridge, 2003), p. 7 The failure of the Clergy to live up to lay expectations of moral superiority (derived from being the agent through whom man communicated with God) and to fulfill the functions they were ordained for opened up the Church to criticisms, from without and within. 'On the eve of the Reformation, laity protested against pervasive clerical fiscalism, absenteeism, mal-administration, and concubinage..."" and were joined in their castigations, by some members of the Clergy, and more notably, the humanists. John Colet, the humanist dean of St Paul's even went as far as to blame ""all the ills of the Church on the secular lifestyles and 'covetousness' of the Clergy"" and urged ""reformation of the Church's estate"". According to Duggan, even ""Catholic and Protestant historians alike largely concur on the (sexual) 'immorality' of many Clergy on the eve of the Reformation."" However, a reconsideration of the reaction to the abuses of the Clergy is necessary. Despite the widespread practice of concubinage, the population of Europe was not overly concerned unless this moral failing on the part of the Clergy impinged on their primary role of administering the sacraments. As Duggan articulates succinctly: ""whether the clergy were 'immoral' or not, the more germane question is whether they ministered adequately to the needs of their flock."" Steve Ozment, Age of Reform: An intellectual and Religious History of Late Medieval and Reformation Europe, (Newhaven, 1980) p, 211. Marshall, Reformation England, p.11 Lawrence G. Duggan, The Unresponsiveness of the Late Medieval Church: A Reconsideration, Sixteen Century Journal, Vol. 9, No. 1 (April 1978), p. 23 Perhaps, it would be plausible to suggest that the more pressing considerations on what was 'wrong' with the Clergy would be ""pluralism and absenteeism, which are frequently construed as abuses and signs of decay and pastoral neglect"" The centrality of the Church in providing ""contact between human beings and the supernatural order of their universe, to explain, direct, and console"" is crucial to the understanding of late medieval religion; and the perceived failure of the Church in providing the above services would shake the foundations on which it was built. The Church was faced with two main dilemmas: firstly, it had inadequate jurisdiction over the appointments to benefices, which increasingly became the prerogative of the rulers. Also, clergymen seeking to further their knowledge and competence often enrolled in the universities, with the obvious consequences of being away from their benefices. Despite pluralism being widespread, it did not necessarily indicate neglect, as absentee beneficiaries were required by canon law to provide a vicar to cater to the needs of the local populace. Ibid, p. 23 Euan Cameron, The European Reformation, (New York, 1991), p. 9 What proved more destructive than the above-mentioned abuses ""were the words and ideas of clerics...born of an impatience with the imperfect established church and a moral vision of purified Christianity."" The more prominent members of this group include John Wycliffe, Erasmus and Luther. It is noteworthy that criticisms of the Church went as far back as to the late 14 th Century (the Lollards) and were not exclusive to the eve of the Reformation. The irony lies in the fact that the Church had effectively 'dug its own grave' by educating these men-empowering them with knowledge which allowed for the formulation of reformist ideas and expectations of change, which clashed, at every level, with the vested interests of the clerical class. Duggan, The Unresponsiveness of the Late Medieval Church: A Reconsideration, p. 25 The fact that the Church was becoming more accessible to the Laity may have opened up the former to criticisms from their congregation. The increased interaction between the Clergy and the Laity may have led to the exposure of church flaws which may seem less apparent previously; and the printing of the provincial decrees and sermons denouncing clerical abuses, by the church, in the vernacular ""might have convinced laymen not only of the widespread efforts of reform but also of the widespread problems of the church ...raising in some circles expectations that could not be realized."" Ibid, p. 14 The Papacy was also not immune to criticisms of 'worldliness', and the Renaissance popes provided the best illustration of this by participating in the struggle for temporal power, concubinage, and as patron of the Arts. Since the thirteenth century, reformers have 'found themselves at cross-purpose with a papacy whose pastoral role had become increasingly confused with its political and economic ambitions."" However, far from being a power-hungry and oppressive apparatus, it can be said that the Papacy had undermined its own power through the setting up of Universities, encouragement of printing, and even patronage of the humanists. The Papacy has often been accused of being lofty and unresponsive, but in reality, was the main impetus behind the improvement of society, largely through the channeling of Church resources into building and maintaining educational institutions that served both the state and the society at large. Most significantly, they 'took under their aegis the devotio moderna the principles of which strikingly foreshadowed the worth and dignity fully accorded laymen by the reformers of the sixteenth century.' Ozment, Age of Reform, p.208 Duggan, The Unresponsiveness of the Late Medieval Church: A Reconsideration, p.16 It is imperative to examine the doctrines and practices (in particular, the deep belief in Purgatory, the sacrament of Penance, and its derivatives such as relics, pilgrimages and most notoriously, indulgences) of the Church, which generated oppositions from the laity and clergy. Interestingly, criticisms of Church practices and doctrines existed in the late fourteenth century, in the form of Lollardy (led by the Oxford theologian John Wycliffe) called for a clear divide between spiritual and temporal power; obedience to the church authorities as far as their behaviour merited; the Bible as the only source of religious authority; condemned the doctrine of transubstantiation; and attacked the veneration of saints (and relics). The humanists also shared the view that the Church had diverged away from the one true religion, embraced misconstrued beliefs and had become too intertwined with the secular world. Convinced that grace and forgiveness could only come from God alone, Luther also harshly condemned the sale of Indulgences in his ninety-five theses. However, the cynicism surrounding the practice of Indulgences were not unique to the eve of the Reformation, and was a pervasive element in literary traditions, as seen in Chaucer's Pardoner's Tale which was written in 1400. It is interesting to note however, that the supply of indulgences was in response to the demands of the laity, and was perhaps a manifestation of the paranoia of securing one's own salvation. What is perhaps significant is that the demanding penitential system and mechanical ritualized religion offered by the Church only provided temporary relief for the laity, and had led instead to the exacerbation of fear and anxiety as reflected in the obsession with the constant absolving of sins. Marshall, Reformation England, p. 16 According to Steve Ozment, one of the chief failings of the Church lay in 'the absence of a distinctive concept of lay religious life and the consequent imposition upon the laity of traditional clerical ideals'; and the proliferation of confraternities, in addition to the earlier movement of the devotio moderna are perhaps indicative of the failures of the church to cater for lay piety. The advocacy of clerical piety as the zenith of religious piety had placed unrealistic expectations on the laity and precluded their search for consolation through the Church. The incongruity of this enterprise was not lost on the increasingly literate laity, who rejected the ""prominent clerical ideals of obedience and sexual impurity"" in favour of ""simplicity, directness and respectful treatment in all spheres of their lives."" Moreover, as mentioned above, the behaviour of the clergy did not correspond to the standards that church traditions had imposed on them, and policies such as enforced celibacy increased the incidence of breaches in moral standards that served to reinforce, amongst the laity, the futility of this endeavour. Ozment, Age of Reform, p. 219 Ibid, p. 220 Some historians have also suggest that the increasing desire amongst the laity for a personal and direct relationship with God, and the subsequent failure of the Church to cater to this aspect led many into the realm of mysticism. Though some have argued that mysticism could be regarded as a form of subversion, it is perhaps more appropriate to think of it as a supplementary to, rather than as a substitute for the institution of the Church. Mysticism of both the fourteenth and sixteenth century coexisted with the Church and may be seen as testament to the religiosity of the Age. There is a possibility that the multiple responsibilities of the Church in society may have led to its resources becoming over-stretched, hence, compromising on the primary role of the Church to minister the sacraments. Swanson puts forth the idea that most of the anti-clericals ""wanted churchmen to behave just as churchmen, rather than officious, interfering, lordly bureaucrats."" However, the Church had for centuries combined both its temporal and spiritual responsibilities without much difficulty, and it appears rather peculiar that the church would find it hard to cope with its dual roles on the eve of the Reformation. Moreover, the increased accessibility of university education provided the necessary training for aspiring civil servants, and this would have taken the burden off the Church as a source of administrative officials, implying perhaps that the Church could concentrate more on its primary, spiritual function. Although there were many problems (both discipline and doctrinal) that the Church faced, it is apparent that these problems had been in existence for the past one or two centuries, and that despite the criticisms, the aim of the reformers was not to subvert the church, but had in mind the desire to change the church into a more responsive, relevant and effective apparatus. Throughout the essay, the writer has tried to suggest that it was the rising expectations of a growing educated class (laity and clergy) and the inability of the church to keep up the pace of reform and to satisfy the needs of the laity - the disparity between expectations and reality- that led to disillusionment. Similarly, the high expectations that the Church set with regards to piety eventually led to the alienation of the people from the Church. Despite the deep-rooted flaws of the church, the attempts at reforming it suggest ""men were seeking salvation and wished to find it with the help of the church."" Hence, in trying to account for the Reformation, it is inadequate to merely examine what was 'wrong' with the church, but requires instead, a broader understanding of the climate and considerations (political, economic and social) of the Age. B.Moeller 'Religious life in Germany on the eve of the Reformation', in G. Strauss (ed.) Pre-Reformation Germany, (London, 1972) p. 27 Social unrests were not unique to the 19 th Century, and the Qing state was by no means inexperienced in putting down such attempts to destabilize the State. Despite the consolidation of power during the reign of the Emperor Kangsi to Qianlong, there remained however, certain underlying grievances that were never fully eradicated, which led to the persistence of the themes such as 'Over-throw the Qing, Restore the Ming', ethnic and religious tensions. The end of the Qianlong era marked the conclusion of the golden age of Qing rule and saw the gradual decline of central power vis-à-vis the provincial power and the emerging western participation in Chinese politics. Though westerners have had a presence in China dating back to the Yuan dynasty, the Opium Wars and the corresponding treaty system marked a watershed for China-West relations, resulting in the change of status quo, of perceptions; and manifested itself in the increasingly intrusive presence of the West, which exposed the many weaknesses of the Qing state, making it more vulnerable to attacks from within. It is in this context that the above question should be addressed, and the writer will examine firstly the internal conditions and entrenched corruption which made it conducive for the outbreak of social unrests, followed by the implications of change brought about by the West, such as the desire to trade, introduction of new technology, ideas, religion, and the pursuit of Imperialism which clashed with Chinese assumptions of the monopoly of civilization, and in turn produced a virulent anti-foreign nationalism which threatened the social order during the period in question. Hence the writer will argue that the social unrests at the beginning of the 19 th Century was due more to the internal rather than external factors; but increasingly, the presence of the West is crucial to the emergence of violent xenophobic nationalism, which account for the social unrests in the later part of the 19 th Century. One of the persistent problems that the Qing failed to eradicate were the deep-rooted ethnic tensions which lent itself to many rebellions at the first half of the 19 th Century. The entrenched sentiments that the Qing were a foreign power residing in China led to the creation of many secret societies, such as the White Lotus Society and the Heaven and Earth Society (Tian-li-jiao), whose rallying cry of 'Overthrow the Qing and Restore the Ming' saw the outbreak of the White Lotus Revolt in 1796, Szechuan and Hubei, which lasted until 1904, as well as the attempt made by Li Wen Cheng to attack the imperial palace in 1813. The prejudices that the Han Chinese held against the Muslims and the latter's desire for autonomy from the Qing, led in the 1820s to the Muslim rebellion led by Jehangir in Turkestan, and subsequently, in the two Muslim rebellions, in Yunnan (1855-1873) and in Gansu and Shaanxi (1862-1873). The Taiping rebellion has its roots in ethnic tensions as well, with Hong Xiuquan and a considerable number of his followers coming from the Hakka (ke jia ren) sub-group, and these 'guest-settlers' were frowned upon by the largely Han population in Guangxi. The anti-Manchu sentiments of the Nian rebellion is perhaps most clearly indicated by their leader Zhang's honorific title of 'Great Han Prince with the Heavenly Mandate'. Jonathan D. Spence, The Search for Modern China (2nd Edition), (New York, 1999), p. 184. Ethnic tensions alone do not cause rebellions, but mixed with deplorable economic and social conditions can be a recipe for disaster. Between 1741-1841, the population increased by three-fold, but the increase in arable land could not keep pace with it, which led to food and land shortages, with the latter commodity increasingly becoming concentrated in the hands of the wealthy. The yearly occurrence of floods and droughts in the provinces along the valleys of Yellow and Yangtze rivers during the end of Daoguang's reign (1821-1850) and the lack of state initiatives in alleviating the situation may have led the population to believe that the mandate of Heaven was passing out of the hands of the Qing dynasty. Also, the Opium trade, conducted by the British led to the great outflow of silver, resulting in the devaluation of copper coins, the inflation in the prices of other commodities, and the corresponding increase in the peasants tax burden which was paid in silver. All these combined events accentuated the miseries of the common people and made for fertile ground for the outbreak of social unrests. Potential rebellion leaders could find amongst the population large numbers of poor peasants and artisans who were willing to throw their weight behind anything that promised better conditions than the existing one. The attractiveness of the Taiping laid in its Christian-Communism ideology and the proclamation that 'nowhere will inequality exist, and no one not be well fed and clothed."" The triggers for the Muslim rebellion in Yunnan 'were the heavy land taxes and extra levies imposed by Peking on the Yunnanese Muslims' who in addition to that, were being ousted out of their sliver and gold mines by the Chinese. Li Chien-nung, The Political History of China, 1840-1928, translated and edited by Teng Ssu-yu and Jeremy Ingalls, (California, 1967) p. 49. Jonathan D. Spence, The Search for Modern China (2nd Edition), (New York, 1999). p. 176. Ibid, p.187. The inability of the State to alleviate the situation plaguing the population, and the ineffectiveness of the Army in putting down the rebellions are perhaps telling of the decay within the Qing central administration, which permeated to the lower level officials. The appalling inertia of the State led Zeng Guofan to condemn the ruinous tendency of officials 'to gloss over, to make up and to steal days of ease."" The two chief ministers under Daoguang's reign, Tsao Chen Yung and Mu-chang-a discouraged officials from talking about national policy, and those who dared petition to the throne were threatened with punishments. It is perhaps not surprising then, to find the character of Chen Tsu-ch'en, the governor of Guangxi who did not bother with administrative affairs, bit prayed instead to Buddha to dispel catastrophes. 'Such was the immediate political background-general lethargy and irresponsibility-which paved the way for Taiping's rapid development."" The corruption and lack of discipline within the Army was so rampant, that by the time of the White Lotus Rebellion, local militias had to be organized to compensate for the ineffectual standing army. Similarly, it was Zeng's Hunan army, Li Hongzhang's Huai army and Zuo Zhongtang's army, that suppressed the Taiping, the Nian and the Muslim rebellions. Also, inner court intrigues dominated the post Xian-feng era, which saw the rise of the Empress Dowager Cixi and her cronies, and in turn compromised the power of the Emperor to act as the unifying force of China. The diversion of resources meant for military and emergency purposes to fund the decadence of the court left China in a weakened state, creating ample opportunities for the outbreak of rebellions. Li Chien-nung, The Political History of China, 1840-1928, (California, 1967), p. 49. Ibid, p.50. The Chinese Pandora box was sitting there, waiting to be unlocked, and when the West came to pry China open, they unleashed torrents of tensions, discontentment and grievances, which when mixed with Western military technology, religion and ideas, became more malignant, and threatened to topple the Qing state. The rise of Capitalism and the nation-state in the West led to the corresponding desire to gain economic advantage and prestige via the acquiring of colonies, and largely accounts for the increased presence of the West, and the advent of the Age of Imperialism, in China and else where (seen also in the 'Scramble for Africa') leading to the creation of anti-foreign nationalism, which is a major source of disturbances during the later half of the 19 th Century. The role of the West became increasingly important because the weak central Qing government were defenseless against the military superiority of the West, as seen in the Opium War. Correspondingly, the treaties that were concluded reflected this imbalance, and. 'under the treaties, China's sovereignty was increasingly impaired."" 'The formative decades of the treaty system in the 1840s and 1850s must therefore be seen as the opening phase in an intricate and portentous growth of foreign influence on Chinese life..."" and the creation of new institutions for Sino-foreign contact. China had to cede Hong Kong permanently to the British, open up Amoy, Fuzhou, Ningpo, Canton and Shanghai for trade, allow the British to fix customs duties, permit extraterritoriality and the establishment of equal basis for official correspondence, all of which were seen as attacks on the sovereignty of China, and the inability of the Qing to protect Chinese territory led to the creation of anti-Manchu as well as anti-foreign sentiments. In particular, the tradition of anti-foreign resistance can be traced to the San-yuan-li incident in Canton 1841, in which '...specific incitements (raping and defiling of temples in San-yuan-li), together with pervasive popular anxieties about the war, helped to transform relatively benign stereotypes about foreigners into xenophobic racism."" Also, the period from 1841-1850 saw the upsurge of social disorders in Canton, and may have precipitated the Taiping Rebellion. 'In general after the Opium War, the prestige of the Manchu government was destroyed and the invitation to rebellion was ever-present...from 1841-1850 there was not a single year free from local uprisings."" In retaliation to the Qing obstinate refusal to honour the treaties, the allied British and French forces stormed Peking, burned the Yuan-Ming-Yuan, and dealt Qing China a big blow to its prestige when Emperor Xian-feng had to flee to Jehol. This demonstrated clearly that the Manchus were no longer able to be guardians of China, and created a power vacuum within China, and made her vulnerable to attacks from within and without. John K. Fairbank, The Creation of the Treaty System, in Dennis Twitchett and John K, Fairbank (ed. ), The Cambridge History of China, Vol. 10 Late Ching 1800-1911, Part I, (New York, 1995), p. 214 Ibid, p.215. Frederic Wakeman Jr, The Canton Trade and the Opium War, in Dennis Twitchett and John K, Fairbank (ed. ), The Cambridge History of China, Vol. 10 Late Ching 1800-1911, Part I, (New York, 1995), p. 202. Frederic Wakeman Jr, Strangers at the Gate: Social Disorder in South China 1839-1861, (Los Angeles, 1966), p. 117-31. Li Chien-nung, The Political History of China, 1840-1928, translated and edited by Teng Ssu-yu and Jeremy Ingalls, (California, 1967) p. 52. Chinese defeat to the Allied British and French forces in 1960 led to the conclusion of a treaty, which granted full official toleration to the missionary, and also opened up the hinterland to the spread of Christianity. 'One result of this new set of circumstances was a considerable broadening and intensification of the tradition of Chinese hostility toward Christianity."" Though the Christian attack on ancestor worship and idolatry offended all Chinese, it is interesting to note that anti-Christian demonstrations were often provoked by the exhortations of the gentry and official class; who saw themselves as defenders of Confucian order and civilisation in a period of disorder, and whom regarded Christianity as the anti-thesis of Confucianism, and as the imperialistic tool of the foreigners. The presence of missionaries who abused their positions by interfering in lawsuits, and in shielding their converts from Chinese law, as well as the flaunting of treaty rights in demanding large amount of indemnity as payment for damages did not endear them to the local populace. The problem was particularly acute with regards to the French Catholic missionaries, and 'the Tientsin Massacre (1870) represented the culmination of a decade of Sino-foreign friction revolving around Christian missionary activities.' Paul A. Cohen, China and Christianity: The Missionary movement and the Growth of Chinese Anti-foreignism 1860-1870, (Massachusetts, 1963), p. 64. Ibid, p. 233. Deep-rooted Chinese ethnocentrism, the resentment against unwanted Western intrusion and the tendency of any society that has been seriously disturbed by internal disorders to seek an external scapegoat accounts largely for the Boxer Rebellion, which occurred when the 19 th Century drew to a close. It is perhaps telling that there was a song that went: 'Learn to be a Boxer, study the Red Lantern. Kill all the foreign devils and make the churches burn."" The churches were the most visible facades of western imperialism and hence were the targets of anti-foreign movements, and for large segment of the population, 'the missionary was the only concrete manifestation of the foreign intrusion and, as such, the only flesh and blood object against which opposition to this intrusion could be directed.' Ibid, p. 269. Cheng Pei-Kai, Michael Lestz and Jonathan D. Spence, The Search for Modern China: A Documentary Collection, (New York, 1999), p. 186. Paul A. Cohen, China and Christianity: The Missionary movement and the Growth of Chinese Anti-foreignism 1860-1870, (Massachusetts, 1963), p. 269. The role of Western influence in inspiring rebellions, and in particular the Taiping, must be considered. Though the Taiping claimed initially to be led by the Christian ideology, in reality, source of it ideas were from the rites of Chou and the Works of Mencius, interwoven with the tenets of Christianity. Also, though Hung Jen Kan had a reform proposal in which western influence can be detected, the Taiping eventually reverted back to the old Chinese corruption, habits of conservatism and of fixed ideas. Hence, though the Taipings were initially trying to identify themselves with their foreign brethren, and sought to employ Western military technology, their adoption of Western ways was very superficial, and hence the West cannot be said to have contributed to the Taiping cause in any significant way. Li Chien-nung, The Political History of China, 1840-1928, translated and edited by Teng Ssu-yu and Jeremy Ingalls, (California, 1967) p. 64 Teng Ssu-yu and John K. Fairbank, China's response to the West: A Documentary Study 1839-1923, (Massachusetts, 1994), p. 57-59. However, the West were particularly influential in helping the Qing to put down the social unrests, such as the employment of the Ever-Victorious Army and Gordon's artillery in supplementing the regular forces and local militias in putting down the Taiping. It must be said that generally, the West were adverse to social unrests, especially in the treaty ports where trade was concentrated, because social instability often interfered with trade and economic prosperity. The open intervention pursued by the West in suppressing the Taiping was to their interests: for the defense of Shanghai, and as a useful bargaining chip with the Qing government when seeking concessions. Also, in the Muslim rebellion in the Kansu, England secretly supported Yakub Beg, the leader of the rebellion, while Russia opposed the expansion of Yakub Beg's power, and under the pretense of maintaining peace along the border, occupied Ili in 1871, subdued the Mohammedan chief and moved troops into Urumchi. Hence when the West moved in to suppress social unrests, they did so only if it was within their interests to do so, should not be regarded as a philanthropic gesture. Li Chien-nung, The Political History of China, 1840-1928, translated and edited by Teng Ssu-yu and Jeremy Ingalls, (California, 1967) p. 110. Perhaps the most glaring exclusion in this essay is the role of Japan in the social unrests of the 19 th Century, but it would be convenient to evaluate Japan on the basis of a foreign power with imperialistic designs on China. The Sino-Japanese War of 1894-5 shook the Chinese self-confidence, and marked an end to the era of 'Self-Strengthening'. Though there was no particular anti-Japanese event, anti-Japanese sentiments took root during this era, and set the stage for more explicit and violent anti-Japanese movements that continue to present time. In conclusion, the social unrests of the early 19 th Century were caused more by internal than external problems, such as underlying ethnic tensions, the prevailing socio-economic conditions, the entrenched corruption and degeneration of the court and the army. However, The Opium War and the Treaty System that was imposed marked a watershed in the Qing-West relations, allowing the West to expose the weaknesses of the Qing State, making it more vulnerable to rebellions, and the rise of Western Imperialism in the quest for territorial gains and in the spread of Christianity, which led to the creation of anti-foreign nationalism, which dominated the face of social unrests in the late 19 th Century. Also, where their interests were concerned, the West would intervene to suppress social unrests in order to safeguard their commercial interests. Hence the parallel streams of development: the decline of the central power of the Qing State, and the growing western presence and penetration gave the West increasing importance in the creation and suppression of social unrests of the 19 th Century.","The Enlightenment, an important feature in most written works on the origins of the French Revolution, has often been credited with the ideology that inspired the French masses to rise up against the monarchy. Though the association has been made, there is nevertheless no direct causal link between the ""siecle des lumieres"" (the century of light) and the revolution. Other factors were present, along with the influence of the Enlightenment, which created the context in which the revolution occurred. To provide a satisfactory answer to the above question, the writer will attempt firstly to examine the ideas of the Enlightenment, and how it contributed to the Revolution; secondly the other factors that culminated in the Storming of the Bastille on the 14 th of July 1789 will be placed under scrutiny; and finally, an alternative interpretation of the relationship between the Enlightenment and the Revolution will be considered. According to the Oxford Dictionary, the Enlightenment was 'a European Intellectual movement of the late 17 th and 18 th centuries emphasising reason and individualism.' 'Man is the single term from which all must be brought back' or as Professor Colin Jones puts it 'human value was the critical yardstick of knowledge employed."" Eric Hobsbawm saw the Enlightenment as being dominated by 'a secular, rationalist and progressive individualism' in which its main objective was to free the individual from the bonds of traditionalism, superstition and an irrational hierarchical class order. The cult of the individual may have led to the estrangement of the self from the Church, (seen as an oppressive and stifling body with traditional rituals and rigid beliefs) hence the dominance of secularism in the ideas of the Enlightenment. Proceeding from there, the criticisms aimed at the Church would have led some to question the basis of its power, and the resultant decrease in its prestige and inviolability. Since the monarch was seen as God's ordained agent on earth, any challenge to the supremacy of the Church would only serve to de-stabilise the King's position as an absolute monarch, which was precisely what happened in 1789 (King Louis XVI by the grace of God, King of France now became Louis, by the Grace of God and the constitutional law of the state, the King of the French). Compact Oxford Dictionary Thesaurus and Word Power Guide (2001) p.292. Dennis Diderot, Encyclopedie, vol. V, p. 641 (article, 'Encyclopedie'). Colin Jones, The Great Nation, France from Louis XV to Napoleon (London, 2002), p.174. Eric Hobsbawm, The Age of Revolution 1789-1848 (United States of America, 1996), p.21. Hobsbawm, Age of Revolution, p. 59. 'Liberty, equality and fraternity of all men' was the slogan of both the Enlightenment and the revolution. It is plausible to suggest that the enlightenment did lead, in reality, to greater equality between certain groups of people. In the salons, academies, Masonic lodges and coffee houses, people debated on equal footing, regardless of their status in society. Madam Geoffrin even went as far as to bar the powerful duc de Richelieu from her salon, 'arguing that wit rather than social rank was the passport for admission into her circles."" But this could have resulted in the resentment amongst the lower classes that thought of themselves as the intellectual, but not social, equals to the aristocrats- resentment for the fact that nepotism and the lack of meritocracy were going to cost them that promotion in the Army or civil service hierarchy. The revolution thus could be borne out of the conflict of interests between the old order, and the new forces that were trying to assert themselves. Colin Jones, The Great Nation, p. 183. Perhaps, the most significant way in which the Enlightenment contributed to the Revolution was that it created a climate of change that was conducive for the revolution to occur. The overall rise in literacy, from 29 to 47 per cent for Men between 1686-90 and 1786-90, and 14 to 27 per cent for women during the same period, led to the rapid and widespread dissemination of the ideas of the Enlightenment. Large-scale diffusion of critical and denunciatory literature (not least in pornographic books targeting the royal family) transformed the representation of the monarchy in the minds of the French, and made them think of themselves as victims of a despotic monarch. Thus, in Roger Chartier's words, regardless of the intent of the 'philosophical books', it succeeded in producing an 'ideological erosion' that may have made the revolution inevitable. According to Darnton, even though the people did not call for a Revolution or foresee 1789, unconsciously however, they prepared for that event by 'desanctifying the symbols and deflating the myths that made the monarchy legitimate in the eyes of its subject."" The increased criticisms of the established order (the Church and the Monarchy) may have therefore poisoned the mentality of the Public Opinion in France that eventually led to revolution. The Encyclopedie also aspired to embody 'the power to change men's common ways of thinking' so as to make a 'revolution...in the minds of men and the national character."" Hence the fostering of the 'critical spirit' amongst the French was also an important consequence of the Enlightenment Roger Chartier, 'Do books make revolutions?' The French Revolution in Social and Political Perspective, ed. Peter Jones, (New York, 1996), p. 168. Robert Darnton, 'A Clandestine Bookseller in the Provinces,' in his The Literary Underground of the Old Regime (Cambridge, Mass., 1982), p. 147. Encyclopedie, vol. v, p.637 (article, Encyclopedie) Roger Chartier however, cautions against this link between philosophical works and revolutionary thought. Using the example of Rousseau, Chartier illustrates the popularity of the philosophes amongst the sans culottes, middle classes and the aristocracy. Moreover, one of the Enlightenment's crowning glories, the Encyclopedie was too expensive to be purchased by anyone other than the notables, and though some were dedicated to the revolutionary cause, the majority was apathetic or hostile to it. Due to the fact that there can be many interpretations to a book, hence, it is impossible to credit too direct a role to books in creating the revolutionary ardour of the French masses. Furthermore, the philosophes were from quite a diverse social background. Rousseau's father was a watchmaker, Voltaire was the son of a notary, and Montesquieu was 'a magistrate in the parlement of Bordeaux, a feudal lord living in a moated castle and an apologist for noble power."" Most of them (excluding Rousseau) believed in the idea of the monarchy as the generator of utilitarian reform, and despite the criticisms of the monarchy and traditional institutions, the philosophes believed that 'the modern state could be improved as it stood."" Perhaps, it was the failure of 'Enlightened Despotism', which resulted in the bourgeoisie transfer of faith from the monarchy to the masses, which led indirectly to the French revolution. William Doyle, The Oxford History of the French Revolution, (Oxford, 2002), p.50. Colin Jones, The Great Nation, p.221. Amongst the other factors that contributed to the outbreak of the revolution, Hobsbawm suggests that it was war and debt that broke the back of the monarchy. He explains that the bankruptcy of the government and the resultant need for tax reforms gave the aristocracy and the parlements a chance to barter with the government. In exchange for the tax reforms, the aristocracy wanted an extension of their privileges, and this led to the forming of the assembly of notables and the calling of the Estates-General (a feudal assembly which last met in 1614). Hobsbawm concluded that the revolution thus began as an aristocratic attempt to recapture the state, but it underestimated the sovereign ideas of the 'Third Estate', and the socio-economic consequences of its political demands. What made the 'Third Estate' a force to be reckoned with was the fact that they not only represented the interests of the middle classes, but also of the urban poor and eventually, the revolutionary peasantry. In my opinion, the socio-economic context of revolutionary France was just as, if not more important than the influence of the Enlightenment. The fact that the revolution broke out in 1789 and not any other year was partly because of the bad harvest in 1788/9, which was reinforced by a particularly harsh winter. The severe economic consequences and the resulting industrial depression drove the urban and rural poor to desperation. The impact was heightened by the fact that in the preceding years, conditions were improving and things were turning out for the better. The expectations of the people were raised, and it came crashing down in 1789. Hobsbawm also suggested that the campaign of propaganda lent an added political dimension to the grievances of the people, and united them behind the deputies of the 'Third Estate'. Once the bastion of monarchical suppression and power (the Bastille) had fallen, there was no turning back. Hobsbawm, The Age of Revolution, p.58. In Roger Chartier's article he puts forward and alternative interpretation of the relationship between the Enlightenment and the revolution. He contemplates the possibility that it was not the Enlightenment that implied revolution, but rather, the converse, that it was the latter that constructed the former. He states that the retrospective construction were many-the 'canonisation' of Voltaire and Rousseau, by the revolutionary assemblies, as the intellectual fore fathers of the revolution, while others such as Buffon and Descartes were relegated; in the quest for legitimacy, political celebrations were held in Year II, honouring the philosophes and martyrs for liberty. The articles in the Declaration of the Rights of Man, which expounds on the freedom of Man, may have led to the belief that it was the Enlightenment that was the ideological inspiration behind the revolution. Perhaps it was a retrospective justification for the revolution and to put the revolutionaries on a moral high ground, since they were now seen not as mere rebels, but as agents of progress. Hence, 'it was the revolution that gave a premonitory and programmatic meaning to certain works, constituted, after the fact, as its origin.' Roger Chartier, 'Do books make Revolutions', p.183. Chartier, 'Do books make Revolutions', p.185. The relationship between the Enlightenment and the revolution is a complex one that does not lend itself readily to simple cause and consequence explanation. There have been mutual exchanges between the two, in the sense that the Enlightenment provided the ideology and legitimacy of the revolution, and on the other hand, the revolution also gave the Enlightenment a prophetic element in that the latter 'predicted' the coming of the former and prepared the people for socio-political change. Much as the Enlightenment has contributed to the revolution, we should not discount the other forces and events that were in place for the revolution to occur, such as the economic depression that led to the politicising of the masses, the bankruptcy of the monarchy, and the personal temperament of King Louis XVI (in particular, how he was so easily manipulated by his Queen and reactionary advisors). Hobsbawm has an interesting perspective to offer, in that he thinks the revolution would have occurred without the philosophes, but they 'made the difference between a mere breakdown of the old regime and the effective and rapid substitution of a new one."" Perhaps we can then say that the Enlightenment was important only after the revolution had become a fait accompli, and that it made its mark on post-revolution re-construction. Hobsbawm, Age of Revolutions, p. 58. The hypothesis put forth by J.C. Davis's Fear, Myth and History has provoked a heated debate that still in want of a closure, and one thinks that it may never be found, for it would be professional suicide for any of the historians involved, especially Davis, to 'recant' and come round to the others' viewpoint. While there is a sense that the debate has been bogged down by personal animosities and the lack of fresh interpretations, it is still of interest today, because of the pertinent ""issues about historical methods raised by Fear, Myth and History"" that ""are important far beyond the particular case of England in the mid-seventeenth century."" It is to this debate that we now turn our attention to, and I have structured the essay as such: the first section deals with historiography, methodology and the implications of Davis's interpretation on the discipline of History, followed by the main body of the essay which will focus on the two questions relating to our topic at hand-why Davis denies the existence of the 'Ranters', and subsequently, why he believes that the 'Ranters' were created by seventeenth century conservatives and twentieth-century Marxists. While Davis's interpretation is thought provoking, it is my opinion that his failure to differentiate between 'Ranter' myth and 'Ranters' has been a stumbling block in this debate. G.E. Aylmer, ""Did the Ranters Exist? "", Past and Present, No. 117 (Nov., 1987), p. 208. Jstor article. Before focusing on the main debate, it is imperative to begin with an examination of some of the held assumptions and employed techniques in Davis's hypothesis. In the area of historiography, Davis seems to have taken E.H. Carr's dictum ""study the historian before you begin to study the facts"" a bit too far when he claims that the 'Ranters' were conjured up by members of the Communist Party Historians Group in order to fit English history to Marxist theory. While it is important to question the agenda of historians, there is a fine line between the above, which is informative, and the latter-pure speculation-which I think that Davis is guilty of. Every historian writes with an agenda, for that is implicit in his/her argument, which in turn moulds his/her interpretation which is-according to Carr-""the lifeblood of history"". One ponders about Davis's own agenda behind his enterprise, but let us not tarry on this for there are other more pertinent issues to discuss. In particular, Hill has raised a very relevant point that Davis's paradigm is flawed and his enterprise doomed from the start, because his desire to find ""a coherent, cohesive group of like-minded Ranters"" is anachronistic, and that he is guilty of looking at the past through eyes of the present. There is much inconsistency in Davis's methodology, seen in the way he views seventeenth century pamphlets with extreme scepticism and yet at the same time, as pointed out by Christopher Hill, seems to have taken Abiezer Coppe's ironic recantations seriously. The implications of this debate are far-reaching, and it has not only questioned how historians ought to view primary sources, but has also affected how we are to understand the function of labels. Labels exist first and foremost to describe an external reality, though, that does not preclude the possibility of that label acquiring associations along the way that exaggerate the former, hence turning it into a myth. In the same way that Protestants used the 'Catholic myth' to consolidate their positions, so the seventeenth century conservatives manipulated what was already in existence to galvanise the population into combating the attack on religious and social order that the English Revolution had brought about. E.H. Carr, What is History?, (London, 1990) p.23. J.C. Davis, ""Fear, Myth and Furore: Reappraising the 'Ranters'"", Past and Present, No. 129 (Nov., 1990), p. 82. Jstor article. Carr, What is History? , p. 28 Davis, Fear, Myth and History: The Ranters and the historians, (Cambridge, 1986), p. 74 Christopher Hill, A Nation of Change and Novelty: Radical politics, religion and literature in seventeenth-century England, (London 1990), p.153 Hill, A Nation of Change and Novelty, p. 180. Davis, Fear, Myth and History, p. 53-7 It is Davis's opinion that the 'Ranters' were a ""projection of deviance"", and that ""there was no Ranter movement, no Ranter sect, no Ranter theology"", but consisted of ""a few relatively isolated individuals of heterogeneous persuasions"" who were ""swept up in the projection of a movement"" and were subsequently ""assigned roles by the historians"" While ""historians are agreed that it is wrong to speak of a Ranter sect or movement"", few have gone so far as to deny the existence of the 'Ranters'. Davis is unique in asserting that because there were no concrete evidences of the 'Ranter's' practical antinomianism, therefore they do not exist. Even if such evidences were in existence, Davis would most likely reject them as sensationalism of the yellow press, or as unreliable stories conjured up by seventeenth century anti-'Ranter' pamphleteers. However, Hill points to evidence that Gerrard Winstanley experienced problems in his Digger colony, due to the presence of drinking, whoring and indolent 'Ranters', and makes a point, which is taken up by Aylmer that the 'Ranters' held an 'anti-work' ethic and lived a ""hippy-like existence"" Therein lies the 'practical antinomianism' that Davis seeks. The 'Ranters' hedonistic 'make love not war' sort of attitude constituted a critique of society, which manifested itself in the breaking of taboos, as opposed to the more taxing alternative of a violent overhaul of society. There is a more cogent explanation proposed by Hill that accounts for the lack of 'practical antinomianism'-it is simply that the Ranters were ""talkers rather than doers."" Ibid, p.124 Bernard Capp, ""Fear, Myth and Furore: Reappraising the 'Ranters'"", Past and Present, No. 140, (Aug., 1993), p.165 Jstor article. Aylmer, ""Did the Ranters Exist? "", p. 209 Christopher Hill, The World Turned Upside Down: Radical Ideas during the English Revolution, (London, 1991), p. 229-30. Hill, A Nation of Change and Novelty, p. 185 Davis's second charge is that the absence of any common defining ideology and close direct links between members of the supposed 'Ranter' core disproves that the 'Ranters' existed either as ""a small group of like-minded individuals, as a sect, or as a large-scale, middle-scale or small movement."" While it is true that the antinomian and spiritualist ideas were not unique to the 'Ranters', according to Capp ""it is clear that something had crystallised from the flux, and that we are no longer dealing with isolated individuals"". Capp substantiates his point with compelling evidence drawn from correspondences between Salmon, Webb and Coppe, which indicated a sense of familiarity to the point of intimacy. Though he concedes that Coppe and Clarkson were never close, nevertheless, he asserts that there were links between both groups, hence refuting Davis's claim that ""the Ranter core shatters and disintegrates."" As mentioned earlier, the search for a seventeenth-century group labelled the 'Ranters' eludes Davis precisely because he employs modern standards and preconceptions to study a seventeenth-century phenomenon. Davis's insistence in finding a certain measure of uniformity or convergence of ideas will not hold up even in the globalised world today, where common church denominational labels mask a whole range of disparate practices and beliefs. Seventeenth-century radical groups were even less likely to converge on not more than a few tenets which roughly separated them from the other movements, not forgetting as well, that there was a great deal of fluidity between the groups, as people shopped around for a belief system that appealed to them. The 'Ranters' were such a loose grouping precisely because of their nature as a subversive group, their wariness of any forms of organised religion and distaste for hierarchy, which makes them not unlike modern anarchist 'groups' in form. Davis, Fear, Myth and History, p. 75 Capp, ""Reappraising the Ranters"", p. 166-7 Davis, Fear, Myth and History, p.75 Davis's scepticism is conceivably fuelled by the tendency of supposed 'Ranter' leaders to recant in the face of persecution. His serious treatment of the ironic recantations of Coppe, Clarkson and others is puzzling in light of his scepticism with regards to seventeenth century anti-'Ranter' writings. In contrast, Hill accounts for this readiness to recant as a feature characteristic of 'Ranters', who, like their sixteenth-century predecessors, the Familists, practiced ""Nicodemism-recanting in the face of overwhelming power"" and warns us against taking their equivocal recantations seriously. This practice can also be seen as the direct consequence of the 'Ranter' belief in 'mortalism', which precludes the immortality of the soul and hence of the appeal of martyrdom. As Hill astutely points out ""...resistance to the death would call for a deeper and more consistently worked out ideology than most Ranters had."" The two historians also disagree as to whether the 'Ranters' were the main targets of the Blasphemy Act of 1650. If Parliament had regarded the ideas and behaviour of the 'Ranters' as a real threat to societal order, then Davis's hypothesis that the Ranters did not exist is weakened. Though Davis is correct in saying that the Blasphemy Act did not mention the Ranters, evidence points towards Clarkson's A Single Eye All Light as a trigger to the body of legislation, which according to McGregor ""was largely a description of Ranter doctrine, owing much to the writings of Coppe and Clarkson."" While there is a possibility that the Parliament was responding to the 'Ranter myth' and not to the 'Ranters' it still reinforces Aylmer's simple concept that there can be ""no smoke without fire"". To take the analogy further, the strength of the fire might be disproportionately small compared to the amount of smoke that is billowing, and the response of the fire fighters might be more than is required to deal with a small fire, but the fact remains that a small fire did exist. Though that does not preclude the possibility that some people may have stoked the fire and fanned the flames, hence creating the 'smoke screen' to mask their presumably sinister agendas. Hill, A Nation of Change and Novelty, p. 179. Hill, The World Turned Upside Down, p. 209. J.F. McGregor, Fear, Myth and Furore: Reappraising the 'Ranters', Past and Present, No. 140 (Aug., 1993), p. 157. In order to justify and perpetuate his hypothesis that the 'Ranters' did not exist, Davis puts forth a series of explanations that sought to provide a plausible list of agendas that necessitated the creation of the 'Ranter' myth. For the seventeenth century conservatives, the 'Ranters' were necessary as the ""projection of deviance"" ""for it is only through deviance that we understand normality."" While this is a compelling argument, and extreme radicalism may alienate other groups and force some into the protective arms of the conservatives, there is also the other possibility that the increased profile and definition of the 'Ranters' may have seduced and ensnared others. No matter how vile the portrayal of the 'Ranters' there is an element of freedom in antinomianism that can be particularly enticing, especially for young men. Hence, this myth is in my opinion a double-edged sword to be wielded at one's own peril. The myth may achieve normality, but it could just as easily go the other way and perpetuate deviant behaviour. The 'Ranter' myth was not the exclusive tool of the conservatives, but was also used to achieve sectarian consolidation: to enhance the acceptability of sects that were not as radical as the 'Ranters', and to induce conformity within sectarian ranks. Because this spectre of Ranterism was conjured up to fulfil a social function, Davis warns us against taking any of the anti-Ranter literature seriously. On the topic of sources, Hill suggests that we can look to seventeenth-century personalities, such as John Reeve and Lodowick Muggleton, who were previously from the Ranter milieu for an accurate account of 'Ranterism'. Notably, Hill focuses on the figure of Bunyan, who was himself initially tempted by Ranter ideas, but whose work's from 1665-1685 took on an anti-'Ranter' slant. Also, despite the many hostile anti-Ranter records, Hill uncovered Ranter nostalgia in Erbery, Byne and Sedgweick, who ""found things to praise in the Ranters..."" though he added that ""they would hardly invent the Ranters for this purpose"". At the very least, there is a consensus that 'Ranter' was a 'buzz-word' in seventeenth-century England, not unlike that of 'globalisation' in our own time, though an admittance that the misuse of the label 'Ranter' existed does not preclude the external reality of a group of 'Ranters'. Davis, Fear, Myth and History, p.124 Ibid, p.111 Ibid, p. 110. Hill, A Nation of Change and Novelty, p. 164 Ibid, p.177. Ibid, p.169. While Davis might possibly defend his claim that the 'Ranters' were created by seventeenth-century conservatives (amongst others), his other claim-that the 'Ranter' spectre was resurrected by A.L. Morton and Christopher Hill, both members of the Communist Party Historians' Group between 1946-1956 in a bid to ""create the history of a popular democratic tradition in English history and culture"" is absurd and deliberately provocative. The allegation that Davis brings against them-that of using theory to dictate history- is an equally valid charge that can be levelled at him. Hill's defence of himself and A.L. Morton in A Nation of Change and Novelty is fairly convincing, and he has managed to place the onus on Davis to explain his own motivation behind the whole enterprise. Furthermore, other historians have joined in the fray, and with a few differences they have reaffirmed the existence of the 'Ranters' and the 'Ranter' myth. While the debate should not end on a 'majority wins' basis, there is a sense that this is quite a lope-sided debate, with Davis fighting the battle on his own. Davis, Fear, Myth and History, p.130 Hill , A Nation of Change and Novelty, p. 190 In conclusion, Davis's hypothesis, while novel, is not entirely convincing. The arguments of the other historians are a lot more persuasive, both in establishing a link between the various individual 'Ranter' leaders (despite the running debate about 'core' vs. 'milieu' which was not dealt with in this essay) and in their efforts to draw out some semblance of similarities between the various groups, while conceding that it impossible to define a coherent set of 'Ranter' ideology. This is not far off from Davis's own assertion that what would make him believe that the 'Ranters' existed would be the coincidence of two sort of evidences: that the existence of a group (or a number of groups loosely linked) can be established and that its shared ideology was reflected behaviourally in practical antinomianism. It bears repeating that Davis's enterprise is flawed for two reasons: his failure to differentiate between 'Ranter' myth and 'Ranters' which has led him to believe that the 'Ranters' were created by seventeenth-century conservatives and later resurrected by twentieth-century Marxist historians; and his employment of modern standards and preconceptions to study a seventeenth-century phenomenon, which explains why his search for the 'Ranters' as a external reality has been so unfruitful. Perhaps one might cheekily suggest that Davis's own 'recantation' is overdue. J.C. Davis, ""Fear, Myth and Furore: Reappraising the 'Ranters': Reply"", Past and Present, No. 140 (Aug., 1993), p. 205. Jstor article. China has long captured the imagination of the West, and still remains an enigma in this present age despite the vicissitudes of time. While technology now permits us to catch a glimpse of China on the news, on the internet, or plausibly, even visit China; this would not have been conceivable in previous centuries, where one of the few ways in which one could come into contact with, and conceive of China was through the objects brought back from overseas trade- which was itself stimulated by a thirst for the mysterious exotic Orient as much as it was for profit. Although these goods were prized for their scarcity, aesthetic value and excellent workmanship, it was not the material alone that captivated, but more importantly, it was the non-material associations that imbued the goods with meaning, desirability and the label of 'luxury'. This fuelled the luxury trade, long underpinned by traditional court luxury, which reached its zenith in the seventeenth and eighteenth centuries, with the establishment of the chartered East India companies and the subsequent creation of a European market for these goods. This essay will attempt to put forth the idea that Europeans were so fascinated by Chinese luxury goods precisely because of the various functions that these goods fulfilled- it fired up the European imagination, giving it the impetus to venture out and explore the world; it played its part in influencing the new taste for the Rococo and in the definition of 'refinement'; it was amenable to the marketing techniques of the East India Companies which further heightened its appeal; and finally it stimulated European industries in their attempt to surpass the East through emulation and innovation. In short, it was through the acquisition of Chinese goods, which defined the 'Other', that one perceives the 'Self' in a clearer light. Maxine Berg, ""In Pursuit of Luxury: Global History and British Consumer Goods in the Eighteenth Century"", in Past and Present 182 (2004), p. 86. Before embarking on the main body of the essay proper, there are some issues that deserve our attention: firstly, the reference to ""seventeenth and eighteenth century Europeans"" in the question should not be taken to mean all Europeans, but was limited to the affluent, by nature of it being Asian luxury goods. And even within this group, there were some such as Mrs. Montagu and the adherents of the Burlington School of Taste who lamented the fall from favour of Palladianism. While it is not the task at hand to examine the interactions between the Orient and the Occident, it must be said that far from being a one-way exchange, the Chinese elite were themselves fascinated by western manufactured goods, especially clocks. It is interesting to note that both cultures used foreign 'luxury' goods in a setting that suited their own agendas, often removing the objects from their original context, transforming the functional into the ornamental. This playful assimilation, also seen in the creation of the Chinoiserie, is perhaps reflective of the confidence of both cultures, which is in stark contrast to the theories of Social Darwinism, which characterised their relations in the nineteenth century. Finally, it is important that we examine the definition of 'luxury goods'. As Vainker astutely points out: despite the high quality of silk, porcelain and lacquer, these goods were not perceived as luxury items, but as functional goods by the affluent Chinese. Instead, what they valued as luxury items would include ""paintings, ritual bronzes, jades, calligraphy and other antiques and collectibles associated with a scholar-official elite and the authority to rule."" Hence, luxury is defined not by high quality goods that are widely available (such as porcelain and silk in China), but by its association with the conferment of social status due to its scarcity (Chinese porcelain and silk in Europe). In a letter from Mrs. Montagu 1749 ""Thus it is happened...we must all seek the barbarous gaudy gout of the Chinese; and the fat-headed Pagods and shaking Mandarins bear the prize from the greatest works of antiquity; and Apollo and Venus must give way to a fat idol with a sconce on his head."" Quoted from Dawn Jacobson, Chinoiserie, (London, 1993), p. 123. Ibid, p.124. Seen in the use of clocks as status symbols (as opposed to time-pieces) in Yongzheng's imperial household; and the French candelabra composed of inverted Chinese whistles and cups mounted on a frame. Seen in the DVD produced on the Encounters: The Meeting of Asia and Europe 1500-1800 exhibition. Picture of French Candelabra in Anna Jackson and Amin Jaffers (ed. ), Encounters: The Meeting of Asia and Europe 1500-1800, (London, 2004), plate 1.9, p. 7. Picture of Yongzheng's concubine with a clock in the background in Evelyn S. Rawski and Jessica Rawson (ed. ), China: The Three Emperors 1662-1795, (London, 2005), bottom left, p. 259. Shelagh Vainker, ""Luxuries or Not? Consumption of Silk and Porcelain in Eighteenth-Century China"", in Maxine Berg and Elizabeth Eger (ed. ), Luxury in the Eighteenth Century: Debates, Desires and Delectable Goods, (New York, 2003), p.214. The seventeenth and eighteenth century fascination with Chinese luxury goods had its genesis in the myth of Cathay, which conjured up ""fantastic visions of luxury and refinement, pleasure and abundance"", and were reinforced by traveller accounts of Marco Polo and the fictional Sir John Mandeville. As Jacobson observed ""this view of the Chinese as being different but in no way inferior, helped to sustain the myth of Cathay, and the vision of a culture that was profound as well as peculiar."" It was this perception that made possible the enchantment and respect that the Europeans had for the advanced Chinese civilisation; which manifested itself in the pursuit of Chinese goods, in the hope that one's collection might plausibly be employed as a prism through which the secret knowledge of the East might be yielded. That China during the Ming dynasty was largely cloistered from the world further enhanced its mystery and appeal, not unlike the coy mistress subtly beckoning to would-be suitors. And they came- in drips and then in droves- the Portuguese trailblazers followed by the Dutch, English and French traders who brought back taster portions of Asian goods, fuelling the imagination of the affluent Europeans further, which in turn encouraged the luxury trade. Jacobson, Chinoiserie, p. 10 Ibid, p. 12-15. Ibid, p.15. It is imperative that we examine how this seemingly perennial fascination that the Europeans have for the exotic Orient featured in the particular context of the seventeenth and eighteenth century, especially in terms of sociability, civility and taste. It is important to note that in the moral debates about luxury at that time, Chinese goods were ""associated not with (Persian) sensuality and excess, but with ethics, harmony and virtue...(and) in possessing things Chinese, they (the philosophes) sought to access levels of civilisation beyond the market."" Hence, it was through the prism of Chinese objects that the Europeans expressed ""their own aspirations to human elegance and refinement."" The employment of taste as ""a new means of displaying prestige"", and the corresponding replacement of traditional forms of opulence (silver and gold) with elegant new 'imitative' objects, such as porcelain are integral in accounting for the vogue in Chinese luxury goods in seventeenth and eighteenth century Europe. This change in taste also reflected the changing relations in society, with the notable rise of the bourgeoisie, who rejected the gaudy decadence of the baroque (associated with the ancien regime) in favour of the lighter, more playful and asymmetrical Rococo, which ""brought together commercial and cultural modernity."" It is perhaps fortunate that the Chinoiserie with its freedom from classical notions of restraint and symmetry found favour with the Rococo, and was in fact a ""vital ingredient in the creation of the new taste."" It was in this context that Chinoiserie-which was the Western romanticised view of the East, produced in part by the imported goods- flourished, and ""in turn dictated the commerce in Asian imports."" Maxine Berg, ""Asian luxuries and the Making of the European Consumer Revolution"", in Maxine Berg and Elizabeth Eger (ed. ), Luxury in the Eighteenth Century: Debates, Desires and Delectable Goods, (New York, 2003), p. 229. Ibid. Ibid, p. 230. Ibid, p. 231. Jacobson, Chinoiserie, p. 57. Maxine Berg, Luxury and Pleasure in Eighteenth century Britain, (Oxford, 2005), p. 51 While fascination with the Orient had always existed, it is necessary to look at how the East India Companies manipulated what was already in existence and transformed it into an obsession with Chinoiserie, as seen from above. As Berg astutely pointed out: these luxury goods ""were a construct of the market, seeming to represent the lives and values of the East, but constructed to meet European preconception of eastern art."" These commodities were prefabricated on the Chinese side to cater to Western markets; and on the part of the East India Companies, which sent out European shapes to be copied, and the initial contentment with Chinese designs soon gave way to a sudden fashion for 'chine de commande' by the end of the seventeenth century. While the East India Companies (EICs) were themselves the early products of the European fascination with China and its goods, they were indispensable in the subsequent promotion of this luxury trade, which was intrinsically tied up with the western enthralment for the East. The EICs were not merely merchants who simply pandered to the tastes of the European elite, but were market movers in the way in which they created a demand for these goods through 'customisation' and effective marketing. It is remarkable how they transformed something as functional and mundane as porcelain by capitalising on its exotic appeal and its association with civility, marketing it as a semi-luxury good, when truth be told, the differentiated decorations on the porcelain pieces belie the fact that they were to some extent, 'mass' produced goods. Moreover, they created new wants- ""the dinner service with complete setting for each person was an invention of the VOC and the EIC which started importing porcelain dinner services in the eighteenth century."" Therefore, though the western fascination for the Orient was present before the seventeenth-century, we have to credit the EICs for their role in elevating it to a whole new dimension with the extension of maritime trade and through successful marketing tactics. Berg, ""Asian luxuries and the Making of the European Consumer Revolution"", p. 228 Christiaan, J.A. Jorg, ""Porcelain for the Dutch"", in Rosemary E. Scott (ed. ), The Porcelain of Jingdezhen: Colloquies on Art and Archaeology in Asia No. 16, (London, 1993), p. 188 and 200. Berg, ""Asian luxuries and the Making of the European Consumer Revolution"", p. 239 While exoticism, taste and marketing definitely contributed to the appeal of Chinese luxury goods to the West, a large measure of this preoccupation was also due to the impressive technical wizardry of the Chinese craftsmen and the secrecy which shrouded the production processes for these goods-especially porcelain- the Arcanum of which was first discovered by the Europeans only in 1709. Perhaps it is useful to point out that the luxury goods that fascinated the West were new, modern luxury comprised of ""technically advanced and refined consumer objects"" such as calicos and porcelain. Hence, the East enthralled the West because the latter realised that much could be learnt from the former; which would allow the West to progress through imitating and perfecting the techniques that they have gleaned from the East. It is important to note that contemporaries such as ""Montesquieu, Hume and Smith wrote of luxury as an aspect of people's desire to better themselves"" and ""...commercial writers were keen to point out what the West could learn from the Orient."" In doing so, they affirmed the moral good of luxuries in contributing to 'Progress', hence alleviating the fears of decadence and degeneration that plagued earlier preoccupations with luxury goods, and indirectly encouraged the fascination for Chinese luxury goods. While it must be said that this desire to learn from the East was more apparent than real, which is illustrated by Berg's observation that ""Asian consumption was transferred to Europe but not Asian production systems"", there appears to be a genuine lesson imported from the East, which is ""the policy of using the fine arts and design in combination with modern manufacturing technique."" Hence, ""paradoxically, in imitating Asian consumer goods, perceived in Europe as luxuries, the British achieved what was actually an Asian success story-new, quality, semi-luxury consumer goods produced with advanced industrial techniques."" Ibid, p. 230 Ibid. Berg, ""In Pursuit of Luxury: Global History and British Consumer Goods in the Eighteenth Century"", p. 86 Berg, ""Asian luxuries and the Making of the European Consumer Revolution"", p. 242. Berg, ""In Pursuit of Luxury: Global History and British Consumer Goods in the Eighteenth Century"", p. 130. In conclusion, this fascination that the Occident had for the Oriental luxury goods was not new, but was closely entwined with the captivating medieval myth of Cathay, and reached new dimensions in the seventeenth and eighteenth century with the establishment of the East India Companies (which were themselves the product of and the further stimulus to the West-East trade born of the Occidental preoccupation with the exotic, mysterious and abundant East) and the creation of European markets for Chinese luxury goods (which was previously the preserve of the European royalty). It is perhaps paradoxical that these Chinese luxury goods fascinate precisely because they have been displaced from their original cultures and grafted into new ones, but continue to give the impression that it was still possible to use them as prisms through which the West can subscribe to notions of Chinese refinement and skill. The fascination for anything 'Chinese' is apparent in the vogue for the Chinoiserie especially in the eighteenth century, and is both reflective of, and contributed to the existing enthrallment that the West had for the East, and how that affected the dominant taste in Europe. The appeal of Chinese luxury goods was also played up by the East India Companies which capitalised on the vogue for Chinoiserie, and created new wants through the successful marketing technique of making the foreign 'relevant' to the West through customisation. Finally, and most importantly, Chinese luxury goods were seen as having the role of ""awakening the imagination"" and were positive elements in stimulating the Europeans to better themselves through emulation and product innovation. Despite being so far removed from their original cultures, that these luxury goods were relevant and fulfilled certain functions given to them by their European possessors accounts to a large extent for the fascination that seventeenth and eighteenth century Europeans had for Chinese luxury goods. Ibid, p.131",True
93,"The Enlightenment, an important feature in most written works on the origins of the French Revolution, has often been credited with the ideology that inspired the French masses to rise up against the monarchy. Though the association has been made, there is nevertheless no direct causal link between the ""siecle des lumieres"" (the century of light) and the revolution. Other factors were present, along with the influence of the Enlightenment, which created the context in which the revolution occurred. To provide a satisfactory answer to the above question, the writer will attempt firstly to examine the ideas of the Enlightenment, and how it contributed to the Revolution; secondly the other factors that culminated in the Storming of the Bastille on the 14 th of July 1789 will be placed under scrutiny; and finally, an alternative interpretation of the relationship between the Enlightenment and the Revolution will be considered. According to the Oxford Dictionary, the Enlightenment was 'a European Intellectual movement of the late 17 th and 18 th centuries emphasising reason and individualism.' 'Man is the single term from which all must be brought back' or as Professor Colin Jones puts it 'human value was the critical yardstick of knowledge employed."" Eric Hobsbawm saw the Enlightenment as being dominated by 'a secular, rationalist and progressive individualism' in which its main objective was to free the individual from the bonds of traditionalism, superstition and an irrational hierarchical class order. The cult of the individual may have led to the estrangement of the self from the Church, (seen as an oppressive and stifling body with traditional rituals and rigid beliefs) hence the dominance of secularism in the ideas of the Enlightenment. Proceeding from there, the criticisms aimed at the Church would have led some to question the basis of its power, and the resultant decrease in its prestige and inviolability. Since the monarch was seen as God's ordained agent on earth, any challenge to the supremacy of the Church would only serve to de-stabilise the King's position as an absolute monarch, which was precisely what happened in 1789 (King Louis XVI by the grace of God, King of France now became Louis, by the Grace of God and the constitutional law of the state, the King of the French). Compact Oxford Dictionary Thesaurus and Word Power Guide (2001) p.292. Dennis Diderot, Encyclopedie, vol. V, p. 641 (article, 'Encyclopedie'). Colin Jones, The Great Nation, France from Louis XV to Napoleon (London, 2002), p.174. Eric Hobsbawm, The Age of Revolution 1789-1848 (United States of America, 1996), p.21. Hobsbawm, Age of Revolution, p. 59. 'Liberty, equality and fraternity of all men' was the slogan of both the Enlightenment and the revolution. It is plausible to suggest that the enlightenment did lead, in reality, to greater equality between certain groups of people. In the salons, academies, Masonic lodges and coffee houses, people debated on equal footing, regardless of their status in society. Madam Geoffrin even went as far as to bar the powerful duc de Richelieu from her salon, 'arguing that wit rather than social rank was the passport for admission into her circles."" But this could have resulted in the resentment amongst the lower classes that thought of themselves as the intellectual, but not social, equals to the aristocrats- resentment for the fact that nepotism and the lack of meritocracy were going to cost them that promotion in the Army or civil service hierarchy. The revolution thus could be borne out of the conflict of interests between the old order, and the new forces that were trying to assert themselves. Colin Jones, The Great Nation, p. 183. Perhaps, the most significant way in which the Enlightenment contributed to the Revolution was that it created a climate of change that was conducive for the revolution to occur. The overall rise in literacy, from 29 to 47 per cent for Men between 1686-90 and 1786-90, and 14 to 27 per cent for women during the same period, led to the rapid and widespread dissemination of the ideas of the Enlightenment. Large-scale diffusion of critical and denunciatory literature (not least in pornographic books targeting the royal family) transformed the representation of the monarchy in the minds of the French, and made them think of themselves as victims of a despotic monarch. Thus, in Roger Chartier's words, regardless of the intent of the 'philosophical books', it succeeded in producing an 'ideological erosion' that may have made the revolution inevitable. According to Darnton, even though the people did not call for a Revolution or foresee 1789, unconsciously however, they prepared for that event by 'desanctifying the symbols and deflating the myths that made the monarchy legitimate in the eyes of its subject."" The increased criticisms of the established order (the Church and the Monarchy) may have therefore poisoned the mentality of the Public Opinion in France that eventually led to revolution. The Encyclopedie also aspired to embody 'the power to change men's common ways of thinking' so as to make a 'revolution...in the minds of men and the national character."" Hence the fostering of the 'critical spirit' amongst the French was also an important consequence of the Enlightenment Roger Chartier, 'Do books make revolutions?' The French Revolution in Social and Political Perspective, ed. Peter Jones, (New York, 1996), p. 168. Robert Darnton, 'A Clandestine Bookseller in the Provinces,' in his The Literary Underground of the Old Regime (Cambridge, Mass., 1982), p. 147. Encyclopedie, vol. v, p.637 (article, Encyclopedie) Roger Chartier however, cautions against this link between philosophical works and revolutionary thought. Using the example of Rousseau, Chartier illustrates the popularity of the philosophes amongst the sans culottes, middle classes and the aristocracy. Moreover, one of the Enlightenment's crowning glories, the Encyclopedie was too expensive to be purchased by anyone other than the notables, and though some were dedicated to the revolutionary cause, the majority was apathetic or hostile to it. Due to the fact that there can be many interpretations to a book, hence, it is impossible to credit too direct a role to books in creating the revolutionary ardour of the French masses. Furthermore, the philosophes were from quite a diverse social background. Rousseau's father was a watchmaker, Voltaire was the son of a notary, and Montesquieu was 'a magistrate in the parlement of Bordeaux, a feudal lord living in a moated castle and an apologist for noble power."" Most of them (excluding Rousseau) believed in the idea of the monarchy as the generator of utilitarian reform, and despite the criticisms of the monarchy and traditional institutions, the philosophes believed that 'the modern state could be improved as it stood."" Perhaps, it was the failure of 'Enlightened Despotism', which resulted in the bourgeoisie transfer of faith from the monarchy to the masses, which led indirectly to the French revolution. William Doyle, The Oxford History of the French Revolution, (Oxford, 2002), p.50. Colin Jones, The Great Nation, p.221. Amongst the other factors that contributed to the outbreak of the revolution, Hobsbawm suggests that it was war and debt that broke the back of the monarchy. He explains that the bankruptcy of the government and the resultant need for tax reforms gave the aristocracy and the parlements a chance to barter with the government. In exchange for the tax reforms, the aristocracy wanted an extension of their privileges, and this led to the forming of the assembly of notables and the calling of the Estates-General (a feudal assembly which last met in 1614). Hobsbawm concluded that the revolution thus began as an aristocratic attempt to recapture the state, but it underestimated the sovereign ideas of the 'Third Estate', and the socio-economic consequences of its political demands. What made the 'Third Estate' a force to be reckoned with was the fact that they not only represented the interests of the middle classes, but also of the urban poor and eventually, the revolutionary peasantry. In my opinion, the socio-economic context of revolutionary France was just as, if not more important than the influence of the Enlightenment. The fact that the revolution broke out in 1789 and not any other year was partly because of the bad harvest in 1788/9, which was reinforced by a particularly harsh winter. The severe economic consequences and the resulting industrial depression drove the urban and rural poor to desperation. The impact was heightened by the fact that in the preceding years, conditions were improving and things were turning out for the better. The expectations of the people were raised, and it came crashing down in 1789. Hobsbawm also suggested that the campaign of propaganda lent an added political dimension to the grievances of the people, and united them behind the deputies of the 'Third Estate'. Once the bastion of monarchical suppression and power (the Bastille) had fallen, there was no turning back. Hobsbawm, The Age of Revolution, p.58. In Roger Chartier's article he puts forward and alternative interpretation of the relationship between the Enlightenment and the revolution. He contemplates the possibility that it was not the Enlightenment that implied revolution, but rather, the converse, that it was the latter that constructed the former. He states that the retrospective construction were many-the 'canonisation' of Voltaire and Rousseau, by the revolutionary assemblies, as the intellectual fore fathers of the revolution, while others such as Buffon and Descartes were relegated; in the quest for legitimacy, political celebrations were held in Year II, honouring the philosophes and martyrs for liberty. The articles in the Declaration of the Rights of Man, which expounds on the freedom of Man, may have led to the belief that it was the Enlightenment that was the ideological inspiration behind the revolution. Perhaps it was a retrospective justification for the revolution and to put the revolutionaries on a moral high ground, since they were now seen not as mere rebels, but as agents of progress. Hence, 'it was the revolution that gave a premonitory and programmatic meaning to certain works, constituted, after the fact, as its origin.' Roger Chartier, 'Do books make Revolutions', p.183. Chartier, 'Do books make Revolutions', p.185. The relationship between the Enlightenment and the revolution is a complex one that does not lend itself readily to simple cause and consequence explanation. There have been mutual exchanges between the two, in the sense that the Enlightenment provided the ideology and legitimacy of the revolution, and on the other hand, the revolution also gave the Enlightenment a prophetic element in that the latter 'predicted' the coming of the former and prepared the people for socio-political change. Much as the Enlightenment has contributed to the revolution, we should not discount the other forces and events that were in place for the revolution to occur, such as the economic depression that led to the politicising of the masses, the bankruptcy of the monarchy, and the personal temperament of King Louis XVI (in particular, how he was so easily manipulated by his Queen and reactionary advisors). Hobsbawm has an interesting perspective to offer, in that he thinks the revolution would have occurred without the philosophes, but they 'made the difference between a mere breakdown of the old regime and the effective and rapid substitution of a new one."" Perhaps we can then say that the Enlightenment was important only after the revolution had become a fait accompli, and that it made its mark on post-revolution re-construction. Hobsbawm, Age of Revolutions, p. 58. The hypothesis put forth by J.C. Davis's Fear, Myth and History has provoked a heated debate that still in want of a closure, and one thinks that it may never be found, for it would be professional suicide for any of the historians involved, especially Davis, to 'recant' and come round to the others' viewpoint. While there is a sense that the debate has been bogged down by personal animosities and the lack of fresh interpretations, it is still of interest today, because of the pertinent ""issues about historical methods raised by Fear, Myth and History"" that ""are important far beyond the particular case of England in the mid-seventeenth century."" It is to this debate that we now turn our attention to, and I have structured the essay as such: the first section deals with historiography, methodology and the implications of Davis's interpretation on the discipline of History, followed by the main body of the essay which will focus on the two questions relating to our topic at hand-why Davis denies the existence of the 'Ranters', and subsequently, why he believes that the 'Ranters' were created by seventeenth century conservatives and twentieth-century Marxists. While Davis's interpretation is thought provoking, it is my opinion that his failure to differentiate between 'Ranter' myth and 'Ranters' has been a stumbling block in this debate. G.E. Aylmer, ""Did the Ranters Exist? "", Past and Present, No. 117 (Nov., 1987), p. 208. Jstor article. Before focusing on the main debate, it is imperative to begin with an examination of some of the held assumptions and employed techniques in Davis's hypothesis. In the area of historiography, Davis seems to have taken E.H. Carr's dictum ""study the historian before you begin to study the facts"" a bit too far when he claims that the 'Ranters' were conjured up by members of the Communist Party Historians Group in order to fit English history to Marxist theory. While it is important to question the agenda of historians, there is a fine line between the above, which is informative, and the latter-pure speculation-which I think that Davis is guilty of. Every historian writes with an agenda, for that is implicit in his/her argument, which in turn moulds his/her interpretation which is-according to Carr-""the lifeblood of history"". One ponders about Davis's own agenda behind his enterprise, but let us not tarry on this for there are other more pertinent issues to discuss. In particular, Hill has raised a very relevant point that Davis's paradigm is flawed and his enterprise doomed from the start, because his desire to find ""a coherent, cohesive group of like-minded Ranters"" is anachronistic, and that he is guilty of looking at the past through eyes of the present. There is much inconsistency in Davis's methodology, seen in the way he views seventeenth century pamphlets with extreme scepticism and yet at the same time, as pointed out by Christopher Hill, seems to have taken Abiezer Coppe's ironic recantations seriously. The implications of this debate are far-reaching, and it has not only questioned how historians ought to view primary sources, but has also affected how we are to understand the function of labels. Labels exist first and foremost to describe an external reality, though, that does not preclude the possibility of that label acquiring associations along the way that exaggerate the former, hence turning it into a myth. In the same way that Protestants used the 'Catholic myth' to consolidate their positions, so the seventeenth century conservatives manipulated what was already in existence to galvanise the population into combating the attack on religious and social order that the English Revolution had brought about. E.H. Carr, What is History?, (London, 1990) p.23. J.C. Davis, ""Fear, Myth and Furore: Reappraising the 'Ranters'"", Past and Present, No. 129 (Nov., 1990), p. 82. Jstor article. Carr, What is History? , p. 28 Davis, Fear, Myth and History: The Ranters and the historians, (Cambridge, 1986), p. 74 Christopher Hill, A Nation of Change and Novelty: Radical politics, religion and literature in seventeenth-century England, (London 1990), p.153 Hill, A Nation of Change and Novelty, p. 180. Davis, Fear, Myth and History, p. 53-7 It is Davis's opinion that the 'Ranters' were a ""projection of deviance"", and that ""there was no Ranter movement, no Ranter sect, no Ranter theology"", but consisted of ""a few relatively isolated individuals of heterogeneous persuasions"" who were ""swept up in the projection of a movement"" and were subsequently ""assigned roles by the historians"" While ""historians are agreed that it is wrong to speak of a Ranter sect or movement"", few have gone so far as to deny the existence of the 'Ranters'. Davis is unique in asserting that because there were no concrete evidences of the 'Ranter's' practical antinomianism, therefore they do not exist. Even if such evidences were in existence, Davis would most likely reject them as sensationalism of the yellow press, or as unreliable stories conjured up by seventeenth century anti-'Ranter' pamphleteers. However, Hill points to evidence that Gerrard Winstanley experienced problems in his Digger colony, due to the presence of drinking, whoring and indolent 'Ranters', and makes a point, which is taken up by Aylmer that the 'Ranters' held an 'anti-work' ethic and lived a ""hippy-like existence"" Therein lies the 'practical antinomianism' that Davis seeks. The 'Ranters' hedonistic 'make love not war' sort of attitude constituted a critique of society, which manifested itself in the breaking of taboos, as opposed to the more taxing alternative of a violent overhaul of society. There is a more cogent explanation proposed by Hill that accounts for the lack of 'practical antinomianism'-it is simply that the Ranters were ""talkers rather than doers."" Ibid, p.124 Bernard Capp, ""Fear, Myth and Furore: Reappraising the 'Ranters'"", Past and Present, No. 140, (Aug., 1993), p.165 Jstor article. Aylmer, ""Did the Ranters Exist? "", p. 209 Christopher Hill, The World Turned Upside Down: Radical Ideas during the English Revolution, (London, 1991), p. 229-30. Hill, A Nation of Change and Novelty, p. 185 Davis's second charge is that the absence of any common defining ideology and close direct links between members of the supposed 'Ranter' core disproves that the 'Ranters' existed either as ""a small group of like-minded individuals, as a sect, or as a large-scale, middle-scale or small movement."" While it is true that the antinomian and spiritualist ideas were not unique to the 'Ranters', according to Capp ""it is clear that something had crystallised from the flux, and that we are no longer dealing with isolated individuals"". Capp substantiates his point with compelling evidence drawn from correspondences between Salmon, Webb and Coppe, which indicated a sense of familiarity to the point of intimacy. Though he concedes that Coppe and Clarkson were never close, nevertheless, he asserts that there were links between both groups, hence refuting Davis's claim that ""the Ranter core shatters and disintegrates."" As mentioned earlier, the search for a seventeenth-century group labelled the 'Ranters' eludes Davis precisely because he employs modern standards and preconceptions to study a seventeenth-century phenomenon. Davis's insistence in finding a certain measure of uniformity or convergence of ideas will not hold up even in the globalised world today, where common church denominational labels mask a whole range of disparate practices and beliefs. Seventeenth-century radical groups were even less likely to converge on not more than a few tenets which roughly separated them from the other movements, not forgetting as well, that there was a great deal of fluidity between the groups, as people shopped around for a belief system that appealed to them. The 'Ranters' were such a loose grouping precisely because of their nature as a subversive group, their wariness of any forms of organised religion and distaste for hierarchy, which makes them not unlike modern anarchist 'groups' in form. Davis, Fear, Myth and History, p. 75 Capp, ""Reappraising the Ranters"", p. 166-7 Davis, Fear, Myth and History, p.75 Davis's scepticism is conceivably fuelled by the tendency of supposed 'Ranter' leaders to recant in the face of persecution. His serious treatment of the ironic recantations of Coppe, Clarkson and others is puzzling in light of his scepticism with regards to seventeenth century anti-'Ranter' writings. In contrast, Hill accounts for this readiness to recant as a feature characteristic of 'Ranters', who, like their sixteenth-century predecessors, the Familists, practiced ""Nicodemism-recanting in the face of overwhelming power"" and warns us against taking their equivocal recantations seriously. This practice can also be seen as the direct consequence of the 'Ranter' belief in 'mortalism', which precludes the immortality of the soul and hence of the appeal of martyrdom. As Hill astutely points out ""...resistance to the death would call for a deeper and more consistently worked out ideology than most Ranters had."" The two historians also disagree as to whether the 'Ranters' were the main targets of the Blasphemy Act of 1650. If Parliament had regarded the ideas and behaviour of the 'Ranters' as a real threat to societal order, then Davis's hypothesis that the Ranters did not exist is weakened. Though Davis is correct in saying that the Blasphemy Act did not mention the Ranters, evidence points towards Clarkson's A Single Eye All Light as a trigger to the body of legislation, which according to McGregor ""was largely a description of Ranter doctrine, owing much to the writings of Coppe and Clarkson."" While there is a possibility that the Parliament was responding to the 'Ranter myth' and not to the 'Ranters' it still reinforces Aylmer's simple concept that there can be ""no smoke without fire"". To take the analogy further, the strength of the fire might be disproportionately small compared to the amount of smoke that is billowing, and the response of the fire fighters might be more than is required to deal with a small fire, but the fact remains that a small fire did exist. Though that does not preclude the possibility that some people may have stoked the fire and fanned the flames, hence creating the 'smoke screen' to mask their presumably sinister agendas. Hill, A Nation of Change and Novelty, p. 179. Hill, The World Turned Upside Down, p. 209. J.F. McGregor, Fear, Myth and Furore: Reappraising the 'Ranters', Past and Present, No. 140 (Aug., 1993), p. 157. In order to justify and perpetuate his hypothesis that the 'Ranters' did not exist, Davis puts forth a series of explanations that sought to provide a plausible list of agendas that necessitated the creation of the 'Ranter' myth. For the seventeenth century conservatives, the 'Ranters' were necessary as the ""projection of deviance"" ""for it is only through deviance that we understand normality."" While this is a compelling argument, and extreme radicalism may alienate other groups and force some into the protective arms of the conservatives, there is also the other possibility that the increased profile and definition of the 'Ranters' may have seduced and ensnared others. No matter how vile the portrayal of the 'Ranters' there is an element of freedom in antinomianism that can be particularly enticing, especially for young men. Hence, this myth is in my opinion a double-edged sword to be wielded at one's own peril. The myth may achieve normality, but it could just as easily go the other way and perpetuate deviant behaviour. The 'Ranter' myth was not the exclusive tool of the conservatives, but was also used to achieve sectarian consolidation: to enhance the acceptability of sects that were not as radical as the 'Ranters', and to induce conformity within sectarian ranks. Because this spectre of Ranterism was conjured up to fulfil a social function, Davis warns us against taking any of the anti-Ranter literature seriously. On the topic of sources, Hill suggests that we can look to seventeenth-century personalities, such as John Reeve and Lodowick Muggleton, who were previously from the Ranter milieu for an accurate account of 'Ranterism'. Notably, Hill focuses on the figure of Bunyan, who was himself initially tempted by Ranter ideas, but whose work's from 1665-1685 took on an anti-'Ranter' slant. Also, despite the many hostile anti-Ranter records, Hill uncovered Ranter nostalgia in Erbery, Byne and Sedgweick, who ""found things to praise in the Ranters..."" though he added that ""they would hardly invent the Ranters for this purpose"". At the very least, there is a consensus that 'Ranter' was a 'buzz-word' in seventeenth-century England, not unlike that of 'globalisation' in our own time, though an admittance that the misuse of the label 'Ranter' existed does not preclude the external reality of a group of 'Ranters'. Davis, Fear, Myth and History, p.124 Ibid, p.111 Ibid, p. 110. Hill, A Nation of Change and Novelty, p. 164 Ibid, p.177. Ibid, p.169. While Davis might possibly defend his claim that the 'Ranters' were created by seventeenth-century conservatives (amongst others), his other claim-that the 'Ranter' spectre was resurrected by A.L. Morton and Christopher Hill, both members of the Communist Party Historians' Group between 1946-1956 in a bid to ""create the history of a popular democratic tradition in English history and culture"" is absurd and deliberately provocative. The allegation that Davis brings against them-that of using theory to dictate history- is an equally valid charge that can be levelled at him. Hill's defence of himself and A.L. Morton in A Nation of Change and Novelty is fairly convincing, and he has managed to place the onus on Davis to explain his own motivation behind the whole enterprise. Furthermore, other historians have joined in the fray, and with a few differences they have reaffirmed the existence of the 'Ranters' and the 'Ranter' myth. While the debate should not end on a 'majority wins' basis, there is a sense that this is quite a lope-sided debate, with Davis fighting the battle on his own. Davis, Fear, Myth and History, p.130 Hill , A Nation of Change and Novelty, p. 190 In conclusion, Davis's hypothesis, while novel, is not entirely convincing. The arguments of the other historians are a lot more persuasive, both in establishing a link between the various individual 'Ranter' leaders (despite the running debate about 'core' vs. 'milieu' which was not dealt with in this essay) and in their efforts to draw out some semblance of similarities between the various groups, while conceding that it impossible to define a coherent set of 'Ranter' ideology. This is not far off from Davis's own assertion that what would make him believe that the 'Ranters' existed would be the coincidence of two sort of evidences: that the existence of a group (or a number of groups loosely linked) can be established and that its shared ideology was reflected behaviourally in practical antinomianism. It bears repeating that Davis's enterprise is flawed for two reasons: his failure to differentiate between 'Ranter' myth and 'Ranters' which has led him to believe that the 'Ranters' were created by seventeenth-century conservatives and later resurrected by twentieth-century Marxist historians; and his employment of modern standards and preconceptions to study a seventeenth-century phenomenon, which explains why his search for the 'Ranters' as a external reality has been so unfruitful. Perhaps one might cheekily suggest that Davis's own 'recantation' is overdue. J.C. Davis, ""Fear, Myth and Furore: Reappraising the 'Ranters': Reply"", Past and Present, No. 140 (Aug., 1993), p. 205. Jstor article. China has long captured the imagination of the West, and still remains an enigma in this present age despite the vicissitudes of time. While technology now permits us to catch a glimpse of China on the news, on the internet, or plausibly, even visit China; this would not have been conceivable in previous centuries, where one of the few ways in which one could come into contact with, and conceive of China was through the objects brought back from overseas trade- which was itself stimulated by a thirst for the mysterious exotic Orient as much as it was for profit. Although these goods were prized for their scarcity, aesthetic value and excellent workmanship, it was not the material alone that captivated, but more importantly, it was the non-material associations that imbued the goods with meaning, desirability and the label of 'luxury'. This fuelled the luxury trade, long underpinned by traditional court luxury, which reached its zenith in the seventeenth and eighteenth centuries, with the establishment of the chartered East India companies and the subsequent creation of a European market for these goods. This essay will attempt to put forth the idea that Europeans were so fascinated by Chinese luxury goods precisely because of the various functions that these goods fulfilled- it fired up the European imagination, giving it the impetus to venture out and explore the world; it played its part in influencing the new taste for the Rococo and in the definition of 'refinement'; it was amenable to the marketing techniques of the East India Companies which further heightened its appeal; and finally it stimulated European industries in their attempt to surpass the East through emulation and innovation. In short, it was through the acquisition of Chinese goods, which defined the 'Other', that one perceives the 'Self' in a clearer light. Maxine Berg, ""In Pursuit of Luxury: Global History and British Consumer Goods in the Eighteenth Century"", in Past and Present 182 (2004), p. 86. Before embarking on the main body of the essay proper, there are some issues that deserve our attention: firstly, the reference to ""seventeenth and eighteenth century Europeans"" in the question should not be taken to mean all Europeans, but was limited to the affluent, by nature of it being Asian luxury goods. And even within this group, there were some such as Mrs. Montagu and the adherents of the Burlington School of Taste who lamented the fall from favour of Palladianism. While it is not the task at hand to examine the interactions between the Orient and the Occident, it must be said that far from being a one-way exchange, the Chinese elite were themselves fascinated by western manufactured goods, especially clocks. It is interesting to note that both cultures used foreign 'luxury' goods in a setting that suited their own agendas, often removing the objects from their original context, transforming the functional into the ornamental. This playful assimilation, also seen in the creation of the Chinoiserie, is perhaps reflective of the confidence of both cultures, which is in stark contrast to the theories of Social Darwinism, which characterised their relations in the nineteenth century. Finally, it is important that we examine the definition of 'luxury goods'. As Vainker astutely points out: despite the high quality of silk, porcelain and lacquer, these goods were not perceived as luxury items, but as functional goods by the affluent Chinese. Instead, what they valued as luxury items would include ""paintings, ritual bronzes, jades, calligraphy and other antiques and collectibles associated with a scholar-official elite and the authority to rule."" Hence, luxury is defined not by high quality goods that are widely available (such as porcelain and silk in China), but by its association with the conferment of social status due to its scarcity (Chinese porcelain and silk in Europe). In a letter from Mrs. Montagu 1749 ""Thus it is happened...we must all seek the barbarous gaudy gout of the Chinese; and the fat-headed Pagods and shaking Mandarins bear the prize from the greatest works of antiquity; and Apollo and Venus must give way to a fat idol with a sconce on his head."" Quoted from Dawn Jacobson, Chinoiserie, (London, 1993), p. 123. Ibid, p.124. Seen in the use of clocks as status symbols (as opposed to time-pieces) in Yongzheng's imperial household; and the French candelabra composed of inverted Chinese whistles and cups mounted on a frame. Seen in the DVD produced on the Encounters: The Meeting of Asia and Europe 1500-1800 exhibition. Picture of French Candelabra in Anna Jackson and Amin Jaffers (ed. ), Encounters: The Meeting of Asia and Europe 1500-1800, (London, 2004), plate 1.9, p. 7. Picture of Yongzheng's concubine with a clock in the background in Evelyn S. Rawski and Jessica Rawson (ed. ), China: The Three Emperors 1662-1795, (London, 2005), bottom left, p. 259. Shelagh Vainker, ""Luxuries or Not? Consumption of Silk and Porcelain in Eighteenth-Century China"", in Maxine Berg and Elizabeth Eger (ed. ), Luxury in the Eighteenth Century: Debates, Desires and Delectable Goods, (New York, 2003), p.214. The seventeenth and eighteenth century fascination with Chinese luxury goods had its genesis in the myth of Cathay, which conjured up ""fantastic visions of luxury and refinement, pleasure and abundance"", and were reinforced by traveller accounts of Marco Polo and the fictional Sir John Mandeville. As Jacobson observed ""this view of the Chinese as being different but in no way inferior, helped to sustain the myth of Cathay, and the vision of a culture that was profound as well as peculiar."" It was this perception that made possible the enchantment and respect that the Europeans had for the advanced Chinese civilisation; which manifested itself in the pursuit of Chinese goods, in the hope that one's collection might plausibly be employed as a prism through which the secret knowledge of the East might be yielded. That China during the Ming dynasty was largely cloistered from the world further enhanced its mystery and appeal, not unlike the coy mistress subtly beckoning to would-be suitors. And they came- in drips and then in droves- the Portuguese trailblazers followed by the Dutch, English and French traders who brought back taster portions of Asian goods, fuelling the imagination of the affluent Europeans further, which in turn encouraged the luxury trade. Jacobson, Chinoiserie, p. 10 Ibid, p. 12-15. Ibid, p.15. It is imperative that we examine how this seemingly perennial fascination that the Europeans have for the exotic Orient featured in the particular context of the seventeenth and eighteenth century, especially in terms of sociability, civility and taste. It is important to note that in the moral debates about luxury at that time, Chinese goods were ""associated not with (Persian) sensuality and excess, but with ethics, harmony and virtue...(and) in possessing things Chinese, they (the philosophes) sought to access levels of civilisation beyond the market."" Hence, it was through the prism of Chinese objects that the Europeans expressed ""their own aspirations to human elegance and refinement."" The employment of taste as ""a new means of displaying prestige"", and the corresponding replacement of traditional forms of opulence (silver and gold) with elegant new 'imitative' objects, such as porcelain are integral in accounting for the vogue in Chinese luxury goods in seventeenth and eighteenth century Europe. This change in taste also reflected the changing relations in society, with the notable rise of the bourgeoisie, who rejected the gaudy decadence of the baroque (associated with the ancien regime) in favour of the lighter, more playful and asymmetrical Rococo, which ""brought together commercial and cultural modernity."" It is perhaps fortunate that the Chinoiserie with its freedom from classical notions of restraint and symmetry found favour with the Rococo, and was in fact a ""vital ingredient in the creation of the new taste."" It was in this context that Chinoiserie-which was the Western romanticised view of the East, produced in part by the imported goods- flourished, and ""in turn dictated the commerce in Asian imports."" Maxine Berg, ""Asian luxuries and the Making of the European Consumer Revolution"", in Maxine Berg and Elizabeth Eger (ed. ), Luxury in the Eighteenth Century: Debates, Desires and Delectable Goods, (New York, 2003), p. 229. Ibid. Ibid, p. 230. Ibid, p. 231. Jacobson, Chinoiserie, p. 57. Maxine Berg, Luxury and Pleasure in Eighteenth century Britain, (Oxford, 2005), p. 51 While fascination with the Orient had always existed, it is necessary to look at how the East India Companies manipulated what was already in existence and transformed it into an obsession with Chinoiserie, as seen from above. As Berg astutely pointed out: these luxury goods ""were a construct of the market, seeming to represent the lives and values of the East, but constructed to meet European preconception of eastern art."" These commodities were prefabricated on the Chinese side to cater to Western markets; and on the part of the East India Companies, which sent out European shapes to be copied, and the initial contentment with Chinese designs soon gave way to a sudden fashion for 'chine de commande' by the end of the seventeenth century. While the East India Companies (EICs) were themselves the early products of the European fascination with China and its goods, they were indispensable in the subsequent promotion of this luxury trade, which was intrinsically tied up with the western enthralment for the East. The EICs were not merely merchants who simply pandered to the tastes of the European elite, but were market movers in the way in which they created a demand for these goods through 'customisation' and effective marketing. It is remarkable how they transformed something as functional and mundane as porcelain by capitalising on its exotic appeal and its association with civility, marketing it as a semi-luxury good, when truth be told, the differentiated decorations on the porcelain pieces belie the fact that they were to some extent, 'mass' produced goods. Moreover, they created new wants- ""the dinner service with complete setting for each person was an invention of the VOC and the EIC which started importing porcelain dinner services in the eighteenth century."" Therefore, though the western fascination for the Orient was present before the seventeenth-century, we have to credit the EICs for their role in elevating it to a whole new dimension with the extension of maritime trade and through successful marketing tactics. Berg, ""Asian luxuries and the Making of the European Consumer Revolution"", p. 228 Christiaan, J.A. Jorg, ""Porcelain for the Dutch"", in Rosemary E. Scott (ed. ), The Porcelain of Jingdezhen: Colloquies on Art and Archaeology in Asia No. 16, (London, 1993), p. 188 and 200. Berg, ""Asian luxuries and the Making of the European Consumer Revolution"", p. 239 While exoticism, taste and marketing definitely contributed to the appeal of Chinese luxury goods to the West, a large measure of this preoccupation was also due to the impressive technical wizardry of the Chinese craftsmen and the secrecy which shrouded the production processes for these goods-especially porcelain- the Arcanum of which was first discovered by the Europeans only in 1709. Perhaps it is useful to point out that the luxury goods that fascinated the West were new, modern luxury comprised of ""technically advanced and refined consumer objects"" such as calicos and porcelain. Hence, the East enthralled the West because the latter realised that much could be learnt from the former; which would allow the West to progress through imitating and perfecting the techniques that they have gleaned from the East. It is important to note that contemporaries such as ""Montesquieu, Hume and Smith wrote of luxury as an aspect of people's desire to better themselves"" and ""...commercial writers were keen to point out what the West could learn from the Orient."" In doing so, they affirmed the moral good of luxuries in contributing to 'Progress', hence alleviating the fears of decadence and degeneration that plagued earlier preoccupations with luxury goods, and indirectly encouraged the fascination for Chinese luxury goods. While it must be said that this desire to learn from the East was more apparent than real, which is illustrated by Berg's observation that ""Asian consumption was transferred to Europe but not Asian production systems"", there appears to be a genuine lesson imported from the East, which is ""the policy of using the fine arts and design in combination with modern manufacturing technique."" Hence, ""paradoxically, in imitating Asian consumer goods, perceived in Europe as luxuries, the British achieved what was actually an Asian success story-new, quality, semi-luxury consumer goods produced with advanced industrial techniques."" Ibid, p. 230 Ibid. Berg, ""In Pursuit of Luxury: Global History and British Consumer Goods in the Eighteenth Century"", p. 86 Berg, ""Asian luxuries and the Making of the European Consumer Revolution"", p. 242. Berg, ""In Pursuit of Luxury: Global History and British Consumer Goods in the Eighteenth Century"", p. 130. In conclusion, this fascination that the Occident had for the Oriental luxury goods was not new, but was closely entwined with the captivating medieval myth of Cathay, and reached new dimensions in the seventeenth and eighteenth century with the establishment of the East India Companies (which were themselves the product of and the further stimulus to the West-East trade born of the Occidental preoccupation with the exotic, mysterious and abundant East) and the creation of European markets for Chinese luxury goods (which was previously the preserve of the European royalty). It is perhaps paradoxical that these Chinese luxury goods fascinate precisely because they have been displaced from their original cultures and grafted into new ones, but continue to give the impression that it was still possible to use them as prisms through which the West can subscribe to notions of Chinese refinement and skill. The fascination for anything 'Chinese' is apparent in the vogue for the Chinoiserie especially in the eighteenth century, and is both reflective of, and contributed to the existing enthrallment that the West had for the East, and how that affected the dominant taste in Europe. The appeal of Chinese luxury goods was also played up by the East India Companies which capitalised on the vogue for Chinoiserie, and created new wants through the successful marketing technique of making the foreign 'relevant' to the West through customisation. Finally, and most importantly, Chinese luxury goods were seen as having the role of ""awakening the imagination"" and were positive elements in stimulating the Europeans to better themselves through emulation and product innovation. Despite being so far removed from their original cultures, that these luxury goods were relevant and fulfilled certain functions given to them by their European possessors accounts to a large extent for the fascination that seventeenth and eighteenth century Europeans had for Chinese luxury goods. Ibid, p.131","Traditional accounts on the causes of the Reformation necessarily included castigation of clerical abuses, such as concubinage, pluralism and absenteeism; the ignorance and poor training of priests and monks; and perhaps even the supposed failure of papal leadership as manifestations of the decay and decline of the Church on the eve of the Reformation. This view, and the more recent revisionist view of the topic in question has informed the main body of debate in scholarly circles, but before embarking on the essay, there are a few points that deserve attention. Firstly, the question suggests that what was 'wrong' with the Church could have led, in some sort of linear equation, to the birth of the Reformation. Following in the footsteps of Peter Marshall, ""the intention is not to sift the evidence for signs of impending Reformation"", and teleological history should be avoided due to the inaccurate interpretations and conclusions which may emerge from that approach. Secondly, it is ambiguous as to what constitutes 'the Church'. According to Robert Swanson, there are two main definitions: ""the restricted, institutional definition, which segregated the 'ecclesiastical' from the 'secular'..."" and ""the body of the faithful which made up the mystical body of the church which was also the body of Christ."" To limit the scope of this essay, the first of Swanson's two definitions will be taken up, noting however, that this requires a generalization of the circumstances in the various national churches. This essay will avoid a direct, simplistic answer to the question but will instead attempt to put the criticisms of the Church into perspective and suggest that there was a certain measure of continuity, rather than exclusivity to the eve of the Reformation; and to locate the essay within the confines of the changing climate of opinion that may have contributed to the perceived problems of the Church. The recurrent themes of Expectations and Reform will be examined in the following context: the Clergy, the Papacy, Doctrines and Practices, and will revolve around the issue of the function of the Church. Peter Marshall, Reformation England 1480-1642, (Cornwall, 2003), p. 2 Robert N. Swanson, Religion and Devotion in Europe 1215-1515, (Cambridge, 2003), p. 7 The failure of the Clergy to live up to lay expectations of moral superiority (derived from being the agent through whom man communicated with God) and to fulfill the functions they were ordained for opened up the Church to criticisms, from without and within. 'On the eve of the Reformation, laity protested against pervasive clerical fiscalism, absenteeism, mal-administration, and concubinage..."" and were joined in their castigations, by some members of the Clergy, and more notably, the humanists. John Colet, the humanist dean of St Paul's even went as far as to blame ""all the ills of the Church on the secular lifestyles and 'covetousness' of the Clergy"" and urged ""reformation of the Church's estate"". According to Duggan, even ""Catholic and Protestant historians alike largely concur on the (sexual) 'immorality' of many Clergy on the eve of the Reformation."" However, a reconsideration of the reaction to the abuses of the Clergy is necessary. Despite the widespread practice of concubinage, the population of Europe was not overly concerned unless this moral failing on the part of the Clergy impinged on their primary role of administering the sacraments. As Duggan articulates succinctly: ""whether the clergy were 'immoral' or not, the more germane question is whether they ministered adequately to the needs of their flock."" Steve Ozment, Age of Reform: An intellectual and Religious History of Late Medieval and Reformation Europe, (Newhaven, 1980) p, 211. Marshall, Reformation England, p.11 Lawrence G. Duggan, The Unresponsiveness of the Late Medieval Church: A Reconsideration, Sixteen Century Journal, Vol. 9, No. 1 (April 1978), p. 23 Perhaps, it would be plausible to suggest that the more pressing considerations on what was 'wrong' with the Clergy would be ""pluralism and absenteeism, which are frequently construed as abuses and signs of decay and pastoral neglect"" The centrality of the Church in providing ""contact between human beings and the supernatural order of their universe, to explain, direct, and console"" is crucial to the understanding of late medieval religion; and the perceived failure of the Church in providing the above services would shake the foundations on which it was built. The Church was faced with two main dilemmas: firstly, it had inadequate jurisdiction over the appointments to benefices, which increasingly became the prerogative of the rulers. Also, clergymen seeking to further their knowledge and competence often enrolled in the universities, with the obvious consequences of being away from their benefices. Despite pluralism being widespread, it did not necessarily indicate neglect, as absentee beneficiaries were required by canon law to provide a vicar to cater to the needs of the local populace. Ibid, p. 23 Euan Cameron, The European Reformation, (New York, 1991), p. 9 What proved more destructive than the above-mentioned abuses ""were the words and ideas of clerics...born of an impatience with the imperfect established church and a moral vision of purified Christianity."" The more prominent members of this group include John Wycliffe, Erasmus and Luther. It is noteworthy that criticisms of the Church went as far back as to the late 14 th Century (the Lollards) and were not exclusive to the eve of the Reformation. The irony lies in the fact that the Church had effectively 'dug its own grave' by educating these men-empowering them with knowledge which allowed for the formulation of reformist ideas and expectations of change, which clashed, at every level, with the vested interests of the clerical class. Duggan, The Unresponsiveness of the Late Medieval Church: A Reconsideration, p. 25 The fact that the Church was becoming more accessible to the Laity may have opened up the former to criticisms from their congregation. The increased interaction between the Clergy and the Laity may have led to the exposure of church flaws which may seem less apparent previously; and the printing of the provincial decrees and sermons denouncing clerical abuses, by the church, in the vernacular ""might have convinced laymen not only of the widespread efforts of reform but also of the widespread problems of the church ...raising in some circles expectations that could not be realized."" Ibid, p. 14 The Papacy was also not immune to criticisms of 'worldliness', and the Renaissance popes provided the best illustration of this by participating in the struggle for temporal power, concubinage, and as patron of the Arts. Since the thirteenth century, reformers have 'found themselves at cross-purpose with a papacy whose pastoral role had become increasingly confused with its political and economic ambitions."" However, far from being a power-hungry and oppressive apparatus, it can be said that the Papacy had undermined its own power through the setting up of Universities, encouragement of printing, and even patronage of the humanists. The Papacy has often been accused of being lofty and unresponsive, but in reality, was the main impetus behind the improvement of society, largely through the channeling of Church resources into building and maintaining educational institutions that served both the state and the society at large. Most significantly, they 'took under their aegis the devotio moderna the principles of which strikingly foreshadowed the worth and dignity fully accorded laymen by the reformers of the sixteenth century.' Ozment, Age of Reform, p.208 Duggan, The Unresponsiveness of the Late Medieval Church: A Reconsideration, p.16 It is imperative to examine the doctrines and practices (in particular, the deep belief in Purgatory, the sacrament of Penance, and its derivatives such as relics, pilgrimages and most notoriously, indulgences) of the Church, which generated oppositions from the laity and clergy. Interestingly, criticisms of Church practices and doctrines existed in the late fourteenth century, in the form of Lollardy (led by the Oxford theologian John Wycliffe) called for a clear divide between spiritual and temporal power; obedience to the church authorities as far as their behaviour merited; the Bible as the only source of religious authority; condemned the doctrine of transubstantiation; and attacked the veneration of saints (and relics). The humanists also shared the view that the Church had diverged away from the one true religion, embraced misconstrued beliefs and had become too intertwined with the secular world. Convinced that grace and forgiveness could only come from God alone, Luther also harshly condemned the sale of Indulgences in his ninety-five theses. However, the cynicism surrounding the practice of Indulgences were not unique to the eve of the Reformation, and was a pervasive element in literary traditions, as seen in Chaucer's Pardoner's Tale which was written in 1400. It is interesting to note however, that the supply of indulgences was in response to the demands of the laity, and was perhaps a manifestation of the paranoia of securing one's own salvation. What is perhaps significant is that the demanding penitential system and mechanical ritualized religion offered by the Church only provided temporary relief for the laity, and had led instead to the exacerbation of fear and anxiety as reflected in the obsession with the constant absolving of sins. Marshall, Reformation England, p. 16 According to Steve Ozment, one of the chief failings of the Church lay in 'the absence of a distinctive concept of lay religious life and the consequent imposition upon the laity of traditional clerical ideals'; and the proliferation of confraternities, in addition to the earlier movement of the devotio moderna are perhaps indicative of the failures of the church to cater for lay piety. The advocacy of clerical piety as the zenith of religious piety had placed unrealistic expectations on the laity and precluded their search for consolation through the Church. The incongruity of this enterprise was not lost on the increasingly literate laity, who rejected the ""prominent clerical ideals of obedience and sexual impurity"" in favour of ""simplicity, directness and respectful treatment in all spheres of their lives."" Moreover, as mentioned above, the behaviour of the clergy did not correspond to the standards that church traditions had imposed on them, and policies such as enforced celibacy increased the incidence of breaches in moral standards that served to reinforce, amongst the laity, the futility of this endeavour. Ozment, Age of Reform, p. 219 Ibid, p. 220 Some historians have also suggest that the increasing desire amongst the laity for a personal and direct relationship with God, and the subsequent failure of the Church to cater to this aspect led many into the realm of mysticism. Though some have argued that mysticism could be regarded as a form of subversion, it is perhaps more appropriate to think of it as a supplementary to, rather than as a substitute for the institution of the Church. Mysticism of both the fourteenth and sixteenth century coexisted with the Church and may be seen as testament to the religiosity of the Age. There is a possibility that the multiple responsibilities of the Church in society may have led to its resources becoming over-stretched, hence, compromising on the primary role of the Church to minister the sacraments. Swanson puts forth the idea that most of the anti-clericals ""wanted churchmen to behave just as churchmen, rather than officious, interfering, lordly bureaucrats."" However, the Church had for centuries combined both its temporal and spiritual responsibilities without much difficulty, and it appears rather peculiar that the church would find it hard to cope with its dual roles on the eve of the Reformation. Moreover, the increased accessibility of university education provided the necessary training for aspiring civil servants, and this would have taken the burden off the Church as a source of administrative officials, implying perhaps that the Church could concentrate more on its primary, spiritual function. Although there were many problems (both discipline and doctrinal) that the Church faced, it is apparent that these problems had been in existence for the past one or two centuries, and that despite the criticisms, the aim of the reformers was not to subvert the church, but had in mind the desire to change the church into a more responsive, relevant and effective apparatus. Throughout the essay, the writer has tried to suggest that it was the rising expectations of a growing educated class (laity and clergy) and the inability of the church to keep up the pace of reform and to satisfy the needs of the laity - the disparity between expectations and reality- that led to disillusionment. Similarly, the high expectations that the Church set with regards to piety eventually led to the alienation of the people from the Church. Despite the deep-rooted flaws of the church, the attempts at reforming it suggest ""men were seeking salvation and wished to find it with the help of the church."" Hence, in trying to account for the Reformation, it is inadequate to merely examine what was 'wrong' with the church, but requires instead, a broader understanding of the climate and considerations (political, economic and social) of the Age. B.Moeller 'Religious life in Germany on the eve of the Reformation', in G. Strauss (ed.) Pre-Reformation Germany, (London, 1972) p. 27 Social unrests were not unique to the 19 th Century, and the Qing state was by no means inexperienced in putting down such attempts to destabilize the State. Despite the consolidation of power during the reign of the Emperor Kangsi to Qianlong, there remained however, certain underlying grievances that were never fully eradicated, which led to the persistence of the themes such as 'Over-throw the Qing, Restore the Ming', ethnic and religious tensions. The end of the Qianlong era marked the conclusion of the golden age of Qing rule and saw the gradual decline of central power vis-à-vis the provincial power and the emerging western participation in Chinese politics. Though westerners have had a presence in China dating back to the Yuan dynasty, the Opium Wars and the corresponding treaty system marked a watershed for China-West relations, resulting in the change of status quo, of perceptions; and manifested itself in the increasingly intrusive presence of the West, which exposed the many weaknesses of the Qing state, making it more vulnerable to attacks from within. It is in this context that the above question should be addressed, and the writer will examine firstly the internal conditions and entrenched corruption which made it conducive for the outbreak of social unrests, followed by the implications of change brought about by the West, such as the desire to trade, introduction of new technology, ideas, religion, and the pursuit of Imperialism which clashed with Chinese assumptions of the monopoly of civilization, and in turn produced a virulent anti-foreign nationalism which threatened the social order during the period in question. Hence the writer will argue that the social unrests at the beginning of the 19 th Century was due more to the internal rather than external factors; but increasingly, the presence of the West is crucial to the emergence of violent xenophobic nationalism, which account for the social unrests in the later part of the 19 th Century. One of the persistent problems that the Qing failed to eradicate were the deep-rooted ethnic tensions which lent itself to many rebellions at the first half of the 19 th Century. The entrenched sentiments that the Qing were a foreign power residing in China led to the creation of many secret societies, such as the White Lotus Society and the Heaven and Earth Society (Tian-li-jiao), whose rallying cry of 'Overthrow the Qing and Restore the Ming' saw the outbreak of the White Lotus Revolt in 1796, Szechuan and Hubei, which lasted until 1904, as well as the attempt made by Li Wen Cheng to attack the imperial palace in 1813. The prejudices that the Han Chinese held against the Muslims and the latter's desire for autonomy from the Qing, led in the 1820s to the Muslim rebellion led by Jehangir in Turkestan, and subsequently, in the two Muslim rebellions, in Yunnan (1855-1873) and in Gansu and Shaanxi (1862-1873). The Taiping rebellion has its roots in ethnic tensions as well, with Hong Xiuquan and a considerable number of his followers coming from the Hakka (ke jia ren) sub-group, and these 'guest-settlers' were frowned upon by the largely Han population in Guangxi. The anti-Manchu sentiments of the Nian rebellion is perhaps most clearly indicated by their leader Zhang's honorific title of 'Great Han Prince with the Heavenly Mandate'. Jonathan D. Spence, The Search for Modern China (2nd Edition), (New York, 1999), p. 184. Ethnic tensions alone do not cause rebellions, but mixed with deplorable economic and social conditions can be a recipe for disaster. Between 1741-1841, the population increased by three-fold, but the increase in arable land could not keep pace with it, which led to food and land shortages, with the latter commodity increasingly becoming concentrated in the hands of the wealthy. The yearly occurrence of floods and droughts in the provinces along the valleys of Yellow and Yangtze rivers during the end of Daoguang's reign (1821-1850) and the lack of state initiatives in alleviating the situation may have led the population to believe that the mandate of Heaven was passing out of the hands of the Qing dynasty. Also, the Opium trade, conducted by the British led to the great outflow of silver, resulting in the devaluation of copper coins, the inflation in the prices of other commodities, and the corresponding increase in the peasants tax burden which was paid in silver. All these combined events accentuated the miseries of the common people and made for fertile ground for the outbreak of social unrests. Potential rebellion leaders could find amongst the population large numbers of poor peasants and artisans who were willing to throw their weight behind anything that promised better conditions than the existing one. The attractiveness of the Taiping laid in its Christian-Communism ideology and the proclamation that 'nowhere will inequality exist, and no one not be well fed and clothed."" The triggers for the Muslim rebellion in Yunnan 'were the heavy land taxes and extra levies imposed by Peking on the Yunnanese Muslims' who in addition to that, were being ousted out of their sliver and gold mines by the Chinese. Li Chien-nung, The Political History of China, 1840-1928, translated and edited by Teng Ssu-yu and Jeremy Ingalls, (California, 1967) p. 49. Jonathan D. Spence, The Search for Modern China (2nd Edition), (New York, 1999). p. 176. Ibid, p.187. The inability of the State to alleviate the situation plaguing the population, and the ineffectiveness of the Army in putting down the rebellions are perhaps telling of the decay within the Qing central administration, which permeated to the lower level officials. The appalling inertia of the State led Zeng Guofan to condemn the ruinous tendency of officials 'to gloss over, to make up and to steal days of ease."" The two chief ministers under Daoguang's reign, Tsao Chen Yung and Mu-chang-a discouraged officials from talking about national policy, and those who dared petition to the throne were threatened with punishments. It is perhaps not surprising then, to find the character of Chen Tsu-ch'en, the governor of Guangxi who did not bother with administrative affairs, bit prayed instead to Buddha to dispel catastrophes. 'Such was the immediate political background-general lethargy and irresponsibility-which paved the way for Taiping's rapid development."" The corruption and lack of discipline within the Army was so rampant, that by the time of the White Lotus Rebellion, local militias had to be organized to compensate for the ineffectual standing army. Similarly, it was Zeng's Hunan army, Li Hongzhang's Huai army and Zuo Zhongtang's army, that suppressed the Taiping, the Nian and the Muslim rebellions. Also, inner court intrigues dominated the post Xian-feng era, which saw the rise of the Empress Dowager Cixi and her cronies, and in turn compromised the power of the Emperor to act as the unifying force of China. The diversion of resources meant for military and emergency purposes to fund the decadence of the court left China in a weakened state, creating ample opportunities for the outbreak of rebellions. Li Chien-nung, The Political History of China, 1840-1928, (California, 1967), p. 49. Ibid, p.50. The Chinese Pandora box was sitting there, waiting to be unlocked, and when the West came to pry China open, they unleashed torrents of tensions, discontentment and grievances, which when mixed with Western military technology, religion and ideas, became more malignant, and threatened to topple the Qing state. The rise of Capitalism and the nation-state in the West led to the corresponding desire to gain economic advantage and prestige via the acquiring of colonies, and largely accounts for the increased presence of the West, and the advent of the Age of Imperialism, in China and else where (seen also in the 'Scramble for Africa') leading to the creation of anti-foreign nationalism, which is a major source of disturbances during the later half of the 19 th Century. The role of the West became increasingly important because the weak central Qing government were defenseless against the military superiority of the West, as seen in the Opium War. Correspondingly, the treaties that were concluded reflected this imbalance, and. 'under the treaties, China's sovereignty was increasingly impaired."" 'The formative decades of the treaty system in the 1840s and 1850s must therefore be seen as the opening phase in an intricate and portentous growth of foreign influence on Chinese life..."" and the creation of new institutions for Sino-foreign contact. China had to cede Hong Kong permanently to the British, open up Amoy, Fuzhou, Ningpo, Canton and Shanghai for trade, allow the British to fix customs duties, permit extraterritoriality and the establishment of equal basis for official correspondence, all of which were seen as attacks on the sovereignty of China, and the inability of the Qing to protect Chinese territory led to the creation of anti-Manchu as well as anti-foreign sentiments. In particular, the tradition of anti-foreign resistance can be traced to the San-yuan-li incident in Canton 1841, in which '...specific incitements (raping and defiling of temples in San-yuan-li), together with pervasive popular anxieties about the war, helped to transform relatively benign stereotypes about foreigners into xenophobic racism."" Also, the period from 1841-1850 saw the upsurge of social disorders in Canton, and may have precipitated the Taiping Rebellion. 'In general after the Opium War, the prestige of the Manchu government was destroyed and the invitation to rebellion was ever-present...from 1841-1850 there was not a single year free from local uprisings."" In retaliation to the Qing obstinate refusal to honour the treaties, the allied British and French forces stormed Peking, burned the Yuan-Ming-Yuan, and dealt Qing China a big blow to its prestige when Emperor Xian-feng had to flee to Jehol. This demonstrated clearly that the Manchus were no longer able to be guardians of China, and created a power vacuum within China, and made her vulnerable to attacks from within and without. John K. Fairbank, The Creation of the Treaty System, in Dennis Twitchett and John K, Fairbank (ed. ), The Cambridge History of China, Vol. 10 Late Ching 1800-1911, Part I, (New York, 1995), p. 214 Ibid, p.215. Frederic Wakeman Jr, The Canton Trade and the Opium War, in Dennis Twitchett and John K, Fairbank (ed. ), The Cambridge History of China, Vol. 10 Late Ching 1800-1911, Part I, (New York, 1995), p. 202. Frederic Wakeman Jr, Strangers at the Gate: Social Disorder in South China 1839-1861, (Los Angeles, 1966), p. 117-31. Li Chien-nung, The Political History of China, 1840-1928, translated and edited by Teng Ssu-yu and Jeremy Ingalls, (California, 1967) p. 52. Chinese defeat to the Allied British and French forces in 1960 led to the conclusion of a treaty, which granted full official toleration to the missionary, and also opened up the hinterland to the spread of Christianity. 'One result of this new set of circumstances was a considerable broadening and intensification of the tradition of Chinese hostility toward Christianity."" Though the Christian attack on ancestor worship and idolatry offended all Chinese, it is interesting to note that anti-Christian demonstrations were often provoked by the exhortations of the gentry and official class; who saw themselves as defenders of Confucian order and civilisation in a period of disorder, and whom regarded Christianity as the anti-thesis of Confucianism, and as the imperialistic tool of the foreigners. The presence of missionaries who abused their positions by interfering in lawsuits, and in shielding their converts from Chinese law, as well as the flaunting of treaty rights in demanding large amount of indemnity as payment for damages did not endear them to the local populace. The problem was particularly acute with regards to the French Catholic missionaries, and 'the Tientsin Massacre (1870) represented the culmination of a decade of Sino-foreign friction revolving around Christian missionary activities.' Paul A. Cohen, China and Christianity: The Missionary movement and the Growth of Chinese Anti-foreignism 1860-1870, (Massachusetts, 1963), p. 64. Ibid, p. 233. Deep-rooted Chinese ethnocentrism, the resentment against unwanted Western intrusion and the tendency of any society that has been seriously disturbed by internal disorders to seek an external scapegoat accounts largely for the Boxer Rebellion, which occurred when the 19 th Century drew to a close. It is perhaps telling that there was a song that went: 'Learn to be a Boxer, study the Red Lantern. Kill all the foreign devils and make the churches burn."" The churches were the most visible facades of western imperialism and hence were the targets of anti-foreign movements, and for large segment of the population, 'the missionary was the only concrete manifestation of the foreign intrusion and, as such, the only flesh and blood object against which opposition to this intrusion could be directed.' Ibid, p. 269. Cheng Pei-Kai, Michael Lestz and Jonathan D. Spence, The Search for Modern China: A Documentary Collection, (New York, 1999), p. 186. Paul A. Cohen, China and Christianity: The Missionary movement and the Growth of Chinese Anti-foreignism 1860-1870, (Massachusetts, 1963), p. 269. The role of Western influence in inspiring rebellions, and in particular the Taiping, must be considered. Though the Taiping claimed initially to be led by the Christian ideology, in reality, source of it ideas were from the rites of Chou and the Works of Mencius, interwoven with the tenets of Christianity. Also, though Hung Jen Kan had a reform proposal in which western influence can be detected, the Taiping eventually reverted back to the old Chinese corruption, habits of conservatism and of fixed ideas. Hence, though the Taipings were initially trying to identify themselves with their foreign brethren, and sought to employ Western military technology, their adoption of Western ways was very superficial, and hence the West cannot be said to have contributed to the Taiping cause in any significant way. Li Chien-nung, The Political History of China, 1840-1928, translated and edited by Teng Ssu-yu and Jeremy Ingalls, (California, 1967) p. 64 Teng Ssu-yu and John K. Fairbank, China's response to the West: A Documentary Study 1839-1923, (Massachusetts, 1994), p. 57-59. However, the West were particularly influential in helping the Qing to put down the social unrests, such as the employment of the Ever-Victorious Army and Gordon's artillery in supplementing the regular forces and local militias in putting down the Taiping. It must be said that generally, the West were adverse to social unrests, especially in the treaty ports where trade was concentrated, because social instability often interfered with trade and economic prosperity. The open intervention pursued by the West in suppressing the Taiping was to their interests: for the defense of Shanghai, and as a useful bargaining chip with the Qing government when seeking concessions. Also, in the Muslim rebellion in the Kansu, England secretly supported Yakub Beg, the leader of the rebellion, while Russia opposed the expansion of Yakub Beg's power, and under the pretense of maintaining peace along the border, occupied Ili in 1871, subdued the Mohammedan chief and moved troops into Urumchi. Hence when the West moved in to suppress social unrests, they did so only if it was within their interests to do so, should not be regarded as a philanthropic gesture. Li Chien-nung, The Political History of China, 1840-1928, translated and edited by Teng Ssu-yu and Jeremy Ingalls, (California, 1967) p. 110. Perhaps the most glaring exclusion in this essay is the role of Japan in the social unrests of the 19 th Century, but it would be convenient to evaluate Japan on the basis of a foreign power with imperialistic designs on China. The Sino-Japanese War of 1894-5 shook the Chinese self-confidence, and marked an end to the era of 'Self-Strengthening'. Though there was no particular anti-Japanese event, anti-Japanese sentiments took root during this era, and set the stage for more explicit and violent anti-Japanese movements that continue to present time. In conclusion, the social unrests of the early 19 th Century were caused more by internal than external problems, such as underlying ethnic tensions, the prevailing socio-economic conditions, the entrenched corruption and degeneration of the court and the army. However, The Opium War and the Treaty System that was imposed marked a watershed in the Qing-West relations, allowing the West to expose the weaknesses of the Qing State, making it more vulnerable to rebellions, and the rise of Western Imperialism in the quest for territorial gains and in the spread of Christianity, which led to the creation of anti-foreign nationalism, which dominated the face of social unrests in the late 19 th Century. Also, where their interests were concerned, the West would intervene to suppress social unrests in order to safeguard their commercial interests. Hence the parallel streams of development: the decline of the central power of the Qing State, and the growing western presence and penetration gave the West increasing importance in the creation and suppression of social unrests of the 19 th Century.",False
94,"The Enlightenment, an important feature in most written works on the origins of the French Revolution, has often been credited with the ideology that inspired the French masses to rise up against the monarchy. Though the association has been made, there is nevertheless no direct causal link between the ""siecle des lumieres"" (the century of light) and the revolution. Other factors were present, along with the influence of the Enlightenment, which created the context in which the revolution occurred. To provide a satisfactory answer to the above question, the writer will attempt firstly to examine the ideas of the Enlightenment, and how it contributed to the Revolution; secondly the other factors that culminated in the Storming of the Bastille on the 14 th of July 1789 will be placed under scrutiny; and finally, an alternative interpretation of the relationship between the Enlightenment and the Revolution will be considered. According to the Oxford Dictionary, the Enlightenment was 'a European Intellectual movement of the late 17 th and 18 th centuries emphasising reason and individualism.' 'Man is the single term from which all must be brought back' or as Professor Colin Jones puts it 'human value was the critical yardstick of knowledge employed."" Eric Hobsbawm saw the Enlightenment as being dominated by 'a secular, rationalist and progressive individualism' in which its main objective was to free the individual from the bonds of traditionalism, superstition and an irrational hierarchical class order. The cult of the individual may have led to the estrangement of the self from the Church, (seen as an oppressive and stifling body with traditional rituals and rigid beliefs) hence the dominance of secularism in the ideas of the Enlightenment. Proceeding from there, the criticisms aimed at the Church would have led some to question the basis of its power, and the resultant decrease in its prestige and inviolability. Since the monarch was seen as God's ordained agent on earth, any challenge to the supremacy of the Church would only serve to de-stabilise the King's position as an absolute monarch, which was precisely what happened in 1789 (King Louis XVI by the grace of God, King of France now became Louis, by the Grace of God and the constitutional law of the state, the King of the French). Compact Oxford Dictionary Thesaurus and Word Power Guide (2001) p.292. Dennis Diderot, Encyclopedie, vol. V, p. 641 (article, 'Encyclopedie'). Colin Jones, The Great Nation, France from Louis XV to Napoleon (London, 2002), p.174. Eric Hobsbawm, The Age of Revolution 1789-1848 (United States of America, 1996), p.21. Hobsbawm, Age of Revolution, p. 59. 'Liberty, equality and fraternity of all men' was the slogan of both the Enlightenment and the revolution. It is plausible to suggest that the enlightenment did lead, in reality, to greater equality between certain groups of people. In the salons, academies, Masonic lodges and coffee houses, people debated on equal footing, regardless of their status in society. Madam Geoffrin even went as far as to bar the powerful duc de Richelieu from her salon, 'arguing that wit rather than social rank was the passport for admission into her circles."" But this could have resulted in the resentment amongst the lower classes that thought of themselves as the intellectual, but not social, equals to the aristocrats- resentment for the fact that nepotism and the lack of meritocracy were going to cost them that promotion in the Army or civil service hierarchy. The revolution thus could be borne out of the conflict of interests between the old order, and the new forces that were trying to assert themselves. Colin Jones, The Great Nation, p. 183. Perhaps, the most significant way in which the Enlightenment contributed to the Revolution was that it created a climate of change that was conducive for the revolution to occur. The overall rise in literacy, from 29 to 47 per cent for Men between 1686-90 and 1786-90, and 14 to 27 per cent for women during the same period, led to the rapid and widespread dissemination of the ideas of the Enlightenment. Large-scale diffusion of critical and denunciatory literature (not least in pornographic books targeting the royal family) transformed the representation of the monarchy in the minds of the French, and made them think of themselves as victims of a despotic monarch. Thus, in Roger Chartier's words, regardless of the intent of the 'philosophical books', it succeeded in producing an 'ideological erosion' that may have made the revolution inevitable. According to Darnton, even though the people did not call for a Revolution or foresee 1789, unconsciously however, they prepared for that event by 'desanctifying the symbols and deflating the myths that made the monarchy legitimate in the eyes of its subject."" The increased criticisms of the established order (the Church and the Monarchy) may have therefore poisoned the mentality of the Public Opinion in France that eventually led to revolution. The Encyclopedie also aspired to embody 'the power to change men's common ways of thinking' so as to make a 'revolution...in the minds of men and the national character."" Hence the fostering of the 'critical spirit' amongst the French was also an important consequence of the Enlightenment Roger Chartier, 'Do books make revolutions?' The French Revolution in Social and Political Perspective, ed. Peter Jones, (New York, 1996), p. 168. Robert Darnton, 'A Clandestine Bookseller in the Provinces,' in his The Literary Underground of the Old Regime (Cambridge, Mass., 1982), p. 147. Encyclopedie, vol. v, p.637 (article, Encyclopedie) Roger Chartier however, cautions against this link between philosophical works and revolutionary thought. Using the example of Rousseau, Chartier illustrates the popularity of the philosophes amongst the sans culottes, middle classes and the aristocracy. Moreover, one of the Enlightenment's crowning glories, the Encyclopedie was too expensive to be purchased by anyone other than the notables, and though some were dedicated to the revolutionary cause, the majority was apathetic or hostile to it. Due to the fact that there can be many interpretations to a book, hence, it is impossible to credit too direct a role to books in creating the revolutionary ardour of the French masses. Furthermore, the philosophes were from quite a diverse social background. Rousseau's father was a watchmaker, Voltaire was the son of a notary, and Montesquieu was 'a magistrate in the parlement of Bordeaux, a feudal lord living in a moated castle and an apologist for noble power."" Most of them (excluding Rousseau) believed in the idea of the monarchy as the generator of utilitarian reform, and despite the criticisms of the monarchy and traditional institutions, the philosophes believed that 'the modern state could be improved as it stood."" Perhaps, it was the failure of 'Enlightened Despotism', which resulted in the bourgeoisie transfer of faith from the monarchy to the masses, which led indirectly to the French revolution. William Doyle, The Oxford History of the French Revolution, (Oxford, 2002), p.50. Colin Jones, The Great Nation, p.221. Amongst the other factors that contributed to the outbreak of the revolution, Hobsbawm suggests that it was war and debt that broke the back of the monarchy. He explains that the bankruptcy of the government and the resultant need for tax reforms gave the aristocracy and the parlements a chance to barter with the government. In exchange for the tax reforms, the aristocracy wanted an extension of their privileges, and this led to the forming of the assembly of notables and the calling of the Estates-General (a feudal assembly which last met in 1614). Hobsbawm concluded that the revolution thus began as an aristocratic attempt to recapture the state, but it underestimated the sovereign ideas of the 'Third Estate', and the socio-economic consequences of its political demands. What made the 'Third Estate' a force to be reckoned with was the fact that they not only represented the interests of the middle classes, but also of the urban poor and eventually, the revolutionary peasantry. In my opinion, the socio-economic context of revolutionary France was just as, if not more important than the influence of the Enlightenment. The fact that the revolution broke out in 1789 and not any other year was partly because of the bad harvest in 1788/9, which was reinforced by a particularly harsh winter. The severe economic consequences and the resulting industrial depression drove the urban and rural poor to desperation. The impact was heightened by the fact that in the preceding years, conditions were improving and things were turning out for the better. The expectations of the people were raised, and it came crashing down in 1789. Hobsbawm also suggested that the campaign of propaganda lent an added political dimension to the grievances of the people, and united them behind the deputies of the 'Third Estate'. Once the bastion of monarchical suppression and power (the Bastille) had fallen, there was no turning back. Hobsbawm, The Age of Revolution, p.58. In Roger Chartier's article he puts forward and alternative interpretation of the relationship between the Enlightenment and the revolution. He contemplates the possibility that it was not the Enlightenment that implied revolution, but rather, the converse, that it was the latter that constructed the former. He states that the retrospective construction were many-the 'canonisation' of Voltaire and Rousseau, by the revolutionary assemblies, as the intellectual fore fathers of the revolution, while others such as Buffon and Descartes were relegated; in the quest for legitimacy, political celebrations were held in Year II, honouring the philosophes and martyrs for liberty. The articles in the Declaration of the Rights of Man, which expounds on the freedom of Man, may have led to the belief that it was the Enlightenment that was the ideological inspiration behind the revolution. Perhaps it was a retrospective justification for the revolution and to put the revolutionaries on a moral high ground, since they were now seen not as mere rebels, but as agents of progress. Hence, 'it was the revolution that gave a premonitory and programmatic meaning to certain works, constituted, after the fact, as its origin.' Roger Chartier, 'Do books make Revolutions', p.183. Chartier, 'Do books make Revolutions', p.185. The relationship between the Enlightenment and the revolution is a complex one that does not lend itself readily to simple cause and consequence explanation. There have been mutual exchanges between the two, in the sense that the Enlightenment provided the ideology and legitimacy of the revolution, and on the other hand, the revolution also gave the Enlightenment a prophetic element in that the latter 'predicted' the coming of the former and prepared the people for socio-political change. Much as the Enlightenment has contributed to the revolution, we should not discount the other forces and events that were in place for the revolution to occur, such as the economic depression that led to the politicising of the masses, the bankruptcy of the monarchy, and the personal temperament of King Louis XVI (in particular, how he was so easily manipulated by his Queen and reactionary advisors). Hobsbawm has an interesting perspective to offer, in that he thinks the revolution would have occurred without the philosophes, but they 'made the difference between a mere breakdown of the old regime and the effective and rapid substitution of a new one."" Perhaps we can then say that the Enlightenment was important only after the revolution had become a fait accompli, and that it made its mark on post-revolution re-construction. Hobsbawm, Age of Revolutions, p. 58. The hypothesis put forth by J.C. Davis's Fear, Myth and History has provoked a heated debate that still in want of a closure, and one thinks that it may never be found, for it would be professional suicide for any of the historians involved, especially Davis, to 'recant' and come round to the others' viewpoint. While there is a sense that the debate has been bogged down by personal animosities and the lack of fresh interpretations, it is still of interest today, because of the pertinent ""issues about historical methods raised by Fear, Myth and History"" that ""are important far beyond the particular case of England in the mid-seventeenth century."" It is to this debate that we now turn our attention to, and I have structured the essay as such: the first section deals with historiography, methodology and the implications of Davis's interpretation on the discipline of History, followed by the main body of the essay which will focus on the two questions relating to our topic at hand-why Davis denies the existence of the 'Ranters', and subsequently, why he believes that the 'Ranters' were created by seventeenth century conservatives and twentieth-century Marxists. While Davis's interpretation is thought provoking, it is my opinion that his failure to differentiate between 'Ranter' myth and 'Ranters' has been a stumbling block in this debate. G.E. Aylmer, ""Did the Ranters Exist? "", Past and Present, No. 117 (Nov., 1987), p. 208. Jstor article. Before focusing on the main debate, it is imperative to begin with an examination of some of the held assumptions and employed techniques in Davis's hypothesis. In the area of historiography, Davis seems to have taken E.H. Carr's dictum ""study the historian before you begin to study the facts"" a bit too far when he claims that the 'Ranters' were conjured up by members of the Communist Party Historians Group in order to fit English history to Marxist theory. While it is important to question the agenda of historians, there is a fine line between the above, which is informative, and the latter-pure speculation-which I think that Davis is guilty of. Every historian writes with an agenda, for that is implicit in his/her argument, which in turn moulds his/her interpretation which is-according to Carr-""the lifeblood of history"". One ponders about Davis's own agenda behind his enterprise, but let us not tarry on this for there are other more pertinent issues to discuss. In particular, Hill has raised a very relevant point that Davis's paradigm is flawed and his enterprise doomed from the start, because his desire to find ""a coherent, cohesive group of like-minded Ranters"" is anachronistic, and that he is guilty of looking at the past through eyes of the present. There is much inconsistency in Davis's methodology, seen in the way he views seventeenth century pamphlets with extreme scepticism and yet at the same time, as pointed out by Christopher Hill, seems to have taken Abiezer Coppe's ironic recantations seriously. The implications of this debate are far-reaching, and it has not only questioned how historians ought to view primary sources, but has also affected how we are to understand the function of labels. Labels exist first and foremost to describe an external reality, though, that does not preclude the possibility of that label acquiring associations along the way that exaggerate the former, hence turning it into a myth. In the same way that Protestants used the 'Catholic myth' to consolidate their positions, so the seventeenth century conservatives manipulated what was already in existence to galvanise the population into combating the attack on religious and social order that the English Revolution had brought about. E.H. Carr, What is History?, (London, 1990) p.23. J.C. Davis, ""Fear, Myth and Furore: Reappraising the 'Ranters'"", Past and Present, No. 129 (Nov., 1990), p. 82. Jstor article. Carr, What is History? , p. 28 Davis, Fear, Myth and History: The Ranters and the historians, (Cambridge, 1986), p. 74 Christopher Hill, A Nation of Change and Novelty: Radical politics, religion and literature in seventeenth-century England, (London 1990), p.153 Hill, A Nation of Change and Novelty, p. 180. Davis, Fear, Myth and History, p. 53-7 It is Davis's opinion that the 'Ranters' were a ""projection of deviance"", and that ""there was no Ranter movement, no Ranter sect, no Ranter theology"", but consisted of ""a few relatively isolated individuals of heterogeneous persuasions"" who were ""swept up in the projection of a movement"" and were subsequently ""assigned roles by the historians"" While ""historians are agreed that it is wrong to speak of a Ranter sect or movement"", few have gone so far as to deny the existence of the 'Ranters'. Davis is unique in asserting that because there were no concrete evidences of the 'Ranter's' practical antinomianism, therefore they do not exist. Even if such evidences were in existence, Davis would most likely reject them as sensationalism of the yellow press, or as unreliable stories conjured up by seventeenth century anti-'Ranter' pamphleteers. However, Hill points to evidence that Gerrard Winstanley experienced problems in his Digger colony, due to the presence of drinking, whoring and indolent 'Ranters', and makes a point, which is taken up by Aylmer that the 'Ranters' held an 'anti-work' ethic and lived a ""hippy-like existence"" Therein lies the 'practical antinomianism' that Davis seeks. The 'Ranters' hedonistic 'make love not war' sort of attitude constituted a critique of society, which manifested itself in the breaking of taboos, as opposed to the more taxing alternative of a violent overhaul of society. There is a more cogent explanation proposed by Hill that accounts for the lack of 'practical antinomianism'-it is simply that the Ranters were ""talkers rather than doers."" Ibid, p.124 Bernard Capp, ""Fear, Myth and Furore: Reappraising the 'Ranters'"", Past and Present, No. 140, (Aug., 1993), p.165 Jstor article. Aylmer, ""Did the Ranters Exist? "", p. 209 Christopher Hill, The World Turned Upside Down: Radical Ideas during the English Revolution, (London, 1991), p. 229-30. Hill, A Nation of Change and Novelty, p. 185 Davis's second charge is that the absence of any common defining ideology and close direct links between members of the supposed 'Ranter' core disproves that the 'Ranters' existed either as ""a small group of like-minded individuals, as a sect, or as a large-scale, middle-scale or small movement."" While it is true that the antinomian and spiritualist ideas were not unique to the 'Ranters', according to Capp ""it is clear that something had crystallised from the flux, and that we are no longer dealing with isolated individuals"". Capp substantiates his point with compelling evidence drawn from correspondences between Salmon, Webb and Coppe, which indicated a sense of familiarity to the point of intimacy. Though he concedes that Coppe and Clarkson were never close, nevertheless, he asserts that there were links between both groups, hence refuting Davis's claim that ""the Ranter core shatters and disintegrates."" As mentioned earlier, the search for a seventeenth-century group labelled the 'Ranters' eludes Davis precisely because he employs modern standards and preconceptions to study a seventeenth-century phenomenon. Davis's insistence in finding a certain measure of uniformity or convergence of ideas will not hold up even in the globalised world today, where common church denominational labels mask a whole range of disparate practices and beliefs. Seventeenth-century radical groups were even less likely to converge on not more than a few tenets which roughly separated them from the other movements, not forgetting as well, that there was a great deal of fluidity between the groups, as people shopped around for a belief system that appealed to them. The 'Ranters' were such a loose grouping precisely because of their nature as a subversive group, their wariness of any forms of organised religion and distaste for hierarchy, which makes them not unlike modern anarchist 'groups' in form. Davis, Fear, Myth and History, p. 75 Capp, ""Reappraising the Ranters"", p. 166-7 Davis, Fear, Myth and History, p.75 Davis's scepticism is conceivably fuelled by the tendency of supposed 'Ranter' leaders to recant in the face of persecution. His serious treatment of the ironic recantations of Coppe, Clarkson and others is puzzling in light of his scepticism with regards to seventeenth century anti-'Ranter' writings. In contrast, Hill accounts for this readiness to recant as a feature characteristic of 'Ranters', who, like their sixteenth-century predecessors, the Familists, practiced ""Nicodemism-recanting in the face of overwhelming power"" and warns us against taking their equivocal recantations seriously. This practice can also be seen as the direct consequence of the 'Ranter' belief in 'mortalism', which precludes the immortality of the soul and hence of the appeal of martyrdom. As Hill astutely points out ""...resistance to the death would call for a deeper and more consistently worked out ideology than most Ranters had."" The two historians also disagree as to whether the 'Ranters' were the main targets of the Blasphemy Act of 1650. If Parliament had regarded the ideas and behaviour of the 'Ranters' as a real threat to societal order, then Davis's hypothesis that the Ranters did not exist is weakened. Though Davis is correct in saying that the Blasphemy Act did not mention the Ranters, evidence points towards Clarkson's A Single Eye All Light as a trigger to the body of legislation, which according to McGregor ""was largely a description of Ranter doctrine, owing much to the writings of Coppe and Clarkson."" While there is a possibility that the Parliament was responding to the 'Ranter myth' and not to the 'Ranters' it still reinforces Aylmer's simple concept that there can be ""no smoke without fire"". To take the analogy further, the strength of the fire might be disproportionately small compared to the amount of smoke that is billowing, and the response of the fire fighters might be more than is required to deal with a small fire, but the fact remains that a small fire did exist. Though that does not preclude the possibility that some people may have stoked the fire and fanned the flames, hence creating the 'smoke screen' to mask their presumably sinister agendas. Hill, A Nation of Change and Novelty, p. 179. Hill, The World Turned Upside Down, p. 209. J.F. McGregor, Fear, Myth and Furore: Reappraising the 'Ranters', Past and Present, No. 140 (Aug., 1993), p. 157. In order to justify and perpetuate his hypothesis that the 'Ranters' did not exist, Davis puts forth a series of explanations that sought to provide a plausible list of agendas that necessitated the creation of the 'Ranter' myth. For the seventeenth century conservatives, the 'Ranters' were necessary as the ""projection of deviance"" ""for it is only through deviance that we understand normality."" While this is a compelling argument, and extreme radicalism may alienate other groups and force some into the protective arms of the conservatives, there is also the other possibility that the increased profile and definition of the 'Ranters' may have seduced and ensnared others. No matter how vile the portrayal of the 'Ranters' there is an element of freedom in antinomianism that can be particularly enticing, especially for young men. Hence, this myth is in my opinion a double-edged sword to be wielded at one's own peril. The myth may achieve normality, but it could just as easily go the other way and perpetuate deviant behaviour. The 'Ranter' myth was not the exclusive tool of the conservatives, but was also used to achieve sectarian consolidation: to enhance the acceptability of sects that were not as radical as the 'Ranters', and to induce conformity within sectarian ranks. Because this spectre of Ranterism was conjured up to fulfil a social function, Davis warns us against taking any of the anti-Ranter literature seriously. On the topic of sources, Hill suggests that we can look to seventeenth-century personalities, such as John Reeve and Lodowick Muggleton, who were previously from the Ranter milieu for an accurate account of 'Ranterism'. Notably, Hill focuses on the figure of Bunyan, who was himself initially tempted by Ranter ideas, but whose work's from 1665-1685 took on an anti-'Ranter' slant. Also, despite the many hostile anti-Ranter records, Hill uncovered Ranter nostalgia in Erbery, Byne and Sedgweick, who ""found things to praise in the Ranters..."" though he added that ""they would hardly invent the Ranters for this purpose"". At the very least, there is a consensus that 'Ranter' was a 'buzz-word' in seventeenth-century England, not unlike that of 'globalisation' in our own time, though an admittance that the misuse of the label 'Ranter' existed does not preclude the external reality of a group of 'Ranters'. Davis, Fear, Myth and History, p.124 Ibid, p.111 Ibid, p. 110. Hill, A Nation of Change and Novelty, p. 164 Ibid, p.177. Ibid, p.169. While Davis might possibly defend his claim that the 'Ranters' were created by seventeenth-century conservatives (amongst others), his other claim-that the 'Ranter' spectre was resurrected by A.L. Morton and Christopher Hill, both members of the Communist Party Historians' Group between 1946-1956 in a bid to ""create the history of a popular democratic tradition in English history and culture"" is absurd and deliberately provocative. The allegation that Davis brings against them-that of using theory to dictate history- is an equally valid charge that can be levelled at him. Hill's defence of himself and A.L. Morton in A Nation of Change and Novelty is fairly convincing, and he has managed to place the onus on Davis to explain his own motivation behind the whole enterprise. Furthermore, other historians have joined in the fray, and with a few differences they have reaffirmed the existence of the 'Ranters' and the 'Ranter' myth. While the debate should not end on a 'majority wins' basis, there is a sense that this is quite a lope-sided debate, with Davis fighting the battle on his own. Davis, Fear, Myth and History, p.130 Hill , A Nation of Change and Novelty, p. 190 In conclusion, Davis's hypothesis, while novel, is not entirely convincing. The arguments of the other historians are a lot more persuasive, both in establishing a link between the various individual 'Ranter' leaders (despite the running debate about 'core' vs. 'milieu' which was not dealt with in this essay) and in their efforts to draw out some semblance of similarities between the various groups, while conceding that it impossible to define a coherent set of 'Ranter' ideology. This is not far off from Davis's own assertion that what would make him believe that the 'Ranters' existed would be the coincidence of two sort of evidences: that the existence of a group (or a number of groups loosely linked) can be established and that its shared ideology was reflected behaviourally in practical antinomianism. It bears repeating that Davis's enterprise is flawed for two reasons: his failure to differentiate between 'Ranter' myth and 'Ranters' which has led him to believe that the 'Ranters' were created by seventeenth-century conservatives and later resurrected by twentieth-century Marxist historians; and his employment of modern standards and preconceptions to study a seventeenth-century phenomenon, which explains why his search for the 'Ranters' as a external reality has been so unfruitful. Perhaps one might cheekily suggest that Davis's own 'recantation' is overdue. J.C. Davis, ""Fear, Myth and Furore: Reappraising the 'Ranters': Reply"", Past and Present, No. 140 (Aug., 1993), p. 205. Jstor article. China has long captured the imagination of the West, and still remains an enigma in this present age despite the vicissitudes of time. While technology now permits us to catch a glimpse of China on the news, on the internet, or plausibly, even visit China; this would not have been conceivable in previous centuries, where one of the few ways in which one could come into contact with, and conceive of China was through the objects brought back from overseas trade- which was itself stimulated by a thirst for the mysterious exotic Orient as much as it was for profit. Although these goods were prized for their scarcity, aesthetic value and excellent workmanship, it was not the material alone that captivated, but more importantly, it was the non-material associations that imbued the goods with meaning, desirability and the label of 'luxury'. This fuelled the luxury trade, long underpinned by traditional court luxury, which reached its zenith in the seventeenth and eighteenth centuries, with the establishment of the chartered East India companies and the subsequent creation of a European market for these goods. This essay will attempt to put forth the idea that Europeans were so fascinated by Chinese luxury goods precisely because of the various functions that these goods fulfilled- it fired up the European imagination, giving it the impetus to venture out and explore the world; it played its part in influencing the new taste for the Rococo and in the definition of 'refinement'; it was amenable to the marketing techniques of the East India Companies which further heightened its appeal; and finally it stimulated European industries in their attempt to surpass the East through emulation and innovation. In short, it was through the acquisition of Chinese goods, which defined the 'Other', that one perceives the 'Self' in a clearer light. Maxine Berg, ""In Pursuit of Luxury: Global History and British Consumer Goods in the Eighteenth Century"", in Past and Present 182 (2004), p. 86. Before embarking on the main body of the essay proper, there are some issues that deserve our attention: firstly, the reference to ""seventeenth and eighteenth century Europeans"" in the question should not be taken to mean all Europeans, but was limited to the affluent, by nature of it being Asian luxury goods. And even within this group, there were some such as Mrs. Montagu and the adherents of the Burlington School of Taste who lamented the fall from favour of Palladianism. While it is not the task at hand to examine the interactions between the Orient and the Occident, it must be said that far from being a one-way exchange, the Chinese elite were themselves fascinated by western manufactured goods, especially clocks. It is interesting to note that both cultures used foreign 'luxury' goods in a setting that suited their own agendas, often removing the objects from their original context, transforming the functional into the ornamental. This playful assimilation, also seen in the creation of the Chinoiserie, is perhaps reflective of the confidence of both cultures, which is in stark contrast to the theories of Social Darwinism, which characterised their relations in the nineteenth century. Finally, it is important that we examine the definition of 'luxury goods'. As Vainker astutely points out: despite the high quality of silk, porcelain and lacquer, these goods were not perceived as luxury items, but as functional goods by the affluent Chinese. Instead, what they valued as luxury items would include ""paintings, ritual bronzes, jades, calligraphy and other antiques and collectibles associated with a scholar-official elite and the authority to rule."" Hence, luxury is defined not by high quality goods that are widely available (such as porcelain and silk in China), but by its association with the conferment of social status due to its scarcity (Chinese porcelain and silk in Europe). In a letter from Mrs. Montagu 1749 ""Thus it is happened...we must all seek the barbarous gaudy gout of the Chinese; and the fat-headed Pagods and shaking Mandarins bear the prize from the greatest works of antiquity; and Apollo and Venus must give way to a fat idol with a sconce on his head."" Quoted from Dawn Jacobson, Chinoiserie, (London, 1993), p. 123. Ibid, p.124. Seen in the use of clocks as status symbols (as opposed to time-pieces) in Yongzheng's imperial household; and the French candelabra composed of inverted Chinese whistles and cups mounted on a frame. Seen in the DVD produced on the Encounters: The Meeting of Asia and Europe 1500-1800 exhibition. Picture of French Candelabra in Anna Jackson and Amin Jaffers (ed. ), Encounters: The Meeting of Asia and Europe 1500-1800, (London, 2004), plate 1.9, p. 7. Picture of Yongzheng's concubine with a clock in the background in Evelyn S. Rawski and Jessica Rawson (ed. ), China: The Three Emperors 1662-1795, (London, 2005), bottom left, p. 259. Shelagh Vainker, ""Luxuries or Not? Consumption of Silk and Porcelain in Eighteenth-Century China"", in Maxine Berg and Elizabeth Eger (ed. ), Luxury in the Eighteenth Century: Debates, Desires and Delectable Goods, (New York, 2003), p.214. The seventeenth and eighteenth century fascination with Chinese luxury goods had its genesis in the myth of Cathay, which conjured up ""fantastic visions of luxury and refinement, pleasure and abundance"", and were reinforced by traveller accounts of Marco Polo and the fictional Sir John Mandeville. As Jacobson observed ""this view of the Chinese as being different but in no way inferior, helped to sustain the myth of Cathay, and the vision of a culture that was profound as well as peculiar."" It was this perception that made possible the enchantment and respect that the Europeans had for the advanced Chinese civilisation; which manifested itself in the pursuit of Chinese goods, in the hope that one's collection might plausibly be employed as a prism through which the secret knowledge of the East might be yielded. That China during the Ming dynasty was largely cloistered from the world further enhanced its mystery and appeal, not unlike the coy mistress subtly beckoning to would-be suitors. And they came- in drips and then in droves- the Portuguese trailblazers followed by the Dutch, English and French traders who brought back taster portions of Asian goods, fuelling the imagination of the affluent Europeans further, which in turn encouraged the luxury trade. Jacobson, Chinoiserie, p. 10 Ibid, p. 12-15. Ibid, p.15. It is imperative that we examine how this seemingly perennial fascination that the Europeans have for the exotic Orient featured in the particular context of the seventeenth and eighteenth century, especially in terms of sociability, civility and taste. It is important to note that in the moral debates about luxury at that time, Chinese goods were ""associated not with (Persian) sensuality and excess, but with ethics, harmony and virtue...(and) in possessing things Chinese, they (the philosophes) sought to access levels of civilisation beyond the market."" Hence, it was through the prism of Chinese objects that the Europeans expressed ""their own aspirations to human elegance and refinement."" The employment of taste as ""a new means of displaying prestige"", and the corresponding replacement of traditional forms of opulence (silver and gold) with elegant new 'imitative' objects, such as porcelain are integral in accounting for the vogue in Chinese luxury goods in seventeenth and eighteenth century Europe. This change in taste also reflected the changing relations in society, with the notable rise of the bourgeoisie, who rejected the gaudy decadence of the baroque (associated with the ancien regime) in favour of the lighter, more playful and asymmetrical Rococo, which ""brought together commercial and cultural modernity."" It is perhaps fortunate that the Chinoiserie with its freedom from classical notions of restraint and symmetry found favour with the Rococo, and was in fact a ""vital ingredient in the creation of the new taste."" It was in this context that Chinoiserie-which was the Western romanticised view of the East, produced in part by the imported goods- flourished, and ""in turn dictated the commerce in Asian imports."" Maxine Berg, ""Asian luxuries and the Making of the European Consumer Revolution"", in Maxine Berg and Elizabeth Eger (ed. ), Luxury in the Eighteenth Century: Debates, Desires and Delectable Goods, (New York, 2003), p. 229. Ibid. Ibid, p. 230. Ibid, p. 231. Jacobson, Chinoiserie, p. 57. Maxine Berg, Luxury and Pleasure in Eighteenth century Britain, (Oxford, 2005), p. 51 While fascination with the Orient had always existed, it is necessary to look at how the East India Companies manipulated what was already in existence and transformed it into an obsession with Chinoiserie, as seen from above. As Berg astutely pointed out: these luxury goods ""were a construct of the market, seeming to represent the lives and values of the East, but constructed to meet European preconception of eastern art."" These commodities were prefabricated on the Chinese side to cater to Western markets; and on the part of the East India Companies, which sent out European shapes to be copied, and the initial contentment with Chinese designs soon gave way to a sudden fashion for 'chine de commande' by the end of the seventeenth century. While the East India Companies (EICs) were themselves the early products of the European fascination with China and its goods, they were indispensable in the subsequent promotion of this luxury trade, which was intrinsically tied up with the western enthralment for the East. The EICs were not merely merchants who simply pandered to the tastes of the European elite, but were market movers in the way in which they created a demand for these goods through 'customisation' and effective marketing. It is remarkable how they transformed something as functional and mundane as porcelain by capitalising on its exotic appeal and its association with civility, marketing it as a semi-luxury good, when truth be told, the differentiated decorations on the porcelain pieces belie the fact that they were to some extent, 'mass' produced goods. Moreover, they created new wants- ""the dinner service with complete setting for each person was an invention of the VOC and the EIC which started importing porcelain dinner services in the eighteenth century."" Therefore, though the western fascination for the Orient was present before the seventeenth-century, we have to credit the EICs for their role in elevating it to a whole new dimension with the extension of maritime trade and through successful marketing tactics. Berg, ""Asian luxuries and the Making of the European Consumer Revolution"", p. 228 Christiaan, J.A. Jorg, ""Porcelain for the Dutch"", in Rosemary E. Scott (ed. ), The Porcelain of Jingdezhen: Colloquies on Art and Archaeology in Asia No. 16, (London, 1993), p. 188 and 200. Berg, ""Asian luxuries and the Making of the European Consumer Revolution"", p. 239 While exoticism, taste and marketing definitely contributed to the appeal of Chinese luxury goods to the West, a large measure of this preoccupation was also due to the impressive technical wizardry of the Chinese craftsmen and the secrecy which shrouded the production processes for these goods-especially porcelain- the Arcanum of which was first discovered by the Europeans only in 1709. Perhaps it is useful to point out that the luxury goods that fascinated the West were new, modern luxury comprised of ""technically advanced and refined consumer objects"" such as calicos and porcelain. Hence, the East enthralled the West because the latter realised that much could be learnt from the former; which would allow the West to progress through imitating and perfecting the techniques that they have gleaned from the East. It is important to note that contemporaries such as ""Montesquieu, Hume and Smith wrote of luxury as an aspect of people's desire to better themselves"" and ""...commercial writers were keen to point out what the West could learn from the Orient."" In doing so, they affirmed the moral good of luxuries in contributing to 'Progress', hence alleviating the fears of decadence and degeneration that plagued earlier preoccupations with luxury goods, and indirectly encouraged the fascination for Chinese luxury goods. While it must be said that this desire to learn from the East was more apparent than real, which is illustrated by Berg's observation that ""Asian consumption was transferred to Europe but not Asian production systems"", there appears to be a genuine lesson imported from the East, which is ""the policy of using the fine arts and design in combination with modern manufacturing technique."" Hence, ""paradoxically, in imitating Asian consumer goods, perceived in Europe as luxuries, the British achieved what was actually an Asian success story-new, quality, semi-luxury consumer goods produced with advanced industrial techniques."" Ibid, p. 230 Ibid. Berg, ""In Pursuit of Luxury: Global History and British Consumer Goods in the Eighteenth Century"", p. 86 Berg, ""Asian luxuries and the Making of the European Consumer Revolution"", p. 242. Berg, ""In Pursuit of Luxury: Global History and British Consumer Goods in the Eighteenth Century"", p. 130. In conclusion, this fascination that the Occident had for the Oriental luxury goods was not new, but was closely entwined with the captivating medieval myth of Cathay, and reached new dimensions in the seventeenth and eighteenth century with the establishment of the East India Companies (which were themselves the product of and the further stimulus to the West-East trade born of the Occidental preoccupation with the exotic, mysterious and abundant East) and the creation of European markets for Chinese luxury goods (which was previously the preserve of the European royalty). It is perhaps paradoxical that these Chinese luxury goods fascinate precisely because they have been displaced from their original cultures and grafted into new ones, but continue to give the impression that it was still possible to use them as prisms through which the West can subscribe to notions of Chinese refinement and skill. The fascination for anything 'Chinese' is apparent in the vogue for the Chinoiserie especially in the eighteenth century, and is both reflective of, and contributed to the existing enthrallment that the West had for the East, and how that affected the dominant taste in Europe. The appeal of Chinese luxury goods was also played up by the East India Companies which capitalised on the vogue for Chinoiserie, and created new wants through the successful marketing technique of making the foreign 'relevant' to the West through customisation. Finally, and most importantly, Chinese luxury goods were seen as having the role of ""awakening the imagination"" and were positive elements in stimulating the Europeans to better themselves through emulation and product innovation. Despite being so far removed from their original cultures, that these luxury goods were relevant and fulfilled certain functions given to them by their European possessors accounts to a large extent for the fascination that seventeenth and eighteenth century Europeans had for Chinese luxury goods. Ibid, p.131","It was in the historical context of the religious and political upheaval of the 1640s, and the subsequent breakdown of censorship (which made fertile ground for the ferment of radical ideas), that Winstanley first made his appearance as a pamphleteer. Considering that Winstanley was only prominent as a writer between 1648-1652, and thereafter faded into oblivion, there seems to be a disproportionate amount of literature written about him, with little consensus over how we should understand such a complex figure. The most prominent debate revolves around Winstanley's evolution as a thinker- while most agree that he started off as a religious thinker, few concur on his predilections (secular or religious or both) in the Law of Freedom in a Platform (1652). While the question above seems to be fairly straightforward, it is in this writer's opinion that the position one takes in the fore-mentioned debate has bearings on how one would view the nature of Winstanley's religious outlook in The New Law of Righteousness (1649). Therefore, while an examination of the primary sources is necessary, it is equally important to consider the interpretations of some historians on Winstanley, and, as far as possible, how their biases feature in the following themes: the nature of Winstanley's millenarianism; his conception of God; his understanding of the Fall and Restoration; and finally his views on the institutionalized church. Before launching into the essay proper, it is perhaps necessary to consider the approach that one should take when reading the pamphlet. While Bernstein's view that Winstanley used ""religious phraseology as a cloak"" has been superceded by writers more sympathetic to Winstanley's religious propensities, there appears to be a dispute between the authors of ""Winstanley: A Case for the Man as He Said He Was"" (with their literal approach to Winstanley's works), and Christopher Hill, whom they accuse of ""making allowances for Winstanley's language"", in his bid ""to establish Winstanley's uniquely secular revolutionary ideology"". This larger dispute about Winstanley's religious phraseology is somewhat defined by their different interpretations of what Winstanley meant by being ""in a trance"", the former taking it literally (a vision/message from God) and the latter, as a seventeenth century manner of speech, to mean ""a moment of clarification in a process of deep thinking"". Their different approaches to Winstanley's religious phraseology are perhaps most amenable to their respective evaluations of Winstanley's development as a thinker. Taking the literal approach allowed Mulligan et al. to assert that Winstanley remained to the end a religious thinker, just as Hill's emphasis on Winstanley's rationality is linked to his interpretation that Winstanley ended up a secular thinker. An understanding of the seventeenth-century mentality is crucial if this is to be a productive study, and it should be noted that unlike the modern age, the belief in a Creator God and the belief in Reason were not antithetical, though Hill's caveat on 'trances' seem to suggest that he thought that the two were. Lotte Mulligan, John K. Graham and Judith Richards, ""Winstanley: A Case for the Man as He Said He Was"", in Journal of Ecclesiastical History, Vol. 28, No. 1 January 1977, p. 58 Ibid, p. 65 ""Rather than treat his Biblical language as a barrier to understanding of Winstanley's thought...the historian might treat that language as the articulation of thought, and through it come closer to understanding what Winstanley meant to himself and his contemporaries."" Ibid, p. 64 George Sabine (ed. ), The Works of Gerrard Winstanley, (New York, 1941), p. 190 Christopher Hill, ""The Religion of Gerrard Winstanley"", in Past and Present Supplement 5, 1978, p. 21 Mulligan et al, ""Winstanley: A Case for the Man as He Said He Was"", p. 60 Christopher Hill, The World Turned Upside Down: Radical Ideas during the English Revolution, (London, 1991), p. 150. ""But if God is everywhere, if matter is God, then there can be no difference between the sacred and the secular: pantheism leads to secularism."" Perhaps one thing that can be agreed upon is that Winstanley held millenarian beliefs, though what type of millenarian he was-radical or traditional-is a matter of dispute. While Juretic saw Winstanley's emphasis in The New Law of Righteousness as primarily millenarian, and as consistent with his previous pamphlets (as opposed to it being a watershed as most scholars have suggested), he dismissed the pre-Digger Winstanley as ""traditional millenarian"" and his ideas as ""common millenarian fare""; and failed to recognise that Winstanley's radicalism preceded his secularisation. Instead, he suggested that the Digger experiment was the key in the radicalisation and secularisation of Winstanley, and in striving to prove the break (as opposed to continuity) between the religious and secular Winstanley, neglected the uniqueness of Winstanley's brand of millenarianism. Also, in trying to set Winstanley firmly within the religiosity of his time by identifying him with his puritan contemporaries, Mulligan et al. compromised on Winstanley's unconventional millenarianism, by arguing that ""he thought of the millennium in actual, not metaphorical terms."" Perhaps Aylmer came closest to understanding Winstanley when he said that ""the real interest of Winstanley lies in the totality of his challenge to established beliefs and systems of value...whatever his starting point as a religious thinker, he was not a normal kind of millenarian..."". Due to Aylmer's stance that Winstanley's ""communism and his theology are literally inseparable in his writings from 1649 on, even though the emphasis varies in different pamphlets"", he was more likely to analyse Winstanley's radical theology rather than dismiss it in favour of rational materialism; and provided a more balanced view to that offered by Mulliganal. A more in-depth examination of Winstanley's religious beliefs is overdue, and it is to this that we now turn our attention. Sabine, Winstanley, ""...now comes the time that the elder sons, that are born after the flesh, shall serve the younger sons, in whom the blessing lies..."" p. 150; ""He is now coming to raign, and the Isles and the Nations of the earth shall all come in unto him..."" p. 152; ""But in the time of Israel's restoration, now beginning, when the King of Righteousness himself shall be Governor in every man..."" p. 190 (emphasis added) George Juretic, ""Digger No Millenarian: The Revolutionising of Gerrard Winstanley"", in Journal of the History of Ideas (36), 1975, p. 270 Ibid, p. 272 Ibid, p. 269 Ibid, p. 269 Mulligan et al., ""Winstanley: A Case for the Man as He Said He Was"", p. 74 ""The spirit of Winstanley's writings throughout had more in common with the apocalyptic visions of his puritan contemporaries than with modern socialist or communist ideologies."" Ibid, p. 65 G. E. Aylmer, ""The Religion of Gerrard Winstanley"", in J. F. McGregor and B. Reay (ed. ), Radical Religion in the English Revolution, (Oxford, 1984), p. 92 Ibid. p. 92-93. A crucial part of one's religious outlook is constituted by how one understands the nature of God, and as with most other things about Winstanley, this is controversial, and is centred on two main positions-God is immanent as opposed to God being immanent and transcendent. The adherents of the former include Zagorin and Hill, who were of the opinion that Winstanley's pantheism, which is manifested in the ""...identification of God with the immanent principle of reason pervading this world"", accounts for and precedes his transition to secularism. This view of Winstanley's ""God"", is most agreeable to their take on Winstanley as a whole, and disposes of the potential problem that the belief in a transcendental God might pose to their reading of Winstanley. Whereas those who take the line that Winstanley was influenced by theology throughout his writing career, assert his belief in an immanent and transcendental God. While both positions can be substantiated by a reading of The New Law of Righteousness, the bone of contention lies in the priority that one should accord to repetition or to a more subtle reading of the text. Although the idea that God is immanent is littered throughout the text, there are a few occasions when God as immanent and transcendent appear: ""The great world...is no other but Christ spread forth in the Creation; and so upholding the Creation by his own wisdom, and power; for he is the maker, the preserver and the restoring Spirit."" It seems to me that if Winstanley believes in a Creator God, it would necessarily imply that God as Creator of the world is able to transcend His creation; though this does not preclude His immanence, nor the equation of Reason with God, as Hill seems to suggest when he said ""only if we forget that the Father is Reason and that Christ's second coming is in sons and daughters can we slip into thinking of an external God."" It not only seems absurd to think that God must be bound by an either/or straitjacket, but it is also simplistic to think in such strict dichotomy. Zagorin's understanding of Winstanley's path follows the progression ""mystic, pantheist, material rationalist"". Seen in Perez Zagorin, A History of Political thought in the English Revolution, (Bristol, 1997), p. 57 ""Winstanley knew no transcendent God, only immanent reason"", in Hill, ""The Religion of Winstanley"", p. 30. See footnote 7 for Hill's position. Zagorin, A History of Political thought in the English Revolution, p. 47 ""Therefore Christ hath the honour above his brethren, to be called the spreading power, because he fills all with himself..."" p. 151/ ""...for he is in all and acts through all"" p. 160/ ""So that, this one Almighty power be spread in the whole creation..."" p. 166 in Sabine, Winstanley. Ibid, p. 164-165 Ibid, p. 204 ""If any man be offended here, let him know, I have obeyed my Maker herein, and I have peace in him."" Hill, ""The Religion of Winstanley"", p. 30 However, Winstanley's belief in a transcendent and immanent God does not make him less of a religious radical, despite what Mulligan et al. seem to imply, and his views on the Fall and the Restoration are testament to that. Mulligan et al. appear keen to make Winstanley out to be a traditional millenarian, and this is reinforced by their interpretation of Winstanley's understanding of the Fall and Restoration, which is fairly similar to the traditional Christian understanding of it. Unlike Hill's radical Winstanley who believed that the introduction of private property constituted the Original Sin, their Winstanley saw property as a symptom, rather than the cause of the Fall. Also, Hill's astute observation that Winstanley was unorthodox in equating the Resurrection and the Second Coming stands in contrast with Mulligan et al. 's conventional reading that Christ's ""appearance would be a literal Second Coming."" The scales seem to be tipped against Mulligan et al. as Zagorin's Winstanley saw Man's redemption ""not in a heaven to come, and not through Christ's vicarious sacrifice, but by the restoration of the absolute law of reason."" Aylmer too, acknowledged Winstanley's unorthodoxy in asserting that the Original Sin was not a historical event, nor was it inherited from Adam, but rather, it was ""the defeat of good by evil, which is repeated in all of us..."" The notion of an internal as opposed to historic and external Fall, Resurrection and Restoration is certainly radical, not to mention also that the Restoration would be complete only when Second Adam (within) kills the First Adam in us, and this inward restoration will be manifested outwardly in the abolishing of Private Property, and the re-establishment of the earth as a common treasury. In using biblical concepts as metaphors to criticise the inequality prevalent in his society, Winstanley ended up creating his own brand of Christian heresy. While the need to substantiate his social critiques and ideas with biblical references is perhaps traditional, in the sense that he acknowledged the authority of the Bible and used it as a legitimising force for his ideas; his use of it is far from conventional, as seen from Winstanley's unorthodox understanding of the Fall and Restoration. ""Winstanley believed in a transcendent God, working through inner transformation"" p. 68 in Mulliganal. ""Winstanley: A Case for the Man as He Said He was"". This makes Winstanley sound almost like an orthodox Christian! Christopher Hill, ""The Religion of Gerrard Winstanley: A Rejoinder"", Past and Present, No. 89, (Nov, 1980), p. 151. Mulligan et al., ""Winstanley: A Case for the Man as He Said He Was"", p. 74 Hill, A Rejoinder, p. 148. This is substantiated with the quote from Sabine, Winstanley, p. 162 ""So that upon the rising up of Christ in sons and daughters, which is his second coming..."" Mulligan et al., ""Winstanley: A Case for the Man as He Said He Was"", p.68 substantiated by quotes from Sabine, Winstanley, Fire in the Bush 1650, p. 466 and 471. The fact that these quotes are not from The New Law of Righteousness does not really matter, since they held that the theological beliefs set out in Truth Lifting Up Its Head are central to Winstanley's thought throughout his writing career. See p. 65 of the same article. Zagorin, A History of Political thought in the English Revolution, p. 47 Aylmer, ""The Religion of Gerrard Winstanley"", p. 95 Can be substantiated by p. 173, 176, 211, 215, 217. in Sabine, Winstanley. Sabine, Winstanley, p. 159 Up till now, the writer has focused on the debates surrounding the nature of Winstanley's religious outlook as a means of conveying the difficulty in assuming a singular stance, and the avoidance of a direct answer to the question so far is an admittance of Winstanley's complexity as a religious thinker. Not forgetting what has been mentioned earlier, there is however, more to be said about Winstanley's views on the institutionalised church, that do not take the form of the above. The lack of attention and controversy concerning Winstanley's anticlericalism (arising from his perceived hypocrisy of the clergy and their monopoly over the interpretation of Scripture, both of which result in the oppression of the people), his suspicion towards institutionalised Christianity, and preference for the personal religious experience, is probably due to the fact that these do not define Winstanley's uniqueness as a religious thinker, but were prevalent in other radical sects. His ambivalence towards an afterlife, belief in the perfectability of man on earth, notion of true worship manifesting itself in love, and the avowed tactic of non-violence echoes that of the Ranters, though the form that it took was different. It is interesting to note that Hill draws out quite a few similarities between Winstanley and the Ranters, and by doing so, sets the religious radicalism of Winstanley within the framework of his time. What remains most distinctive about Winstanley's religious outlook in The New Law of Righteousness is his concept of Restoration being tied up with that of the re-establishment of the earth as a common treasury. Hence it is the communism that is embedded in Winstanley's religious outlook that stimulates the reading of The New Law of Righteousness, since as Zagorin puts in ""it is in his religious evolution ...that we must seek the source of his political theory."" Thus, The New Law of Righteousness has often been read less for itself, and more for its part in the grand scheme of Winstanley's development as a political thinker. Therefore, as mentioned earlier, the different interpretations of the nature of Winstanley's religious outlook reflects the agenda of each historian in trying to conceive of a holistic entity that is Winstanley. Hill, The World Turned Upside Down, p. 206-7 Zagorin, A History of Political thought in the English Revolution, p. 43 In conclusion, it is not possible to take The New Law of Righteousness in isolation, and any interpretations made about the nature of Winstanley's religious outlook necessarily has repercussions on how one understands his evolution as a thinker. Such interactions make it exceedingly difficult to give a straightforward answer to the question. Also, disagreements over how the text should be read has not made it easier to decipher the nature of Winstanley's religious outlook, as the study of the text seem to support more than one interpretation. The debates surrounding Winstanley's millenarianism, his understanding of God, his views on the Fall and Restoration; and the lack of attention given to his stance on the institutionalised Church all illustrate the importance of the agenda of the historian in his treatment of primary sources.",True
95,"It was in the historical context of the religious and political upheaval of the 1640s, and the subsequent breakdown of censorship (which made fertile ground for the ferment of radical ideas), that Winstanley first made his appearance as a pamphleteer. Considering that Winstanley was only prominent as a writer between 1648-1652, and thereafter faded into oblivion, there seems to be a disproportionate amount of literature written about him, with little consensus over how we should understand such a complex figure. The most prominent debate revolves around Winstanley's evolution as a thinker- while most agree that he started off as a religious thinker, few concur on his predilections (secular or religious or both) in the Law of Freedom in a Platform (1652). While the question above seems to be fairly straightforward, it is in this writer's opinion that the position one takes in the fore-mentioned debate has bearings on how one would view the nature of Winstanley's religious outlook in The New Law of Righteousness (1649). Therefore, while an examination of the primary sources is necessary, it is equally important to consider the interpretations of some historians on Winstanley, and, as far as possible, how their biases feature in the following themes: the nature of Winstanley's millenarianism; his conception of God; his understanding of the Fall and Restoration; and finally his views on the institutionalized church. Before launching into the essay proper, it is perhaps necessary to consider the approach that one should take when reading the pamphlet. While Bernstein's view that Winstanley used ""religious phraseology as a cloak"" has been superceded by writers more sympathetic to Winstanley's religious propensities, there appears to be a dispute between the authors of ""Winstanley: A Case for the Man as He Said He Was"" (with their literal approach to Winstanley's works), and Christopher Hill, whom they accuse of ""making allowances for Winstanley's language"", in his bid ""to establish Winstanley's uniquely secular revolutionary ideology"". This larger dispute about Winstanley's religious phraseology is somewhat defined by their different interpretations of what Winstanley meant by being ""in a trance"", the former taking it literally (a vision/message from God) and the latter, as a seventeenth century manner of speech, to mean ""a moment of clarification in a process of deep thinking"". Their different approaches to Winstanley's religious phraseology are perhaps most amenable to their respective evaluations of Winstanley's development as a thinker. Taking the literal approach allowed Mulligan et al. to assert that Winstanley remained to the end a religious thinker, just as Hill's emphasis on Winstanley's rationality is linked to his interpretation that Winstanley ended up a secular thinker. An understanding of the seventeenth-century mentality is crucial if this is to be a productive study, and it should be noted that unlike the modern age, the belief in a Creator God and the belief in Reason were not antithetical, though Hill's caveat on 'trances' seem to suggest that he thought that the two were. Lotte Mulligan, John K. Graham and Judith Richards, ""Winstanley: A Case for the Man as He Said He Was"", in Journal of Ecclesiastical History, Vol. 28, No. 1 January 1977, p. 58 Ibid, p. 65 ""Rather than treat his Biblical language as a barrier to understanding of Winstanley's thought...the historian might treat that language as the articulation of thought, and through it come closer to understanding what Winstanley meant to himself and his contemporaries."" Ibid, p. 64 George Sabine (ed. ), The Works of Gerrard Winstanley, (New York, 1941), p. 190 Christopher Hill, ""The Religion of Gerrard Winstanley"", in Past and Present Supplement 5, 1978, p. 21 Mulligan et al, ""Winstanley: A Case for the Man as He Said He Was"", p. 60 Christopher Hill, The World Turned Upside Down: Radical Ideas during the English Revolution, (London, 1991), p. 150. ""But if God is everywhere, if matter is God, then there can be no difference between the sacred and the secular: pantheism leads to secularism."" Perhaps one thing that can be agreed upon is that Winstanley held millenarian beliefs, though what type of millenarian he was-radical or traditional-is a matter of dispute. While Juretic saw Winstanley's emphasis in The New Law of Righteousness as primarily millenarian, and as consistent with his previous pamphlets (as opposed to it being a watershed as most scholars have suggested), he dismissed the pre-Digger Winstanley as ""traditional millenarian"" and his ideas as ""common millenarian fare""; and failed to recognise that Winstanley's radicalism preceded his secularisation. Instead, he suggested that the Digger experiment was the key in the radicalisation and secularisation of Winstanley, and in striving to prove the break (as opposed to continuity) between the religious and secular Winstanley, neglected the uniqueness of Winstanley's brand of millenarianism. Also, in trying to set Winstanley firmly within the religiosity of his time by identifying him with his puritan contemporaries, Mulligan et al. compromised on Winstanley's unconventional millenarianism, by arguing that ""he thought of the millennium in actual, not metaphorical terms."" Perhaps Aylmer came closest to understanding Winstanley when he said that ""the real interest of Winstanley lies in the totality of his challenge to established beliefs and systems of value...whatever his starting point as a religious thinker, he was not a normal kind of millenarian..."". Due to Aylmer's stance that Winstanley's ""communism and his theology are literally inseparable in his writings from 1649 on, even though the emphasis varies in different pamphlets"", he was more likely to analyse Winstanley's radical theology rather than dismiss it in favour of rational materialism; and provided a more balanced view to that offered by Mulliganal. A more in-depth examination of Winstanley's religious beliefs is overdue, and it is to this that we now turn our attention. Sabine, Winstanley, ""...now comes the time that the elder sons, that are born after the flesh, shall serve the younger sons, in whom the blessing lies..."" p. 150; ""He is now coming to raign, and the Isles and the Nations of the earth shall all come in unto him..."" p. 152; ""But in the time of Israel's restoration, now beginning, when the King of Righteousness himself shall be Governor in every man..."" p. 190 (emphasis added) George Juretic, ""Digger No Millenarian: The Revolutionising of Gerrard Winstanley"", in Journal of the History of Ideas (36), 1975, p. 270 Ibid, p. 272 Ibid, p. 269 Ibid, p. 269 Mulligan et al., ""Winstanley: A Case for the Man as He Said He Was"", p. 74 ""The spirit of Winstanley's writings throughout had more in common with the apocalyptic visions of his puritan contemporaries than with modern socialist or communist ideologies."" Ibid, p. 65 G. E. Aylmer, ""The Religion of Gerrard Winstanley"", in J. F. McGregor and B. Reay (ed. ), Radical Religion in the English Revolution, (Oxford, 1984), p. 92 Ibid. p. 92-93. A crucial part of one's religious outlook is constituted by how one understands the nature of God, and as with most other things about Winstanley, this is controversial, and is centred on two main positions-God is immanent as opposed to God being immanent and transcendent. The adherents of the former include Zagorin and Hill, who were of the opinion that Winstanley's pantheism, which is manifested in the ""...identification of God with the immanent principle of reason pervading this world"", accounts for and precedes his transition to secularism. This view of Winstanley's ""God"", is most agreeable to their take on Winstanley as a whole, and disposes of the potential problem that the belief in a transcendental God might pose to their reading of Winstanley. Whereas those who take the line that Winstanley was influenced by theology throughout his writing career, assert his belief in an immanent and transcendental God. While both positions can be substantiated by a reading of The New Law of Righteousness, the bone of contention lies in the priority that one should accord to repetition or to a more subtle reading of the text. Although the idea that God is immanent is littered throughout the text, there are a few occasions when God as immanent and transcendent appear: ""The great world...is no other but Christ spread forth in the Creation; and so upholding the Creation by his own wisdom, and power; for he is the maker, the preserver and the restoring Spirit."" It seems to me that if Winstanley believes in a Creator God, it would necessarily imply that God as Creator of the world is able to transcend His creation; though this does not preclude His immanence, nor the equation of Reason with God, as Hill seems to suggest when he said ""only if we forget that the Father is Reason and that Christ's second coming is in sons and daughters can we slip into thinking of an external God."" It not only seems absurd to think that God must be bound by an either/or straitjacket, but it is also simplistic to think in such strict dichotomy. Zagorin's understanding of Winstanley's path follows the progression ""mystic, pantheist, material rationalist"". Seen in Perez Zagorin, A History of Political thought in the English Revolution, (Bristol, 1997), p. 57 ""Winstanley knew no transcendent God, only immanent reason"", in Hill, ""The Religion of Winstanley"", p. 30. See footnote 7 for Hill's position. Zagorin, A History of Political thought in the English Revolution, p. 47 ""Therefore Christ hath the honour above his brethren, to be called the spreading power, because he fills all with himself..."" p. 151/ ""...for he is in all and acts through all"" p. 160/ ""So that, this one Almighty power be spread in the whole creation..."" p. 166 in Sabine, Winstanley. Ibid, p. 164-165 Ibid, p. 204 ""If any man be offended here, let him know, I have obeyed my Maker herein, and I have peace in him."" Hill, ""The Religion of Winstanley"", p. 30 However, Winstanley's belief in a transcendent and immanent God does not make him less of a religious radical, despite what Mulligan et al. seem to imply, and his views on the Fall and the Restoration are testament to that. Mulligan et al. appear keen to make Winstanley out to be a traditional millenarian, and this is reinforced by their interpretation of Winstanley's understanding of the Fall and Restoration, which is fairly similar to the traditional Christian understanding of it. Unlike Hill's radical Winstanley who believed that the introduction of private property constituted the Original Sin, their Winstanley saw property as a symptom, rather than the cause of the Fall. Also, Hill's astute observation that Winstanley was unorthodox in equating the Resurrection and the Second Coming stands in contrast with Mulligan et al. 's conventional reading that Christ's ""appearance would be a literal Second Coming."" The scales seem to be tipped against Mulligan et al. as Zagorin's Winstanley saw Man's redemption ""not in a heaven to come, and not through Christ's vicarious sacrifice, but by the restoration of the absolute law of reason."" Aylmer too, acknowledged Winstanley's unorthodoxy in asserting that the Original Sin was not a historical event, nor was it inherited from Adam, but rather, it was ""the defeat of good by evil, which is repeated in all of us..."" The notion of an internal as opposed to historic and external Fall, Resurrection and Restoration is certainly radical, not to mention also that the Restoration would be complete only when Second Adam (within) kills the First Adam in us, and this inward restoration will be manifested outwardly in the abolishing of Private Property, and the re-establishment of the earth as a common treasury. In using biblical concepts as metaphors to criticise the inequality prevalent in his society, Winstanley ended up creating his own brand of Christian heresy. While the need to substantiate his social critiques and ideas with biblical references is perhaps traditional, in the sense that he acknowledged the authority of the Bible and used it as a legitimising force for his ideas; his use of it is far from conventional, as seen from Winstanley's unorthodox understanding of the Fall and Restoration. ""Winstanley believed in a transcendent God, working through inner transformation"" p. 68 in Mulliganal. ""Winstanley: A Case for the Man as He Said He was"". This makes Winstanley sound almost like an orthodox Christian! Christopher Hill, ""The Religion of Gerrard Winstanley: A Rejoinder"", Past and Present, No. 89, (Nov, 1980), p. 151. Mulligan et al., ""Winstanley: A Case for the Man as He Said He Was"", p. 74 Hill, A Rejoinder, p. 148. This is substantiated with the quote from Sabine, Winstanley, p. 162 ""So that upon the rising up of Christ in sons and daughters, which is his second coming..."" Mulligan et al., ""Winstanley: A Case for the Man as He Said He Was"", p.68 substantiated by quotes from Sabine, Winstanley, Fire in the Bush 1650, p. 466 and 471. The fact that these quotes are not from The New Law of Righteousness does not really matter, since they held that the theological beliefs set out in Truth Lifting Up Its Head are central to Winstanley's thought throughout his writing career. See p. 65 of the same article. Zagorin, A History of Political thought in the English Revolution, p. 47 Aylmer, ""The Religion of Gerrard Winstanley"", p. 95 Can be substantiated by p. 173, 176, 211, 215, 217. in Sabine, Winstanley. Sabine, Winstanley, p. 159 Up till now, the writer has focused on the debates surrounding the nature of Winstanley's religious outlook as a means of conveying the difficulty in assuming a singular stance, and the avoidance of a direct answer to the question so far is an admittance of Winstanley's complexity as a religious thinker. Not forgetting what has been mentioned earlier, there is however, more to be said about Winstanley's views on the institutionalised church, that do not take the form of the above. The lack of attention and controversy concerning Winstanley's anticlericalism (arising from his perceived hypocrisy of the clergy and their monopoly over the interpretation of Scripture, both of which result in the oppression of the people), his suspicion towards institutionalised Christianity, and preference for the personal religious experience, is probably due to the fact that these do not define Winstanley's uniqueness as a religious thinker, but were prevalent in other radical sects. His ambivalence towards an afterlife, belief in the perfectability of man on earth, notion of true worship manifesting itself in love, and the avowed tactic of non-violence echoes that of the Ranters, though the form that it took was different. It is interesting to note that Hill draws out quite a few similarities between Winstanley and the Ranters, and by doing so, sets the religious radicalism of Winstanley within the framework of his time. What remains most distinctive about Winstanley's religious outlook in The New Law of Righteousness is his concept of Restoration being tied up with that of the re-establishment of the earth as a common treasury. Hence it is the communism that is embedded in Winstanley's religious outlook that stimulates the reading of The New Law of Righteousness, since as Zagorin puts in ""it is in his religious evolution ...that we must seek the source of his political theory."" Thus, The New Law of Righteousness has often been read less for itself, and more for its part in the grand scheme of Winstanley's development as a political thinker. Therefore, as mentioned earlier, the different interpretations of the nature of Winstanley's religious outlook reflects the agenda of each historian in trying to conceive of a holistic entity that is Winstanley. Hill, The World Turned Upside Down, p. 206-7 Zagorin, A History of Political thought in the English Revolution, p. 43 In conclusion, it is not possible to take The New Law of Righteousness in isolation, and any interpretations made about the nature of Winstanley's religious outlook necessarily has repercussions on how one understands his evolution as a thinker. Such interactions make it exceedingly difficult to give a straightforward answer to the question. Also, disagreements over how the text should be read has not made it easier to decipher the nature of Winstanley's religious outlook, as the study of the text seem to support more than one interpretation. The debates surrounding Winstanley's millenarianism, his understanding of God, his views on the Fall and Restoration; and the lack of attention given to his stance on the institutionalised Church all illustrate the importance of the agenda of the historian in his treatment of primary sources.","The Enlightenment, an important feature in most written works on the origins of the French Revolution, has often been credited with the ideology that inspired the French masses to rise up against the monarchy. Though the association has been made, there is nevertheless no direct causal link between the ""siecle des lumieres"" (the century of light) and the revolution. Other factors were present, along with the influence of the Enlightenment, which created the context in which the revolution occurred. To provide a satisfactory answer to the above question, the writer will attempt firstly to examine the ideas of the Enlightenment, and how it contributed to the Revolution; secondly the other factors that culminated in the Storming of the Bastille on the 14 th of July 1789 will be placed under scrutiny; and finally, an alternative interpretation of the relationship between the Enlightenment and the Revolution will be considered. According to the Oxford Dictionary, the Enlightenment was 'a European Intellectual movement of the late 17 th and 18 th centuries emphasising reason and individualism.' 'Man is the single term from which all must be brought back' or as Professor Colin Jones puts it 'human value was the critical yardstick of knowledge employed."" Eric Hobsbawm saw the Enlightenment as being dominated by 'a secular, rationalist and progressive individualism' in which its main objective was to free the individual from the bonds of traditionalism, superstition and an irrational hierarchical class order. The cult of the individual may have led to the estrangement of the self from the Church, (seen as an oppressive and stifling body with traditional rituals and rigid beliefs) hence the dominance of secularism in the ideas of the Enlightenment. Proceeding from there, the criticisms aimed at the Church would have led some to question the basis of its power, and the resultant decrease in its prestige and inviolability. Since the monarch was seen as God's ordained agent on earth, any challenge to the supremacy of the Church would only serve to de-stabilise the King's position as an absolute monarch, which was precisely what happened in 1789 (King Louis XVI by the grace of God, King of France now became Louis, by the Grace of God and the constitutional law of the state, the King of the French). Compact Oxford Dictionary Thesaurus and Word Power Guide (2001) p.292. Dennis Diderot, Encyclopedie, vol. V, p. 641 (article, 'Encyclopedie'). Colin Jones, The Great Nation, France from Louis XV to Napoleon (London, 2002), p.174. Eric Hobsbawm, The Age of Revolution 1789-1848 (United States of America, 1996), p.21. Hobsbawm, Age of Revolution, p. 59. 'Liberty, equality and fraternity of all men' was the slogan of both the Enlightenment and the revolution. It is plausible to suggest that the enlightenment did lead, in reality, to greater equality between certain groups of people. In the salons, academies, Masonic lodges and coffee houses, people debated on equal footing, regardless of their status in society. Madam Geoffrin even went as far as to bar the powerful duc de Richelieu from her salon, 'arguing that wit rather than social rank was the passport for admission into her circles."" But this could have resulted in the resentment amongst the lower classes that thought of themselves as the intellectual, but not social, equals to the aristocrats- resentment for the fact that nepotism and the lack of meritocracy were going to cost them that promotion in the Army or civil service hierarchy. The revolution thus could be borne out of the conflict of interests between the old order, and the new forces that were trying to assert themselves. Colin Jones, The Great Nation, p. 183. Perhaps, the most significant way in which the Enlightenment contributed to the Revolution was that it created a climate of change that was conducive for the revolution to occur. The overall rise in literacy, from 29 to 47 per cent for Men between 1686-90 and 1786-90, and 14 to 27 per cent for women during the same period, led to the rapid and widespread dissemination of the ideas of the Enlightenment. Large-scale diffusion of critical and denunciatory literature (not least in pornographic books targeting the royal family) transformed the representation of the monarchy in the minds of the French, and made them think of themselves as victims of a despotic monarch. Thus, in Roger Chartier's words, regardless of the intent of the 'philosophical books', it succeeded in producing an 'ideological erosion' that may have made the revolution inevitable. According to Darnton, even though the people did not call for a Revolution or foresee 1789, unconsciously however, they prepared for that event by 'desanctifying the symbols and deflating the myths that made the monarchy legitimate in the eyes of its subject."" The increased criticisms of the established order (the Church and the Monarchy) may have therefore poisoned the mentality of the Public Opinion in France that eventually led to revolution. The Encyclopedie also aspired to embody 'the power to change men's common ways of thinking' so as to make a 'revolution...in the minds of men and the national character."" Hence the fostering of the 'critical spirit' amongst the French was also an important consequence of the Enlightenment Roger Chartier, 'Do books make revolutions?' The French Revolution in Social and Political Perspective, ed. Peter Jones, (New York, 1996), p. 168. Robert Darnton, 'A Clandestine Bookseller in the Provinces,' in his The Literary Underground of the Old Regime (Cambridge, Mass., 1982), p. 147. Encyclopedie, vol. v, p.637 (article, Encyclopedie) Roger Chartier however, cautions against this link between philosophical works and revolutionary thought. Using the example of Rousseau, Chartier illustrates the popularity of the philosophes amongst the sans culottes, middle classes and the aristocracy. Moreover, one of the Enlightenment's crowning glories, the Encyclopedie was too expensive to be purchased by anyone other than the notables, and though some were dedicated to the revolutionary cause, the majority was apathetic or hostile to it. Due to the fact that there can be many interpretations to a book, hence, it is impossible to credit too direct a role to books in creating the revolutionary ardour of the French masses. Furthermore, the philosophes were from quite a diverse social background. Rousseau's father was a watchmaker, Voltaire was the son of a notary, and Montesquieu was 'a magistrate in the parlement of Bordeaux, a feudal lord living in a moated castle and an apologist for noble power."" Most of them (excluding Rousseau) believed in the idea of the monarchy as the generator of utilitarian reform, and despite the criticisms of the monarchy and traditional institutions, the philosophes believed that 'the modern state could be improved as it stood."" Perhaps, it was the failure of 'Enlightened Despotism', which resulted in the bourgeoisie transfer of faith from the monarchy to the masses, which led indirectly to the French revolution. William Doyle, The Oxford History of the French Revolution, (Oxford, 2002), p.50. Colin Jones, The Great Nation, p.221. Amongst the other factors that contributed to the outbreak of the revolution, Hobsbawm suggests that it was war and debt that broke the back of the monarchy. He explains that the bankruptcy of the government and the resultant need for tax reforms gave the aristocracy and the parlements a chance to barter with the government. In exchange for the tax reforms, the aristocracy wanted an extension of their privileges, and this led to the forming of the assembly of notables and the calling of the Estates-General (a feudal assembly which last met in 1614). Hobsbawm concluded that the revolution thus began as an aristocratic attempt to recapture the state, but it underestimated the sovereign ideas of the 'Third Estate', and the socio-economic consequences of its political demands. What made the 'Third Estate' a force to be reckoned with was the fact that they not only represented the interests of the middle classes, but also of the urban poor and eventually, the revolutionary peasantry. In my opinion, the socio-economic context of revolutionary France was just as, if not more important than the influence of the Enlightenment. The fact that the revolution broke out in 1789 and not any other year was partly because of the bad harvest in 1788/9, which was reinforced by a particularly harsh winter. The severe economic consequences and the resulting industrial depression drove the urban and rural poor to desperation. The impact was heightened by the fact that in the preceding years, conditions were improving and things were turning out for the better. The expectations of the people were raised, and it came crashing down in 1789. Hobsbawm also suggested that the campaign of propaganda lent an added political dimension to the grievances of the people, and united them behind the deputies of the 'Third Estate'. Once the bastion of monarchical suppression and power (the Bastille) had fallen, there was no turning back. Hobsbawm, The Age of Revolution, p.58. In Roger Chartier's article he puts forward and alternative interpretation of the relationship between the Enlightenment and the revolution. He contemplates the possibility that it was not the Enlightenment that implied revolution, but rather, the converse, that it was the latter that constructed the former. He states that the retrospective construction were many-the 'canonisation' of Voltaire and Rousseau, by the revolutionary assemblies, as the intellectual fore fathers of the revolution, while others such as Buffon and Descartes were relegated; in the quest for legitimacy, political celebrations were held in Year II, honouring the philosophes and martyrs for liberty. The articles in the Declaration of the Rights of Man, which expounds on the freedom of Man, may have led to the belief that it was the Enlightenment that was the ideological inspiration behind the revolution. Perhaps it was a retrospective justification for the revolution and to put the revolutionaries on a moral high ground, since they were now seen not as mere rebels, but as agents of progress. Hence, 'it was the revolution that gave a premonitory and programmatic meaning to certain works, constituted, after the fact, as its origin.' Roger Chartier, 'Do books make Revolutions', p.183. Chartier, 'Do books make Revolutions', p.185. The relationship between the Enlightenment and the revolution is a complex one that does not lend itself readily to simple cause and consequence explanation. There have been mutual exchanges between the two, in the sense that the Enlightenment provided the ideology and legitimacy of the revolution, and on the other hand, the revolution also gave the Enlightenment a prophetic element in that the latter 'predicted' the coming of the former and prepared the people for socio-political change. Much as the Enlightenment has contributed to the revolution, we should not discount the other forces and events that were in place for the revolution to occur, such as the economic depression that led to the politicising of the masses, the bankruptcy of the monarchy, and the personal temperament of King Louis XVI (in particular, how he was so easily manipulated by his Queen and reactionary advisors). Hobsbawm has an interesting perspective to offer, in that he thinks the revolution would have occurred without the philosophes, but they 'made the difference between a mere breakdown of the old regime and the effective and rapid substitution of a new one."" Perhaps we can then say that the Enlightenment was important only after the revolution had become a fait accompli, and that it made its mark on post-revolution re-construction. Hobsbawm, Age of Revolutions, p. 58. The hypothesis put forth by J.C. Davis's Fear, Myth and History has provoked a heated debate that still in want of a closure, and one thinks that it may never be found, for it would be professional suicide for any of the historians involved, especially Davis, to 'recant' and come round to the others' viewpoint. While there is a sense that the debate has been bogged down by personal animosities and the lack of fresh interpretations, it is still of interest today, because of the pertinent ""issues about historical methods raised by Fear, Myth and History"" that ""are important far beyond the particular case of England in the mid-seventeenth century."" It is to this debate that we now turn our attention to, and I have structured the essay as such: the first section deals with historiography, methodology and the implications of Davis's interpretation on the discipline of History, followed by the main body of the essay which will focus on the two questions relating to our topic at hand-why Davis denies the existence of the 'Ranters', and subsequently, why he believes that the 'Ranters' were created by seventeenth century conservatives and twentieth-century Marxists. While Davis's interpretation is thought provoking, it is my opinion that his failure to differentiate between 'Ranter' myth and 'Ranters' has been a stumbling block in this debate. G.E. Aylmer, ""Did the Ranters Exist? "", Past and Present, No. 117 (Nov., 1987), p. 208. Jstor article. Before focusing on the main debate, it is imperative to begin with an examination of some of the held assumptions and employed techniques in Davis's hypothesis. In the area of historiography, Davis seems to have taken E.H. Carr's dictum ""study the historian before you begin to study the facts"" a bit too far when he claims that the 'Ranters' were conjured up by members of the Communist Party Historians Group in order to fit English history to Marxist theory. While it is important to question the agenda of historians, there is a fine line between the above, which is informative, and the latter-pure speculation-which I think that Davis is guilty of. Every historian writes with an agenda, for that is implicit in his/her argument, which in turn moulds his/her interpretation which is-according to Carr-""the lifeblood of history"". One ponders about Davis's own agenda behind his enterprise, but let us not tarry on this for there are other more pertinent issues to discuss. In particular, Hill has raised a very relevant point that Davis's paradigm is flawed and his enterprise doomed from the start, because his desire to find ""a coherent, cohesive group of like-minded Ranters"" is anachronistic, and that he is guilty of looking at the past through eyes of the present. There is much inconsistency in Davis's methodology, seen in the way he views seventeenth century pamphlets with extreme scepticism and yet at the same time, as pointed out by Christopher Hill, seems to have taken Abiezer Coppe's ironic recantations seriously. The implications of this debate are far-reaching, and it has not only questioned how historians ought to view primary sources, but has also affected how we are to understand the function of labels. Labels exist first and foremost to describe an external reality, though, that does not preclude the possibility of that label acquiring associations along the way that exaggerate the former, hence turning it into a myth. In the same way that Protestants used the 'Catholic myth' to consolidate their positions, so the seventeenth century conservatives manipulated what was already in existence to galvanise the population into combating the attack on religious and social order that the English Revolution had brought about. E.H. Carr, What is History?, (London, 1990) p.23. J.C. Davis, ""Fear, Myth and Furore: Reappraising the 'Ranters'"", Past and Present, No. 129 (Nov., 1990), p. 82. Jstor article. Carr, What is History? , p. 28 Davis, Fear, Myth and History: The Ranters and the historians, (Cambridge, 1986), p. 74 Christopher Hill, A Nation of Change and Novelty: Radical politics, religion and literature in seventeenth-century England, (London 1990), p.153 Hill, A Nation of Change and Novelty, p. 180. Davis, Fear, Myth and History, p. 53-7 It is Davis's opinion that the 'Ranters' were a ""projection of deviance"", and that ""there was no Ranter movement, no Ranter sect, no Ranter theology"", but consisted of ""a few relatively isolated individuals of heterogeneous persuasions"" who were ""swept up in the projection of a movement"" and were subsequently ""assigned roles by the historians"" While ""historians are agreed that it is wrong to speak of a Ranter sect or movement"", few have gone so far as to deny the existence of the 'Ranters'. Davis is unique in asserting that because there were no concrete evidences of the 'Ranter's' practical antinomianism, therefore they do not exist. Even if such evidences were in existence, Davis would most likely reject them as sensationalism of the yellow press, or as unreliable stories conjured up by seventeenth century anti-'Ranter' pamphleteers. However, Hill points to evidence that Gerrard Winstanley experienced problems in his Digger colony, due to the presence of drinking, whoring and indolent 'Ranters', and makes a point, which is taken up by Aylmer that the 'Ranters' held an 'anti-work' ethic and lived a ""hippy-like existence"" Therein lies the 'practical antinomianism' that Davis seeks. The 'Ranters' hedonistic 'make love not war' sort of attitude constituted a critique of society, which manifested itself in the breaking of taboos, as opposed to the more taxing alternative of a violent overhaul of society. There is a more cogent explanation proposed by Hill that accounts for the lack of 'practical antinomianism'-it is simply that the Ranters were ""talkers rather than doers."" Ibid, p.124 Bernard Capp, ""Fear, Myth and Furore: Reappraising the 'Ranters'"", Past and Present, No. 140, (Aug., 1993), p.165 Jstor article. Aylmer, ""Did the Ranters Exist? "", p. 209 Christopher Hill, The World Turned Upside Down: Radical Ideas during the English Revolution, (London, 1991), p. 229-30. Hill, A Nation of Change and Novelty, p. 185 Davis's second charge is that the absence of any common defining ideology and close direct links between members of the supposed 'Ranter' core disproves that the 'Ranters' existed either as ""a small group of like-minded individuals, as a sect, or as a large-scale, middle-scale or small movement."" While it is true that the antinomian and spiritualist ideas were not unique to the 'Ranters', according to Capp ""it is clear that something had crystallised from the flux, and that we are no longer dealing with isolated individuals"". Capp substantiates his point with compelling evidence drawn from correspondences between Salmon, Webb and Coppe, which indicated a sense of familiarity to the point of intimacy. Though he concedes that Coppe and Clarkson were never close, nevertheless, he asserts that there were links between both groups, hence refuting Davis's claim that ""the Ranter core shatters and disintegrates."" As mentioned earlier, the search for a seventeenth-century group labelled the 'Ranters' eludes Davis precisely because he employs modern standards and preconceptions to study a seventeenth-century phenomenon. Davis's insistence in finding a certain measure of uniformity or convergence of ideas will not hold up even in the globalised world today, where common church denominational labels mask a whole range of disparate practices and beliefs. Seventeenth-century radical groups were even less likely to converge on not more than a few tenets which roughly separated them from the other movements, not forgetting as well, that there was a great deal of fluidity between the groups, as people shopped around for a belief system that appealed to them. The 'Ranters' were such a loose grouping precisely because of their nature as a subversive group, their wariness of any forms of organised religion and distaste for hierarchy, which makes them not unlike modern anarchist 'groups' in form. Davis, Fear, Myth and History, p. 75 Capp, ""Reappraising the Ranters"", p. 166-7 Davis, Fear, Myth and History, p.75 Davis's scepticism is conceivably fuelled by the tendency of supposed 'Ranter' leaders to recant in the face of persecution. His serious treatment of the ironic recantations of Coppe, Clarkson and others is puzzling in light of his scepticism with regards to seventeenth century anti-'Ranter' writings. In contrast, Hill accounts for this readiness to recant as a feature characteristic of 'Ranters', who, like their sixteenth-century predecessors, the Familists, practiced ""Nicodemism-recanting in the face of overwhelming power"" and warns us against taking their equivocal recantations seriously. This practice can also be seen as the direct consequence of the 'Ranter' belief in 'mortalism', which precludes the immortality of the soul and hence of the appeal of martyrdom. As Hill astutely points out ""...resistance to the death would call for a deeper and more consistently worked out ideology than most Ranters had."" The two historians also disagree as to whether the 'Ranters' were the main targets of the Blasphemy Act of 1650. If Parliament had regarded the ideas and behaviour of the 'Ranters' as a real threat to societal order, then Davis's hypothesis that the Ranters did not exist is weakened. Though Davis is correct in saying that the Blasphemy Act did not mention the Ranters, evidence points towards Clarkson's A Single Eye All Light as a trigger to the body of legislation, which according to McGregor ""was largely a description of Ranter doctrine, owing much to the writings of Coppe and Clarkson."" While there is a possibility that the Parliament was responding to the 'Ranter myth' and not to the 'Ranters' it still reinforces Aylmer's simple concept that there can be ""no smoke without fire"". To take the analogy further, the strength of the fire might be disproportionately small compared to the amount of smoke that is billowing, and the response of the fire fighters might be more than is required to deal with a small fire, but the fact remains that a small fire did exist. Though that does not preclude the possibility that some people may have stoked the fire and fanned the flames, hence creating the 'smoke screen' to mask their presumably sinister agendas. Hill, A Nation of Change and Novelty, p. 179. Hill, The World Turned Upside Down, p. 209. J.F. McGregor, Fear, Myth and Furore: Reappraising the 'Ranters', Past and Present, No. 140 (Aug., 1993), p. 157. In order to justify and perpetuate his hypothesis that the 'Ranters' did not exist, Davis puts forth a series of explanations that sought to provide a plausible list of agendas that necessitated the creation of the 'Ranter' myth. For the seventeenth century conservatives, the 'Ranters' were necessary as the ""projection of deviance"" ""for it is only through deviance that we understand normality."" While this is a compelling argument, and extreme radicalism may alienate other groups and force some into the protective arms of the conservatives, there is also the other possibility that the increased profile and definition of the 'Ranters' may have seduced and ensnared others. No matter how vile the portrayal of the 'Ranters' there is an element of freedom in antinomianism that can be particularly enticing, especially for young men. Hence, this myth is in my opinion a double-edged sword to be wielded at one's own peril. The myth may achieve normality, but it could just as easily go the other way and perpetuate deviant behaviour. The 'Ranter' myth was not the exclusive tool of the conservatives, but was also used to achieve sectarian consolidation: to enhance the acceptability of sects that were not as radical as the 'Ranters', and to induce conformity within sectarian ranks. Because this spectre of Ranterism was conjured up to fulfil a social function, Davis warns us against taking any of the anti-Ranter literature seriously. On the topic of sources, Hill suggests that we can look to seventeenth-century personalities, such as John Reeve and Lodowick Muggleton, who were previously from the Ranter milieu for an accurate account of 'Ranterism'. Notably, Hill focuses on the figure of Bunyan, who was himself initially tempted by Ranter ideas, but whose work's from 1665-1685 took on an anti-'Ranter' slant. Also, despite the many hostile anti-Ranter records, Hill uncovered Ranter nostalgia in Erbery, Byne and Sedgweick, who ""found things to praise in the Ranters..."" though he added that ""they would hardly invent the Ranters for this purpose"". At the very least, there is a consensus that 'Ranter' was a 'buzz-word' in seventeenth-century England, not unlike that of 'globalisation' in our own time, though an admittance that the misuse of the label 'Ranter' existed does not preclude the external reality of a group of 'Ranters'. Davis, Fear, Myth and History, p.124 Ibid, p.111 Ibid, p. 110. Hill, A Nation of Change and Novelty, p. 164 Ibid, p.177. Ibid, p.169. While Davis might possibly defend his claim that the 'Ranters' were created by seventeenth-century conservatives (amongst others), his other claim-that the 'Ranter' spectre was resurrected by A.L. Morton and Christopher Hill, both members of the Communist Party Historians' Group between 1946-1956 in a bid to ""create the history of a popular democratic tradition in English history and culture"" is absurd and deliberately provocative. The allegation that Davis brings against them-that of using theory to dictate history- is an equally valid charge that can be levelled at him. Hill's defence of himself and A.L. Morton in A Nation of Change and Novelty is fairly convincing, and he has managed to place the onus on Davis to explain his own motivation behind the whole enterprise. Furthermore, other historians have joined in the fray, and with a few differences they have reaffirmed the existence of the 'Ranters' and the 'Ranter' myth. While the debate should not end on a 'majority wins' basis, there is a sense that this is quite a lope-sided debate, with Davis fighting the battle on his own. Davis, Fear, Myth and History, p.130 Hill , A Nation of Change and Novelty, p. 190 In conclusion, Davis's hypothesis, while novel, is not entirely convincing. The arguments of the other historians are a lot more persuasive, both in establishing a link between the various individual 'Ranter' leaders (despite the running debate about 'core' vs. 'milieu' which was not dealt with in this essay) and in their efforts to draw out some semblance of similarities between the various groups, while conceding that it impossible to define a coherent set of 'Ranter' ideology. This is not far off from Davis's own assertion that what would make him believe that the 'Ranters' existed would be the coincidence of two sort of evidences: that the existence of a group (or a number of groups loosely linked) can be established and that its shared ideology was reflected behaviourally in practical antinomianism. It bears repeating that Davis's enterprise is flawed for two reasons: his failure to differentiate between 'Ranter' myth and 'Ranters' which has led him to believe that the 'Ranters' were created by seventeenth-century conservatives and later resurrected by twentieth-century Marxist historians; and his employment of modern standards and preconceptions to study a seventeenth-century phenomenon, which explains why his search for the 'Ranters' as a external reality has been so unfruitful. Perhaps one might cheekily suggest that Davis's own 'recantation' is overdue. J.C. Davis, ""Fear, Myth and Furore: Reappraising the 'Ranters': Reply"", Past and Present, No. 140 (Aug., 1993), p. 205. Jstor article. China has long captured the imagination of the West, and still remains an enigma in this present age despite the vicissitudes of time. While technology now permits us to catch a glimpse of China on the news, on the internet, or plausibly, even visit China; this would not have been conceivable in previous centuries, where one of the few ways in which one could come into contact with, and conceive of China was through the objects brought back from overseas trade- which was itself stimulated by a thirst for the mysterious exotic Orient as much as it was for profit. Although these goods were prized for their scarcity, aesthetic value and excellent workmanship, it was not the material alone that captivated, but more importantly, it was the non-material associations that imbued the goods with meaning, desirability and the label of 'luxury'. This fuelled the luxury trade, long underpinned by traditional court luxury, which reached its zenith in the seventeenth and eighteenth centuries, with the establishment of the chartered East India companies and the subsequent creation of a European market for these goods. This essay will attempt to put forth the idea that Europeans were so fascinated by Chinese luxury goods precisely because of the various functions that these goods fulfilled- it fired up the European imagination, giving it the impetus to venture out and explore the world; it played its part in influencing the new taste for the Rococo and in the definition of 'refinement'; it was amenable to the marketing techniques of the East India Companies which further heightened its appeal; and finally it stimulated European industries in their attempt to surpass the East through emulation and innovation. In short, it was through the acquisition of Chinese goods, which defined the 'Other', that one perceives the 'Self' in a clearer light. Maxine Berg, ""In Pursuit of Luxury: Global History and British Consumer Goods in the Eighteenth Century"", in Past and Present 182 (2004), p. 86. Before embarking on the main body of the essay proper, there are some issues that deserve our attention: firstly, the reference to ""seventeenth and eighteenth century Europeans"" in the question should not be taken to mean all Europeans, but was limited to the affluent, by nature of it being Asian luxury goods. And even within this group, there were some such as Mrs. Montagu and the adherents of the Burlington School of Taste who lamented the fall from favour of Palladianism. While it is not the task at hand to examine the interactions between the Orient and the Occident, it must be said that far from being a one-way exchange, the Chinese elite were themselves fascinated by western manufactured goods, especially clocks. It is interesting to note that both cultures used foreign 'luxury' goods in a setting that suited their own agendas, often removing the objects from their original context, transforming the functional into the ornamental. This playful assimilation, also seen in the creation of the Chinoiserie, is perhaps reflective of the confidence of both cultures, which is in stark contrast to the theories of Social Darwinism, which characterised their relations in the nineteenth century. Finally, it is important that we examine the definition of 'luxury goods'. As Vainker astutely points out: despite the high quality of silk, porcelain and lacquer, these goods were not perceived as luxury items, but as functional goods by the affluent Chinese. Instead, what they valued as luxury items would include ""paintings, ritual bronzes, jades, calligraphy and other antiques and collectibles associated with a scholar-official elite and the authority to rule."" Hence, luxury is defined not by high quality goods that are widely available (such as porcelain and silk in China), but by its association with the conferment of social status due to its scarcity (Chinese porcelain and silk in Europe). In a letter from Mrs. Montagu 1749 ""Thus it is happened...we must all seek the barbarous gaudy gout of the Chinese; and the fat-headed Pagods and shaking Mandarins bear the prize from the greatest works of antiquity; and Apollo and Venus must give way to a fat idol with a sconce on his head."" Quoted from Dawn Jacobson, Chinoiserie, (London, 1993), p. 123. Ibid, p.124. Seen in the use of clocks as status symbols (as opposed to time-pieces) in Yongzheng's imperial household; and the French candelabra composed of inverted Chinese whistles and cups mounted on a frame. Seen in the DVD produced on the Encounters: The Meeting of Asia and Europe 1500-1800 exhibition. Picture of French Candelabra in Anna Jackson and Amin Jaffers (ed. ), Encounters: The Meeting of Asia and Europe 1500-1800, (London, 2004), plate 1.9, p. 7. Picture of Yongzheng's concubine with a clock in the background in Evelyn S. Rawski and Jessica Rawson (ed. ), China: The Three Emperors 1662-1795, (London, 2005), bottom left, p. 259. Shelagh Vainker, ""Luxuries or Not? Consumption of Silk and Porcelain in Eighteenth-Century China"", in Maxine Berg and Elizabeth Eger (ed. ), Luxury in the Eighteenth Century: Debates, Desires and Delectable Goods, (New York, 2003), p.214. The seventeenth and eighteenth century fascination with Chinese luxury goods had its genesis in the myth of Cathay, which conjured up ""fantastic visions of luxury and refinement, pleasure and abundance"", and were reinforced by traveller accounts of Marco Polo and the fictional Sir John Mandeville. As Jacobson observed ""this view of the Chinese as being different but in no way inferior, helped to sustain the myth of Cathay, and the vision of a culture that was profound as well as peculiar."" It was this perception that made possible the enchantment and respect that the Europeans had for the advanced Chinese civilisation; which manifested itself in the pursuit of Chinese goods, in the hope that one's collection might plausibly be employed as a prism through which the secret knowledge of the East might be yielded. That China during the Ming dynasty was largely cloistered from the world further enhanced its mystery and appeal, not unlike the coy mistress subtly beckoning to would-be suitors. And they came- in drips and then in droves- the Portuguese trailblazers followed by the Dutch, English and French traders who brought back taster portions of Asian goods, fuelling the imagination of the affluent Europeans further, which in turn encouraged the luxury trade. Jacobson, Chinoiserie, p. 10 Ibid, p. 12-15. Ibid, p.15. It is imperative that we examine how this seemingly perennial fascination that the Europeans have for the exotic Orient featured in the particular context of the seventeenth and eighteenth century, especially in terms of sociability, civility and taste. It is important to note that in the moral debates about luxury at that time, Chinese goods were ""associated not with (Persian) sensuality and excess, but with ethics, harmony and virtue...(and) in possessing things Chinese, they (the philosophes) sought to access levels of civilisation beyond the market."" Hence, it was through the prism of Chinese objects that the Europeans expressed ""their own aspirations to human elegance and refinement."" The employment of taste as ""a new means of displaying prestige"", and the corresponding replacement of traditional forms of opulence (silver and gold) with elegant new 'imitative' objects, such as porcelain are integral in accounting for the vogue in Chinese luxury goods in seventeenth and eighteenth century Europe. This change in taste also reflected the changing relations in society, with the notable rise of the bourgeoisie, who rejected the gaudy decadence of the baroque (associated with the ancien regime) in favour of the lighter, more playful and asymmetrical Rococo, which ""brought together commercial and cultural modernity."" It is perhaps fortunate that the Chinoiserie with its freedom from classical notions of restraint and symmetry found favour with the Rococo, and was in fact a ""vital ingredient in the creation of the new taste."" It was in this context that Chinoiserie-which was the Western romanticised view of the East, produced in part by the imported goods- flourished, and ""in turn dictated the commerce in Asian imports."" Maxine Berg, ""Asian luxuries and the Making of the European Consumer Revolution"", in Maxine Berg and Elizabeth Eger (ed. ), Luxury in the Eighteenth Century: Debates, Desires and Delectable Goods, (New York, 2003), p. 229. Ibid. Ibid, p. 230. Ibid, p. 231. Jacobson, Chinoiserie, p. 57. Maxine Berg, Luxury and Pleasure in Eighteenth century Britain, (Oxford, 2005), p. 51 While fascination with the Orient had always existed, it is necessary to look at how the East India Companies manipulated what was already in existence and transformed it into an obsession with Chinoiserie, as seen from above. As Berg astutely pointed out: these luxury goods ""were a construct of the market, seeming to represent the lives and values of the East, but constructed to meet European preconception of eastern art."" These commodities were prefabricated on the Chinese side to cater to Western markets; and on the part of the East India Companies, which sent out European shapes to be copied, and the initial contentment with Chinese designs soon gave way to a sudden fashion for 'chine de commande' by the end of the seventeenth century. While the East India Companies (EICs) were themselves the early products of the European fascination with China and its goods, they were indispensable in the subsequent promotion of this luxury trade, which was intrinsically tied up with the western enthralment for the East. The EICs were not merely merchants who simply pandered to the tastes of the European elite, but were market movers in the way in which they created a demand for these goods through 'customisation' and effective marketing. It is remarkable how they transformed something as functional and mundane as porcelain by capitalising on its exotic appeal and its association with civility, marketing it as a semi-luxury good, when truth be told, the differentiated decorations on the porcelain pieces belie the fact that they were to some extent, 'mass' produced goods. Moreover, they created new wants- ""the dinner service with complete setting for each person was an invention of the VOC and the EIC which started importing porcelain dinner services in the eighteenth century."" Therefore, though the western fascination for the Orient was present before the seventeenth-century, we have to credit the EICs for their role in elevating it to a whole new dimension with the extension of maritime trade and through successful marketing tactics. Berg, ""Asian luxuries and the Making of the European Consumer Revolution"", p. 228 Christiaan, J.A. Jorg, ""Porcelain for the Dutch"", in Rosemary E. Scott (ed. ), The Porcelain of Jingdezhen: Colloquies on Art and Archaeology in Asia No. 16, (London, 1993), p. 188 and 200. Berg, ""Asian luxuries and the Making of the European Consumer Revolution"", p. 239 While exoticism, taste and marketing definitely contributed to the appeal of Chinese luxury goods to the West, a large measure of this preoccupation was also due to the impressive technical wizardry of the Chinese craftsmen and the secrecy which shrouded the production processes for these goods-especially porcelain- the Arcanum of which was first discovered by the Europeans only in 1709. Perhaps it is useful to point out that the luxury goods that fascinated the West were new, modern luxury comprised of ""technically advanced and refined consumer objects"" such as calicos and porcelain. Hence, the East enthralled the West because the latter realised that much could be learnt from the former; which would allow the West to progress through imitating and perfecting the techniques that they have gleaned from the East. It is important to note that contemporaries such as ""Montesquieu, Hume and Smith wrote of luxury as an aspect of people's desire to better themselves"" and ""...commercial writers were keen to point out what the West could learn from the Orient."" In doing so, they affirmed the moral good of luxuries in contributing to 'Progress', hence alleviating the fears of decadence and degeneration that plagued earlier preoccupations with luxury goods, and indirectly encouraged the fascination for Chinese luxury goods. While it must be said that this desire to learn from the East was more apparent than real, which is illustrated by Berg's observation that ""Asian consumption was transferred to Europe but not Asian production systems"", there appears to be a genuine lesson imported from the East, which is ""the policy of using the fine arts and design in combination with modern manufacturing technique."" Hence, ""paradoxically, in imitating Asian consumer goods, perceived in Europe as luxuries, the British achieved what was actually an Asian success story-new, quality, semi-luxury consumer goods produced with advanced industrial techniques."" Ibid, p. 230 Ibid. Berg, ""In Pursuit of Luxury: Global History and British Consumer Goods in the Eighteenth Century"", p. 86 Berg, ""Asian luxuries and the Making of the European Consumer Revolution"", p. 242. Berg, ""In Pursuit of Luxury: Global History and British Consumer Goods in the Eighteenth Century"", p. 130. In conclusion, this fascination that the Occident had for the Oriental luxury goods was not new, but was closely entwined with the captivating medieval myth of Cathay, and reached new dimensions in the seventeenth and eighteenth century with the establishment of the East India Companies (which were themselves the product of and the further stimulus to the West-East trade born of the Occidental preoccupation with the exotic, mysterious and abundant East) and the creation of European markets for Chinese luxury goods (which was previously the preserve of the European royalty). It is perhaps paradoxical that these Chinese luxury goods fascinate precisely because they have been displaced from their original cultures and grafted into new ones, but continue to give the impression that it was still possible to use them as prisms through which the West can subscribe to notions of Chinese refinement and skill. The fascination for anything 'Chinese' is apparent in the vogue for the Chinoiserie especially in the eighteenth century, and is both reflective of, and contributed to the existing enthrallment that the West had for the East, and how that affected the dominant taste in Europe. The appeal of Chinese luxury goods was also played up by the East India Companies which capitalised on the vogue for Chinoiserie, and created new wants through the successful marketing technique of making the foreign 'relevant' to the West through customisation. Finally, and most importantly, Chinese luxury goods were seen as having the role of ""awakening the imagination"" and were positive elements in stimulating the Europeans to better themselves through emulation and product innovation. Despite being so far removed from their original cultures, that these luxury goods were relevant and fulfilled certain functions given to them by their European possessors accounts to a large extent for the fascination that seventeenth and eighteenth century Europeans had for Chinese luxury goods. Ibid, p.131",False
96,"A disease slowly destroys your oligodendroglia cellsOligodendroglia cells are held in the sheath of Schwann, which covers myelin and provides insulation to the neurons in the central nervous system. The axon of each neuron is protected by a myelin sheath, this ensures the transmission of signals and allows nerve impulses to be propagated to the next neuron. This mylelination has a strong impact on behaviours as it profoundly increases the velocity of nerve impulses and thereby affects the temporal order of events in the nervous system (Biological Psychology, 2002, p. 69). Normal transmissions are so fast and automatic that it is hard to accept a delay between a thought and a muscle contraction. The speed of nerve conduction due to the presence of the oligodendroglia cells is up to approximately 260 miles per hour (Psychology 5 th ed., 1999 p.51). The myelin sheath around the axon is made up from 2 principle proteins, the myelin basic protein and the proteolipid protein. Without the oligodendroglia cells forming myelin, nerve signals would become slowed or even completely blocked, thus causing symptoms of Multiple Sclerosis. In MS T-cells from the immune system become sensitised to the myelin basic protein and begin to attack it, this causes the body to try and repair the situation. Therefore in the initial stages of MS the oligodendroglia cells are not destroyed, but produce more myelin to compensate for the myelin being attacked. This causes the symptoms of MS to disappear, but only until the myelin degenerates again. In later stages of MS the oligodendroglia cells are destroyed, this produces multiple demyelinated areas on the axons of the neurons and eventually a person with MS develops areas of scarring where the healthy myelin once used to be. These areas of scaring cause lesions to appear in seemingly random areas of the CNS white matter and because the severity of MS depends on the location and extent to which these lesions occur, the type of symptoms can vary ( URL ). Myelination slows down the speed of conduction of neural impulses and thus can cause slowness of movement and loss of sensory abilities (Biological Psychology, 2002, p. 69). Also with the insulation gone, the messages being carried by the axons are no longer kept separate and this results in the sensory disorders and the loss of muscular control (Physiology Of Behaviour, 1991 p.29). Other symptoms of multiple sclerosis include numbness, difficulty with speech, muscle spasms and coordination problems. These problems with movement occur because the loss of the oligodendroglia cells weaken the electrical impulses from the brain and sometimes fail to transmit them at all. b) A tumour destroys the fusiform gyrus in your right temporal lobeThe fusiform gyrus can be found in the medial part of the occipital lobe situated in the right temporal lobe. It is used in performing sensory discrimination of emotional stimuli and the activation of the fusiform has been reported during the perception of faces. Also several studies have demonstrated that a particular region within the fusiform gyrus, the fusiform face area, responds in a highly selective way to face stimuli. The fusiform gyrus has more than one function, it holds the functional region responsible for colour, and is used in the identification of faces and in the recognition of facial expressions ( URL ). Damage to the fusiform gyrus can disrupt an individual's ability to retrieve information especially visual memories, such as the remembered face of a friend. If a tumour destroys the fusiform gyrus of the right temporal lobe then the left visual field will be affected. A lesion in the fusiform gyrus is associated with the loss of colour perception; this is known a central achromatopsia. Patients are usually aware of the deficit and report the world as being grey or 'dirty' and it can also affect one colour more than another (braincampus.com). Another consequence of damage to the fusiform gyrus is prosopagnosia; this is a disorder where the patient is unable recognise faces. Patients can identify faces but they do not recognise whose face is present before them, they may not even recognise their own face in the mirror. Prosopagnosia involves both the temporal and parietal lobes but mainly originates from damage to the fusiform gyrus. Even though patients have difficulty in recognising faces there is a marked difference if a face is made psychologically 'stronger' by for example, presenting faces that are linked. Patients with prosopagnosia may also have difficulty in distinguishing between particular objects of the same class, e.g. makes of cars (Physiology Of Behaviour, 1991, p.187). The fusiform gyrus is also related to facial expressions. A person with a damaged fusiform gyrus also has problems in recognising facial expressions of basic emotions and my find it difficult distinguishing between anger and frustration. c) The cells in the substantia nigra degenerateThe substantia nigra is a group of neurons in the region of the midbrain whose axons go to the basal ganglia. The cell bodies of the dopamine system lie within this region and are involved in motor control via the basal ganglia and in particular to the striatum cells of the substantia nigra. The nerve cells in the substantia nigra communicate with other cells on the nearby striatum by releasing the neurotransmitter dopamine at the nerve terminals, which is also synthesised there and is the area of the brain that controls movement and balance ( URL ). The basal ganglia usually impose constraints on motor control, but the absence of these constraints yields persistent excess of movement or slowness of movement and marked changes in muscle tone (Biological Psychology, 2002, p.352). The degeneration of the cells in the substantia nigra causes involuntary repetitive movements of the hand and arm, now known as Parkinson's disease. The cell bodies of dopamine that originate in the substantia nigra are no longer produced if the cells in the substantia nigra degenerate, this therefore causes a pronounced decrease in the dopamine content of the basal ganglia. People with Parkinson's disease inexplicably lose more than 80% of the dopamine producing cells in the substantia nigra ( URL ). Without dopamine the substantia nigra striatum cannot send out certain messages as they are unable to travel across the nerve connection, and it is this that leads to Parkinson's. The disease is described as a severe disorder of movement that involves tremors of the fingers, slowness of movement and problems with maintaining posture. There can also be a loss of facial muscle tone and general difficulty in all motor efforts. There are many symptoms for Parkinson's disease but they vary substantially from person to person, making diagnosis quite difficult. L-dopa (the precursor to dopamine) is administered to increase levels of dopamine in the brain in order to reduce the symptoms of Parkinson's disease. L-dopa enhances dopamine levels of the surviving cells but because the cell bodies in the brain stem degenerate, dopamine-containing terminals also disappear. Eventually too few dopamine-containing neurons remain in the substantia nigra to be influenced by the intake of L-dopa (Biological Psychology, 2002, p.353). Therefore L-dopa only relieves the symptoms and because the degeneration of the substantia nigra is progressive, there is no cure as yet. Recent reports have suggested that a more permanent cure would be to transplant new dopamine neurons from embryonic brains (Principles Of Biopsychology, 2001, p.26). d) Your mamillary bodies cease to functionThe mamillary bodies receive information from the hippocampus and are reciprocally connected to the anterior thalamic nucleus and the midbrain. These structures seem to play an important role in the memory process and have been described as 'a narrow funnel through which connections from the midbrain as well as the temporal lobe neo cortex and limbic system gain access to the frontal lobes.' (Mair et al., 1979). Korsakoff's syndrome occurs due to neuronal loss in the mamillary bodies and is associated with cell shrinkage in a particular nucleus of the thalamus and the mamillary bodies. Mairal. (1979) examined 2 Korskoff patients and found that both brains showed shrunken, diseased mamillary bodies. It is distinguished by confusion and a deficit in storing new information (anterograde amnesia). This type of amnesia is usually caused by chronic alcoholism. The alcohol prevents the liver from metabolising the vitamin thiamine, which causes a thiamine deficiency, which in turn leads to the breakdown and loss of neurons, and because neurons are not replaced the damage is irreversible. Therefore Korsakoff's involves a progressive degeneration of brain tissue (Principles Of Biopsychology, 2001, p.88). The dysfunction of the mamillary bodies causes the patient to fail to recall many items or events in the past, if the item is presented again, the patient does not feel familiar with it. They often show disorientation to time and place and may confabulate (fill a gap in memory with a falsification that they seem to accept as true) (Biological Psychology, 2002, p.541). Patients with this disorder usually suffer from some retrograde amnesia as well, but their primary deficit is the inability to store any new information in long-term memory. The symptoms are similar to those of people with damage to the prefrontal cortex, including apathy, confusion and memory impairment. Like most patients with damage to the frontal-lobes, Korsakoff's patients have trouble recalling the temporal order of events, for example they cannot recall which of a series of world events happened the furthest in the past or which one happened most recently, they also can not recall which events from their own lives happened a long time ago and which ones happened recently. Treatment with thiamine can sometimes improve the conditions, but the longer a person has remained thiamine deficient before treatment, the poorer the chances for recovery.","The authoritarian personality is a personality type that predisposes a person to obey unquestionably. People with this personality type are said to be prejudice against minority groups and believe strongly in obeying authority. They also have a general belief in the importance of power and dominance, and the insignificance of those below them. This is because they feel weak and therefore feel they need to be part of a powerful group and have a powerful leader. The authoritarian personality has shown a trend in personality patterns formed in childhood. Research has found that those who score highly on authoritarianism tend to describe their childhood as being dominated by stern harsh fathers who insisted on absolute obedience. This theory is predominately a psychodynamic one as it is based on the idea of 'reaction formation', as a forbidden impulse is re-channelled into a safer course. In effect these minority groups are used as scapegoats as they are considered to be weak and powerless. In this case, the child is unable to express their anger towards their father and thus resulting in them redirecting their anger towards much safer targets such as minority groups (Gleitman, Fridlund & Reisberg, 1998). Defence mechanisms are used to hide their true feelings, for example they will repress their aggressive or sexual feelings and project those traits onto others. There is also a tendency to view the outside world as being populated with dangerous enemies who have to be attacked before they themselves are attacked. This personality consists of a lot of distrust for others. This negative view of people leads to the conclusion that harsh laws and order is necessary in order to control people. These conclusions were however based on a limited sample of white middle class Californians; therefore it is questionable whether the theory can be applied to other cultural or socio-economic groups. Also the information on the patients's childhood was drawn from their own recollections and so the authenticity of the results cannot be certain. The theory can also be seen as being incomplete. It explains why some individuals become fascists but it doesn't explain why racism and fascism became a widespread phenomenon. Some research has shown that even the fascist elites have had different types of education than that which would create authoritarian personalities. Also, it fails to study how society itself influences the rise of hate and acceptance of authority. It is claimed that a person's attitude to authority is influenced by their attitude to their parents. (Adorno, Frenkel-Brunswick, Levinson & Sanford, 1950). However, variations in the degree of racism have been shown to be unrelated to type of upbringing (Sidanius et al., 1986). The authoritarian personality is supported by the fact that the studies focus on the important relationship between social and political attitudes (Brown, 1965). Minority prejudice does tend to go with authoritarianism and people with this type of personality do also tend to hold certain political views such as, they are more obedient to authority and tend to accept the attitudes of those in power (Elm & Milgram, 1996). However prejudice has varied historically, therefore authoritarianism may just be a result of societal times and not ones family, as suggested by the psychodynamic approach. This calls into question the causality between authoritarianism and prejudice. Further support comes from evidence that authoritarianism is more pronounced among people with less education (Chrishe, 1954). The theory supports this as working class parents are more likely than upper class parents to stress obedience to authority as they themselves are more likely to obey to the authority of a boss or a supervisor (Kelly, 1969). And it has been found that authoritarian parents tend to produce dominated children who themselves become authoritarian parents (Adorno et al.). The theory of the authoritarian personality (Adorno et al.) is based on empirical research and determines susceptibility to prejudice and also patterns of belief and ideology. The theory was based on a research project where people who were believed to be highly authoritarian were interviewed; it was found that these people shared certain common traits. The purpose of the questionnaire was to measure anti-Semitism and to determine correlates of anti-Semitism. The answers from 2000 questionnaires were classified in order to put each individual on a scale of A-S (anti-Semitism), E (ethnocentrism), PEC (political-economic conservatism) and F (potentially fascist). These results helped to form the f-scale by assigning psychometric properties to the scales. The f-scale was validated by various means such as discriminated validation, conformity methods etc. It was developed as a personality inventory that measured the central attributes of authoritarian personality. The scale consisted of 30 items covering such issues as obedience & respect for authority, aggression towards deviant groups such as homosexuals, and general ethnocentrism. The main criticism of the f-scale is a methodological one, all the items are worded in the same direction, and therefore agreeing with a statement would always mean that you support the authoritarian viewpoint. Therefore if you scored highly on the f-scale, there if no way of determining whether you are authoritarian or just acquiescent. The more recent Right Wing Authoritarianism (RWA) scale (Altemeyer, 1996) claims that it has fixed the problem of acquiescence. This scale uses cognitive theory to account for authoritarianism instead of the psychodynamic approach that is used in the f-scale. The authoritarian sees the world in black and white terms, being unable or unwilling to tolerate cognitive ambiguity. In other words people with a high RWA will have a different cognitive set to those with a low RWA. It is based on the idea that authoritarianism is a personality trait consisting of three covarying components, Conventionalism, authoritarian submission and authoritarian aggression. The validity of the f-scale has come under question a number of times. Even though research suggests that the f-scale does not measure what it sets out to measure, it does measure something that seems to have a significant effect on many other variables (Ray, 1990). Gabennesch (1972) suggested that a high f score represents narrowness of world-view and a narrow breadth of perspective. The scale has also been described as a collection of 'Victorian values' (Hartmann, 1977); the attitudes were considered old-fashioned even when the f-scale was complied. Therefore we are unable to be sure if the scale is even socially relevant to today's society. What is considered as an authority personality type may not be applicable to today's culture either. In the 20's, obedience to authority was the norm and was actually expected of children. However, now, children are encouraged to be more autonomous. Therefore if someone were to score highly on the f-scale it could just mean that they are old fashioned and not authoritarian. Sanford (1977) suggested that due to changing times, interested researchers should attempt to define a new from of authoritarian personality that is related more to current social issues. Relational studies have shown when correlating f-scale to various prejudice measures in South Africa the relation has been poor. The scale fails to explain prejudice in societies where prejudice is the norm (e.g. South Africa & Southern USA) as it only looks at the individual. Ray (1980) found a 0.59 correlation however, Orpen & Van der Schyff (1972) only found a 0.05 correlation. This implies that maybe the concept of authoritarianism would be more appropriate if it was applied only to societies where prejudice is normative. It has been suggested that the potential for authoritarianism is actually quite high, given the right circumstances (Zimbardo, 1971). It is estimated that at least 80% of us have prejudices. Also, Milgram's study of obedience (1971) suggests that 65% of us would physically hurt someone if told to do so by an authority figure. This suggests that maybe there are parts of an authoritarian personality in all of us. Research into the authoritarian personality is extremely important as the trait can lead to anti-democratic beliefs, ethnocentrism and prejudice. These types of social behaviour can have a detrimental effect on society as we have seen many times in history, for example in Nazi Germany or fascist Italy. The authoritarian personality theory (Adorno et al., 1950) has been widely criticised as being inaccurate. However, even with its errors the authoritarian personality theory has pioneered a lot of research in this field, which in time will hopefully help to improve and expand on the theory.",True
97,"The authoritarian personality is a personality type that predisposes a person to obey unquestionably. People with this personality type are said to be prejudice against minority groups and believe strongly in obeying authority. They also have a general belief in the importance of power and dominance, and the insignificance of those below them. This is because they feel weak and therefore feel they need to be part of a powerful group and have a powerful leader. The authoritarian personality has shown a trend in personality patterns formed in childhood. Research has found that those who score highly on authoritarianism tend to describe their childhood as being dominated by stern harsh fathers who insisted on absolute obedience. This theory is predominately a psychodynamic one as it is based on the idea of 'reaction formation', as a forbidden impulse is re-channelled into a safer course. In effect these minority groups are used as scapegoats as they are considered to be weak and powerless. In this case, the child is unable to express their anger towards their father and thus resulting in them redirecting their anger towards much safer targets such as minority groups (Gleitman, Fridlund & Reisberg, 1998). Defence mechanisms are used to hide their true feelings, for example they will repress their aggressive or sexual feelings and project those traits onto others. There is also a tendency to view the outside world as being populated with dangerous enemies who have to be attacked before they themselves are attacked. This personality consists of a lot of distrust for others. This negative view of people leads to the conclusion that harsh laws and order is necessary in order to control people. These conclusions were however based on a limited sample of white middle class Californians; therefore it is questionable whether the theory can be applied to other cultural or socio-economic groups. Also the information on the patients's childhood was drawn from their own recollections and so the authenticity of the results cannot be certain. The theory can also be seen as being incomplete. It explains why some individuals become fascists but it doesn't explain why racism and fascism became a widespread phenomenon. Some research has shown that even the fascist elites have had different types of education than that which would create authoritarian personalities. Also, it fails to study how society itself influences the rise of hate and acceptance of authority. It is claimed that a person's attitude to authority is influenced by their attitude to their parents. (Adorno, Frenkel-Brunswick, Levinson & Sanford, 1950). However, variations in the degree of racism have been shown to be unrelated to type of upbringing (Sidanius et al., 1986). The authoritarian personality is supported by the fact that the studies focus on the important relationship between social and political attitudes (Brown, 1965). Minority prejudice does tend to go with authoritarianism and people with this type of personality do also tend to hold certain political views such as, they are more obedient to authority and tend to accept the attitudes of those in power (Elm & Milgram, 1996). However prejudice has varied historically, therefore authoritarianism may just be a result of societal times and not ones family, as suggested by the psychodynamic approach. This calls into question the causality between authoritarianism and prejudice. Further support comes from evidence that authoritarianism is more pronounced among people with less education (Chrishe, 1954). The theory supports this as working class parents are more likely than upper class parents to stress obedience to authority as they themselves are more likely to obey to the authority of a boss or a supervisor (Kelly, 1969). And it has been found that authoritarian parents tend to produce dominated children who themselves become authoritarian parents (Adorno et al.). The theory of the authoritarian personality (Adorno et al.) is based on empirical research and determines susceptibility to prejudice and also patterns of belief and ideology. The theory was based on a research project where people who were believed to be highly authoritarian were interviewed; it was found that these people shared certain common traits. The purpose of the questionnaire was to measure anti-Semitism and to determine correlates of anti-Semitism. The answers from 2000 questionnaires were classified in order to put each individual on a scale of A-S (anti-Semitism), E (ethnocentrism), PEC (political-economic conservatism) and F (potentially fascist). These results helped to form the f-scale by assigning psychometric properties to the scales. The f-scale was validated by various means such as discriminated validation, conformity methods etc. It was developed as a personality inventory that measured the central attributes of authoritarian personality. The scale consisted of 30 items covering such issues as obedience & respect for authority, aggression towards deviant groups such as homosexuals, and general ethnocentrism. The main criticism of the f-scale is a methodological one, all the items are worded in the same direction, and therefore agreeing with a statement would always mean that you support the authoritarian viewpoint. Therefore if you scored highly on the f-scale, there if no way of determining whether you are authoritarian or just acquiescent. The more recent Right Wing Authoritarianism (RWA) scale (Altemeyer, 1996) claims that it has fixed the problem of acquiescence. This scale uses cognitive theory to account for authoritarianism instead of the psychodynamic approach that is used in the f-scale. The authoritarian sees the world in black and white terms, being unable or unwilling to tolerate cognitive ambiguity. In other words people with a high RWA will have a different cognitive set to those with a low RWA. It is based on the idea that authoritarianism is a personality trait consisting of three covarying components, Conventionalism, authoritarian submission and authoritarian aggression. The validity of the f-scale has come under question a number of times. Even though research suggests that the f-scale does not measure what it sets out to measure, it does measure something that seems to have a significant effect on many other variables (Ray, 1990). Gabennesch (1972) suggested that a high f score represents narrowness of world-view and a narrow breadth of perspective. The scale has also been described as a collection of 'Victorian values' (Hartmann, 1977); the attitudes were considered old-fashioned even when the f-scale was complied. Therefore we are unable to be sure if the scale is even socially relevant to today's society. What is considered as an authority personality type may not be applicable to today's culture either. In the 20's, obedience to authority was the norm and was actually expected of children. However, now, children are encouraged to be more autonomous. Therefore if someone were to score highly on the f-scale it could just mean that they are old fashioned and not authoritarian. Sanford (1977) suggested that due to changing times, interested researchers should attempt to define a new from of authoritarian personality that is related more to current social issues. Relational studies have shown when correlating f-scale to various prejudice measures in South Africa the relation has been poor. The scale fails to explain prejudice in societies where prejudice is the norm (e.g. South Africa & Southern USA) as it only looks at the individual. Ray (1980) found a 0.59 correlation however, Orpen & Van der Schyff (1972) only found a 0.05 correlation. This implies that maybe the concept of authoritarianism would be more appropriate if it was applied only to societies where prejudice is normative. It has been suggested that the potential for authoritarianism is actually quite high, given the right circumstances (Zimbardo, 1971). It is estimated that at least 80% of us have prejudices. Also, Milgram's study of obedience (1971) suggests that 65% of us would physically hurt someone if told to do so by an authority figure. This suggests that maybe there are parts of an authoritarian personality in all of us. Research into the authoritarian personality is extremely important as the trait can lead to anti-democratic beliefs, ethnocentrism and prejudice. These types of social behaviour can have a detrimental effect on society as we have seen many times in history, for example in Nazi Germany or fascist Italy. The authoritarian personality theory (Adorno et al., 1950) has been widely criticised as being inaccurate. However, even with its errors the authoritarian personality theory has pioneered a lot of research in this field, which in time will hopefully help to improve and expand on the theory.","A disease slowly destroys your oligodendroglia cellsOligodendroglia cells are held in the sheath of Schwann, which covers myelin and provides insulation to the neurons in the central nervous system. The axon of each neuron is protected by a myelin sheath, this ensures the transmission of signals and allows nerve impulses to be propagated to the next neuron. This mylelination has a strong impact on behaviours as it profoundly increases the velocity of nerve impulses and thereby affects the temporal order of events in the nervous system (Biological Psychology, 2002, p. 69). Normal transmissions are so fast and automatic that it is hard to accept a delay between a thought and a muscle contraction. The speed of nerve conduction due to the presence of the oligodendroglia cells is up to approximately 260 miles per hour (Psychology 5 th ed., 1999 p.51). The myelin sheath around the axon is made up from 2 principle proteins, the myelin basic protein and the proteolipid protein. Without the oligodendroglia cells forming myelin, nerve signals would become slowed or even completely blocked, thus causing symptoms of Multiple Sclerosis. In MS T-cells from the immune system become sensitised to the myelin basic protein and begin to attack it, this causes the body to try and repair the situation. Therefore in the initial stages of MS the oligodendroglia cells are not destroyed, but produce more myelin to compensate for the myelin being attacked. This causes the symptoms of MS to disappear, but only until the myelin degenerates again. In later stages of MS the oligodendroglia cells are destroyed, this produces multiple demyelinated areas on the axons of the neurons and eventually a person with MS develops areas of scarring where the healthy myelin once used to be. These areas of scaring cause lesions to appear in seemingly random areas of the CNS white matter and because the severity of MS depends on the location and extent to which these lesions occur, the type of symptoms can vary ( URL ). Myelination slows down the speed of conduction of neural impulses and thus can cause slowness of movement and loss of sensory abilities (Biological Psychology, 2002, p. 69). Also with the insulation gone, the messages being carried by the axons are no longer kept separate and this results in the sensory disorders and the loss of muscular control (Physiology Of Behaviour, 1991 p.29). Other symptoms of multiple sclerosis include numbness, difficulty with speech, muscle spasms and coordination problems. These problems with movement occur because the loss of the oligodendroglia cells weaken the electrical impulses from the brain and sometimes fail to transmit them at all. b) A tumour destroys the fusiform gyrus in your right temporal lobeThe fusiform gyrus can be found in the medial part of the occipital lobe situated in the right temporal lobe. It is used in performing sensory discrimination of emotional stimuli and the activation of the fusiform has been reported during the perception of faces. Also several studies have demonstrated that a particular region within the fusiform gyrus, the fusiform face area, responds in a highly selective way to face stimuli. The fusiform gyrus has more than one function, it holds the functional region responsible for colour, and is used in the identification of faces and in the recognition of facial expressions ( URL ). Damage to the fusiform gyrus can disrupt an individual's ability to retrieve information especially visual memories, such as the remembered face of a friend. If a tumour destroys the fusiform gyrus of the right temporal lobe then the left visual field will be affected. A lesion in the fusiform gyrus is associated with the loss of colour perception; this is known a central achromatopsia. Patients are usually aware of the deficit and report the world as being grey or 'dirty' and it can also affect one colour more than another (braincampus.com). Another consequence of damage to the fusiform gyrus is prosopagnosia; this is a disorder where the patient is unable recognise faces. Patients can identify faces but they do not recognise whose face is present before them, they may not even recognise their own face in the mirror. Prosopagnosia involves both the temporal and parietal lobes but mainly originates from damage to the fusiform gyrus. Even though patients have difficulty in recognising faces there is a marked difference if a face is made psychologically 'stronger' by for example, presenting faces that are linked. Patients with prosopagnosia may also have difficulty in distinguishing between particular objects of the same class, e.g. makes of cars (Physiology Of Behaviour, 1991, p.187). The fusiform gyrus is also related to facial expressions. A person with a damaged fusiform gyrus also has problems in recognising facial expressions of basic emotions and my find it difficult distinguishing between anger and frustration. c) The cells in the substantia nigra degenerateThe substantia nigra is a group of neurons in the region of the midbrain whose axons go to the basal ganglia. The cell bodies of the dopamine system lie within this region and are involved in motor control via the basal ganglia and in particular to the striatum cells of the substantia nigra. The nerve cells in the substantia nigra communicate with other cells on the nearby striatum by releasing the neurotransmitter dopamine at the nerve terminals, which is also synthesised there and is the area of the brain that controls movement and balance ( URL ). The basal ganglia usually impose constraints on motor control, but the absence of these constraints yields persistent excess of movement or slowness of movement and marked changes in muscle tone (Biological Psychology, 2002, p.352). The degeneration of the cells in the substantia nigra causes involuntary repetitive movements of the hand and arm, now known as Parkinson's disease. The cell bodies of dopamine that originate in the substantia nigra are no longer produced if the cells in the substantia nigra degenerate, this therefore causes a pronounced decrease in the dopamine content of the basal ganglia. People with Parkinson's disease inexplicably lose more than 80% of the dopamine producing cells in the substantia nigra ( URL ). Without dopamine the substantia nigra striatum cannot send out certain messages as they are unable to travel across the nerve connection, and it is this that leads to Parkinson's. The disease is described as a severe disorder of movement that involves tremors of the fingers, slowness of movement and problems with maintaining posture. There can also be a loss of facial muscle tone and general difficulty in all motor efforts. There are many symptoms for Parkinson's disease but they vary substantially from person to person, making diagnosis quite difficult. L-dopa (the precursor to dopamine) is administered to increase levels of dopamine in the brain in order to reduce the symptoms of Parkinson's disease. L-dopa enhances dopamine levels of the surviving cells but because the cell bodies in the brain stem degenerate, dopamine-containing terminals also disappear. Eventually too few dopamine-containing neurons remain in the substantia nigra to be influenced by the intake of L-dopa (Biological Psychology, 2002, p.353). Therefore L-dopa only relieves the symptoms and because the degeneration of the substantia nigra is progressive, there is no cure as yet. Recent reports have suggested that a more permanent cure would be to transplant new dopamine neurons from embryonic brains (Principles Of Biopsychology, 2001, p.26). d) Your mamillary bodies cease to functionThe mamillary bodies receive information from the hippocampus and are reciprocally connected to the anterior thalamic nucleus and the midbrain. These structures seem to play an important role in the memory process and have been described as 'a narrow funnel through which connections from the midbrain as well as the temporal lobe neo cortex and limbic system gain access to the frontal lobes.' (Mair et al., 1979). Korsakoff's syndrome occurs due to neuronal loss in the mamillary bodies and is associated with cell shrinkage in a particular nucleus of the thalamus and the mamillary bodies. Mairal. (1979) examined 2 Korskoff patients and found that both brains showed shrunken, diseased mamillary bodies. It is distinguished by confusion and a deficit in storing new information (anterograde amnesia). This type of amnesia is usually caused by chronic alcoholism. The alcohol prevents the liver from metabolising the vitamin thiamine, which causes a thiamine deficiency, which in turn leads to the breakdown and loss of neurons, and because neurons are not replaced the damage is irreversible. Therefore Korsakoff's involves a progressive degeneration of brain tissue (Principles Of Biopsychology, 2001, p.88). The dysfunction of the mamillary bodies causes the patient to fail to recall many items or events in the past, if the item is presented again, the patient does not feel familiar with it. They often show disorientation to time and place and may confabulate (fill a gap in memory with a falsification that they seem to accept as true) (Biological Psychology, 2002, p.541). Patients with this disorder usually suffer from some retrograde amnesia as well, but their primary deficit is the inability to store any new information in long-term memory. The symptoms are similar to those of people with damage to the prefrontal cortex, including apathy, confusion and memory impairment. Like most patients with damage to the frontal-lobes, Korsakoff's patients have trouble recalling the temporal order of events, for example they cannot recall which of a series of world events happened the furthest in the past or which one happened most recently, they also can not recall which events from their own lives happened a long time ago and which ones happened recently. Treatment with thiamine can sometimes improve the conditions, but the longer a person has remained thiamine deficient before treatment, the poorer the chances for recovery.",False
98,"The authoritarian personality is a personality type that predisposes a person to obey unquestionably. People with this personality type are said to be prejudice against minority groups and believe strongly in obeying authority. They also have a general belief in the importance of power and dominance, and the insignificance of those below them. This is because they feel weak and therefore feel they need to be part of a powerful group and have a powerful leader. The authoritarian personality has shown a trend in personality patterns formed in childhood. Research has found that those who score highly on authoritarianism tend to describe their childhood as being dominated by stern harsh fathers who insisted on absolute obedience. This theory is predominately a psychodynamic one as it is based on the idea of 'reaction formation', as a forbidden impulse is re-channelled into a safer course. In effect these minority groups are used as scapegoats as they are considered to be weak and powerless. In this case, the child is unable to express their anger towards their father and thus resulting in them redirecting their anger towards much safer targets such as minority groups (Gleitman, Fridlund & Reisberg, 1998). Defence mechanisms are used to hide their true feelings, for example they will repress their aggressive or sexual feelings and project those traits onto others. There is also a tendency to view the outside world as being populated with dangerous enemies who have to be attacked before they themselves are attacked. This personality consists of a lot of distrust for others. This negative view of people leads to the conclusion that harsh laws and order is necessary in order to control people. These conclusions were however based on a limited sample of white middle class Californians; therefore it is questionable whether the theory can be applied to other cultural or socio-economic groups. Also the information on the patients's childhood was drawn from their own recollections and so the authenticity of the results cannot be certain. The theory can also be seen as being incomplete. It explains why some individuals become fascists but it doesn't explain why racism and fascism became a widespread phenomenon. Some research has shown that even the fascist elites have had different types of education than that which would create authoritarian personalities. Also, it fails to study how society itself influences the rise of hate and acceptance of authority. It is claimed that a person's attitude to authority is influenced by their attitude to their parents. (Adorno, Frenkel-Brunswick, Levinson & Sanford, 1950). However, variations in the degree of racism have been shown to be unrelated to type of upbringing (Sidanius et al., 1986). The authoritarian personality is supported by the fact that the studies focus on the important relationship between social and political attitudes (Brown, 1965). Minority prejudice does tend to go with authoritarianism and people with this type of personality do also tend to hold certain political views such as, they are more obedient to authority and tend to accept the attitudes of those in power (Elm & Milgram, 1996). However prejudice has varied historically, therefore authoritarianism may just be a result of societal times and not ones family, as suggested by the psychodynamic approach. This calls into question the causality between authoritarianism and prejudice. Further support comes from evidence that authoritarianism is more pronounced among people with less education (Chrishe, 1954). The theory supports this as working class parents are more likely than upper class parents to stress obedience to authority as they themselves are more likely to obey to the authority of a boss or a supervisor (Kelly, 1969). And it has been found that authoritarian parents tend to produce dominated children who themselves become authoritarian parents (Adorno et al.). The theory of the authoritarian personality (Adorno et al.) is based on empirical research and determines susceptibility to prejudice and also patterns of belief and ideology. The theory was based on a research project where people who were believed to be highly authoritarian were interviewed; it was found that these people shared certain common traits. The purpose of the questionnaire was to measure anti-Semitism and to determine correlates of anti-Semitism. The answers from 2000 questionnaires were classified in order to put each individual on a scale of A-S (anti-Semitism), E (ethnocentrism), PEC (political-economic conservatism) and F (potentially fascist). These results helped to form the f-scale by assigning psychometric properties to the scales. The f-scale was validated by various means such as discriminated validation, conformity methods etc. It was developed as a personality inventory that measured the central attributes of authoritarian personality. The scale consisted of 30 items covering such issues as obedience & respect for authority, aggression towards deviant groups such as homosexuals, and general ethnocentrism. The main criticism of the f-scale is a methodological one, all the items are worded in the same direction, and therefore agreeing with a statement would always mean that you support the authoritarian viewpoint. Therefore if you scored highly on the f-scale, there if no way of determining whether you are authoritarian or just acquiescent. The more recent Right Wing Authoritarianism (RWA) scale (Altemeyer, 1996) claims that it has fixed the problem of acquiescence. This scale uses cognitive theory to account for authoritarianism instead of the psychodynamic approach that is used in the f-scale. The authoritarian sees the world in black and white terms, being unable or unwilling to tolerate cognitive ambiguity. In other words people with a high RWA will have a different cognitive set to those with a low RWA. It is based on the idea that authoritarianism is a personality trait consisting of three covarying components, Conventionalism, authoritarian submission and authoritarian aggression. The validity of the f-scale has come under question a number of times. Even though research suggests that the f-scale does not measure what it sets out to measure, it does measure something that seems to have a significant effect on many other variables (Ray, 1990). Gabennesch (1972) suggested that a high f score represents narrowness of world-view and a narrow breadth of perspective. The scale has also been described as a collection of 'Victorian values' (Hartmann, 1977); the attitudes were considered old-fashioned even when the f-scale was complied. Therefore we are unable to be sure if the scale is even socially relevant to today's society. What is considered as an authority personality type may not be applicable to today's culture either. In the 20's, obedience to authority was the norm and was actually expected of children. However, now, children are encouraged to be more autonomous. Therefore if someone were to score highly on the f-scale it could just mean that they are old fashioned and not authoritarian. Sanford (1977) suggested that due to changing times, interested researchers should attempt to define a new from of authoritarian personality that is related more to current social issues. Relational studies have shown when correlating f-scale to various prejudice measures in South Africa the relation has been poor. The scale fails to explain prejudice in societies where prejudice is the norm (e.g. South Africa & Southern USA) as it only looks at the individual. Ray (1980) found a 0.59 correlation however, Orpen & Van der Schyff (1972) only found a 0.05 correlation. This implies that maybe the concept of authoritarianism would be more appropriate if it was applied only to societies where prejudice is normative. It has been suggested that the potential for authoritarianism is actually quite high, given the right circumstances (Zimbardo, 1971). It is estimated that at least 80% of us have prejudices. Also, Milgram's study of obedience (1971) suggests that 65% of us would physically hurt someone if told to do so by an authority figure. This suggests that maybe there are parts of an authoritarian personality in all of us. Research into the authoritarian personality is extremely important as the trait can lead to anti-democratic beliefs, ethnocentrism and prejudice. These types of social behaviour can have a detrimental effect on society as we have seen many times in history, for example in Nazi Germany or fascist Italy. The authoritarian personality theory (Adorno et al., 1950) has been widely criticised as being inaccurate. However, even with its errors the authoritarian personality theory has pioneered a lot of research in this field, which in time will hopefully help to improve and expand on the theory.","Over the last twenty years qualitative research has risen from an approach of the study of social issues to a widely practiced method for understanding how people view themselves and the world around them. Qualitative research involves findings that are not arrived at by statistical or other quantitative procedures. The data used in qualitative research varies in many form, the main focus of the essay will be on verbal report data and discourse analysis. Discourse analysis focuses on verbal report data; this consists of texts, both oral and written and on the roles and strategies of the speakers and the readers who participate in that text. It does this by studying the organization of language above the sentence or the clause, and therefore examines larger linguistic units, such as conversational exchanges or written texts. It is, therefore, primarily concerned with the use of language in social contexts and is the leading method in social psychology. Qualitative research, using verbal reports, has made many contributions to psychology. The first is that it enables us to gain a better understanding of social life and social interaction from our study of social texts. It does this by emphasizing how social reality is linguistically constructed (Potter & Wetherell, 1987). Coyle (1995) suggests that 'language in the form of discourses is seen as constituting the building blocks of social reality.' Discourse analysis sees language as a constructive tool from which people create a version of events by selecting from the range of linguistic resources available to them. The way in which they use language, and what they gain from the resulting constructions, is then analysed. This, in turn, helps to 'negate the cognitive reductionism that characterizes much of what passes for social psychology and offers the possibility of a truly social psychology' (Coyle, 1995). The methodology used in discourse analysis does, however, have its flaws. Discourse analysts are sometimes accused of putting words in the mouths of those whose discourse is being analysed. This is especially true in verbal interviews. An interview can be a very good way of getting an idea of the way in which people use language, however, it is important for the interviewer not to confuse their own constructions of the word with those of the patients' (Potter, 1996). Patients may also find that it is advantageous for them to under-report or over-report their thoughts or feelings when giving verbal reports. Due to the influence an interviewer can have on a patient, discourse analysts have been increasingly turning to the study of records of natural interaction (e.g. conversations, newspapers and recordings of counselling sessions) as opposed to interviews where even the most open-ended interview can have set topics and themes. Also individuals may not be aware of the consequences their language has, therefore asking participants to comment on their own analysis is inappropriate, which is a method some psychologists prefer to use. Qualitative research can also play a primary role in hypothesis generation without the need to appeal to a post modernist philosophy, which questions the need for set criteria exists in judging the products of qualitative research (Denzin & Lincoln, 1994, as cited in Merrick, 1993). In theoretical sciences the depth of knowledge from past experiments and theories constrain the choice of sensible hypotheses (Burt & Oaksfield, 1999). Therefore it is suggested, that in the light of recent views that considering psychology as a science, we need qualitative research to fill the gap in relation to the lack of theories. Qualitative methods systemize and give structure to the process of hypothesis generation. The resulting hypothesis can then be subject to objective testing in a truly scientific way. However we do have to be aware that it is difficult to conduct objective experimental tests using qualitative research. People may not always be willing or comfortable with sharing their experiences with the experimenter. In this case, it may be more appropriate to change the type of verbal report used. For example, people may feel more at ease verbally expressing their feelings in the form of writing. Whether qualitative work can ever being considered a scientific method is questionable. It has been proposed that there is no way to establish the validity or reliability of scientific claims or observations in qualitative work, (Jessor, 1996, as cited in Merrick, 1993). Validity, within a discourse analytic approach, can be achieved through the presentation of the source texts and analytical processes, thus making clear the researcher's interpretation (Potter & Wetherell, 1987). Provision of these source texts enables readers to evaluate and assess the adequacy of the analysis presented (Potter, 1996), the reader can then decide whether the interpretation is valid and could even offer their own interpretation of the data. Therefore discourse analysis requires a balanced compromise between the presentations of enough source texts to allow the reader to assess the validity of the researcher's analysis, without inundating them with detail. It is also important to bear in mind the difficulty involved in presenting large amounts of data in research reports due to submission guidelines of psychology journals. Hopefully as research into discourse analysis increases, journals might adapt their guideline in order to cater more for qualitative research (Coyle, 1995). Reliability of a method can be easily established when experimental work is involved, as is can, as a method, be just a matter of test and re-test, however this cannot be done with qualitative research. An alternative possibility is to combine a group of studies together in order to build upon previous work. (Potter, 1996) this can then be used to check if similar results have been found. The criticisms associated with qualitative research indicate that we need to formulate a set of criteria that allows the quality of analysis to be evaluated (Denzin & Lincoln, 1994, as cited in Merrick, 1993). Having said this, it is also possible that we could evaluate the research, especially in terms of validity, by using the above-mentioned method of allowing the readers to judge the interpretations themselves. The use of verbal reports and discourse analysis has increased in popularity as a research method and has resulted in many contributions to the study of psychology. Nevertheless, this method does have its problems. Discourse analysis does not have an answer to any psychological query; therefore it cannot be plugged into a particular pre-defined question. Therefore discourse analysis should not be seen as just a research method but also as a whole perspective on social life, (Potter, 1996). Also, there is the hope that in the future, discourse analysis will continue to develop further and offer considerable more contributions to the various aspects of psychology.",True
99,"Over the last twenty years qualitative research has risen from an approach of the study of social issues to a widely practiced method for understanding how people view themselves and the world around them. Qualitative research involves findings that are not arrived at by statistical or other quantitative procedures. The data used in qualitative research varies in many form, the main focus of the essay will be on verbal report data and discourse analysis. Discourse analysis focuses on verbal report data; this consists of texts, both oral and written and on the roles and strategies of the speakers and the readers who participate in that text. It does this by studying the organization of language above the sentence or the clause, and therefore examines larger linguistic units, such as conversational exchanges or written texts. It is, therefore, primarily concerned with the use of language in social contexts and is the leading method in social psychology. Qualitative research, using verbal reports, has made many contributions to psychology. The first is that it enables us to gain a better understanding of social life and social interaction from our study of social texts. It does this by emphasizing how social reality is linguistically constructed (Potter & Wetherell, 1987). Coyle (1995) suggests that 'language in the form of discourses is seen as constituting the building blocks of social reality.' Discourse analysis sees language as a constructive tool from which people create a version of events by selecting from the range of linguistic resources available to them. The way in which they use language, and what they gain from the resulting constructions, is then analysed. This, in turn, helps to 'negate the cognitive reductionism that characterizes much of what passes for social psychology and offers the possibility of a truly social psychology' (Coyle, 1995). The methodology used in discourse analysis does, however, have its flaws. Discourse analysts are sometimes accused of putting words in the mouths of those whose discourse is being analysed. This is especially true in verbal interviews. An interview can be a very good way of getting an idea of the way in which people use language, however, it is important for the interviewer not to confuse their own constructions of the word with those of the patients' (Potter, 1996). Patients may also find that it is advantageous for them to under-report or over-report their thoughts or feelings when giving verbal reports. Due to the influence an interviewer can have on a patient, discourse analysts have been increasingly turning to the study of records of natural interaction (e.g. conversations, newspapers and recordings of counselling sessions) as opposed to interviews where even the most open-ended interview can have set topics and themes. Also individuals may not be aware of the consequences their language has, therefore asking participants to comment on their own analysis is inappropriate, which is a method some psychologists prefer to use. Qualitative research can also play a primary role in hypothesis generation without the need to appeal to a post modernist philosophy, which questions the need for set criteria exists in judging the products of qualitative research (Denzin & Lincoln, 1994, as cited in Merrick, 1993). In theoretical sciences the depth of knowledge from past experiments and theories constrain the choice of sensible hypotheses (Burt & Oaksfield, 1999). Therefore it is suggested, that in the light of recent views that considering psychology as a science, we need qualitative research to fill the gap in relation to the lack of theories. Qualitative methods systemize and give structure to the process of hypothesis generation. The resulting hypothesis can then be subject to objective testing in a truly scientific way. However we do have to be aware that it is difficult to conduct objective experimental tests using qualitative research. People may not always be willing or comfortable with sharing their experiences with the experimenter. In this case, it may be more appropriate to change the type of verbal report used. For example, people may feel more at ease verbally expressing their feelings in the form of writing. Whether qualitative work can ever being considered a scientific method is questionable. It has been proposed that there is no way to establish the validity or reliability of scientific claims or observations in qualitative work, (Jessor, 1996, as cited in Merrick, 1993). Validity, within a discourse analytic approach, can be achieved through the presentation of the source texts and analytical processes, thus making clear the researcher's interpretation (Potter & Wetherell, 1987). Provision of these source texts enables readers to evaluate and assess the adequacy of the analysis presented (Potter, 1996), the reader can then decide whether the interpretation is valid and could even offer their own interpretation of the data. Therefore discourse analysis requires a balanced compromise between the presentations of enough source texts to allow the reader to assess the validity of the researcher's analysis, without inundating them with detail. It is also important to bear in mind the difficulty involved in presenting large amounts of data in research reports due to submission guidelines of psychology journals. Hopefully as research into discourse analysis increases, journals might adapt their guideline in order to cater more for qualitative research (Coyle, 1995). Reliability of a method can be easily established when experimental work is involved, as is can, as a method, be just a matter of test and re-test, however this cannot be done with qualitative research. An alternative possibility is to combine a group of studies together in order to build upon previous work. (Potter, 1996) this can then be used to check if similar results have been found. The criticisms associated with qualitative research indicate that we need to formulate a set of criteria that allows the quality of analysis to be evaluated (Denzin & Lincoln, 1994, as cited in Merrick, 1993). Having said this, it is also possible that we could evaluate the research, especially in terms of validity, by using the above-mentioned method of allowing the readers to judge the interpretations themselves. The use of verbal reports and discourse analysis has increased in popularity as a research method and has resulted in many contributions to the study of psychology. Nevertheless, this method does have its problems. Discourse analysis does not have an answer to any psychological query; therefore it cannot be plugged into a particular pre-defined question. Therefore discourse analysis should not be seen as just a research method but also as a whole perspective on social life, (Potter, 1996). Also, there is the hope that in the future, discourse analysis will continue to develop further and offer considerable more contributions to the various aspects of psychology.","The authoritarian personality is a personality type that predisposes a person to obey unquestionably. People with this personality type are said to be prejudice against minority groups and believe strongly in obeying authority. They also have a general belief in the importance of power and dominance, and the insignificance of those below them. This is because they feel weak and therefore feel they need to be part of a powerful group and have a powerful leader. The authoritarian personality has shown a trend in personality patterns formed in childhood. Research has found that those who score highly on authoritarianism tend to describe their childhood as being dominated by stern harsh fathers who insisted on absolute obedience. This theory is predominately a psychodynamic one as it is based on the idea of 'reaction formation', as a forbidden impulse is re-channelled into a safer course. In effect these minority groups are used as scapegoats as they are considered to be weak and powerless. In this case, the child is unable to express their anger towards their father and thus resulting in them redirecting their anger towards much safer targets such as minority groups (Gleitman, Fridlund & Reisberg, 1998). Defence mechanisms are used to hide their true feelings, for example they will repress their aggressive or sexual feelings and project those traits onto others. There is also a tendency to view the outside world as being populated with dangerous enemies who have to be attacked before they themselves are attacked. This personality consists of a lot of distrust for others. This negative view of people leads to the conclusion that harsh laws and order is necessary in order to control people. These conclusions were however based on a limited sample of white middle class Californians; therefore it is questionable whether the theory can be applied to other cultural or socio-economic groups. Also the information on the patients's childhood was drawn from their own recollections and so the authenticity of the results cannot be certain. The theory can also be seen as being incomplete. It explains why some individuals become fascists but it doesn't explain why racism and fascism became a widespread phenomenon. Some research has shown that even the fascist elites have had different types of education than that which would create authoritarian personalities. Also, it fails to study how society itself influences the rise of hate and acceptance of authority. It is claimed that a person's attitude to authority is influenced by their attitude to their parents. (Adorno, Frenkel-Brunswick, Levinson & Sanford, 1950). However, variations in the degree of racism have been shown to be unrelated to type of upbringing (Sidanius et al., 1986). The authoritarian personality is supported by the fact that the studies focus on the important relationship between social and political attitudes (Brown, 1965). Minority prejudice does tend to go with authoritarianism and people with this type of personality do also tend to hold certain political views such as, they are more obedient to authority and tend to accept the attitudes of those in power (Elm & Milgram, 1996). However prejudice has varied historically, therefore authoritarianism may just be a result of societal times and not ones family, as suggested by the psychodynamic approach. This calls into question the causality between authoritarianism and prejudice. Further support comes from evidence that authoritarianism is more pronounced among people with less education (Chrishe, 1954). The theory supports this as working class parents are more likely than upper class parents to stress obedience to authority as they themselves are more likely to obey to the authority of a boss or a supervisor (Kelly, 1969). And it has been found that authoritarian parents tend to produce dominated children who themselves become authoritarian parents (Adorno et al.). The theory of the authoritarian personality (Adorno et al.) is based on empirical research and determines susceptibility to prejudice and also patterns of belief and ideology. The theory was based on a research project where people who were believed to be highly authoritarian were interviewed; it was found that these people shared certain common traits. The purpose of the questionnaire was to measure anti-Semitism and to determine correlates of anti-Semitism. The answers from 2000 questionnaires were classified in order to put each individual on a scale of A-S (anti-Semitism), E (ethnocentrism), PEC (political-economic conservatism) and F (potentially fascist). These results helped to form the f-scale by assigning psychometric properties to the scales. The f-scale was validated by various means such as discriminated validation, conformity methods etc. It was developed as a personality inventory that measured the central attributes of authoritarian personality. The scale consisted of 30 items covering such issues as obedience & respect for authority, aggression towards deviant groups such as homosexuals, and general ethnocentrism. The main criticism of the f-scale is a methodological one, all the items are worded in the same direction, and therefore agreeing with a statement would always mean that you support the authoritarian viewpoint. Therefore if you scored highly on the f-scale, there if no way of determining whether you are authoritarian or just acquiescent. The more recent Right Wing Authoritarianism (RWA) scale (Altemeyer, 1996) claims that it has fixed the problem of acquiescence. This scale uses cognitive theory to account for authoritarianism instead of the psychodynamic approach that is used in the f-scale. The authoritarian sees the world in black and white terms, being unable or unwilling to tolerate cognitive ambiguity. In other words people with a high RWA will have a different cognitive set to those with a low RWA. It is based on the idea that authoritarianism is a personality trait consisting of three covarying components, Conventionalism, authoritarian submission and authoritarian aggression. The validity of the f-scale has come under question a number of times. Even though research suggests that the f-scale does not measure what it sets out to measure, it does measure something that seems to have a significant effect on many other variables (Ray, 1990). Gabennesch (1972) suggested that a high f score represents narrowness of world-view and a narrow breadth of perspective. The scale has also been described as a collection of 'Victorian values' (Hartmann, 1977); the attitudes were considered old-fashioned even when the f-scale was complied. Therefore we are unable to be sure if the scale is even socially relevant to today's society. What is considered as an authority personality type may not be applicable to today's culture either. In the 20's, obedience to authority was the norm and was actually expected of children. However, now, children are encouraged to be more autonomous. Therefore if someone were to score highly on the f-scale it could just mean that they are old fashioned and not authoritarian. Sanford (1977) suggested that due to changing times, interested researchers should attempt to define a new from of authoritarian personality that is related more to current social issues. Relational studies have shown when correlating f-scale to various prejudice measures in South Africa the relation has been poor. The scale fails to explain prejudice in societies where prejudice is the norm (e.g. South Africa & Southern USA) as it only looks at the individual. Ray (1980) found a 0.59 correlation however, Orpen & Van der Schyff (1972) only found a 0.05 correlation. This implies that maybe the concept of authoritarianism would be more appropriate if it was applied only to societies where prejudice is normative. It has been suggested that the potential for authoritarianism is actually quite high, given the right circumstances (Zimbardo, 1971). It is estimated that at least 80% of us have prejudices. Also, Milgram's study of obedience (1971) suggests that 65% of us would physically hurt someone if told to do so by an authority figure. This suggests that maybe there are parts of an authoritarian personality in all of us. Research into the authoritarian personality is extremely important as the trait can lead to anti-democratic beliefs, ethnocentrism and prejudice. These types of social behaviour can have a detrimental effect on society as we have seen many times in history, for example in Nazi Germany or fascist Italy. The authoritarian personality theory (Adorno et al., 1950) has been widely criticised as being inaccurate. However, even with its errors the authoritarian personality theory has pioneered a lot of research in this field, which in time will hopefully help to improve and expand on the theory.",False
